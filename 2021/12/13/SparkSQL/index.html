<!DOCTYPE html>


<html lang="zh-CN">
  

    <head>
      <meta charset="utf-8" />
        
      <meta
        name="viewport"
        content="width=device-width, initial-scale=1, maximum-scale=1"
      />
      <title>SparkSQL |  anzhen.tech</title>
  <meta name="generator" content="hexo-theme-ayer">
      
      <link rel="shortcut icon" href="/favicon.ico" />
       
<link rel="stylesheet" href="/dist/main.css">

      <link
        rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/Shen-Yu/cdn/css/remixicon.min.css"
      />
      
<link rel="stylesheet" href="/css/custom.css">
 
      <script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script>
       
 
<script>
var _hmt = _hmt || [];
(function() {
	var hm = document.createElement("script");
	hm.src = "https://hm.baidu.com/hm.js?20878462c8c8a6915b11b2d93a956d26";
	var s = document.getElementsByTagName("script")[0]; 
	s.parentNode.insertBefore(hm, s);
})();
</script>


      <link
        rel="stylesheet"
        href="https://cdn.jsdelivr.net/npm/@sweetalert2/theme-bulma@5.0.1/bulma.min.css"
      />
      <script src="https://cdn.jsdelivr.net/npm/sweetalert2@11.0.19/dist/sweetalert2.min.js"></script>

      <!-- mermaid -->
      
      <style>
        .swal2-styled.swal2-confirm {
          font-size: 1.6rem;
        }
      </style>
      <meta name="baidu-site-verification" content="code-mBRwXRLQqk" />
      <meta name="google-site-verification" content="bqavzWFaou2XjWPiLJI2ZoQwSGDVv_wZFPMkWKjEAz0" />
      
    <link rel="alternate" href="/atom.xml" title="anzhen.tech" type="application/atom+xml">
</head>
  </html>
</html>


<body>
  <div id="app">
    
      
    <main class="content on">
      <section class="outer">
  <article
  id="post-SparkSQL"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h1 class="article-title sea-center" style="border-left:0" itemprop="name">
  SparkSQL
</h1>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2021/12/13/SparkSQL/" class="article-date">
  <time datetime="2021-12-13T14:33:38.000Z" itemprop="datePublished">2021-12-13</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/categories/Spark/">Spark</a>
  </div>
  
<div class="word_count">
    <span class="post-time">
        <span class="post-meta-item-icon">
            <i class="ri-quill-pen-line"></i>
            <span class="post-meta-item-text"> 字数统计:</span>
            <span class="post-count">10.1k</span>
        </span>
    </span>

    <span class="post-time">
        &nbsp; | &nbsp;
        <span class="post-meta-item-icon">
            <i class="ri-book-open-line"></i>
            <span class="post-meta-item-text"> 阅读时长≈</span>
            <span class="post-count">46 分钟</span>
        </span>
    </span>
</div>
 
    </div>
      
    <div class="tocbot"></div>




  
    <div class="article-entry" itemprop="articleBody">
       
  <h1 id="Spark-SQL"><a href="#Spark-SQL" class="headerlink" title="Spark SQL"></a>Spark SQL</h1><h1 id="第1章-SparkSQL概述"><a href="#第1章-SparkSQL概述" class="headerlink" title="第1章 SparkSQL概述"></a>第1章 SparkSQL概述</h1><h2 id="1-1-SparkSQL是什么"><a href="#1-1-SparkSQL是什么" class="headerlink" title="1.1 SparkSQL是什么"></a>1.1 SparkSQL是什么</h2><p>Spark SQL是Spark用于结构化数据(structured data)处理的Spark模块。</p>
<h2 id="1-2-Hive-and-SparkSQL"><a href="#1-2-Hive-and-SparkSQL" class="headerlink" title="1.2 Hive and SparkSQL"></a>1.2 Hive and SparkSQL</h2><ul>
<li><p>SparkSQL的前身是Shark，给熟悉RDBMS但又不理解MapReduce的技术人员提供快速上手的工具。</p>
</li>
<li><p>Hive是早期唯一运行在Hadoop上的SQL-on-Hadoop工具。但是MapReduce计算过程中大量的中间磁盘落地过程消耗了大量的I/O，降低的运行效率，为了提高SQL-on-Hadoop的效率，大量的SQL-on-Hadoop工具开始产生，其中表现较为突出的是：</p>
<ul>
<li>Drill</li>
<li>Impala</li>
<li>Shark</li>
</ul>
</li>
<li><p>其中Shark是伯克利实验室Spark生态环境的组件之一，是基于Hive所开发的工具，它修改了下图所示的右下角的内存管理、物理计划、执行三个模块，并使之能运行在Spark引擎上。<br>  <img src="https://s2.loli.net/2021/12/13/7jzrtMVpEQx9Lcn.jpg"></p>
</li>
<li><p>Shark的出现，使得SQL-on-Hadoop的性能比Hive有了10-100倍的提高。<br>  <img src="https://s2.loli.net/2021/12/13/W7HwSAcQyZvsVGX.jpg"></p>
</li>
<li><p>但是，随着Spark的发展，对于野心勃勃的Spark团队来说，Shark对于Hive的太多依赖（如采用Hive的语法解析器、查询优化器等等），制约了Spark的One Stack Rule Them All的既定方针，制约了Spark各个组件的相互集成，所以提出了SparkSQL项目。SparkSQL抛弃原有Shark的代码，汲取了Shark的一些优点，如内存列存储（In-Memory Columnar Storage）、Hive兼容性等，重新开发了SparkSQL代码；由于摆脱了对Hive的依赖性，SparkSQL无论在数据兼容、性能优化、组件扩展方面都得到了极大的方便，真可谓“退一步，海阔天空”。</p>
<ul>
<li>数据兼容方面 SparkSQL不但兼容Hive，还可以从RDD、parquet文件、JSON文件中获取数据，未来版本甚至支持获取RDBMS数据以及cassandra等NOSQL数据；</li>
<li>性能优化方面 除了采取In-Memory Columnar Storage、byte-code generation等优化技术外、将会引进Cost Model对查询进行动态评估、获取最佳物理计划等等；</li>
<li>组件扩展方面 无论是SQL的语法解析器、分析器还是优化器都可以重新定义，进行扩展。</li>
</ul>
</li>
<li><p>2014年6月1日Shark项目和SparkSQL项目的主持人Reynold Xin宣布：停止对Shark的开发，团队将所有资源放SparkSQL项目上，至此，Shark的发展画上了句话，但也因此发展出两个支线：SparkSQL和Hive on Spark。</p>
</li>
<li><p>其中SparkSQL作为Spark生态的一员继续发展，而不再受限于Hive，只是兼容Hive；而Hive on Spark是一个Hive的发展计划，该计划将Spark作为Hive的底层引擎之一，也就是说，Hive将不再受限于一个引擎，可以采用Map-Reduce、Tez、Spark等引擎。</p>
</li>
<li><p><font color ='blue' >对于开发人员来讲，SparkSQL可以简化RDD的开发，提高开发效率</font>，且执行效率非常快，所以实际工作中，基本上采用的就是SparkSQL。Spark SQL为了简化RDD的开发，提高开发效率，提供了2个编程抽象，类似Spark Core中的RDD</p>
<ul>
<li>DataFrame</li>
<li>DataSet</li>
</ul>
</li>
</ul>
<h2 id="1-3-SparkSQL特点"><a href="#1-3-SparkSQL特点" class="headerlink" title="1.3 SparkSQL特点"></a>1.3 SparkSQL特点</h2><h3 id="1-3-1-易整合"><a href="#1-3-1-易整合" class="headerlink" title="1.3.1 易整合"></a>1.3.1 易整合</h3><p>无缝的整合了 SQL 查询和 Spark 编程<br><img src="https://s2.loli.net/2021/12/13/8sTmDI6NikRp1OV.jpg"></p>
<h3 id="1-3-2-统一的数据访问"><a href="#1-3-2-统一的数据访问" class="headerlink" title="1.3.2 统一的数据访问"></a>1.3.2 统一的数据访问</h3><p>使用相同的方式连接不同的数据源<br><img src="https://s2.loli.net/2021/12/13/EBJyWkd2ipO6stN.jpg"></p>
<h3 id="1-3-3-兼容Hive"><a href="#1-3-3-兼容Hive" class="headerlink" title="1.3.3 兼容Hive"></a>1.3.3 兼容Hive</h3><p>在已有的仓库上直接运行 SQL 或者 HiveQL<br><img src="https://s2.loli.net/2021/12/13/AeYlHzuBoViQvbf.jpg"></p>
<h3 id="1-3-4-标准数据连接"><a href="#1-3-4-标准数据连接" class="headerlink" title="1.3.4 标准数据连接"></a>1.3.4 标准数据连接</h3><p>通过 JDBC 或者 ODBC 来连接<br><img src="https://s2.loli.net/2021/12/13/Trg6ZLADS4laVnI.jpg"></p>
<h2 id="1-4-DataFrame是什么"><a href="#1-4-DataFrame是什么" class="headerlink" title="1.4 DataFrame是什么"></a>1.4 DataFrame是什么</h2><ul>
<li><p>在Spark中，DataFrame是一种以RDD为基础的分布式数据集，类似于传统数据库中的二维表格。DataFrame与RDD的主要区别在于，前者带有schema元信息，即DataFrame所表示的二维表数据集的每一列都带有名称和类型。这使得Spark SQL得以洞察更多的结构信息，从而对藏于DataFrame背后的数据源以及作用于DataFrame之上的变换进行了针对性的优化，最终达到大幅提升运行时效率的目标。反观RDD，由于无从得知所存数据元素的具体内部结构，Spark Core只能在stage层面进行简单、通用的流水线优化。</p>
</li>
<li><p>同时，与Hive类似，DataFrame也支持嵌套数据类型（struct、array和map）。从 API 易用性的角度上看，DataFrame API提供的是一套高层的关系操作，比函数式的RDD API 要更加友好，门槛更低。<br><img src="https://s2.loli.net/2021/12/13/aGlVyieYRHDZhrg.jpg"></p>
</li>
<li><p>上图直观地体现了DataFrame和RDD的区别。</p>
<ul>
<li>左侧的RDD[Person]虽然以Person为类型参数，但Spark框架本身不了解Person类的内部结构。而右侧的DataFrame却提供了详细的结构信息，使得 Spark SQL 可以清楚地知道该数据集中包含哪些列，每列的名称和类型各是什么。</li>
<li>DataFrame是为数据提供了Schema的视图。可以把它当做数据库中的一张表来对待</li>
<li>DataFrame也是懒执行的，但性能上比RDD要高，主要原因：优化的执行计划，即查询计划通过Spark catalyst optimiser进行优化。比如下面一个例子:<br>  <img src="https://s2.loli.net/2021/12/13/6ANB9eFt2KlXIaO.jpg"><br><img src="https://s2.loli.net/2021/12/13/gtdbslanASHGu79.jpg"></li>
</ul>
</li>
<li><p>为了说明查询优化，我们来看上图展示的人口数据分析的示例。图中构造了两个DataFrame，将它们join之后又做了一次filter操作。如果原封不动地执行这个执行计划，最终的执行效率是不高的。因为join是一个代价较大的操作，也可能会产生一个较大的数据集。如果我们能将filter下推到 join下方，先对DataFrame进行过滤，再join过滤后的较小的结果集，便可以有效缩短执行时间。而Spark SQL的查询优化器正是这样做的。简而言之，逻辑查询计划优化就是一个利用基于关系代数的等价变换，将高成本的操作替换为低成本操作的过程。<br><img src="https://s2.loli.net/2021/12/13/NrzleGmhZ6S5PvA.jpg"></p>
</li>
</ul>
<h2 id="1-5-DataSet是什么"><a href="#1-5-DataSet是什么" class="headerlink" title="1.5 DataSet是什么"></a>1.5 DataSet是什么</h2><p>DataSet是分布式数据集合。DataSet是Spark 1.6中添加的一个新抽象，是DataFrame的一个扩展。它提供了RDD的优势（强类型，使用强大的lambda函数的能力）以及Spark SQL优化执行引擎的优点。DataSet也可以使用功能性的转换（操作map，flatMap，filter等等）。</p>
<ul>
<li>DataSet是DataFrame API的一个扩展，是SparkSQL最新的数据抽象</li>
<li>用户友好的API风格，既具有类型安全检查也具有DataFrame的查询优化特性；</li>
<li>用样例类来对DataSet中定义数据的结构信息，样例类中每个属性的名称直接映射到DataSet中的字段名称；</li>
<li>DataSet是强类型的。比如可以有DataSet[Car]，DataSet[Person]。</li>
<li>DataFrame是DataSet的特列，DataFrame=DataSet[Row] ，所以可以通过as方法将DataFrame转换为DataSet。Row是一个类型，跟Car、Person这些的类型一样，所有的表结构信息都用Row来表示。获取数据时需要指定顺序</li>
</ul>
<h1 id="第2章-SparkSQL核心编程"><a href="#第2章-SparkSQL核心编程" class="headerlink" title="第2章 SparkSQL核心编程"></a>第2章 SparkSQL核心编程</h1><p>本节重点学习如何使用 Spark SQL所提供的 DataFrame和DataSet模型进行编程。以及了解它们之间的关系和转换，关于具体的SQL语法不是我们的重点。</p>
<h2 id="2-1-新的起点"><a href="#2-1-新的起点" class="headerlink" title="2.1 新的起点"></a>2.1 新的起点</h2><p>Spark Core中，如果想要执行应用程序，需要首先构建上下文环境对象SparkContext，Spark SQL其实可以理解为对Spark Core的一种封装，不仅仅在模型上进行了封装，上下文环境对象也进行了封装。<br>在老的版本中，SparkSQL提供两种SQL查询起始点：一个叫SQLContext，用于Spark自己提供的SQL查询；一个叫HiveContext，用于连接Hive的查询。<br>SparkSession是Spark最新的SQL查询起始点，实质上是SQLContext和HiveContext的组合，所以在SQLContex和HiveContext上可用的API在SparkSession上同样是可以使用的。SparkSession内部封装了SparkContext，所以计算实际上是由sparkContext完成的。当我们使用 spark-shell 的时候, spark框架会自动的创建一个名称叫做spark的SparkSession对象, 就像我们以前可以自动获取到一个sc来表示SparkContext对象一样<br><img src="https://s2.loli.net/2021/12/13/651jx9Y7BymkDhN.jpg"></p>
<h2 id="2-2-DataFrame"><a href="#2-2-DataFrame" class="headerlink" title="2.2 DataFrame"></a>2.2 DataFrame</h2><p>Spark SQL的DataFrame API 允许我们使用 DataFrame 而不用必须去注册临时表或者生成 SQL 表达式。DataFrame API 既有 transformation操作也有action操作。</p>
<h3 id="2-2-1-创建DataFrame"><a href="#2-2-1-创建DataFrame" class="headerlink" title="2.2.1 创建DataFrame"></a>2.2.1 创建DataFrame</h3><p>在Spark SQL中SparkSession是创建DataFrame和执行SQL的入口，创建DataFrame有三种方式：通过Spark的数据源进行创建；从一个存在的RDD进行转换；还可以从Hive Table进行查询返回。</p>
<ol>
<li>从Spark数据源进行创建<ul>
<li>查看Spark支持创建文件的数据源格式  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; spark.read.</span><br><span class="line">csv   format   jdbc   json   load   option   options   orc   parquet   schema   table   text   textFile</span><br><span class="line"></span><br><span class="line">scala&gt; spark.read.</span><br></pre></td></tr></table></figure></li>
<li>在spark的bin/data目录中创建user.json文件  <figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&#123;&#x27;id&#x27;:<span class="number">1</span>,&#x27;name&#x27;:&#x27;zhangsan&#x27;,&#x27;age&#x27;:<span class="number">20</span>&#125;</span><br><span class="line">&#123;&#x27;id&#x27;:<span class="number">2</span>,&#x27;name&#x27;:&#x27;lisi&#x27;,&#x27;age&#x27;:<span class="number">30</span>&#125;</span><br><span class="line">&#123;&#x27;id&#x27;:<span class="number">3</span>,&#x27;name&#x27;:&#x27;wangwu&#x27;,&#x27;age&#x27;:<span class="number">40</span>&#125;</span><br><span class="line">&#123;&#x27;id&#x27;:<span class="number">4</span>,&#x27;name&#x27;:&#x27;zhaoliu&#x27;,&#x27;age&#x27;:<span class="number">50</span>&#125;</span><br></pre></td></tr></table></figure></li>
<li>读取json文件创建DataFrame  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> df = spark.read.json(<span class="string">&quot;data/user.json&quot;</span>)</span><br><span class="line">df: org.apache.spark.sql.<span class="type">DataFrame</span> = [age: bigint， username: string]</span><br></pre></td></tr></table></figure>
  注意：如果从内存中获取数据，spark可以知道数据类型具体是什么。如果是数字，默认作为Int处理；但是从文件中读取的数字，不能确定是什么类型，所以用bigint接收，可以和Long类型转换，但是和Int不能进行转换</li>
<li>展示结果:show  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; df.show</span><br><span class="line">+---+---+--------+</span><br><span class="line">|age| id|    name|</span><br><span class="line">+---+---+--------+</span><br><span class="line">| 20|  1|zhangsan|</span><br><span class="line">| 30|  2|    lisi|</span><br><span class="line">| 40|  3|  wangwu|</span><br><span class="line">| 50|  4| zhaoliu|</span><br><span class="line">+---+---+--------+</span><br><span class="line">scala&gt;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>从RDD进行转换<br> 在后续章节中讨论</li>
<li>从Hive Table进行查询返回<br> 在后续章节中讨论</li>
</ol>
<h3 id="2-2-2-SQL语法"><a href="#2-2-2-SQL语法" class="headerlink" title="2.2.2 SQL语法"></a>2.2.2 SQL语法</h3><p>SQL语法风格是指我们查询数据的时候使用SQL语句来查询，这种风格的查询必须要有临时视图或者全局视图来辅助</p>
<ol>
<li>读取JSON文件创建DataFrame <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> df = spark.read.json(<span class="string">&quot;data/user.json&quot;</span>)</span><br><span class="line">df: org.apache.spark.sql.<span class="type">DataFrame</span> = [age: bigint， username: string]</span><br></pre></td></tr></table></figure></li>
<li>对DataFrame创建一个临时表(视图) <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; df.createOrReplaceTempView(<span class="string">&quot;user&quot;</span>)</span><br></pre></td></tr></table></figure></li>
<li>通过SQL语句实现查询全表,结果展示 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; spark.sql(<span class="string">&quot;select * from user&quot;</span>).show</span><br><span class="line">+---+---+--------+</span><br><span class="line">|age| id|    name|</span><br><span class="line">+---+---+--------+</span><br><span class="line">| 20|  1|zhangsan|</span><br><span class="line">| 30|  2|    lisi|</span><br><span class="line">| 40|  3|  wangwu|</span><br><span class="line">| 50|  4| zhaoliu|</span><br><span class="line">+---+---+--------+</span><br><span class="line">scala&gt;</span><br></pre></td></tr></table></figure>
 注意：普通临时表是Session范围内的，如果想应用范围内有效，可以使用全局临时表。使用全局临时表时需要全路径访问，如：global_temp.people</li>
<li>对于DataFrame创建一个全局表 <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; df.createGlobalTempView(<span class="string">&quot;user&quot;</span>)</span><br></pre></td></tr></table></figure></li>
<li>通过SQL语句实现查询全表 <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; spark.sql(<span class="string">&quot;SELECT * FROM global_temp.user&quot;</span>).show()</span><br><span class="line">+---+---+--------+</span><br><span class="line">|age| id|    name|</span><br><span class="line">+---+---+--------+</span><br><span class="line">| <span class="number">20</span>|  <span class="number">1</span>|zhangsan|</span><br><span class="line">| <span class="number">30</span>|  <span class="number">2</span>|    lisi|</span><br><span class="line">| <span class="number">40</span>|  <span class="number">3</span>|  wangwu|</span><br><span class="line">| <span class="number">50</span>|  <span class="number">4</span>| zhaoliu|</span><br><span class="line">+---+---+--------+</span><br><span class="line">scala&gt; spark.newSession.sql(<span class="string">&quot;SELECT * FROM global_temp.user&quot;</span>).show()</span><br><span class="line">+---+---+--------+</span><br><span class="line">|age| id|    name|</span><br><span class="line">+---+---+--------+</span><br><span class="line">| <span class="number">20</span>|  <span class="number">1</span>|zhangsan|</span><br><span class="line">| <span class="number">30</span>|  <span class="number">2</span>|    lisi|</span><br><span class="line">| <span class="number">40</span>|  <span class="number">3</span>|  wangwu|</span><br><span class="line">| <span class="number">50</span>|  <span class="number">4</span>| zhaoliu|</span><br><span class="line">+---+---+--------+</span><br><span class="line">scala&gt;</span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="2-2-3-DSL语法"><a href="#2-2-3-DSL语法" class="headerlink" title="2.2.3 DSL语法"></a>2.2.3 DSL语法</h3><p>DataFrame提供一个特定领域语言(domain-specific language, DSL)去管理结构化的数据。可以在 Scala, Java, Python 和 R 中使用 DSL，使用 DSL 语法风格不必去创建临时视图了</p>
<ol>
<li>创建一个DataFrame <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> df = spark.read.json(<span class="string">&quot;data/user.json&quot;</span>)</span><br><span class="line">df: org.apache.spark.sql.<span class="type">DataFrame</span> = [age: bigint， name: string]</span><br></pre></td></tr></table></figure></li>
<li>查看DataFrame的Schema信息 <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; df.printSchema</span><br><span class="line">root</span><br><span class="line"> |-- age: <span class="type">Long</span> (nullable = <span class="literal">true</span>)</span><br><span class="line"> |-- username: string (nullable = <span class="literal">true</span>)</span><br></pre></td></tr></table></figure></li>
<li>只查看”name”列数据， <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; df.select(<span class="string">&quot;name&quot;</span>).show()</span><br><span class="line">+--------+</span><br><span class="line">|    name|</span><br><span class="line">+--------+</span><br><span class="line">|zhangsan|</span><br><span class="line">|    lisi|</span><br><span class="line">|  wangwu|</span><br><span class="line">| zhaoliu|</span><br><span class="line">+--------+</span><br></pre></td></tr></table></figure></li>
<li>查看”name”列数据以及”age+1”数据<br> 注意:涉及到运算的时候, 每列都必须使用$, 或者采用引号表达式：单引号+字段名 <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; df.select($<span class="string">&quot;name&quot;</span>,$<span class="string">&quot;age&quot;</span> + <span class="number">1</span>).show</span><br><span class="line">+--------+---------+</span><br><span class="line">|    name|(age + <span class="number">1</span>)|</span><br><span class="line">+--------+---------+</span><br><span class="line">|zhangsan|       <span class="number">21</span>|</span><br><span class="line">|    lisi|       <span class="number">31</span>|</span><br><span class="line">|  wangwu|       <span class="number">41</span>|</span><br><span class="line">| zhaoliu|       <span class="number">51</span>|</span><br><span class="line">+--------+---------+</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">scala&gt; df.select(<span class="symbol">&#x27;name</span>, <span class="symbol">&#x27;age</span> + <span class="number">1</span>).show()</span><br><span class="line">+--------+---------+</span><br><span class="line">|    name|(age + <span class="number">1</span>)|</span><br><span class="line">+--------+---------+</span><br><span class="line">|zhangsan|       <span class="number">21</span>|</span><br><span class="line">|    lisi|       <span class="number">31</span>|</span><br><span class="line">|  wangwu|       <span class="number">41</span>|</span><br><span class="line">| zhaoliu|       <span class="number">51</span>|</span><br><span class="line">+--------+---------+</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">scala&gt; df.select(<span class="symbol">&#x27;name</span>, <span class="symbol">&#x27;age</span> + <span class="number">1</span> as <span class="string">&quot;newage&quot;</span>).show()</span><br><span class="line">+--------+------+</span><br><span class="line">|    name|newage|</span><br><span class="line">+--------+------+</span><br><span class="line">|zhangsan|    <span class="number">21</span>|</span><br><span class="line">|    lisi|    <span class="number">31</span>|</span><br><span class="line">|  wangwu|    <span class="number">41</span>|</span><br><span class="line">| zhaoliu|    <span class="number">51</span>|</span><br><span class="line">+--------+------+</span><br></pre></td></tr></table></figure></li>
<li>查看”age”大于”30”的数据 <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; df.filter($<span class="string">&quot;age&quot;</span>&gt;<span class="number">30</span>).show</span><br><span class="line">+---+---+-------+</span><br><span class="line">|age| id|   name|</span><br><span class="line">+---+---+-------+</span><br><span class="line">| <span class="number">40</span>|  <span class="number">3</span>| wangwu|</span><br><span class="line">| <span class="number">50</span>|  <span class="number">4</span>|zhaoliu|</span><br><span class="line">+---+---+-------+</span><br></pre></td></tr></table></figure></li>
<li>按照”age”分组，查看数据条数 <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; df.groupBy(<span class="string">&quot;age&quot;</span>).count.show</span><br><span class="line">+---+-----+</span><br><span class="line">|age|count|</span><br><span class="line">+---+-----+</span><br><span class="line">| <span class="number">50</span>|    <span class="number">1</span>|</span><br><span class="line">| <span class="number">30</span>|    <span class="number">1</span>|</span><br><span class="line">| <span class="number">20</span>|    <span class="number">1</span>|</span><br><span class="line">| <span class="number">40</span>|    <span class="number">1</span>|</span><br><span class="line">+---+-----+</span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="2-2-4-RDD转换为DataFrame"><a href="#2-2-4-RDD转换为DataFrame" class="headerlink" title="2.2.4 RDD转换为DataFrame"></a>2.2.4 RDD转换为DataFrame</h3><ul>
<li>在IDEA中开发程序时，如果需要RDD与DF或者DS之间互相操作，那么需要引入 <code>import spark.implicits._</code><ul>
<li>这里的spark不是Scala中的包名，而是创建的sparkSession对象的变量名称，所以必须先创建SparkSession对象再导入。这里的spark对象不能使用var声明，因为Scala只支持val修饰的对象的引入。</li>
</ul>
</li>
<li>spark-shell中无需导入，自动完成此操作。  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> idRDD = sc.textFile(<span class="string">&quot;data/id.txt&quot;</span>)</span><br><span class="line">scala&gt; idRDD.toDF(<span class="string">&quot;id&quot;</span>).show</span><br><span class="line">+---+</span><br><span class="line">| id|</span><br><span class="line">+---+</span><br><span class="line">|  <span class="number">1</span>|</span><br><span class="line">|  <span class="number">2</span>|</span><br><span class="line">|  <span class="number">3</span>|</span><br><span class="line">|  <span class="number">4</span>|</span><br><span class="line">+---+</span><br></pre></td></tr></table></figure></li>
<li>实际开发中，一般通过样例类将RDD转换为DataFrame  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">User</span>(<span class="params">name:<span class="type">String</span>, age:<span class="type">Int</span></span>)</span></span><br><span class="line">defined <span class="class"><span class="keyword">class</span> <span class="title">User</span></span></span><br><span class="line">scala&gt; sc.makeRDD(<span class="type">List</span>((<span class="string">&quot;zhangsan&quot;</span>,<span class="number">30</span>), (<span class="string">&quot;lisi&quot;</span>,<span class="number">40</span>))).map(t=&gt;<span class="type">User</span>(t._1, t._2)).toDF.show</span><br><span class="line">+--------+---+</span><br><span class="line">|     name|age|</span><br><span class="line">+--------+---+</span><br><span class="line">|zhangsan| <span class="number">30</span>|</span><br><span class="line">|    lisi| <span class="number">40</span>|</span><br><span class="line">+--------+---+</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="2-2-5-DataFrame转换为RDD"><a href="#2-2-5-DataFrame转换为RDD" class="headerlink" title="2.2.5 DataFrame转换为RDD"></a>2.2.5 DataFrame转换为RDD</h3><p>DataFrame其实就是对RDD的封装，所以可以直接获取内部的RDD</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> df = sc.makeRDD(<span class="type">List</span>((<span class="string">&quot;zhangsan&quot;</span>,<span class="number">30</span>), (<span class="string">&quot;lisi&quot;</span>,<span class="number">40</span>))).map(t=&gt;<span class="type">User</span>(t._1, t._2)).toDF</span><br><span class="line">df: org.apache.spark.sql.<span class="type">DataFrame</span> = [name: string, age: int]</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> rdd = df.rdd</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[org.apache.spark.sql.<span class="type">Row</span>] = <span class="type">MapPartitionsRDD</span>[<span class="number">46</span>] at rdd at &lt;console&gt;:<span class="number">25</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> array = rdd.collect</span><br><span class="line">array: <span class="type">Array</span>[org.apache.spark.sql.<span class="type">Row</span>] = <span class="type">Array</span>([zhangsan,<span class="number">30</span>], [lisi,<span class="number">40</span>])</span><br></pre></td></tr></table></figure>
<blockquote>
<p>注意：此时得到的RDD存储类型为Row</p>
</blockquote>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; array(<span class="number">0</span>)</span><br><span class="line">res28: org.apache.spark.sql.<span class="type">Row</span> = [zhangsan,<span class="number">30</span>]</span><br><span class="line">scala&gt; array(<span class="number">0</span>)(<span class="number">0</span>)</span><br><span class="line">res29: <span class="type">Any</span> = zhangsan</span><br><span class="line">scala&gt; array(<span class="number">0</span>).getAs[<span class="type">String</span>](<span class="string">&quot;name&quot;</span>)</span><br><span class="line">res30: <span class="type">String</span> = zhangsan</span><br></pre></td></tr></table></figure>

<h2 id="2-3-DataSet"><a href="#2-3-DataSet" class="headerlink" title="2.3 DataSet"></a>2.3 DataSet</h2><p>DataSet是具有强类型的数据集合，需要提供对应的类型信息。</p>
<h3 id="2-3-1-创建DataSet"><a href="#2-3-1-创建DataSet" class="headerlink" title="2.3.1 创建DataSet"></a>2.3.1 创建DataSet</h3><ol>
<li>使用样例类序列创建DataSet <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Person</span>(<span class="params">name: <span class="type">String</span>, age: <span class="type">Long</span></span>)</span></span><br><span class="line">defined <span class="class"><span class="keyword">class</span> <span class="title">Person</span></span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> caseClassDS = <span class="type">Seq</span>(<span class="type">Person</span>(<span class="string">&quot;zhangsan&quot;</span>,<span class="number">2</span>)).toDS()</span><br><span class="line"></span><br><span class="line">caseClassDS: org.apache.spark.sql.<span class="type">Dataset</span>[<span class="type">Person</span>] = [name: string, age: <span class="type">Long</span>]</span><br><span class="line"></span><br><span class="line">scala&gt; caseClassDS.show</span><br><span class="line">+---------+---+</span><br><span class="line">|     name|age|</span><br><span class="line">+---------+---+</span><br><span class="line">| zhangsan|  <span class="number">2</span>|</span><br><span class="line">+---------+---+</span><br></pre></td></tr></table></figure></li>
<li>使用基本类型的序列创建DataSet <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> ds = <span class="type">Seq</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>).toDS</span><br><span class="line">ds: org.apache.spark.sql.<span class="type">Dataset</span>[<span class="type">Int</span>] = [value: int]</span><br><span class="line"></span><br><span class="line">scala&gt; ds.show</span><br><span class="line">+-----+</span><br><span class="line">|value|</span><br><span class="line">+-----+</span><br><span class="line">|    <span class="number">1</span>|</span><br><span class="line">|    <span class="number">2</span>|</span><br><span class="line">|    <span class="number">3</span>|</span><br><span class="line">|    <span class="number">4</span>|</span><br><span class="line">|    <span class="number">5</span>|</span><br><span class="line">+-----+</span><br></pre></td></tr></table></figure>
<blockquote>
<p>注意：在实际使用的时候，很少用到把序列转换成DataSet，更多的是通过RDD来得到DataSet</p>
</blockquote>
</li>
</ol>
<h3 id="2-3-2-RDD转换为DataSet"><a href="#2-3-2-RDD转换为DataSet" class="headerlink" title="2.3.2 RDD转换为DataSet"></a>2.3.2 RDD转换为DataSet</h3><p>SparkSQL能够自动将包含有case类的RDD转换成DataSet，case类定义了table的结构，case类属性通过反射变成了表的列名。Case类可以包含诸如Seq或者Array等复杂的结构。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">User</span>(<span class="params">name:<span class="type">String</span>, age:<span class="type">Int</span></span>)</span></span><br><span class="line">defined <span class="class"><span class="keyword">class</span> <span class="title">User</span></span></span><br><span class="line"></span><br><span class="line">scala&gt; sc.makeRDD(<span class="type">List</span>((<span class="string">&quot;zhangsan&quot;</span>,<span class="number">30</span>), (<span class="string">&quot;lisi&quot;</span>,<span class="number">49</span>))).map(t=&gt;<span class="type">User</span>(t._1, t._2)).toDS</span><br><span class="line">res11: org.apache.spark.sql.<span class="type">Dataset</span>[<span class="type">User</span>] = [name: string, age: int]</span><br></pre></td></tr></table></figure>

<h3 id="2-3-3-DataSet转换为RDD"><a href="#2-3-3-DataSet转换为RDD" class="headerlink" title="2.3.3 DataSet转换为RDD"></a>2.3.3 DataSet转换为RDD</h3><p>DataSet其实也是对RDD的封装，所以可以直接获取内部的RDD</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">User</span>(<span class="params">name:<span class="type">String</span>, age:<span class="type">Int</span></span>)</span></span><br><span class="line">defined <span class="class"><span class="keyword">class</span> <span class="title">User</span></span></span><br><span class="line"></span><br><span class="line">scala&gt; sc.makeRDD(<span class="type">List</span>((<span class="string">&quot;zhangsan&quot;</span>,<span class="number">30</span>), (<span class="string">&quot;lisi&quot;</span>,<span class="number">49</span>))).map(t=&gt;<span class="type">User</span>(t._1, t._2)).toDS</span><br><span class="line">res11: org.apache.spark.sql.<span class="type">Dataset</span>[<span class="type">User</span>] = [name: string, age: int]</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> rdd = res11.rdd</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">User</span>] = <span class="type">MapPartitionsRDD</span>[<span class="number">51</span>] at rdd at &lt;console&gt;:<span class="number">25</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd.collect</span><br><span class="line">res12: <span class="type">Array</span>[<span class="type">User</span>] = <span class="type">Array</span>(<span class="type">User</span>(zhangsan,<span class="number">30</span>), <span class="type">User</span>(lisi,<span class="number">49</span>))</span><br></pre></td></tr></table></figure>

<h2 id="2-4-DataFrame和DataSet转换"><a href="#2-4-DataFrame和DataSet转换" class="headerlink" title="2.4 DataFrame和DataSet转换"></a>2.4 DataFrame和DataSet转换</h2><p>DataFrame其实是DataSet的特例，所以它们之间是可以互相转换的。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">type</span> <span class="title">DataFrame</span> </span>= <span class="type">Dataset</span>[<span class="type">Row</span>]</span><br></pre></td></tr></table></figure>
<ul>
<li>DataFrame转换为DataSet  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">User</span>(<span class="params">name:<span class="type">String</span>, age:<span class="type">Int</span></span>)</span></span><br><span class="line">defined <span class="class"><span class="keyword">class</span> <span class="title">User</span></span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> df = sc.makeRDD(<span class="type">List</span>((<span class="string">&quot;zhangsan&quot;</span>,<span class="number">30</span>), (<span class="string">&quot;lisi&quot;</span>,<span class="number">49</span>))).toDF(<span class="string">&quot;name&quot;</span>,<span class="string">&quot;age&quot;</span>)</span><br><span class="line">df: org.apache.spark.sql.<span class="type">DataFrame</span> = [name: string, age: int]</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> ds = df.as[<span class="type">User</span>]</span><br><span class="line">ds: org.apache.spark.sql.<span class="type">Dataset</span>[<span class="type">User</span>] = [name: string, age: int]</span><br></pre></td></tr></table></figure></li>
<li>DataSet转换为DataFrame  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> ds = df.as[<span class="type">User</span>]</span><br><span class="line">ds: org.apache.spark.sql.<span class="type">Dataset</span>[<span class="type">User</span>] = [name: string, age: int]</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> df = ds.toDF</span><br><span class="line">df: org.apache.spark.sql.<span class="type">DataFrame</span> = [name: string, age: int]</span><br></pre></td></tr></table></figure></li>
</ul>
<h2 id="2-5-RDD、DataFrame、DataSet三者的关系"><a href="#2-5-RDD、DataFrame、DataSet三者的关系" class="headerlink" title="2.5 RDD、DataFrame、DataSet三者的关系"></a>2.5 RDD、DataFrame、DataSet三者的关系</h2><p>在SparkSQL中Spark为我们提供了两个新的抽象，分别是DataFrame和DataSet。他们和RDD有什么区别呢？首先从版本的产生上来看：</p>
<ul>
<li>Spark1.0 =&gt; RDD </li>
<li>Spark1.3 =&gt; DataFrame</li>
<li>Spark1.6 =&gt; Dataset<br>如果同样的数据都给到这三个数据结构，他们分别计算之后，都会给出相同的结果。不同是的他们的执行效率和执行方式。在后期的Spark版本中，DataSet有可能会逐步取代RDD和DataFrame成为唯一的API接口。</li>
</ul>
<h3 id="2-5-1-三者的共性"><a href="#2-5-1-三者的共性" class="headerlink" title="2.5.1 三者的共性"></a>2.5.1 三者的共性</h3><ul>
<li>RDD、DataFrame、DataSet全都是spark平台下的分布式弹性数据集，为处理超大型数据提供便利;</li>
<li>三者都有惰性机制，在进行创建、转换，如map方法时，不会立即执行，只有在遇到Action如foreach时，三者才会开始遍历运算;</li>
<li>三者有许多共同的函数，如filter，排序等;</li>
<li>在对DataFrame和Dataset进行操作许多操作都需要这个包:import spark.implicits._（在创建好SparkSession对象后尽量直接导入）</li>
<li>三者都会根据 Spark 的内存情况自动缓存运算，这样即使数据量很大，也不用担心会内存溢出</li>
<li>三者都有partition的概念</li>
<li>DataFrame和DataSet均可使用模式匹配获取各个字段的值和类型</li>
</ul>
<h3 id="2-5-2-三者的区别"><a href="#2-5-2-三者的区别" class="headerlink" title="2.5.2 三者的区别"></a>2.5.2 三者的区别</h3><ol>
<li>RDD<ul>
<li>RDD一般和spark mllib同时使用</li>
<li>RDD不支持sparksql操作</li>
</ul>
</li>
<li>DataFrame<ul>
<li>与RDD和Dataset不同，DataFrame每一行的类型固定为Row，每一列的值没法直接访问，只有通过解析才能获取各个字段的值</li>
<li>DataFrame与DataSet一般不与 spark mllib 同时使用</li>
<li>DataFrame与DataSet均支持 SparkSQL 的操作，比如select，groupby之类，还能注册临时表/视窗，进行 sql 语句操作</li>
<li>DataFrame与DataSet支持一些特别方便的保存方式，比如保存成csv，可以带上表头，这样每一列的字段名一目了然(后面专门讲解)</li>
</ul>
</li>
<li>DataSet<ul>
<li>Dataset和DataFrame拥有完全相同的成员函数，区别只是每一行的数据类型不同。 DataFrame其实就是DataSet的一个特例  type DataFrame = Dataset[Row]</li>
<li>DataFrame也可以叫Dataset[Row],每一行的类型是Row，不解析，每一行究竟有哪些字段，各个字段又是什么类型都无从得知，只能用上面提到的getAS方法或者共性中的第七条提到的模式匹配拿出特定字段。而Dataset中，每一行是什么类型是不一定的，在自定义了case class之后可以很自由的获得每一行的信息</li>
</ul>
</li>
</ol>
<h3 id="2-5-3-三者的互相转换"><a href="#2-5-3-三者的互相转换" class="headerlink" title="2.5.3 三者的互相转换"></a>2.5.3 三者的互相转换</h3><p><img src="https://s2.loli.net/2021/12/13/Xl9isUamgIWY7MR.jpg"></p>
<h2 id="2-6-IDEA开发SparkSQL"><a href="#2-6-IDEA开发SparkSQL" class="headerlink" title="2.6 IDEA开发SparkSQL"></a>2.6 IDEA开发SparkSQL</h2><p>实际开发中，都是使用IDEA进行开发的。</p>
<h3 id="2-6-1-添加依赖"><a href="#2-6-1-添加依赖" class="headerlink" title="2.6.1 添加依赖"></a>2.6.1 添加依赖</h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-sql_2.12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.0.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h3 id="2-6-2-代码实现"><a href="#2-6-2-代码实现" class="headerlink" title="2.6.2 代码实现"></a>2.6.2 代码实现</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SparkSQL01_Demo</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">//创建上下文环境配置对象</span></span><br><span class="line">    <span class="keyword">val</span> conf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">&quot;local[*]&quot;</span>).setAppName(<span class="string">&quot;SparkSQL01_Demo&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//创建SparkSession对象</span></span><br><span class="line">    <span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span>.builder().config(conf).getOrCreate()</span><br><span class="line">    <span class="comment">//RDD=&gt;DataFrame=&gt;DataSet转换需要引入隐式转换规则，否则无法转换</span></span><br><span class="line">    <span class="comment">//spark不是包名，是上下文环境对象名</span></span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line">    <span class="comment">//读取json文件 创建DataFrame  &#123;&quot;username&quot;: &quot;lisi&quot;,&quot;age&quot;: 18&#125;</span></span><br><span class="line">    <span class="keyword">val</span> df: <span class="type">DataFrame</span> = spark.read.json(<span class="string">&quot;input/test.json&quot;</span>)</span><br><span class="line">    <span class="comment">//df.show()</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">//SQL风格语法</span></span><br><span class="line">    df.createOrReplaceTempView(<span class="string">&quot;user&quot;</span>)</span><br><span class="line">    <span class="comment">//spark.sql(&quot;select avg(age) from user&quot;).show</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">//DSL风格语法</span></span><br><span class="line">    <span class="comment">//df.select(&quot;username&quot;,&quot;age&quot;).show()</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">//*****RDD=&gt;DataFrame=&gt;DataSet*****</span></span><br><span class="line">    <span class="comment">//RDD</span></span><br><span class="line">    <span class="keyword">val</span> rdd1: <span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">String</span>, <span class="type">Int</span>)] = spark.sparkContext.makeRDD(<span class="type">List</span>((<span class="number">1</span>,<span class="string">&quot;zhangsan&quot;</span>,<span class="number">30</span>),(<span class="number">2</span>,<span class="string">&quot;lisi&quot;</span>,<span class="number">28</span>),(<span class="number">3</span>,<span class="string">&quot;wangwu&quot;</span>,<span class="number">20</span>)))</span><br><span class="line"></span><br><span class="line">    <span class="comment">//DataFrame</span></span><br><span class="line">    <span class="keyword">val</span> df1: <span class="type">DataFrame</span> = rdd1.toDF(<span class="string">&quot;id&quot;</span>,<span class="string">&quot;name&quot;</span>,<span class="string">&quot;age&quot;</span>)</span><br><span class="line">    <span class="comment">//df1.show()</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">//DateSet</span></span><br><span class="line">    <span class="keyword">val</span> ds1: <span class="type">Dataset</span>[<span class="type">User</span>] = df1.as[<span class="type">User</span>]</span><br><span class="line">    <span class="comment">//ds1.show()</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">//*****DataSet=&gt;DataFrame=&gt;RDD*****</span></span><br><span class="line">    <span class="comment">//DataFrame</span></span><br><span class="line">    <span class="keyword">val</span> df2: <span class="type">DataFrame</span> = ds1.toDF()</span><br><span class="line"></span><br><span class="line">    <span class="comment">//RDD  返回的RDD类型为Row，里面提供的getXXX方法可以获取字段值，类似jdbc处理结果集，但是索引从0开始</span></span><br><span class="line">    <span class="keyword">val</span> rdd2: <span class="type">RDD</span>[<span class="type">Row</span>] = df2.rdd</span><br><span class="line">    <span class="comment">//rdd2.foreach(a=&gt;println(a.getString(1)))</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">//*****RDD=&gt;DataSet*****</span></span><br><span class="line">    rdd1.map&#123;</span><br><span class="line">      <span class="keyword">case</span> (id,name,age)=&gt;<span class="type">User</span>(id,name,age)</span><br><span class="line">    &#125;.toDS()</span><br><span class="line"></span><br><span class="line">    <span class="comment">//*****DataSet=&gt;=&gt;RDD*****</span></span><br><span class="line">    ds1.rdd</span><br><span class="line"></span><br><span class="line">    <span class="comment">//释放资源</span></span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">User</span>(<span class="params">id:<span class="type">Int</span>,name:<span class="type">String</span>,age:<span class="type">Int</span></span>)</span></span><br></pre></td></tr></table></figure>

<h2 id="2-7-用户自定义函数"><a href="#2-7-用户自定义函数" class="headerlink" title="2.7 用户自定义函数"></a>2.7 用户自定义函数</h2><p>用户可以通过spark.udf功能添加自定义函数，实现自定义功能。</p>
<h3 id="2-7-1-UDF"><a href="#2-7-1-UDF" class="headerlink" title="2.7.1 UDF"></a>2.7.1 UDF</h3><ol>
<li>创建DataFrame <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> df = spark.read.json(<span class="string">&quot;data/user.json&quot;</span>)</span><br><span class="line">df: org.apache.spark.sql.<span class="type">DataFrame</span> = [age: bigint， username: string]</span><br></pre></td></tr></table></figure></li>
<li>注册UDF <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; spark.udf.register(<span class="string">&quot;addName&quot;</span>,(x:<span class="type">String</span>)=&gt; <span class="string">&quot;Name:&quot;</span>+x)</span><br><span class="line">res9: org.apache.spark.sql.expressions.<span class="type">UserDefinedFunction</span> = <span class="type">UserDefinedFunction</span>(&lt;function1&gt;,<span class="type">StringType</span>,<span class="type">Some</span>(<span class="type">List</span>(<span class="type">StringType</span>)))</span><br></pre></td></tr></table></figure></li>
<li>创建临时表 <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; df.createOrReplaceTempView(<span class="string">&quot;people&quot;</span>)</span><br></pre></td></tr></table></figure></li>
<li>应用UDF <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; spark.sql(<span class="string">&quot;Select addName(name),age from people&quot;</span>).show()</span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="2-7-2-UDAF"><a href="#2-7-2-UDAF" class="headerlink" title="2.7.2 UDAF"></a>2.7.2 UDAF</h3><p>强类型的Dataset和弱类型的DataFrame都提供了相关的聚合函数， 如 count()，countDistinct()，avg()，max()，min()。除此之外，用户可以设定自己的自定义聚合函数。通过继承UserDefinedAggregateFunction来实现用户自定义弱类型聚合函数。从Spark3.0版本后，UserDefinedAggregateFunction已经不推荐使用了。可以统一采用强类型聚合函数Aggregator<br>需求：计算平均工资<br>一个需求可以采用很多种不同的方法实现需求</p>
<ol>
<li><p>实现方式 - RDD</p>
 <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> conf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">&quot;app&quot;</span>).setMaster(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> sc: <span class="type">SparkContext</span> = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"><span class="keyword">val</span> res: (<span class="type">Int</span>, <span class="type">Int</span>) = sc.makeRDD(<span class="type">List</span>((<span class="string">&quot;zhangsan&quot;</span>, <span class="number">20</span>), (<span class="string">&quot;lisi&quot;</span>, <span class="number">30</span>), (<span class="string">&quot;wangw&quot;</span>, <span class="number">40</span>))).map &#123;</span><br><span class="line">  <span class="keyword">case</span> (name, age) =&gt; &#123;</span><br><span class="line">    (age, <span class="number">1</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;.reduce &#123;</span><br><span class="line">  (t1, t2) =&gt; &#123;</span><br><span class="line">    (t1._1 + t2._1, t1._2 + t2._2)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">println(res._1/res._2)</span><br><span class="line"><span class="comment">// 关闭连接</span></span><br><span class="line">sc.stop()</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
<li><p>实现方式 - 累加器</p>
 <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyAC</span> <span class="keyword">extends</span> <span class="title">AccumulatorV2</span>[<span class="type">Int</span>,<span class="type">Int</span>]</span>&#123;</span><br><span class="line">  <span class="keyword">var</span> sum:<span class="type">Int</span> = <span class="number">0</span></span><br><span class="line">  <span class="keyword">var</span> count:<span class="type">Int</span> = <span class="number">0</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">isZero</span></span>: <span class="type">Boolean</span> = &#123;</span><br><span class="line">    <span class="keyword">return</span> sum ==<span class="number">0</span> &amp;&amp; count == <span class="number">0</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">copy</span></span>(): <span class="type">AccumulatorV2</span>[<span class="type">Int</span>, <span class="type">Int</span>] = &#123;</span><br><span class="line">    <span class="keyword">val</span> newMyAc = <span class="keyword">new</span> <span class="type">MyAC</span></span><br><span class="line">    newMyAc.sum = <span class="keyword">this</span>.sum</span><br><span class="line">    newMyAc.count = <span class="keyword">this</span>.count</span><br><span class="line">    newMyAc</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">reset</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">    sum =<span class="number">0</span></span><br><span class="line">    count = <span class="number">0</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">add</span></span>(v: <span class="type">Int</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    sum += v</span><br><span class="line">    count += <span class="number">1</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">merge</span></span>(other: <span class="type">AccumulatorV2</span>[<span class="type">Int</span>, <span class="type">Int</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    other <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> o:<span class="type">MyAC</span>=&gt;&#123;</span><br><span class="line">        sum += o.sum</span><br><span class="line">        count += o.count</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">case</span> _=&gt;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">value</span></span>: <span class="type">Int</span> = sum/count</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li><p>实现方式 - UDAF - 弱类型</p>
 <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">udaf_avg</span> </span>= &#123;</span><br><span class="line">    println(<span class="string">&quot;*********udf**********&quot;</span>)</span><br><span class="line">    <span class="comment">//</span></span><br><span class="line">    <span class="comment">//val spark = new SparkSession()</span></span><br><span class="line">    <span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span>.builder()</span><br><span class="line">        .master(<span class="string">&quot;local&quot;</span>)</span><br><span class="line">        .appName(<span class="string">&quot;Word Count&quot;</span>)</span><br><span class="line">        .getOrCreate()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> userDF: <span class="type">DataFrame</span> = spark.read.json(<span class="string">&quot;data/user.json&quot;</span>)</span><br><span class="line">    println(<span class="string">&quot;*********userDF.show**********&quot;</span>)</span><br><span class="line">    userDF.show</span><br><span class="line"></span><br><span class="line">    userDF.createOrReplaceTempView(<span class="string">&quot;user&quot;</span>)</span><br><span class="line"></span><br><span class="line">    println(<span class="string">&quot;*********select name from user**********&quot;</span>)</span><br><span class="line">    spark.sql(<span class="string">&quot;select id,age,name from user&quot;</span>).show</span><br><span class="line"></span><br><span class="line">    <span class="comment">//SQL中调用自定义函数 UDAF</span></span><br><span class="line">    spark.udf.register(<span class="string">&quot;udaf_avg&quot;</span>,functions.udaf(<span class="keyword">new</span> <span class="type">MyAveragUDAF</span>))</span><br><span class="line">    spark.sql(<span class="string">&quot;select udaf_avg(age) from user&quot;</span>).show</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    spark.stop()</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">定义类继承UserDefinedAggregateFunction，并重写其中方法</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyAveragUDAF</span> <span class="keyword">extends</span> <span class="title">UserDefinedAggregateFunction</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 聚合函数输入参数的数据类型</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">inputSchema</span></span>: <span class="type">StructType</span> = <span class="type">StructType</span>(<span class="type">Array</span>(<span class="type">StructField</span>(<span class="string">&quot;age&quot;</span>,<span class="type">IntegerType</span>)))</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 聚合函数缓冲区中值的数据类型(age,count)</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">bufferSchema</span></span>: <span class="type">StructType</span> = &#123;</span><br><span class="line">    <span class="type">StructType</span>(<span class="type">Array</span>(<span class="type">StructField</span>(<span class="string">&quot;sum&quot;</span>,<span class="type">LongType</span>),<span class="type">StructField</span>(<span class="string">&quot;count&quot;</span>,<span class="type">LongType</span>)))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 函数返回值的数据类型</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">dataType</span></span>: <span class="type">DataType</span> = <span class="type">DoubleType</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// 稳定性：对于相同的输入是否一直返回相同的输出。</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">deterministic</span></span>: <span class="type">Boolean</span> = <span class="literal">true</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// 函数缓冲区初始化</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">initialize</span></span>(buffer: <span class="type">MutableAggregationBuffer</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// 存年龄的总和</span></span><br><span class="line">    buffer(<span class="number">0</span>) = <span class="number">0</span>L</span><br><span class="line">    <span class="comment">// 存年龄的个数</span></span><br><span class="line">    buffer(<span class="number">1</span>) = <span class="number">0</span>L</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 更新缓冲区中的数据</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">update</span></span>(buffer: <span class="type">MutableAggregationBuffer</span>,input: <span class="type">Row</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">if</span> (!input.isNullAt(<span class="number">0</span>)) &#123;</span><br><span class="line">      buffer(<span class="number">0</span>) = buffer.getLong(<span class="number">0</span>) + input.getInt(<span class="number">0</span>)</span><br><span class="line">      buffer(<span class="number">1</span>) = buffer.getLong(<span class="number">1</span>) + <span class="number">1</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 合并缓冲区</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">merge</span></span>(buffer1: <span class="type">MutableAggregationBuffer</span>,buffer2: <span class="type">Row</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    buffer1(<span class="number">0</span>) = buffer1.getLong(<span class="number">0</span>) + buffer2.getLong(<span class="number">0</span>)</span><br><span class="line">    buffer1(<span class="number">1</span>) = buffer1.getLong(<span class="number">1</span>) + buffer2.getLong(<span class="number">1</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 计算最终结果</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">evaluate</span></span>(buffer: <span class="type">Row</span>): <span class="type">Double</span> = buffer.getLong(<span class="number">0</span>).toDouble / buffer.getLong(<span class="number">1</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li><p>实现方式 - UDAF - 强类型</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">udaf_avg</span> </span>= &#123;</span><br><span class="line">    println(<span class="string">&quot;*********udf**********&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span>.builder()</span><br><span class="line">        .master(<span class="string">&quot;local&quot;</span>)</span><br><span class="line">        .appName(<span class="string">&quot;Word Count&quot;</span>)</span><br><span class="line">        .getOrCreate()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">val</span> df: <span class="type">DataFrame</span> = spark.read.json(<span class="string">&quot;data/user.json&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//封装为DataSet</span></span><br><span class="line">    <span class="keyword">val</span> ds: <span class="type">Dataset</span>[<span class="type">User01</span>] = df.as[<span class="type">User01</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//创建聚合函数</span></span><br><span class="line">    <span class="keyword">var</span> myAgeUdaf1 = <span class="keyword">new</span> <span class="type">MyAveragUDAF1</span></span><br><span class="line">    <span class="comment">//将聚合函数转换为查询的列</span></span><br><span class="line">    <span class="keyword">val</span> col: <span class="type">TypedColumn</span>[<span class="type">User01</span>, <span class="type">Double</span>] = myAgeUdaf1.toColumn</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//查询</span></span><br><span class="line">    ds.select(col).show()</span><br><span class="line">    <span class="type">Spark3</span><span class="number">.0</span>版本可以采用强类型的<span class="type">Aggregate</span>方式代替<span class="type">UserDefinedAggregateFunction</span> </span><br><span class="line">    <span class="comment">// TODO 创建UDAF函数</span></span><br><span class="line">    <span class="keyword">val</span> udaf = <span class="keyword">new</span> <span class="type">MyAvgAgeUDAF</span></span><br><span class="line">    <span class="comment">// TODO 注册到SparkSQL中</span></span><br><span class="line">    spark.udf.register(<span class="string">&quot;avgAge&quot;</span>, functions.udaf(udaf))</span><br><span class="line">    <span class="comment">// TODO 在SQL中使用聚合函数</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 定义用户的自定义聚合函数</span></span><br><span class="line">    spark.sql(<span class="string">&quot;select avgAge(age) from user&quot;</span>).show</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// **************************************************</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Buff</span>(<span class="params"> var sum:<span class="type">Long</span>, var cnt:<span class="type">Long</span> </span>)</span></span><br><span class="line">    <span class="comment">// totalage, count</span></span><br><span class="line">    <span class="class"><span class="keyword">class</span> <span class="title">MyAvgAgeUDAF</span> <span class="keyword">extends</span> <span class="title">Aggregator</span>[<span class="type">Long</span>, <span class="type">Buff</span>, <span class="type">Double</span>]</span>&#123;</span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">zero</span></span>: <span class="type">Buff</span> = <span class="type">Buff</span>(<span class="number">0</span>,<span class="number">0</span>)</span><br><span class="line">    </span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">reduce</span></span>(b: <span class="type">Buff</span>, a: <span class="type">Long</span>): <span class="type">Buff</span> = &#123;</span><br><span class="line">            b.sum += a</span><br><span class="line">            b.cnt += <span class="number">1</span></span><br><span class="line">            b</span><br><span class="line">        &#125;</span><br><span class="line">    </span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">merge</span></span>(b1: <span class="type">Buff</span>, b2: <span class="type">Buff</span>): <span class="type">Buff</span> = &#123;</span><br><span class="line">            b1.sum += b2.sum</span><br><span class="line">            b1.cnt += b2.cnt</span><br><span class="line">            b1</span><br><span class="line">        &#125;</span><br><span class="line">    </span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">finish</span></span>(reduction: <span class="type">Buff</span>): <span class="type">Double</span> = &#123;</span><br><span class="line">            reduction.sum.toDouble/reduction.cnt</span><br><span class="line">        &#125;</span><br><span class="line">    </span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">bufferEncoder</span></span>: <span class="type">Encoder</span>[<span class="type">Buff</span>] = <span class="type">Encoders</span>.product</span><br><span class="line">    </span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">outputEncoder</span></span>: <span class="type">Encoder</span>[<span class="type">Double</span>] = <span class="type">Encoders</span>.scalaDouble</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    spark.stop()</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">//输入数据类型</span></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">User01</span>(<span class="params">username:<span class="type">String</span>,age:<span class="type">Long</span></span>)</span></span><br><span class="line"><span class="comment">//缓存类型</span></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">AgeBuffer</span>(<span class="params">var sum:<span class="type">Long</span>,var count:<span class="type">Long</span></span>)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * 定义类继承org.apache.spark.sql.expressions.Aggregator</span></span><br><span class="line"><span class="comment">  * 重写类中的方法</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyAveragUDAF1</span> <span class="keyword">extends</span> <span class="title">Aggregator</span>[<span class="type">User01</span>,<span class="type">AgeBuffer</span>,<span class="type">Double</span>]</span>&#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">zero</span></span>: <span class="type">AgeBuffer</span> = &#123;</span><br><span class="line">    <span class="type">AgeBuffer</span>(<span class="number">0</span>L,<span class="number">0</span>L)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">reduce</span></span>(b: <span class="type">AgeBuffer</span>, a: <span class="type">User01</span>): <span class="type">AgeBuffer</span> = &#123;</span><br><span class="line">    b.sum = b.sum + a.age</span><br><span class="line">    b.count = b.count + <span class="number">1</span></span><br><span class="line">    b</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">merge</span></span>(b1: <span class="type">AgeBuffer</span>, b2: <span class="type">AgeBuffer</span>): <span class="type">AgeBuffer</span> = &#123;</span><br><span class="line">    b1.sum = b1.sum + b2.sum</span><br><span class="line">    b1.count = b1.count + b2.count</span><br><span class="line">    b1</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">finish</span></span>(buff: <span class="type">AgeBuffer</span>): <span class="type">Double</span> = &#123;</span><br><span class="line">    buff.sum.toDouble/buff.count</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">//DataSet默认额编解码器，用于序列化，固定写法</span></span><br><span class="line">  <span class="comment">//自定义类型就是product自带类型根据类型选择</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">bufferEncoder</span></span>: <span class="type">Encoder</span>[<span class="type">AgeBuffer</span>] = &#123;</span><br><span class="line">    <span class="type">Encoders</span>.product</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">outputEncoder</span></span>: <span class="type">Encoder</span>[<span class="type">Double</span>] = &#123;</span><br><span class="line">    <span class="type">Encoders</span>.scalaDouble</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="2-8-数据的加载和保存"><a href="#2-8-数据的加载和保存" class="headerlink" title="2.8 数据的加载和保存"></a>2.8 数据的加载和保存</h2><h3 id="2-8-1-通用的加载和保存方式"><a href="#2-8-1-通用的加载和保存方式" class="headerlink" title="2.8.1 通用的加载和保存方式"></a>2.8.1 通用的加载和保存方式</h3><p>SparkSQL提供了通用的保存数据和数据加载的方式。这里的通用指的是使用相同的API，根据不同的参数读取和保存不同格式的数据，SparkSQL默认读取和保存的文件格式为parquet</p>
<ol>
<li><p>加载数据</p>
<ul>
<li><p>spark.read.load 是加载数据的通用方法</p>
  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; spark.read.</span><br><span class="line">csv   format   jdbc   json   load   option   options   orc   parquet   schema   table   text   textFile</span><br></pre></td></tr></table></figure></li>
<li><p>如果读取不同格式的数据，可以对不同的数据格式进行设定</p>
  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; spark.read.format(<span class="string">&quot;…&quot;</span>)[.option(<span class="string">&quot;…&quot;</span>)].load(<span class="string">&quot;…&quot;</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>format(“…”)：指定加载的数据类型，包括”csv”、”jdbc”、”json”、”orc”、”parquet”和”textFile”。</li>
<li>load(“…”)：在”csv”、”jdbc”、”json”、”orc”、”parquet”和”textFile”格式下需要传入加载数据的路径。</li>
<li>option(“…”)：在”jdbc”格式下需要传入JDBC相应参数，url、user、password和dbtable</li>
</ul>
</li>
<li><p>前面都是使用read API 先把文件加载到 DataFrame然后再查询，其实，我们也可以直接在文件上进行查询:  文件格式.<code>文件路径</code></p>
  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scala&gt;spark.sql(<span class="string">&quot;select * from json.`/opt/module/data/user.json`&quot;</span>).show</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>保存数据</p>
<ul>
<li><p>df.write.save 是保存数据的通用方法</p>
  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt;df.write.</span><br><span class="line">csv  jdbc   json  orc   parquet textFile… …</span><br></pre></td></tr></table></figure></li>
<li><p>如果保存不同格式的数据，可以对不同的数据格式进行设定</p>
  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scala&gt;df.write.format(<span class="string">&quot;…&quot;</span>)[.option(<span class="string">&quot;…&quot;</span>)].save(<span class="string">&quot;…&quot;</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>format(“…”)：指定保存的数据类型，包括”csv”、”jdbc”、”json”、”orc”、”parquet”和”textFile”。</li>
<li>save (“…”)：在”csv”、”orc”、”parquet”和”textFile”格式下需要传入保存数据的路径。</li>
<li>option(“…”)：在”jdbc”格式下需要传入JDBC相应参数，url、user、password和dbtable</li>
</ul>
</li>
<li><p>保存操作可以使用 SaveMode, 用来指明如何处理数据，使用mode()方法来设置。</p>
</li>
<li><p>有一点很重要: 这些 SaveMode 都是没有加锁的, 也不是原子操作。</p>
</li>
<li><p>SaveMode是一个枚举类，其中的常量包括：</p>
<table>
<thead>
<tr>
<th>Scala/Java</th>
<th>Any</th>
<th>Language</th>
<th>Meaning</th>
</tr>
</thead>
<tbody><tr>
<td>SaveMode.ErrorIfExists(default)</td>
<td>“error”(default)</td>
<td>如果文件已经存在则抛出异常</td>
<td></td>
</tr>
<tr>
<td>SaveMode.Append</td>
<td>“append”</td>
<td>如果文件已经存在则追加</td>
<td></td>
</tr>
<tr>
<td>SaveMode.Overwrite</td>
<td>“overwrite”</td>
<td>如果文件已经存在则覆盖</td>
<td></td>
</tr>
<tr>
<td>SaveMode.Ignore</td>
<td>“ignore”</td>
<td>如果文件已经存在则忽略</td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.write.mode(<span class="string">&quot;append&quot;</span>).json(<span class="string">&quot;/opt/module/data/output&quot;</span>)</span><br></pre></td></tr></table></figure></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody></table>
</li>
</ul>
</li>
</ol>
<h3 id="2-8-2-Parquet"><a href="#2-8-2-Parquet" class="headerlink" title="2.8.2 Parquet"></a>2.8.2 Parquet</h3><p>Spark SQL的默认数据源为Parquet格式。Parquet是一种能够有效存储嵌套数据的列式存储格式。<br>数据源为Parquet文件时，Spark SQL可以方便的执行所有的操作，不需要使用format。修改配置项spark.sql.sources.default，可修改默认数据源格式。</p>
<ol>
<li>加载数据 <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> df = spark.read.load(<span class="string">&quot;examples/src/main/resources/users.parquet&quot;</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; df.show</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
<li>保存数据 <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">var</span> df = spark.read.json(<span class="string">&quot;/opt/module/data/input/people.json&quot;</span>)</span><br><span class="line"><span class="comment">//保存为parquet格式</span></span><br><span class="line">scala&gt; df.write.mode(<span class="string">&quot;append&quot;</span>).save(<span class="string">&quot;/opt/module/data/output&quot;</span>)</span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="2-8-3-JSON"><a href="#2-8-3-JSON" class="headerlink" title="2.8.3 JSON"></a>2.8.3 JSON</h3><p>Spark SQL 能够自动推测JSON数据集的结构，并将它加载为一个Dataset[Row]. 可以通过SparkSession.read.json()去加载JSON 文件。</p>
<blockquote>
<p>注意：Spark读取的JSON文件不是传统的JSON文件，每一行都应该是一个JSON串。格式如下：</p>
</blockquote>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="attr">&quot;name&quot;</span>:<span class="string">&quot;Michael&quot;</span>&#125;</span><br><span class="line">&#123;<span class="attr">&quot;name&quot;</span>:<span class="string">&quot;Andy&quot;</span>, <span class="attr">&quot;age&quot;</span>:<span class="number">30</span>&#125;</span><br><span class="line">[&#123;<span class="attr">&quot;name&quot;</span>:<span class="string">&quot;Justin&quot;</span>, <span class="attr">&quot;age&quot;</span>:<span class="number">19</span>&#125;,&#123;<span class="attr">&quot;name&quot;</span>:<span class="string">&quot;Justin&quot;</span>, <span class="attr">&quot;age&quot;</span>:<span class="number">19</span>&#125;]</span><br></pre></td></tr></table></figure>
<ol>
<li>导入隐式转换 <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> spark.implicits._</span><br></pre></td></tr></table></figure></li>
<li>加载JSON文件 <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> path = <span class="string">&quot;/opt/module/spark-local/people.json&quot;</span></span><br><span class="line"><span class="keyword">val</span> peopleDF = spark.read.json(path)</span><br></pre></td></tr></table></figure></li>
<li>创建临时表 <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">peopleDF.createOrReplaceTempView(<span class="string">&quot;people&quot;</span>)</span><br></pre></td></tr></table></figure></li>
<li>数据查询 <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> teenagerNamesDF = spark.sql(<span class="string">&quot;SELECT name FROM people WHERE age BETWEEN 13 AND 19&quot;</span>)</span><br><span class="line">teenagerNamesDF.show()</span><br><span class="line">+------+</span><br><span class="line">|  name|</span><br><span class="line">+------+</span><br><span class="line">|<span class="type">Justin</span>|</span><br><span class="line">+------+</span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="2-8-4-CSV"><a href="#2-8-4-CSV" class="headerlink" title="2.8.4 CSV"></a>2.8.4 CSV</h3><p>Spark SQL可以配置CSV文件的列表信息，读取CSV文件,CSV文件的第一行设置为数据列</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">spark.read</span><br><span class="line">    .format(<span class="string">&quot;csv&quot;</span>)</span><br><span class="line">    .option(<span class="string">&quot;sep&quot;</span>, <span class="string">&quot;;&quot;</span>)</span><br><span class="line">    .option(<span class="string">&quot;inferSchema&quot;</span>, <span class="string">&quot;true&quot;</span>)</span><br><span class="line">    .option(<span class="string">&quot;header&quot;</span>, <span class="string">&quot;true&quot;</span>)</span><br><span class="line">    .load(<span class="string">&quot;data/user.csv&quot;</span>)</span><br></pre></td></tr></table></figure>

<h3 id="2-8-5-MySQL"><a href="#2-8-5-MySQL" class="headerlink" title="2.8.5 MySQL"></a>2.8.5 MySQL</h3><p>Spark SQL可以通过JDBC从关系型数据库中读取数据的方式创建DataFrame，通过对DataFrame一系列的计算后，还可以将数据再写回关系型数据库中。如果使用spark-shell操作，可在启动shell时指定相关的数据库驱动路径或者将相关的数据库驱动放到spark的类路径下。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">bin/spark-shell </span><br><span class="line">--jars mysql-connector-java-5.1.27-bin.jar</span><br></pre></td></tr></table></figure>
<p>我们这里只演示在Idea中通过JDBC对Mysql进行操作</p>
<ol>
<li><p>导入依赖</p>
 <figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>mysql<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>mysql-connector-java<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>5.1.27<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure></li>
<li><p>读取数据</p>
 <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> conf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">&quot;local[*]&quot;</span>).setAppName(<span class="string">&quot;SparkSQL&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">//创建SparkSession对象</span></span><br><span class="line"><span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span>.builder().config(conf).getOrCreate()</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line"><span class="comment">//方式1：通用的load方法读取</span></span><br><span class="line">spark.read.format(<span class="string">&quot;jdbc&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;url&quot;</span>, <span class="string">&quot;jdbc:mysql://linux1:3306/spark-sql&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;driver&quot;</span>, <span class="string">&quot;com.mysql.jdbc.Driver&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;user&quot;</span>, <span class="string">&quot;root&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;password&quot;</span>, <span class="string">&quot;123123&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;dbtable&quot;</span>, <span class="string">&quot;user&quot;</span>)</span><br><span class="line">  .load().show</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">//方式2:通用的load方法读取 参数另一种形式</span></span><br><span class="line">spark.read.format(<span class="string">&quot;jdbc&quot;</span>)</span><br><span class="line">  .options(<span class="type">Map</span>(<span class="string">&quot;url&quot;</span>-&gt;<span class="string">&quot;jdbc:mysql://linux1:3306/spark-sql?user=root&amp;password=123123&quot;</span>,</span><br><span class="line">    <span class="string">&quot;dbtable&quot;</span>-&gt;<span class="string">&quot;user&quot;</span>,<span class="string">&quot;driver&quot;</span>-&gt;<span class="string">&quot;com.mysql.jdbc.Driver&quot;</span>)).load().show</span><br><span class="line"></span><br><span class="line"><span class="comment">//方式3:使用jdbc方法读取</span></span><br><span class="line"><span class="keyword">val</span> props: <span class="type">Properties</span> = <span class="keyword">new</span> <span class="type">Properties</span>()</span><br><span class="line">props.setProperty(<span class="string">&quot;user&quot;</span>, <span class="string">&quot;root&quot;</span>)</span><br><span class="line">props.setProperty(<span class="string">&quot;password&quot;</span>, <span class="string">&quot;123123&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> df: <span class="type">DataFrame</span> = spark.read.jdbc(<span class="string">&quot;jdbc:mysql://linux1:3306/spark-sql&quot;</span>, <span class="string">&quot;user&quot;</span>, props)</span><br><span class="line">df.show</span><br><span class="line"></span><br><span class="line"><span class="comment">//释放资源</span></span><br><span class="line">spark.stop()    </span><br></pre></td></tr></table></figure></li>
<li><p>写入数据</p>
 <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">User2</span>(<span class="params">name: <span class="type">String</span>, age: <span class="type">Long</span></span>)</span></span><br><span class="line">。。。</span><br><span class="line"><span class="keyword">val</span> conf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">&quot;local[*]&quot;</span>).setAppName(<span class="string">&quot;SparkSQL&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">//创建SparkSession对象</span></span><br><span class="line"><span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span>.builder().config(conf).getOrCreate()</span><br><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> rdd: <span class="type">RDD</span>[<span class="type">User2</span>] = spark.sparkContext.makeRDD(<span class="type">List</span>(<span class="type">User2</span>(<span class="string">&quot;lisi&quot;</span>, <span class="number">20</span>), <span class="type">User2</span>(<span class="string">&quot;zs&quot;</span>, <span class="number">30</span>)))</span><br><span class="line"><span class="keyword">val</span> ds: <span class="type">Dataset</span>[<span class="type">User2</span>] = rdd.toDS</span><br><span class="line"><span class="comment">//方式1：通用的方式  format指定写出类型</span></span><br><span class="line"> </span><br><span class="line"><span class="comment">//方式2：通过jdbc方法</span></span><br><span class="line"><span class="keyword">val</span> props: <span class="type">Properties</span> = <span class="keyword">new</span> <span class="type">Properties</span>()</span><br><span class="line">props.setProperty(<span class="string">&quot;user&quot;</span>, <span class="string">&quot;root&quot;</span>)</span><br><span class="line">props.setProperty(<span class="string">&quot;password&quot;</span>, <span class="string">&quot;123123&quot;</span>)</span><br><span class="line">ds.write.mode(<span class="type">SaveMode</span>.<span class="type">Append</span>).jdbc(<span class="string">&quot;jdbc:mysql://linux1:3306/spark-sql&quot;</span>, <span class="string">&quot;user&quot;</span>, props)</span><br><span class="line"></span><br><span class="line"><span class="comment">//释放资源</span></span><br><span class="line">spark.stop()</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="2-8-6-Hive"><a href="#2-8-6-Hive" class="headerlink" title="2.8.6 Hive"></a>2.8.6 Hive</h3><ul>
<li>Apache Hive 是 Hadoop 上的 SQL 引擎，Spark SQL编译时可以包含 Hive 支持，也可以不包含。包含 Hive 支持的 Spark SQL 可以支持 Hive 表访问、UDF (用户自定义函数)以及 Hive 查询语言(HiveQL/HQL)等。需要强调的一点是，如果要在 Spark SQL 中包含Hive 的库，并不需要事先安装 Hive。一般来说，最好还是在编译Spark SQL时引入Hive支持，这样就可以使用这些特性了。如果你下载的是二进制版本的 Spark，它应该已经在编译时添加了 Hive 支持。</li>
<li>若要把 Spark SQL 连接到一个部署好的 Hive 上，你必须把 hive-site.xml 复制到 Spark的配置文件目录中($SPARK_HOME/conf)。即使没有部署好 Hive，Spark SQL 也可以运行。 需要注意的是，如果你没有部署好Hive，Spark SQL 会在当前的工作目录中创建出自己的 Hive 元数据仓库，叫作 metastore_db。此外，如果你尝试使用 HiveQL 中的 CREATE TABLE (并非 CREATE EXTERNAL TABLE)语句来创建表，这些表会被放在你默认的文件系统中的 /user/hive/warehouse 目录中(如果你的 classpath 中有配好的 hdfs-site.xml，默认的文件系统就是 HDFS，否则就是本地文件系统)。</li>
<li>spark-shell默认是Hive支持的；代码中是默认不支持的，需要手动指定（加一个参数即可）。</li>
</ul>
<ol>
<li><p>内嵌的HIVE<br> 如果使用 Spark 内嵌的 Hive, 则什么都不用做, 直接使用即可.<br> Hive 的元数据存储在 derby 中, 默认仓库地址:$SPARK_HOME/spark-warehouse</p>
 <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; spark.sql(<span class="string">&quot;show tables&quot;</span>).show</span><br><span class="line">。。。</span><br><span class="line">+--------+---------+-----------+</span><br><span class="line">|database|tableName|isTemporary|</span><br><span class="line">+--------+---------+-----------+</span><br><span class="line">+--------+---------+-----------+</span><br><span class="line"></span><br><span class="line">scala&gt; spark.sql(<span class="string">&quot;create table aa(id int)&quot;</span>)</span><br><span class="line"></span><br><span class="line">。。。</span><br><span class="line"></span><br><span class="line">scala&gt; spark.sql(<span class="string">&quot;show tables&quot;</span>).show</span><br><span class="line">+--------+---------+-----------+</span><br><span class="line">|database|tableName|isTemporary|</span><br><span class="line">+--------+---------+-----------+</span><br><span class="line">| <span class="keyword">default</span>|       aa|      <span class="literal">false</span>|</span><br><span class="line">+--------+---------+-----------+</span><br></pre></td></tr></table></figure>
<p> 向表加载本地数据</p>
 <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; spark.sql(<span class="string">&quot;load data local inpath &#x27;input/ids.txt&#x27; into table aa&quot;</span>)</span><br><span class="line"></span><br><span class="line">。。。</span><br><span class="line"></span><br><span class="line">scala&gt; spark.sql(<span class="string">&quot;select * from aa&quot;</span>).show</span><br><span class="line">+---+</span><br><span class="line">| id|</span><br><span class="line">+---+</span><br><span class="line">|  <span class="number">1</span>|</span><br><span class="line">|  <span class="number">2</span>|</span><br><span class="line">|  <span class="number">3</span>|</span><br><span class="line">|  <span class="number">4</span>|</span><br><span class="line">+---+</span><br></pre></td></tr></table></figure>
<p> 在实际使用中, 几乎没有任何人会使用内置的 Hive</p>
</li>
<li><p>外部的HIVE<br> 如果想连接外部已经部署好的Hive，需要通过以下几个步骤：</p>
<ul>
<li>Spark要接管Hive需要把hive-site.xml拷贝到conf/目录下</li>
<li>把Mysql的驱动copy到jars/目录下</li>
<li>如果访问不到hdfs，则需要把core-site.xml和hdfs-site.xml拷贝到conf/目录下</li>
<li>重启spark-shell  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; spark.sql(<span class="string">&quot;show tables&quot;</span>).show</span><br><span class="line"><span class="number">20</span>/<span class="number">04</span>/<span class="number">25</span> <span class="number">22</span>:<span class="number">05</span>:<span class="number">14</span> <span class="type">WARN</span> <span class="type">ObjectStore</span>: <span class="type">Failed</span> to get database global_temp, returning <span class="type">NoSuchObjectException</span></span><br><span class="line">+--------+--------------------+-----------+</span><br><span class="line">|database|           tableName|isTemporary|</span><br><span class="line">+--------+--------------------+-----------+</span><br><span class="line">| <span class="keyword">default</span>|                 emp|      <span class="literal">false</span>|</span><br><span class="line">| <span class="keyword">default</span>|hive_hbase_emp_table|      <span class="literal">false</span>|</span><br><span class="line">| <span class="keyword">default</span>| relevance_hbase_emp|      <span class="literal">false</span>|</span><br><span class="line">| <span class="keyword">default</span>|          staff_hive|      <span class="literal">false</span>|</span><br><span class="line">| <span class="keyword">default</span>|                 ttt|      <span class="literal">false</span>|</span><br><span class="line">| <span class="keyword">default</span>|   user_visit_action|      <span class="literal">false</span>|</span><br><span class="line">+--------+--------------------+-----------+</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>运行Spark SQL CLI<br> Spark SQL CLI可以很方便的在本地运行Hive元数据服务以及从命令行执行查询任务。在Spark目录下执行如下命令启动Spark SQL CLI，直接执行SQL语句，类似一Hive窗口bin/spark-sql</p>
</li>
<li><p>运行Spark beeline<br> Spark Thrift Server是Spark社区基于HiveServer2实现的一个Thrift服务。旨在无缝兼容HiveServer2。因为Spark Thrift Server的接口和协议都和HiveServer2完全一致，因此我们部署好Spark Thrift Server后，可以直接使用hive的beeline访问Spark Thrift Server执行相关语句。Spark Thrift Server的目的也只是取代HiveServer2，因此它依旧可以和Hive Metastore进行交互，获取到hive的元数据。<br> 如果想连接Thrift Server，需要通过以下几个步骤：</p>
<ul>
<li>Spark要接管Hive需要把hive-site.xml拷贝到conf/目录下</li>
<li>把Mysql的驱动copy到jars/目录下</li>
<li>如果访问不到hdfs，则需要把core-site.xml和hdfs-site.xml拷贝到conf/目录下</li>
<li>启动Thrift Server  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sbin/start-thriftserver.sh</span><br></pre></td></tr></table></figure></li>
<li>使用beeline连接Thrift Server  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/beeline -u jdbc:hive2://linux1:10000 -n root</span><br></pre></td></tr></table></figure>
  <img src="https://s2.loli.net/2021/12/13/8Brzyl7RJUVqhba.jpg"></li>
</ul>
<p> 如果连接有问题，可以尝试在hive-site.xml文件中增加如下内容：</p>
 <figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.server2.enable.doAs<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li>
<li><p>代码操作Hive</p>
<ol>
<li>导入依赖 <figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-hive_2.12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.0.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hive<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hive-exec<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.2.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>mysql<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>mysql-connector-java<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>5.1.27<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure></li>
</ol>
</li>
<li><p>将hive-site.xml文件拷贝到项目的resources目录中，代码实现</p>
 <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//创建SparkSession</span></span><br><span class="line"><span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span></span><br><span class="line">  .builder()</span><br><span class="line">  .enableHiveSupport()</span><br><span class="line">  .master(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">  .appName(<span class="string">&quot;sql&quot;</span>)</span><br><span class="line">  .getOrCreate()</span><br></pre></td></tr></table></figure>
<blockquote>
<p>注意：在开发工具中创建数据库默认是在本地仓库，通过参数修改数据库仓库的地址: </p>
   <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">config(<span class="string">&quot;spark.sql.warehouse.dir&quot;</span>, <span class="string">&quot;hdfs://linux1:8020/user/hive/warehouse&quot;</span>)</span><br></pre></td></tr></table></figure>
<p> 如果在执行操作时，出现如下错误：<br> <img src="https://s2.loli.net/2021/12/13/ZmY8fq3BuMUjN16.jpg"><br> 可以代码最前面增加如下代码解决(此处的root改为hadoop用户名称)：</p>
 <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">System</span>.setProperty(<span class="string">&quot;HADOOP_USER_NAME&quot;</span>, <span class="string">&quot;root&quot;</span>)</span><br></pre></td></tr></table></figure></blockquote>
</li>
</ol>
<h1 id="第3章-SparkSQL项目实战"><a href="#第3章-SparkSQL项目实战" class="headerlink" title="第3章 SparkSQL项目实战"></a>第3章 SparkSQL项目实战</h1><h3 id="3-1-数据准备"><a href="#3-1-数据准备" class="headerlink" title="3.1 数据准备"></a>3.1 数据准备</h3><p>我们这次 Spark-sql 操作中所有的数据均来自 Hive，首先在 Hive 中创建表,，并导入数据。<br>一共有3张表： 1张用户行为表，1张城市表，1 张产品表</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 用户行为表</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> `user_visit_action`(</span><br><span class="line">  `<span class="type">date</span>` string,	</span><br><span class="line">  `user_id` <span class="type">bigint</span>,</span><br><span class="line">  `session_id` string,</span><br><span class="line">  `page_id` <span class="type">bigint</span>,</span><br><span class="line">  `action_time` string,</span><br><span class="line">  `search_keyword` string,</span><br><span class="line">  `click_category_id` <span class="type">bigint</span>,</span><br><span class="line">  `click_product_id` <span class="type">bigint</span>,</span><br><span class="line">  `order_category_ids` string,</span><br><span class="line">  `order_product_ids` string,</span><br><span class="line">  `pay_category_ids` string,</span><br><span class="line">  `pay_product_ids` string,</span><br><span class="line">  `city_id` <span class="type">bigint</span>)</span><br><span class="line"><span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span>;</span><br><span class="line">load data <span class="keyword">local</span> inpath <span class="string">&#x27;input/user_visit_action.txt&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span> user_visit_action;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 城市表</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> `product_info`(</span><br><span class="line">  `product_id` <span class="type">bigint</span>,</span><br><span class="line">  `product_name` string,</span><br><span class="line">  `extend_info` string)</span><br><span class="line"><span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span>;</span><br><span class="line">load data <span class="keyword">local</span> inpath <span class="string">&#x27;input/product_info.txt&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span> product_info;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 产品表</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> `city_info`(</span><br><span class="line">  `city_id` <span class="type">bigint</span>,</span><br><span class="line">  `city_name` string,</span><br><span class="line">  `area` string)</span><br><span class="line"><span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span>;</span><br><span class="line">load data <span class="keyword">local</span> inpath <span class="string">&#x27;input/city_info.txt&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span> city_info;</span><br></pre></td></tr></table></figure>

<h2 id="3-2-需求：各区域热门商品-Top3"><a href="#3-2-需求：各区域热门商品-Top3" class="headerlink" title="3.2 需求：各区域热门商品 Top3"></a>3.2 需求：各区域热门商品 Top3</h2><h3 id="3-2-1-需求简介"><a href="#3-2-1-需求简介" class="headerlink" title="3.2.1 需求简介"></a>3.2.1 需求简介</h3><p> 这里的热门商品是从点击量的维度来看的，计算各个区域前三大热门商品，并备注上每个商品在主要城市中的分布比例，超过两个城市用其他显示。例如：<br>|地区        |商品名称      |点击次数      |城市备注       |<br>|———|———–|———–|————–|<br>|华北        |商品A         |100000      |北京21.2%，天津13.2%，其他65.6%    |<br>|华北        |商品P         |80200      |北京63.0%，太原10%，其他27.0%    |<br>|华北        |商品M         |40000         |北京63.0%，太原10%，其他27.0%    |<br>|东北        |商品J         |92000         |大连28%，辽宁17.0%，其他 55.0%   |</p>
<h3 id="3-2-2-需求分析"><a href="#3-2-2-需求分析" class="headerlink" title="3.2.2 需求分析"></a>3.2.2 需求分析</h3><ul>
<li>查询出来所有的点击记录，并与 city_info 表连接，得到每个城市所在的地区，与 Product_info 表连接得到产品名称</li>
<li>按照地区和商品 id 分组，统计出每个商品在每个地区的总点击次数</li>
<li>每个地区内按照点击次数降序排列</li>
<li>只取前三名</li>
<li>城市备注需要自定义 UDAF 函数</li>
</ul>
<h3 id="3-2-3-功能实现"><a href="#3-2-3-功能实现" class="headerlink" title="3.2.3 功能实现"></a>3.2.3 功能实现</h3><ul>
<li>连接三张表的数据，获取完整的数据（只有点击）</li>
<li>将数据根据地区，商品名称分组</li>
<li>统计商品点击次数总和,取Top3</li>
<li>实现自定义聚合函数显示备注<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.bigdata.spark.sql</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.expressions.<span class="type">Aggregator</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.&#123;<span class="type">Encoder</span>, <span class="type">Encoders</span>, <span class="type">SparkSession</span>, functions&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> scala.collection.mutable</span><br><span class="line"><span class="keyword">import</span> scala.collection.mutable.<span class="type">ListBuffer</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SparkSQL_Hive</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="type">System</span>.setProperty(<span class="string">&quot;HADOOP_USER_NAME&quot;</span>, <span class="string">&quot;atguigu&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">//by_sql</span></span><br><span class="line">        by_sql_split</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">by_sql_split</span> </span>= &#123;</span><br><span class="line">        <span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span></span><br><span class="line">            .builder()</span><br><span class="line">            .enableHiveSupport()</span><br><span class="line">            .master(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">            .appName(<span class="string">&quot;sql&quot;</span>)</span><br><span class="line">            .config(<span class="string">&quot;spark.sql.warehouse.dir&quot;</span>, <span class="string">&quot;hdfs://mycluster/user/hive/warehouse&quot;</span>)</span><br><span class="line">            .getOrCreate()</span><br><span class="line"></span><br><span class="line">        spark.sql(<span class="string">&quot;use spark_sql&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">//1. 关联三张表</span></span><br><span class="line">        spark.sql(</span><br><span class="line">            <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">              |select ac.*,</span></span><br><span class="line"><span class="string">              |       pi.product_name,</span></span><br><span class="line"><span class="string">              |       ci.area,</span></span><br><span class="line"><span class="string">              |       ci.city_name</span></span><br><span class="line"><span class="string">              |from user_visit_action ac</span></span><br><span class="line"><span class="string">              |         left join city_info ci on ac.city_id = ci.city_id</span></span><br><span class="line"><span class="string">              |         left join product_info pi on ac.click_product_id = pi.product_id</span></span><br><span class="line"><span class="string">              |where ac.click_product_id != -1</span></span><br><span class="line"><span class="string">              |&quot;&quot;&quot;</span>.stripMargin).createTempView(<span class="string">&quot;t1&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">//2. 商品区域作为整体，统计点击数量（（区域，商品），sum）</span></span><br><span class="line">        spark.udf.register(<span class="string">&quot;cityRemark&quot;</span>, functions.udaf(<span class="keyword">new</span> <span class="type">CityRemarkUDAF</span>()))</span><br><span class="line">        spark.sql(</span><br><span class="line">            <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">              |select area,</span></span><br><span class="line"><span class="string">              |       product_name,</span></span><br><span class="line"><span class="string">              |       count(*) as clickCount,</span></span><br><span class="line"><span class="string">              |       cityRemark(city_name)</span></span><br><span class="line"><span class="string">              |from t1</span></span><br><span class="line"><span class="string">              |group by area, product_name</span></span><br><span class="line"><span class="string">              |&quot;&quot;&quot;</span>.stripMargin).createTempView(<span class="string">&quot;t2&quot;</span>)</span><br><span class="line">        <span class="comment">//3. 将统计结果进行结构的转换（区域，（商品，sum））</span></span><br><span class="line">        <span class="comment">//4. 按区域进行分组（区域，Iter[（商品，sum）,（商品，sum）,（商品，sum）]）</span></span><br><span class="line">        spark.sql(</span><br><span class="line">            <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">              |select *,</span></span><br><span class="line"><span class="string">              |       rank() over (partition by area order by clickCount desc ) as rank</span></span><br><span class="line"><span class="string">              |from t2</span></span><br><span class="line"><span class="string">              |&quot;&quot;&quot;</span>.stripMargin).createTempView(<span class="string">&quot;t3&quot;</span>)</span><br><span class="line">        <span class="comment">//5. 分组后的数据根据点击量排序</span></span><br><span class="line">        spark.sql(</span><br><span class="line">            <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">              |select * from t3 where rank &lt;=3</span></span><br><span class="line"><span class="string">              |&quot;&quot;&quot;</span>.stripMargin)</span><br><span class="line">            .show(<span class="literal">false</span>)</span><br><span class="line"></span><br><span class="line">        spark.stop()</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">CityRemarkBuff</span>(<span class="params">var total: <span class="type">Long</span>, cityMap: mutable.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">Long</span>]</span>)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="class"><span class="keyword">class</span> <span class="title">CityRemarkUDAF</span> <span class="keyword">extends</span> <span class="title">Aggregator</span>[<span class="type">String</span>, <span class="type">CityRemarkBuff</span>, <span class="type">String</span>] </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">zero</span></span>: <span class="type">CityRemarkBuff</span> = &#123;</span><br><span class="line">            <span class="type">CityRemarkBuff</span>(<span class="number">0</span>L, mutable.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">Long</span>]())</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">reduce</span></span>(buff: <span class="type">CityRemarkBuff</span>, city: <span class="type">String</span>): <span class="type">CityRemarkBuff</span> = &#123;</span><br><span class="line">            buff.total += <span class="number">1</span>L</span><br><span class="line">            buff.cityMap.update(city, buff.cityMap.getOrElse(city, <span class="number">0</span>L) + <span class="number">1</span>L)</span><br><span class="line">            buff</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">merge</span></span>(b1: <span class="type">CityRemarkBuff</span>, b2: <span class="type">CityRemarkBuff</span>): <span class="type">CityRemarkBuff</span> = &#123;</span><br><span class="line">            b1.total += b2.total</span><br><span class="line">            b2.cityMap.foreach &#123;</span><br><span class="line">                <span class="keyword">case</span> (city, count) =&gt; &#123;</span><br><span class="line">                    b1.cityMap.update(city, b1.cityMap.getOrElse(city, <span class="number">0</span>L) + count)</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            b1</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">finish</span></span>(reduction: <span class="type">CityRemarkBuff</span>): <span class="type">String</span> = &#123;</span><br><span class="line">            <span class="keyword">val</span> list: <span class="type">ListBuffer</span>[<span class="type">String</span>] = <span class="type">ListBuffer</span>[<span class="type">String</span>]()</span><br><span class="line">            <span class="keyword">val</span> total: <span class="type">Long</span> = reduction.total</span><br><span class="line">            <span class="keyword">val</span> sortCity: <span class="type">List</span>[(<span class="type">String</span>, <span class="type">Long</span>)] = reduction.cityMap.toList.sortBy(_._2)</span><br><span class="line">            <span class="keyword">val</span> top2: <span class="type">List</span>[(<span class="type">String</span>, <span class="type">Long</span>)] = sortCity.take(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">var</span> remain =<span class="number">100</span>L</span><br><span class="line"></span><br><span class="line">            top2.foreach &#123;</span><br><span class="line">                <span class="keyword">case</span> (city, count) =&gt; &#123;</span><br><span class="line">                    <span class="keyword">val</span> percent: <span class="type">Long</span> = count * <span class="number">100</span> / total</span><br><span class="line">                    remain -= percent</span><br><span class="line">                    list.append(<span class="string">s&quot;<span class="subst">$&#123;city&#125;</span> <span class="subst">$percent</span>%&quot;</span>)</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span> (sortCity.length&gt;<span class="number">2</span>) &#123;</span><br><span class="line">                list.append(<span class="string">s&quot;其他 <span class="subst">$&#123;remain&#125;</span>%&quot;</span>)</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            list.mkString(<span class="string">&quot;, &quot;</span>)</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">bufferEncoder</span></span>: <span class="type">Encoder</span>[<span class="type">CityRemarkBuff</span>] = <span class="type">Encoders</span>.product</span><br><span class="line"></span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">outputEncoder</span></span>: <span class="type">Encoder</span>[<span class="type">String</span>] = <span class="type">Encoders</span>.<span class="type">STRING</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">by_sql</span> </span>= &#123;</span><br><span class="line">        <span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span></span><br><span class="line">            .builder()</span><br><span class="line">            .enableHiveSupport()</span><br><span class="line">            .master(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">            .appName(<span class="string">&quot;sql&quot;</span>)</span><br><span class="line">            .config(<span class="string">&quot;spark.sql.warehouse.dir&quot;</span>, <span class="string">&quot;hdfs://mycluster/user/hive/warehouse&quot;</span>)</span><br><span class="line">            .getOrCreate()</span><br><span class="line"></span><br><span class="line">        spark.sql(<span class="string">&quot;use spark_sql&quot;</span>)</span><br><span class="line"></span><br><span class="line">        spark.sql(</span><br><span class="line">            <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">              |select *</span></span><br><span class="line"><span class="string">              |from (</span></span><br><span class="line"><span class="string">              |         select *,</span></span><br><span class="line"><span class="string">              |                rank() over (partition by area order by clickCount desc ) as rank</span></span><br><span class="line"><span class="string">              |         from (</span></span><br><span class="line"><span class="string">              |                  select area,</span></span><br><span class="line"><span class="string">              |                         product_name,</span></span><br><span class="line"><span class="string">              |                         count(*) as clickCount</span></span><br><span class="line"><span class="string">              |                  from (</span></span><br><span class="line"><span class="string">              |                           select ac.*,</span></span><br><span class="line"><span class="string">              |                                  pi.product_name,</span></span><br><span class="line"><span class="string">              |                                  ci.area,</span></span><br><span class="line"><span class="string">              |                                  ci.city_name</span></span><br><span class="line"><span class="string">              |                           from user_visit_action ac</span></span><br><span class="line"><span class="string">              |                                    left join city_info ci on ac.city_id = ci.city_id</span></span><br><span class="line"><span class="string">              |                                    left join product_info pi on ac.click_product_id = pi.product_id</span></span><br><span class="line"><span class="string">              |                           where ac.click_product_id != -1</span></span><br><span class="line"><span class="string">              |                       ) t1</span></span><br><span class="line"><span class="string">              |                  group by area, product_name</span></span><br><span class="line"><span class="string">              |              ) t2</span></span><br><span class="line"><span class="string">              |     ) t3</span></span><br><span class="line"><span class="string">              |where rank &lt;= 3</span></span><br><span class="line"><span class="string">              |&quot;&quot;&quot;</span>.stripMargin).show()</span><br><span class="line"></span><br><span class="line">        spark.stop()</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">by_rdd</span> </span>= &#123;</span><br><span class="line">        <span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span></span><br><span class="line">            .builder()</span><br><span class="line">            .enableHiveSupport()</span><br><span class="line">            .master(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">            .appName(<span class="string">&quot;sql&quot;</span>)</span><br><span class="line">            .config(<span class="string">&quot;spark.sql.warehouse.dir&quot;</span>, <span class="string">&quot;hdfs://mycluster/user/hive/warehouse&quot;</span>)</span><br><span class="line">            .getOrCreate()</span><br><span class="line"></span><br><span class="line">        spark.sql(<span class="string">&quot;use spark_sql&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">//1. 关联三张表</span></span><br><span class="line">        <span class="comment">//2. 商品区域作为整体，统计点击数量（（区域，商品），sum）</span></span><br><span class="line">        <span class="comment">//3. 将统计结果进行结构的转换（区域，（商品，sum））</span></span><br><span class="line">        <span class="comment">//4. 按区域进行分组（区域，Iter[（商品，sum）,（商品，sum）,（商品，sum）]）</span></span><br><span class="line">        <span class="comment">//5. 分组后的数据根据点击量排序</span></span><br><span class="line">        spark.stop()</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 导入数据</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">import_data</span> </span>= &#123;</span><br><span class="line">        <span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span></span><br><span class="line">            .builder()</span><br><span class="line">            .enableHiveSupport()</span><br><span class="line">            .master(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">            .appName(<span class="string">&quot;sql&quot;</span>)</span><br><span class="line">            .config(<span class="string">&quot;spark.sql.warehouse.dir&quot;</span>, <span class="string">&quot;hdfs://mycluster/user/hive/warehouse&quot;</span>)</span><br><span class="line">            .getOrCreate()</span><br><span class="line"></span><br><span class="line">        spark.sql(<span class="string">&quot;use spark_sql&quot;</span>)</span><br><span class="line"></span><br><span class="line">        spark.sql(</span><br><span class="line">            <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">              |CREATE TABLE `user_visit_action`(</span></span><br><span class="line"><span class="string">              |  `date` string,</span></span><br><span class="line"><span class="string">              |  `user_id` bigint,</span></span><br><span class="line"><span class="string">              |  `session_id` string,</span></span><br><span class="line"><span class="string">              |  `page_id` bigint,</span></span><br><span class="line"><span class="string">              |  `action_time` string,</span></span><br><span class="line"><span class="string">              |  `search_keyword` string,</span></span><br><span class="line"><span class="string">              |  `click_category_id` bigint,</span></span><br><span class="line"><span class="string">              |  `click_product_id` bigint,</span></span><br><span class="line"><span class="string">              |  `order_category_ids` string,</span></span><br><span class="line"><span class="string">              |  `order_product_ids` string,</span></span><br><span class="line"><span class="string">              |  `pay_category_ids` string,</span></span><br><span class="line"><span class="string">              |  `pay_product_ids` string,</span></span><br><span class="line"><span class="string">              |  `city_id` bigint)</span></span><br><span class="line"><span class="string">              |row format delimited fields terminated by &#x27;\t&#x27;;</span></span><br><span class="line"><span class="string">              |&quot;&quot;&quot;</span>.stripMargin)</span><br><span class="line">        spark.sql(</span><br><span class="line">            <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">              |CREATE TABLE `city_info`(</span></span><br><span class="line"><span class="string">              |  `city_id` bigint,</span></span><br><span class="line"><span class="string">              |  `city_name` string,</span></span><br><span class="line"><span class="string">              |  `area` string)</span></span><br><span class="line"><span class="string">              |row format delimited fields terminated by &#x27;\t&#x27;;</span></span><br><span class="line"><span class="string">              |&quot;&quot;&quot;</span>.stripMargin)</span><br><span class="line">        spark.sql(</span><br><span class="line">            <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">              |CREATE TABLE `product_info`(</span></span><br><span class="line"><span class="string">              |  `product_id` bigint,</span></span><br><span class="line"><span class="string">              |  `product_name` string,</span></span><br><span class="line"><span class="string">              |  `extend_info` string)</span></span><br><span class="line"><span class="string">              |row format delimited fields terminated by &#x27;\t&#x27;;</span></span><br><span class="line"><span class="string">              |</span></span><br><span class="line"><span class="string">              |&quot;&quot;&quot;</span>.stripMargin)</span><br><span class="line">        spark.sql(<span class="string">&quot;load data local inpath &#x27;data/user_visit_action.txt&#x27; into table user_visit_action;&quot;</span>)</span><br><span class="line">        spark.sql(<span class="string">&quot;load data local inpath &#x27;data/city_info.txt&#x27; into table city_info;&quot;</span>)</span><br><span class="line">        spark.sql(<span class="string">&quot;load data local inpath &#x27;data/product_info.txt&#x27; into table product_info;&quot;</span>)</span><br><span class="line">        spark.sql(<span class="string">&quot;show tables&quot;</span>).show()</span><br><span class="line">        spark.sql(<span class="string">&quot;select * from  city_info&quot;</span>).show()</span><br><span class="line"></span><br><span class="line">        spark.stop()</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">init</span> </span>= &#123;</span><br><span class="line">        <span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span></span><br><span class="line">            .builder()</span><br><span class="line">            .enableHiveSupport()</span><br><span class="line">            .master(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">            .appName(<span class="string">&quot;sql&quot;</span>)</span><br><span class="line">            .getOrCreate()</span><br><span class="line"></span><br><span class="line">        spark.stop()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul>
 
      <!-- reward -->
      
      <div id="reword-out">
        <div id="reward-btn">
          打赏
        </div>
      </div>
      
    </div>
    

    <!-- copyright -->
    
    <div class="declare">
      <ul class="post-copyright">
        <li>
          <i class="ri-copyright-line"></i>
          <strong>版权声明： </strong>
          
          本博客所有文章除特别声明外，著作权归作者所有。转载请注明出处！
          
        </li>
      </ul>
    </div>
    
    <footer class="article-footer">
       
<div class="share-btn">
      <span class="share-sns share-outer">
        <i class="ri-share-forward-line"></i>
        分享
      </span>
      <div class="share-wrap">
        <i class="arrow"></i>
        <div class="share-icons">
          
          <a class="weibo share-sns" href="javascript:;" data-type="weibo">
            <i class="ri-weibo-fill"></i>
          </a>
          <a class="weixin share-sns wxFab" href="javascript:;" data-type="weixin">
            <i class="ri-wechat-fill"></i>
          </a>
          <a class="qq share-sns" href="javascript:;" data-type="qq">
            <i class="ri-qq-fill"></i>
          </a>
          <a class="douban share-sns" href="javascript:;" data-type="douban">
            <i class="ri-douban-line"></i>
          </a>
          <!-- <a class="qzone share-sns" href="javascript:;" data-type="qzone">
            <i class="icon icon-qzone"></i>
          </a> -->
          
          <a class="facebook share-sns" href="javascript:;" data-type="facebook">
            <i class="ri-facebook-circle-fill"></i>
          </a>
          <a class="twitter share-sns" href="javascript:;" data-type="twitter">
            <i class="ri-twitter-fill"></i>
          </a>
          <a class="google share-sns" href="javascript:;" data-type="google">
            <i class="ri-google-fill"></i>
          </a>
        </div>
      </div>
</div>

<div class="wx-share-modal">
    <a class="modal-close" href="javascript:;"><i class="ri-close-circle-line"></i></a>
    <p>扫一扫，分享到微信</p>
    <div class="wx-qrcode">
      <img src="//api.qrserver.com/v1/create-qr-code/?size=150x150&data=https://anzhen-tech.gitee.io/2021/12/13/SparkSQL/" alt="微信分享二维码">
    </div>
</div>

<div id="share-mask"></div>  
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Spark/" rel="tag">Spark</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/SparkSQL/" rel="tag">SparkSQL</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/" rel="tag">大数据</a></li></ul>

    </footer>
  </div>

   
  <nav class="article-nav">
    
      <a href="/2022/04/24/%E4%BB%80%E4%B9%88%E6%98%AF%E5%BE%AE%E6%9C%8D%E5%8A%A1/" class="article-nav-link">
        <strong class="article-nav-caption">上一篇</strong>
        <div class="article-nav-title">
          
            什么是微服务
          
        </div>
      </a>
    
    
      <a href="/2021/12/11/SparkCore/" class="article-nav-link">
        <strong class="article-nav-caption">下一篇</strong>
        <div class="article-nav-title">SparkCore</div>
      </a>
    
  </nav>

   
<!-- valine评论 -->
<div id="vcomments-box">
  <div id="vcomments"></div>
</div>
<script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/valine@1.4.14/dist/Valine.min.js"></script>
<script>
  new Valine({
    el: "#vcomments",
    app_id: "XCKHv09pYxF5EmF2ezNgFfLS-gzGzoHsz",
    app_key: "gyCHBp787fNNfXDiHGIcj7Am",
    path: window.location.pathname,
    avatar: "monsterid",
    placeholder: "给我的文章加点评论吧~",
    recordIP: true,
  });
  const infoEle = document.querySelector("#vcomments .info");
  if (infoEle && infoEle.childNodes && infoEle.childNodes.length > 0) {
    infoEle.childNodes.forEach(function (item) {
      item.parentNode.removeChild(item);
    });
  }
</script>
<style>
  #vcomments-box {
    padding: 5px 30px;
  }

  @media screen and (max-width: 800px) {
    #vcomments-box {
      padding: 5px 0px;
    }
  }

  #vcomments-box #vcomments {
    background-color: #fff;
  }

  .v .vlist .vcard .vh {
    padding-right: 20px;
  }

  .v .vlist .vcard {
    padding-left: 10px;
  }
</style>

 
   
     
</article>

</section>
      <footer class="footer">
  <div class="outer">
    <ul>
      <li>
        Copyrights &copy;
        2019-2022
        <i class="ri-heart-fill heart_icon"></i> Anzhen
      </li>
    </ul>
    <ul>
      <li>
        
      </li>
    </ul>
    <ul>
      <li>
        
        
        <span>
  <span><i class="ri-user-3-fill"></i>访问人数:<span id="busuanzi_value_site_uv"></span></span>
  <span class="division">|</span>
  <span><i class="ri-eye-fill"></i>浏览次数:<span id="busuanzi_value_page_pv"></span></span>
</span>
        
      </li>
    </ul>
    <ul>
      
    </ul>
    <ul>
      
    </ul>
    <ul>
      <li>
        <!-- cnzz统计 -->
        
        <script type="text/javascript" src=''></script>
        
      </li>
    </ul>
  </div>
</footer>    
    </main>
    <div class="float_btns">
      <div class="totop" id="totop">
  <i class="ri-arrow-up-line"></i>
</div>

<div class="todark" id="todark">
  <i class="ri-moon-line"></i>
</div>

    </div>
    <aside class="sidebar on">
      <button class="navbar-toggle"></button>
<nav class="navbar">
  
  <div class="logo">
    <a href="/"><img src="/images/ayer-side.svg" alt="anzhen.tech"></a>
  </div>
  
  <ul class="nav nav-main">
    
    <li class="nav-item">
      <a class="nav-item-link" href="/">主页</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/archives">归档</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/categories">分类</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags">标签</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags/HDFS">HDFS</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags/Yarn">Yarn</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags/MR">MR</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags/Hive">Hive</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags/%E6%95%B0%E6%8D%AE%E9%87%87%E9%9B%86">数据采集</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags/HBase">HBase</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags/Kafka">Kafka</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags/Spark">Spark</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags/Flink">Flink</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags/MySQL">MySQL</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags/Java">Java</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags/interview">面试宝典</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/2019/11/07/about-me">关于我</a>
    </li>
    
  </ul>
</nav>
<nav class="navbar navbar-bottom">
  <ul class="nav">
    <li class="nav-item">
      
      <a class="nav-item-link nav-item-search"  title="搜索">
        <i class="ri-search-line"></i>
      </a>
      
      
      <a class="nav-item-link" target="_blank" href="/atom.xml" title="RSS Feed">
        <i class="ri-rss-line"></i>
      </a>
      
    </li>
  </ul>
</nav>
<div class="search-form-wrap">
  <div class="local-search local-search-plugin">
  <input type="search" id="local-search-input" class="local-search-input" placeholder="Search...">
  <div id="local-search-result" class="local-search-result"></div>
</div>
</div>
    </aside>
    <div id="mask"></div>

<!-- #reward -->
<div id="reward">
  <span class="close"><i class="ri-close-line"></i></span>
  <p class="reward-p"><i class="ri-cup-line"></i>请我喝杯咖啡吧~</p>
  <div class="reward-box">
    
    <div class="reward-item">
      <img class="reward-img" src="/images/alipay.png">
      <span class="reward-type">支付宝</span>
    </div>
    
    
    <div class="reward-item">
      <img class="reward-img" src="/images/wechat.png">
      <span class="reward-type">微信</span>
    </div>
    
  </div>
</div>
    
<script src="/js/jquery-3.6.0.min.js"></script>
 
<script src="/js/lazyload.min.js"></script>

<!-- Tocbot -->
 
<script src="/js/tocbot.min.js"></script>

<script>
  tocbot.init({
    tocSelector: ".tocbot",
    contentSelector: ".article-entry",
    headingSelector: "h1, h2, h3, h4, h5, h6",
    hasInnerContainers: true,
    scrollSmooth: true,
    scrollContainer: "main",
    positionFixedSelector: ".tocbot",
    positionFixedClass: "is-position-fixed",
    fixedSidebarOffset: "auto",
  });
</script>

<script src="https://cdn.jsdelivr.net/npm/jquery-modal@0.9.2/jquery.modal.min.js"></script>
<link
  rel="stylesheet"
  href="https://cdn.jsdelivr.net/npm/jquery-modal@0.9.2/jquery.modal.min.css"
/>
<script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js"></script>

<script src="/dist/main.js"></script>

<!-- ImageViewer -->
 <!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                <!--  Controls are self-explanatory. Order can be changed. -->

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" style="display:none" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css">
<script src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js"></script>

<script>
    function viewer_init() {
        let pswpElement = document.querySelectorAll('.pswp')[0];
        let $imgArr = document.querySelectorAll(('.article-entry img:not(.reward-img)'))

        $imgArr.forEach(($em, i) => {
            $em.onclick = () => {
                // slider展开状态
                // todo: 这样不好，后面改成状态
                if (document.querySelector('.left-col.show')) return
                let items = []
                $imgArr.forEach(($em2, i2) => {
                    let img = $em2.getAttribute('data-idx', i2)
                    let src = $em2.getAttribute('data-target') || $em2.getAttribute('src')
                    let title = $em2.getAttribute('alt')
                    // 获得原图尺寸
                    const image = new Image()
                    image.src = src
                    items.push({
                        src: src,
                        w: image.width || $em2.width,
                        h: image.height || $em2.height,
                        title: title
                    })
                })
                var gallery = new PhotoSwipe(pswpElement, PhotoSwipeUI_Default, items, {
                    index: parseInt(i)
                });
                gallery.init()
            }
        })
    }
    viewer_init()
</script> 
<!-- MathJax -->
 <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
      tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
  });

  MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
      for(i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
      }
  });
</script>

<script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.6/unpacked/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script>
  var ayerConfig = {
    mathjax: true,
  };
</script>

<!-- Katex -->

<!-- busuanzi  -->
 
<script src="/js/busuanzi-2.3.pure.min.js"></script>
 
<!-- ClickLove -->

<!-- ClickBoom1 -->

<!-- ClickBoom2 -->

<!-- CodeCopy -->
 
<link rel="stylesheet" href="/css/clipboard.css">
 <script src="https://cdn.jsdelivr.net/npm/clipboard@2/dist/clipboard.min.js"></script>
<script>
  function wait(callback, seconds) {
    var timelag = null;
    timelag = window.setTimeout(callback, seconds);
  }
  !function (e, t, a) {
    var initCopyCode = function(){
      var copyHtml = '';
      copyHtml += '<button class="btn-copy" data-clipboard-snippet="">';
      copyHtml += '<i class="ri-file-copy-2-line"></i><span>COPY</span>';
      copyHtml += '</button>';
      $(".highlight .code pre").before(copyHtml);
      $(".article pre code").before(copyHtml);
      var clipboard = new ClipboardJS('.btn-copy', {
        target: function(trigger) {
          return trigger.nextElementSibling;
        }
      });
      clipboard.on('success', function(e) {
        let $btn = $(e.trigger);
        $btn.addClass('copied');
        let $icon = $($btn.find('i'));
        $icon.removeClass('ri-file-copy-2-line');
        $icon.addClass('ri-checkbox-circle-line');
        let $span = $($btn.find('span'));
        $span[0].innerText = 'COPIED';
        
        wait(function () { // 等待两秒钟后恢复
          $icon.removeClass('ri-checkbox-circle-line');
          $icon.addClass('ri-file-copy-2-line');
          $span[0].innerText = 'COPY';
        }, 2000);
      });
      clipboard.on('error', function(e) {
        e.clearSelection();
        let $btn = $(e.trigger);
        $btn.addClass('copy-failed');
        let $icon = $($btn.find('i'));
        $icon.removeClass('ri-file-copy-2-line');
        $icon.addClass('ri-time-line');
        let $span = $($btn.find('span'));
        $span[0].innerText = 'COPY FAILED';
        
        wait(function () { // 等待两秒钟后恢复
          $icon.removeClass('ri-time-line');
          $icon.addClass('ri-file-copy-2-line');
          $span[0].innerText = 'COPY';
        }, 2000);
      });
    }
    initCopyCode();
  }(window, document);
</script>
 
<!-- CanvasBackground -->

<script>
  if (window.mermaid) {
    mermaid.initialize({ theme: "forest" });
  }
</script>


    
    <div id="music">
    
    
    
    <iframe frameborder="no" border="1" marginwidth="0" marginheight="0" width="200" height="52"
        src="//music.163.com/outchain/player?type=2&id=318916815&auto=1&height=32"></iframe>
</div>

<style>
    #music {
        position: fixed;
        right: 15px;
        bottom: 0;
        z-index: 998;
    }
</style>
    
    

  </div>
<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>