<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Docker</title>
    <url>/2022/04/24/Docker/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="Docker"><a href="#Docker" class="headerlink" title="Docker"></a>Docker</h1><!-- GFM-TOC -->
<ul>
<li><a href="#docker">Docker</a><ul>
<li><a href="#%E4%B8%80%E8%A7%A3%E5%86%B3%E7%9A%84%E9%97%AE%E9%A2%98">一、解决的问题</a></li>
<li><a href="#%E4%BA%8C%E4%B8%8E%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%9A%84%E6%AF%94%E8%BE%83">二、与虚拟机的比较</a><ul>
<li><a href="#%E5%90%AF%E5%8A%A8%E9%80%9F%E5%BA%A6">启动速度</a></li>
<li><a href="#%E5%8D%A0%E7%94%A8%E8%B5%84%E6%BA%90">占用资源</a></li>
</ul>
</li>
<li><a href="#%E4%B8%89%E4%BC%98%E5%8A%BF">三、优势</a><ul>
<li><a href="#%E6%9B%B4%E5%AE%B9%E6%98%93%E8%BF%81%E7%A7%BB">更容易迁移</a></li>
<li><a href="#%E6%9B%B4%E5%AE%B9%E6%98%93%E7%BB%B4%E6%8A%A4">更容易维护</a></li>
<li><a href="#%E6%9B%B4%E5%AE%B9%E6%98%93%E6%89%A9%E5%B1%95">更容易扩展</a></li>
</ul>
</li>
<li><a href="#%E5%9B%9B%E4%BD%BF%E7%94%A8%E5%9C%BA%E6%99%AF">四、使用场景</a><ul>
<li><a href="#%E6%8C%81%E7%BB%AD%E9%9B%86%E6%88%90">持续集成</a></li>
<li><a href="#%E6%8F%90%E4%BE%9B%E5%8F%AF%E4%BC%B8%E7%BC%A9%E7%9A%84%E4%BA%91%E6%9C%8D%E5%8A%A1">提供可伸缩的云服务</a></li>
<li><a href="#%E6%90%AD%E5%BB%BA%E5%BE%AE%E6%9C%8D%E5%8A%A1%E6%9E%B6%E6%9E%84">搭建微服务架构</a></li>
</ul>
</li>
<li><a href="#%E4%BA%94%E9%95%9C%E5%83%8F%E4%B8%8E%E5%AE%B9%E5%99%A8">五、镜像与容器</a></li>
<li><a href="#%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99">参考资料</a><!-- GFM-TOC --></li>
</ul>
</li>
</ul>
<h2 id="一、解决的问题"><a href="#一、解决的问题" class="headerlink" title="一、解决的问题"></a>一、解决的问题</h2><p>由于不同的机器有不同的操作系统，以及不同的库和组件，在将一个应用部署到多台机器上需要进行大量的环境配置操作。</p>
<p>Docker 主要解决环境配置问题，它是一种虚拟化技术，对进程进行隔离，被隔离的进程独立于宿主操作系统和其它隔离的进程。使用 Docker 可以不修改应用程序代码，不需要开发人员学习特定环境下的技术，就能够将现有的应用程序部署在其它机器上。</p>
<div align="center"> <img src="https://cs-notes-1256109796.cos.ap-guangzhou.myqcloud.com/011f3ef6-d824-4d43-8b2c-36dab8eaaa72-1.png" width="400px"/> </div><br>

<h2 id="二、与虚拟机的比较"><a href="#二、与虚拟机的比较" class="headerlink" title="二、与虚拟机的比较"></a>二、与虚拟机的比较</h2><p>虚拟机也是一种虚拟化技术，它与 Docker 最大的区别在于它是通过模拟硬件，并在硬件上安装操作系统来实现。</p>
<div align="center"> <img src="https://cs-notes-1256109796.cos.ap-guangzhou.myqcloud.com/be608a77-7b7f-4f8e-87cc-f2237270bf69.png" width="500"/> </div><br>

<h3 id="启动速度"><a href="#启动速度" class="headerlink" title="启动速度"></a>启动速度</h3><p>启动虚拟机需要先启动虚拟机的操作系统，再启动应用，这个过程非常慢；</p>
<p>而启动 Docker 相当于启动宿主操作系统上的一个进程。</p>
<h3 id="占用资源"><a href="#占用资源" class="headerlink" title="占用资源"></a>占用资源</h3><p>虚拟机是一个完整的操作系统，需要占用大量的磁盘、内存和 CPU 资源，一台机器只能开启几十个的虚拟机。</p>
<p>而 Docker 只是一个进程，只需要将应用以及相关的组件打包，在运行时占用很少的资源，一台机器可以开启成千上万个 Docker。</p>
<h2 id="三、优势"><a href="#三、优势" class="headerlink" title="三、优势"></a>三、优势</h2><p>除了启动速度快以及占用资源少之外，Docker 具有以下优势：</p>
<h3 id="更容易迁移"><a href="#更容易迁移" class="headerlink" title="更容易迁移"></a>更容易迁移</h3><p>提供一致性的运行环境。已经打包好的应用可以在不同的机器上进行迁移，而不用担心环境变化导致无法运行。</p>
<h3 id="更容易维护"><a href="#更容易维护" class="headerlink" title="更容易维护"></a>更容易维护</h3><p>使用分层技术和镜像，使得应用可以更容易复用重复的部分。复用程度越高，维护工作也越容易。</p>
<h3 id="更容易扩展"><a href="#更容易扩展" class="headerlink" title="更容易扩展"></a>更容易扩展</h3><p>可以使用基础镜像进一步扩展得到新的镜像，并且官方和开源社区提供了大量的镜像，通过扩展这些镜像可以非常容易得到我们想要的镜像。</p>
<h2 id="四、使用场景"><a href="#四、使用场景" class="headerlink" title="四、使用场景"></a>四、使用场景</h2><h3 id="持续集成"><a href="#持续集成" class="headerlink" title="持续集成"></a>持续集成</h3><p>持续集成指的是频繁地将代码集成到主干上，这样能够更快地发现错误。</p>
<p>Docker 具有轻量级以及隔离性的特点，在将代码集成到一个 Docker 中不会对其它 Docker 产生影响。</p>
<h3 id="提供可伸缩的云服务"><a href="#提供可伸缩的云服务" class="headerlink" title="提供可伸缩的云服务"></a>提供可伸缩的云服务</h3><p>根据应用的负载情况，可以很容易地增加或者减少 Docker。</p>
<h3 id="搭建微服务架构"><a href="#搭建微服务架构" class="headerlink" title="搭建微服务架构"></a>搭建微服务架构</h3><p>Docker 轻量级的特点使得它很适合用于部署、维护、组合微服务。</p>
<h2 id="五、镜像与容器"><a href="#五、镜像与容器" class="headerlink" title="五、镜像与容器"></a>五、镜像与容器</h2><p>镜像是一种静态的结构，可以看成面向对象里面的类，而容器是镜像的一个实例。</p>
<p>镜像包含着容器运行时所需要的代码以及其它组件，它是一种分层结构，每一层都是只读的（read-only layers）。构建镜像时，会一层一层构建，前一层是后一层的基础。镜像的这种分层存储结构很适合镜像的复用以及定制。</p>
<p>构建容器时，通过在镜像的基础上添加一个可写层（writable layer），用来保存着容器运行过程中的修改。</p>
<div align="center"> <img src="https://cs-notes-1256109796.cos.ap-guangzhou.myqcloud.com/docker-filesystems-busyboxrw.png"/> </div><br>

<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul>
<li><a href="https://blog.docker.com/2017/08/docker-101-introduction-docker-webinar-recap/">DOCKER 101: INTRODUCTION TO DOCKER WEBINAR RECAP</a></li>
<li><a href="http://www.ruanyifeng.com/blog/2018/02/docker-tutorial.html">Docker 入门教程</a></li>
<li><a href="http://www.bogotobogo.com/DevOps/Docker/Docker_Container_vs_Virtual_Machine.php">Docker container vs Virtual machine</a></li>
<li><a href="https://linoxide.com/linux-how-to/dockerfile-create-docker-container/">How to Create Docker Container using Dockerfile</a></li>
<li><a href="http://www.cnblogs.com/sammyliu/p/5877964.html">理解 Docker（2）：Docker 镜像</a></li>
<li><a href="https://yeasy.gitbooks.io/docker_practice/introduction/why.html">为什么要使用 Docker？</a></li>
<li><a href="https://www.docker.com/what-docker">What is Docker</a></li>
<li><a href="http://www.ruanyifeng.com/blog/2015/09/continuous-integration.html">持续集成是什么？</a></li>
</ul>
]]></content>
      <categories>
        <category>容器</category>
      </categories>
      <tags>
        <tag>微服务</tag>
        <tag>面试宝典</tag>
      </tags>
  </entry>
  <entry>
    <title>什么是微服务</title>
    <url>/2022/04/24/%E4%BB%80%E4%B9%88%E6%98%AF%E5%BE%AE%E6%9C%8D%E5%8A%A1/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><blockquote>
<p>摘要: 原创出处 <a href="https://blog.csdn.net/wuxiaobingandbob/article/details/78642020">https://blog.csdn.net/wuxiaobingandbob/article/details/78642020</a> 「武晓兵」欢迎转载，保留摘要，谢谢！</p>
</blockquote>
<p>[TOC]</p>
<h1 id="一、微服务介绍"><a href="#一、微服务介绍" class="headerlink" title="一、微服务介绍"></a>一、微服务介绍</h1><h2 id="1-什么是微服务"><a href="#1-什么是微服务" class="headerlink" title="1. 什么是微服务"></a>1. 什么是微服务</h2><ul>
<li>在介绍微服务时，首先得先理解什么是微服务，顾名思义，微服务得从两个方面去理解，什么是”微”、什么是”服务”， 微 狭义来讲就是体积小、著名的”2 pizza 团队”很好的诠释了这一解释（2 pizza 团队最早是亚马逊 CEO Bezos提出来的，意思是说单个服务的设计，所有参与人从设计、开发、测试、运维所有人加起来 只需要2个披萨就够了 ）。 而所谓服务，一定要区别于系统，服务一个或者一组相对较小且独立的功能单元，是用户可以感知最小功能集。</li>
</ul>
<h2 id="2-微服务由来"><a href="#2-微服务由来" class="headerlink" title="2. 微服务由来"></a>2. 微服务由来</h2><ul>
<li>微服务最早由Martin Fowler与James Lewis于2014年共同提出，微服务架构风格是一种使用一套小服务来开发单个应用的方式途径，每个服务运行在自己的进程中，并使用轻量级机制通信，通常是HTTP API，这些服务基于业务能力构建，并能够通过自动化部署机制来独立部署，这些服务使用不同的编程语言实现，以及不同数据存储技术，并保持最低限度的集中式管理。</li>
</ul>
<h2 id="3-为什么需要微服务？"><a href="#3-为什么需要微服务？" class="headerlink" title="3. 为什么需要微服务？"></a>3. 为什么需要微服务？</h2><ul>
<li>在传统的IT行业软件大多都是各种独立系统的堆砌，这些系统的问题总结来说就是扩展性差，可靠性不高，维护成本高。到后面引入了SOA服务化，但是，由于 SOA 早期均使用了总线模式，这种总线模式是与某种技术栈强绑定的，比如：J2EE。这导致很多企业的遗留系统很难对接，切换时间太长，成本太高，新系统稳定性的收敛也需要一些时间。最终 SOA 看起来很美，但却成为了企业级奢侈品，中小公司都望而生畏。</li>
</ul>
<h3 id="3-1-最期的单体架构带来的问题"><a href="#3-1-最期的单体架构带来的问题" class="headerlink" title="3.1 最期的单体架构带来的问题"></a>3.1 最期的单体架构带来的问题</h3><p>单体架构在规模比较小的情况下工作情况良好，但是随着系统规模的扩大，它暴露出来的问题也越来越多，主要有以下几点：</p>
<ol>
<li>复杂性逐渐变高<br> 比如有的项目有几十万行代码，各个模块之间区别比较模糊，逻辑比较混乱，代码越多复杂性越高，越难解决遇到的问题。</li>
<li>技术债务逐渐上升<br> 公司的人员流动是再正常不过的事情，有的员工在离职之前，疏于代码质量的自我管束，导致留下来很多坑，由于单体项目代码量庞大的惊人，留下的坑很难被发觉，这就给新来的员工带来很大的烦恼，人员流动越大所留下的坑越多，也就是所谓的技术债务越来越多。</li>
<li>部署速度逐渐变慢<br> 这个就很好理解了，单体架构模块非常多，代码量非常庞大，导致部署项目所花费的时间越来越多，曾经有的项目启动就要一二十分钟，这是多么恐怖的事情啊，启动几次项目一天的时间就过去了，留给开发者开发的时间就非常少了。</li>
<li>阻碍技术创新<br> 比如以前的某个项目使用struts2写的，由于各个模块之间有着千丝万缕的联系，代码量大，逻辑不够清楚，如果现在想用spring mvc来重构这个项目将是非常困难的，付出的成本将非常大，所以更多的时候公司不得不硬着头皮继续使用老的struts架构，这就阻碍了技术的创新。</li>
<li>无法按需伸缩<br> 比如说电影模块是CPU密集型的模块，而订单模块是IO密集型的模块，假如我们要提升订单模块的性能，比如加大内存、增加硬盘，但是由于所有的模块都在一个架构下，因此我们在扩展订单模块的性能时不得不考虑其它模块的因素，因为我们不能因为扩展某个模块的性能而损害其它模块的性能，从而无法按需进行伸缩。<h3 id="3-2-微服务与单体架构区别"><a href="#3-2-微服务与单体架构区别" class="headerlink" title="3.2 微服务与单体架构区别"></a>3.2 微服务与单体架构区别</h3></li>
<li>单体架构所有的模块全都耦合在一块，代码量大，维护困难，微服务每个模块就相当于一个单独的项目，代码量明显减少，遇到问题也相对来说比较好解决。</li>
<li>单体架构所有的模块都共用一个数据库，存储方式比较单一，微服务每个模块都可以使用不同的存储方式（比如有的用redis，有的用mysql等），数据库也是单个模块对应自己的数据库。</li>
<li>单体架构所有的模块开发所使用的技术一样，微服务每个模块都可以使用不同的开发技术，开发模式更灵活。<br><img src="https://s2.loli.net/2022/04/24/Bia5f6csAu3DVPk.jpg"></li>
</ol>
<h3 id="3-3-微服务与SOA区别"><a href="#3-3-微服务与SOA区别" class="headerlink" title="3.3 微服务与SOA区别"></a>3.3 微服务与SOA区别</h3><p>微服务，从本质意义上看，还是 SOA 架构。但内涵有所不同，微服务并不绑定某种特殊的技术，在一个微服务的系统中，可以有 Java 编写的服务，也可以有 Python编写的服务，他们是靠Restful架构风格统一成一个系统的。所以微服务本身与具体技术实现无关，扩展性强。</p>
<h2 id="4-微服务本质"><a href="#4-微服务本质" class="headerlink" title="4. 微服务本质"></a>4. 微服务本质</h2><ol>
<li>微服务，关键其实不仅仅是微服务本身，而是系统要提供一套基础的架构，这种架构使得微服务可以独立的部署、运行、升级，不仅如此，这个系统架构还让微服务与微服务之间在结构上“松耦合”，而在功能上则表现为一个统一的整体。这种所谓的“统一的整体”表现出来的是统一风格的界面，统一的权限管理，统一的安全策略，统一的上线过程，统一的日志和审计方法，统一的调度方式，统一的访问入口等等。</li>
<li>微服务的目的是有效的拆分应用，实现敏捷开发和部署 。</li>
<li>微服务提倡的理念团队间应该是 inter-operate, not integrate 。inter-operate是定义好系统的边界和接口，在一个团队内全栈，让团队自治，原因就是因为如果团队按照这样的方式组建，将沟通的成本维持在系统内部，每个子系统就会更加内聚，彼此的依赖耦合能变弱，跨系统的沟通成本也就能降低。</li>
</ol>
<h2 id="5-什么样的项目适合微服务"><a href="#5-什么样的项目适合微服务" class="headerlink" title="5. 什么样的项目适合微服务"></a>5. 什么样的项目适合微服务</h2><p>微服务可以按照业务功能本身的独立性来划分，如果系统提供的业务是非常底层的，如：操作系统内核、存储系统、网络系统、数据库系统等等，这类系统都偏底层，功能和功能之间有着紧密的配合关系，如果强制拆分为较小的服务单元，会让集成工作量急剧上升，并且这种人为的切割无法带来业务上的真正的隔离，所以无法做到独立部署和运行，也就不适合做成微服务了。<br>能不能做成微服务，取决于四个要素：</p>
<ul>
<li>小：微服务体积小，2 pizza 团队。</li>
<li>独：能够独立的部署和运行。</li>
<li>轻：使用轻量级的通信机制和架构。</li>
<li>松：为服务之间是松耦合的。</li>
</ul>
<h2 id="6-微服务折分与设计"><a href="#6-微服务折分与设计" class="headerlink" title="6. 微服务折分与设计"></a>6. 微服务折分与设计</h2><ol>
<li>从单体式结构转向微服务架构中会持续碰到服务边界划分的问题：比如，我们有user 服务来提供用户的基础信息，那么用户的头像和图片等是应该单独划分为一个新的service更好还是应该合并到user服务里呢？如果服务的粒度划分的过粗，那就回到了单体式的老路；如果过细，那服务间调用的开销就变得不可忽视了，管理难度也会指数级增加。目前为止还没有一个可以称之为服务边界划分的标准，只能根据不同的业务系统加以调节</li>
<li>拆分的大原则是当一块业务不依赖或极少依赖其它服务，有独立的业务语义，为超过2个的其他服务或客户端提供数据，那么它就应该被拆分成一个独立的服务模块。</li>
</ol>
<h3 id="6-1-微服务设计原则"><a href="#6-1-微服务设计原则" class="headerlink" title="6.1 微服务设计原则"></a>6.1 微服务设计原则</h3><ul>
<li>单一职责原则<ul>
<li>意思是每个微服务只需要实现自己的业务逻辑就可以了，比如订单管理模块，它只需要处理订单的业务逻辑就可以了，其它的不必考虑。</li>
</ul>
</li>
<li>服务自治原则<ul>
<li>意思是每个微服务从开发、测试、运维等都是独立的，包括存储的数据库也都是独立的，自己就有一套完整的流程，我们完全可以把它当成一个项目来对待。不必依赖于其它模块。</li>
</ul>
</li>
<li>轻量级通信原则<ul>
<li>首先是通信的语言非常的轻量，第二，该通信方式需要是跨语言、跨平台的，之所以要跨平台、跨语言就是为了让每个微服务都有足够的独立性，可以不受技术的钳制。</li>
</ul>
</li>
<li>接口明确原则<ul>
<li>由于微服务之间可能存在着调用关系，为了尽量避免以后由于某个微服务的接口变化而导致其它微服务都做调整，在设计之初就要考虑到所有情况，让接口尽量做的更通用，更灵活，从而尽量避免其它模块也做调整。</li>
</ul>
</li>
</ul>
<h2 id="7-微服务优势与缺点"><a href="#7-微服务优势与缺点" class="headerlink" title="7. 微服务优势与缺点"></a>7. 微服务优势与缺点</h2><h2 id="7-1-特性"><a href="#7-1-特性" class="headerlink" title="7.1 特性"></a>7.1 特性</h2><ol>
<li>每个微服务可独立运行在自己的进程里；</li>
<li>一系列独立运行的微服务共同构建起了整个系统；</li>
<li>每个服务为独立的业务开发，一个微服务一般完成某个特定的功能，比如：订单管理，用户管理等；</li>
<li>微服务之间通过一些轻量级的通信机制进行通信，例如通过REST API或者RPC的方式进行调用。</li>
</ol>
<h2 id="7-2-特点"><a href="#7-2-特点" class="headerlink" title="7.2 特点"></a>7.2 特点</h2><ul>
<li>易于开发和维护<br>  由于微服务单个模块就相当于一个项目，开发这个模块我们就只需关心这个模块的逻辑即可，代码量和逻辑复杂度都会降低，从而易于开发和维护。</li>
<li>启动较快<br>  这是相对单个微服务来讲的，相比于启动单体架构的整个项目，启动某个模块的服务速度明显是要快很多的。</li>
<li>局部修改容易部署<br>  在开发中发现了一个问题，如果是单体架构的话，我们就需要重新发布并启动整个项目，非常耗时间，但是微服务则不同，哪个模块出现了bug我们只需要解决那个模块的bug就可以了，解决完bug之后，我们只需要重启这个模块的服务即可，部署相对简单，不必重启整个项目从而大大节约时间。</li>
<li>技术栈不受限<br>  比如订单微服务和电影微服务原来都是用java写的，现在我们想把电影微服务改成nodeJs技术，这是完全可以的，而且由于所关注的只是电影的逻辑而已，因此技术更换的成本也就会少很多。</li>
<li>按需伸缩<br>  我们上面说了单体架构在想扩展某个模块的性能时不得不考虑到其它模块的性能会不会受影响，对于我们微服务来讲，完全不是问题，电影模块通过什么方式来提升性能不必考虑其它模块的情况。</li>
</ul>
<h2 id="7-3-缺点"><a href="#7-3-缺点" class="headerlink" title="7.3 缺点"></a>7.3 缺点</h2><ul>
<li>运维要求较高<br>  对于单体架构来讲，我们只需要维护好这一个项目就可以了，但是对于微服务架构来讲，由于项目是由多个微服务构成的，每个模块出现问题都会造成整个项目运行出现异常，想要知道是哪个模块造成的问题往往是不容易的，因为我们无法一步一步通过debug的方式来跟踪，这就对运维人员提出了很高的要求。</li>
<li>分布式的复杂性<br>  对于单体架构来讲，我们可以不使用分布式，但是对于微服务架构来说，分布式几乎是必会用的技术，由于分布式本身的复杂性，导致微服务架构也变得复杂起来。</li>
<li>接口调整成本高<br>  比如，用户微服务是要被订单微服务和电影微服务所调用的，一旦用户微服务的接口发生大的变动，那么所有依赖它的微服务都要做相应的调整，由于微服务可能非常多，那么调整接口所造成的成本将会明显提高。</li>
<li>重复劳动<br>  对于单体架构来讲，如果某段业务被多个模块所共同使用，我们便可以抽象成一个工具类，被所有模块直接调用，但是微服务却无法这样做，因为这个微服务的工具类是不能被其它微服务所直接调用的，从而我们便不得不在每个微服务上都建这么一个工具类，从而导致代码的重复。</li>
</ul>
<h2 id="8-微服务开发框架"><a href="#8-微服务开发框架" class="headerlink" title="8. 微服务开发框架"></a>8. 微服务开发框架</h2><p>目前微服务的开发框架，最常用的有以下四个：</p>
<ol>
<li>Spring Cloud：<a href="http://projects.spring.io/spring-cloud%EF%BC%88%E7%8E%B0%E5%9C%A8%E9%9D%9E%E5%B8%B8%E6%B5%81%E8%A1%8C%E7%9A%84%E5%BE%AE%E6%9C%8D%E5%8A%A1%E6%9E%B6%E6%9E%84%EF%BC%89">http://projects.spring.io/spring-cloud（现在非常流行的微服务架构）</a></li>
<li>Dubbo：http：//dubbo.io</li>
<li>Dropwizard：<a href="http://www.dropwizard.io/">http://www.dropwizard.io</a> （关注单个微服务的开发）</li>
<li>Consul、etcd&amp;etc.（微服务的模块）</li>
</ol>
<h2 id="9-Sprint-cloud-和-Sprint-boot区别"><a href="#9-Sprint-cloud-和-Sprint-boot区别" class="headerlink" title="9. Sprint cloud 和 Sprint boot区别"></a>9. Sprint cloud 和 Sprint boot区别</h2><ul>
<li>Spring Boot:<br>  旨在简化创建产品级的Spring应用和服务，简化了配置文件，使用嵌入式web服务器，含有诸多开箱即用微服务功能，可以和spring cloud联合部署。</li>
<li>Spring Cloud：<br>  微服务工具包，为开发者提供了在分布式系统的配置管理、服务发现、断路器、智能路由、微代理、控制总线等开发工具包。</li>
</ul>
<h1 id="二、微服务实践先知"><a href="#二、微服务实践先知" class="headerlink" title="二、微服务实践先知"></a>二、微服务实践先知</h1><ol>
<li>客户端如何访问这些服务？（API Gateway）<br> 传统的开发方式，所有的服务都是本地的，UI可以直接调用，现在按功能拆分成独立的服务，跑在独立的一般都在独立的虚拟机上的 Java进程了。客户端UI如何访问他的？后台有N个服务，前台就需要记住管理N个服务，一个服务下线/更新/升级，前台就要重新部署，这明显不服务我们 拆分的理念，特别当前台是移动应用的时候，通常业务变化的节奏更快。另外，N个小服务的调用也是一个不小的网络开销。还有一般微服务在系统内部，通常是无状态的，用户登录信息和权限管理最好有一个统一的地方维护管理（OAuth）。<br> 所以，一般在后台N个服务和UI之间一般会一个代理或者叫API Gateway，他的作用包括<ul>
<li>提供统一服务入口，让微服务对前台透明</li>
<li>聚合后台的服务，节省流量，提升性能</li>
<li>提供安全，过滤，流控等API管理功能</li>
<li>我的理解其实这个API Gateway可以有很多广义的实现办法，可以是一个软硬一体的盒子，也可以是一个简单的MVC框架，甚至是一个Node.js的服务端。他们最重要的作用是为前台（通常是移动应用）提供后台服务的聚合，提供一个统一的服务出口，解除他们之间的耦合，不过API Gateway也有可能成为单点故障点或者性能的瓶颈。<br><img src="https://s2.loli.net/2022/04/24/G9xJ5zelBuQ4pba.jpg"></li>
</ul>
</li>
</ol>
<ol start="2">
<li><p>服务之间如何通信？（服务调用）</p>
<ul>
<li><p>因为所有的微服务都是独立的Java进程跑在独立的虚拟机上，所以服务间的通行就是IPC（inter process communication），已经有很多成熟的方案。现在基本最通用的有两种方式。这几种方式，展开来讲都可以写本书，而且大家一般都比较熟悉细节了， 就不展开讲了。</p>
<ul>
<li>REST（JAX-RS，Spring Boot）</li>
<li>RPC（Thrift, Dubbo）</li>
<li>异步消息调用(Kafka, Notify)<br><img src="https://s2.loli.net/2022/04/24/B6C7fkb1KFGhdwj.jpg"></li>
</ul>
</li>
<li><p>一般同步调用比较简单，一致性强，但是容易出调用问题，性能体验上也会差些，特别是调用层次多的时候。RESTful和RPC的比较也是一个很有意 思的话题。一般REST基于HTTP，更容易实现，更容易被接受，服务端实现技术也更灵活些，各个语言都能支持，同时能跨客户端，对客户端没有特殊的要 求，只要封装了HTTP的SDK就能调用，所以相对使用的广一些。RPC也有自己的优点，传输协议更高效，安全更可控，特别在一个公司内部，如果有统一个的开发规范和统一的服务框架时，他的开发效率优势更明显些。就看各自的技术积累实际条件，自己的选择了。</p>
</li>
<li><p>而异步消息的方式在分布式系统中有特别广泛的应用，他既能减低调用服务之间的耦合，又能成为调用之间的缓冲，确保消息积压不会冲垮被调用方，同时能 保证调用方的服务体验，继续干自己该干的活，不至于被后台性能拖慢。不过需要付出的代价是一致性的减弱，需要接受数据最终一致性；还有就是后台服务一般要 实现幂等性，因为消息发送出于性能的考虑一般会有重复（保证消息的被收到且仅收到一次对性能是很大的考验）；最后就是必须引入一个独立的broker，如 果公司内部没有技术积累，对broker分布式管理也是一个很大的挑战。</p>
</li>
</ul>
</li>
</ol>
<h2 id="3-这么多服务怎么查找？（服务发现）"><a href="#3-这么多服务怎么查找？（服务发现）" class="headerlink" title="3. 这么多服务怎么查找？（服务发现）"></a>3. 这么多服务怎么查找？（服务发现）</h2><p>​在微服务架构中，一般每一个服务都是有多个拷贝，来做负载均衡。一个服务随时可能下线，也可能应对临时访问压力增加新的服务节点。服务之间如何相互 感知？服务如何管理？这就是服务发现的问题了。一般有两类做法，也各有优缺点。基本都是通过zookeeper等类似技术做服务注册信息的分布式管理。当 服务上线时，服务提供者将自己的服务信息注册到ZK（或类似框架），并通过心跳维持长链接，实时更新链接信息。服务调用者通过ZK寻址，根据可定制算法，找到一个服务，还可以将服务信息缓存在本地以提高性能。当服务下线时，ZK会发通知给服务客户端。</p>
<ul>
<li>客户端做：**优点是架构简单，扩展灵活，只对服务注册器依赖。缺点是客户端要维护所有调用服务的地址，有技术难度，一般大公司都有成熟的内部框架支持，比如Dubbo。</li>
<li>服务端做：**优点是简单，所有服务对于前台调用方透明，一般在小公司在云服务上部署的应用采用的比较多。<br><img src="https://s2.loli.net/2022/04/24/VQy3MpGcgh95W6b.jpg"></li>
</ul>
<h2 id="4-服务挂了怎么办？"><a href="#4-服务挂了怎么办？" class="headerlink" title="4. 服务挂了怎么办？"></a>4. 服务挂了怎么办？</h2><p>​分布式最大的特性就是网络是不可靠 的。通过微服务拆分能降低这个风险，不过如果没有特别的保障，结局肯定是噩梦。我们刚遇到一个线上故障就是一个很不起眼的SQL计数功能，在访问量上升 时，导致数据库load彪高，影响了所在应用的性能，从而影响所有调用这个应用服务的前台应用。所以当我们的系统是由一系列的服务调用链组成的时候，我们必须确保任一环节出问题都不至于影响整体链路。相应的手段有很多：</p>
<ul>
<li>重试机制</li>
<li>限流</li>
<li>熔断机制</li>
<li>负载均衡</li>
<li>降级（本地缓存） 这些方法基本上都很明确通用，就不详细说明了。比如Netflix的Hystrix：<a href="https://github.com/Netflix/Hystrix">https://github.com/Netflix/Hystrix</a><br><img src="https://s2.loli.net/2022/04/24/z4Xmd7uniY2faxp.jpg"></li>
</ul>
<h2 id="5-微服务需要考虑的问题"><a href="#5-微服务需要考虑的问题" class="headerlink" title="5. 微服务需要考虑的问题"></a>5. 微服务需要考虑的问题</h2><p>这里有一个图非常好的总结微服务架构需要考虑的问题，包括</p>
<ul>
<li>API Gateway</li>
<li>服务间调用</li>
<li>服务发现</li>
<li>服务容错</li>
<li>服务部署</li>
<li>数据调用<br><img src="https://s2.loli.net/2022/04/24/SQoiTBtRH41eMWs.jpg"></li>
</ul>
<h1 id="三、微服务重要部件"><a href="#三、微服务重要部件" class="headerlink" title="三、微服务重要部件"></a>三、微服务重要部件</h1><h2 id="1-微服务基本能力"><a href="#1-微服务基本能力" class="headerlink" title="1. 微服务基本能力"></a>1. 微服务基本能力</h2><p><img src="https://s2.loli.net/2022/04/24/fzsK3VWQErGm9IC.jpg"></p>
<h2 id="2-服务注册中心"><a href="#2-服务注册中心" class="headerlink" title="2. 服务注册中心"></a>2. 服务注册中心</h2><p>服务之间需要创建一种服务发现机制，用于帮助服务之间互相感知彼此的存在。服务启动时会将自身的服务信息注册到注册中心，并订阅自己需要消费的服务。<br>服务注册中心是服务发现的核心。它保存了各个可用服务实例的网络地址（IPAddress和Port）。服务注册中心必须要有高可用性和实时更新功能。上面提到的 Netflix Eureka 就是一个服务注册中心。它提供了服务注册和查询服务信息的REST API。服务通过使用POST请求注册自己的IPAddress和Port。每30秒发送一个PUT请求刷新注册信息。通过DELETE请求注销服务。客户端通过GET请求获取可用的服务实例信息。 Netflix的高可用（Netflix achieves high availability ）是通过在Amazon EC2运行多个实例来实现的,每一个Eureka服务都有一个弹性IP Address。当Eureka服务启动时，有DNS服务器动态的分配。Eureka客户端通过查询 DNS来获取Eureka的网络地址（IP Address和Port）。一般情况下，都是返回和客户端在同一个可用区Eureka服务器地址。 其他能够作为服务注册中心的有：</p>
<ul>
<li>etcd —– 高可用，分布式，强一致性的，key-value，Kubernetes和Cloud Foundry都是使用了etcd。</li>
<li>consul —–一个用于discovering和configuring的工具。它提供了允许客户端注册和发现服务的API。Consul可以进行服务健康检查，以确定服务的可用性。</li>
<li>zookeeper —— 在分布式应用中被广泛使用，高性能的协调服务。 Apache Zookeeper 最初为Hadoop的一个子项目，但现在是一个顶级项目。</li>
</ul>
<h3 id="2-1-zookeeper服务注册和发现"><a href="#2-1-zookeeper服务注册和发现" class="headerlink" title="2.1 zookeeper服务注册和发现"></a>2.1 zookeeper服务注册和发现</h3><ul>
<li><p>简单来讲，zookeeper可以充当一个服务注册表（Service Registry），让多个服务提供者形成一个集群，让服务消费者通过服务注册表获取具体的服务访问地址（ip+端口）去访问具体的服务提供者。如下图所示：<br><img src="https://s2.loli.net/2022/04/24/l3oW46QESiPCVdY.jpg"></p>
</li>
<li><p>具体来说，zookeeper就是个分布式文件系统，每当一个服务提供者部署后都要将自己的服务注册到zookeeper的某一路径上: /{service}/{version}/{ip:port}, 比如我们的HelloWorldService部署到两台机器，那么zookeeper上就会创建两条目录：分别为/HelloWorldService/1.0.0/100.19.20.01:16888 /HelloWorldService/1.0.0/100.19.20.02:16888。</p>
</li>
<li><p>zookeeper提供了“心跳检测”功能，它会定时向各个服务提供者发送一个请求（实际上建立的是一个 socket 长连接），如果长期没有响应，服务中心就认为该服务提供者已经“挂了”，并将其剔除，比如100.19.20.02这台机器如果宕机了，那么zookeeper上的路径就会只剩/HelloWorldService/1.0.0/100.19.20.01:16888。</p>
</li>
<li><p>服务消费者会去监听相应路径（/HelloWorldService/1.0.0），一旦路径上的数据有任务变化（增加或减少），zookeeper都会通知服务消费方服务提供者地址列表已经发生改变，从而进行更新。</p>
</li>
<li><p>更为重要的是zookeeper 与生俱来的容错容灾能力（比如leader选举），可以确保服务注册表的高可用性。</p>
</li>
</ul>
<h2 id="3-负载均衡"><a href="#3-负载均衡" class="headerlink" title="3. 负载均衡"></a>3. 负载均衡</h2><p>服务高可用的保证手段，为了保证高可用，每一个微服务都需要部署多个服务实例来提供服务。此时客户端进行服务的负载均衡。</p>
<h3 id="3-1-负载均衡的常见策略"><a href="#3-1-负载均衡的常见策略" class="headerlink" title="3.1 负载均衡的常见策略"></a>3.1 负载均衡的常见策略</h3><h4 id="3-1-1-随机"><a href="#3-1-1-随机" class="headerlink" title="3.1.1 随机"></a>3.1.1 随机</h4><p>把来自网络的请求随机分配给内部中的多个服务器。</p>
<h4 id="3-1-2-轮询"><a href="#3-1-2-轮询" class="headerlink" title="3.1.2 轮询"></a>3.1.2 轮询</h4><p>每一个来自网络中的请求，轮流分配给内部的服务器，从1到N然后重新开始。此种负载均衡算法适合服务器组内部的服务器都具有相同的配置并且平均服务请求相对均衡的情况。</p>
<h4 id="3-1-3-加权轮询"><a href="#3-1-3-加权轮询" class="headerlink" title="3.1.3 加权轮询"></a>3.1.3 加权轮询</h4><p>根据服务器的不同处理能力，给每个服务器分配不同的权值，使其能够接受相应权值数的服务请求。例如：服务器A的权值被设计成1，B的权值是3，C的权值是6，则服务器A、B、C将分别接受到10%、30％、60％的服务请求。此种均衡算法能确保高性能的服务器得到更多的使用率，避免低性能的服务器负载过重。</p>
<h4 id="3-1-4-IP-Hash"><a href="#3-1-4-IP-Hash" class="headerlink" title="3.1.4 IP Hash"></a>3.1.4 IP Hash</h4><p>这种方式通过生成请求源IP的哈希值，并通过这个哈希值来找到正确的真实服务器。这意味着对于同一主机来说他对应的服务器总是相同。使用这种方式，你不需要保存任何源IP。但是需要注意，这种方式可能导致服务器负载不平衡。</p>
<h4 id="3-1-5-最少连接数"><a href="#3-1-5-最少连接数" class="headerlink" title="3.1.5 最少连接数"></a>3.1.5 最少连接数</h4><p>客户端的每一次请求服务在服务器停留的时间可能会有较大的差异，随着工作时间加长，如果采用简单的轮循或随机均衡算法，每一台服务器上的连接进程可能会产生极大的不同，并没有达到真正的负载均衡。最少连接数均衡算法对内部中需负载的每一台服务器都有一个数据记录，记录当前该服务器正在处理的连接数量，当有新的服务连接请求时，将把当前请求分配给连接数最少的服务器，使均衡更加符合实际情况，负载更加均衡。此种均衡算法适合长时处理的请求服务，如FTP。</p>
<h2 id="4-容错"><a href="#4-容错" class="headerlink" title="4. 容错"></a>4. 容错</h2><p>容错，这个词的理解，直面意思就是可以容下错误，不让错误再次扩张，让这个错误产生的影响在一个固定的边界之内，“千里之堤毁于蚁穴”我们用容错的方式就是让这种蚁穴不要变大。那么我们常见的降级，限流，熔断器，超时重试等等都是容错的方法。</p>
<p>在调用服务集群时，如果一个微服务调用异常，如超时，连接异常，网络异常等，则根据容错策略进行服务容错。目前支持的服务容错策略有快速失败，失效切换。如果连续失败多次则直接熔断，不再发起调用。这样可以避免一个服务异常拖垮所有依赖于他的服务。</p>
<h2 id="4-1-容错策略"><a href="#4-1-容错策略" class="headerlink" title="4.1 容错策略"></a>4.1 容错策略</h2><h4 id="4-1-1-快速失败"><a href="#4-1-1-快速失败" class="headerlink" title="4.1.1 快速失败"></a>4.1.1 快速失败</h4><p>服务只发起一次待用，失败立即报错。通常用于非幂等下性的写操作</p>
<h4 id="4-1-2-失效切换"><a href="#4-1-2-失效切换" class="headerlink" title="4.1.2 失效切换"></a>4.1.2 失效切换</h4><p>服务发起调用，当出现失败后，重试其他服务器。通常用于读操作，但重试会带来更长时间的延迟。重试的次数通常是可以设置的</p>
<h4 id="4-1-3-失败安全"><a href="#4-1-3-失败安全" class="headerlink" title="4.1.3 失败安全"></a>4.1.3 失败安全</h4><p>失败安全， 当服务调用出现异常时，直接忽略。通常用于写入日志等操作。</p>
<h4 id="4-1-4-失败自动恢复"><a href="#4-1-4-失败自动恢复" class="headerlink" title="4.1.4 失败自动恢复"></a>4.1.4 失败自动恢复</h4><p>当服务调用出现异常时，记录失败请求，定时重发。通常用于消息通知。</p>
<h4 id="4-1-5-forking-Cluster"><a href="#4-1-5-forking-Cluster" class="headerlink" title="4.1.5 forking Cluster"></a>4.1.5 forking Cluster</h4><p>并行调用多个服务器，只要有一个成功，即返回。通常用于实时性较高的读操作。可以通过forks=n来设置最大并行数。</p>
<h4 id="4-1-6-广播调用"><a href="#4-1-6-广播调用" class="headerlink" title="4.1.6 广播调用"></a>4.1.6 广播调用</h4><p>广播调用所有提供者，逐个调用，任何一台失败则失败。通常用于通知所有提供者更新缓存或日志等本地资源信息。</p>
<h2 id="5-熔断"><a href="#5-熔断" class="headerlink" title="5. 熔断"></a>5. 熔断</h2><p>熔断技术可以说是一种“智能化的容错”，当调用满足失败次数，失败比例就会触发熔断器打开，有程序自动切断当前的RPC调用,来防止错误进一步扩大。实现一个熔断器主要是考虑三种模式，关闭，打开，半开。各个状态的转换如下图。<br><img src="https://s2.loli.net/2022/04/24/1ZeirbPv8AFEfxu.jpg"></p>
<p>我们在处理异常的时候，要根据具体的业务情况来决定处理方式，比如我们调用商品接口，对方只是临时做了降级处理，那么作为网关调用就要切到可替换的服务上来执行或者获取托底数据，给用户友好提示。还有要区分异常的类型，比如依赖的服务崩溃了，这个可能需要花费比较久的时间来解决。也可能是由于服务器负载临时过高导致超时。作为熔断器应该能够甄别这种异常类型，从而根据具体的错误类型调整熔断策略。增加手动设置，在失败的服务恢复时间不确定的情况下，管理员可以手动强制切换熔断状态。最后，熔断器的使用场景是调用可能失败的远程服务程序或者共享资源。如果是本地缓存本地私有资源，使用熔断器则会增加系统的额外开销。还要注意，熔断器不能作为应用程序中业务逻辑的异常处理替代品。</p>
<p>有一些异常比较顽固，突然发生，无法预测，而且很难恢复，并且还会导致级联失败（举个例子，假设一个服务集群的负载非常高，如果这时候集群的一部分挂掉了，还占了很大一部分资源，整个集群都有可能遭殃）。如果我们这时还是不断进行重试的话，结果大多都是失败的。因此，此时我们的应用需要立即进入失败状态(fast-fail)，并采取合适的方法进行恢复。</p>
<p>我们可以用状态机来实现CircuitBreaker，它有以下三种状态：</p>
<ul>
<li>关闭( Closed )：默认情况下Circuit Breaker是关闭的，此时允许操作执行。CircuitBreaker内部记录着最近失败的次数，如果对应的操作执行失败，次数就会续一次。如果在某个时间段内，失败次数（或者失败比率）达到阈值，CircuitBreaker会转换到开启( Open )状态。在开启状态中，Circuit Breaker会启用一个超时计时器，设这个计时器的目的是给集群相应的时间来恢复故障。当计时器时间到的时候，CircuitBreaker会转换到半开启( Half-Open )状态。</li>
<li>开启( Open )：在此状态下，执行对应的操作将会立即失败并且立即抛出异常。</li>
<li>半开启( Half-Open )：在此状态下，Circuit Breaker会允许执行一定数量的操作。如果所有操作全部成功，CircuitBreaker就会假定故障已经恢复，它就会转换到关闭状态，并且重置失败次数。如果其中 任意一次 操作失败了，Circuit Breaker就会认为故障仍然存在，所以它会转换到开启状态并再次开启计时器（再给系统一些时间使其从失败中恢复）</li>
</ul>
<ol start="6">
<li>限流和降级<br>​ 保证核心服务的稳定性。为了保证核心服务的稳定性，随着访问量的不断增加，需要为系统能够处理的服务数量设置一个极限阀值，超过这个阀值的请求则直接拒绝。同时，为了保证核心服务的可用，可以对否些非核心服务进行降级，通过限制服务的最大访问量进行限流，通过管理控制台对单个微服务进行人工降级</li>
</ol>
<h2 id="7-SLA"><a href="#7-SLA" class="headerlink" title="7. SLA"></a>7. SLA</h2><p>SLA：Service-LevelAgreement的缩写，意思是服务等级协议。 是关于网络服务供应商和客户间的一份合同，其中定义了服务类型、服务质量和客户付款等术语。 典型的SLA包括以下项目：</p>
<ul>
<li>分配给客户的最小带宽；</li>
<li>客户带宽极限；</li>
<li>能同时服务的客户数目；</li>
<li>在可能影响用户行为的网络变化之前的通知安排；</li>
<li>拨入访问可用性；</li>
<li>运用统计学；</li>
<li>服务供应商支持的最小网络利用性能，如99.9%有效工作时间或每天最多为1分钟的停机时间；</li>
<li>各类客户的流量优先权；</li>
<li>客户技术支持和服务；</li>
<li>惩罚规定，为服务供应商不能满足 SLA需求所指定。</li>
</ul>
<h2 id="8-API网关"><a href="#8-API网关" class="headerlink" title="8. API网关"></a>8. API网关</h2><p>这里说的网关是指API网关，直面意思是将所有API调用统一接入到API网关层，有网关层统一接入和输出。一个网关的基本功能有：统一接入、安全防护、协议适配、流量管控、长短链接支持、容错能力。有了网关之后，各个API服务提供团队可以专注于自己的的业务逻辑处理，而API网关更专注于安全、流量、路由等问题。</p>
<h2 id="9-多级缓存"><a href="#9-多级缓存" class="headerlink" title="9. 多级缓存"></a>9. 多级缓存</h2><p>​最简单的缓存就是查一次数据库然后将数据写入缓存比如redis中并设置过期时间。因为有过期失效因此我们要关注下缓存的穿透率，这个穿透率的计算公式，比如查询方法queryOrder(调用次数1000/1s)里面嵌套查询DB方法queryProductFromDb(调用次数300/s)，那么redis的穿透率就是300/1000,在这种使用缓存的方式下，是要重视穿透率的，穿透率大了说明缓存的效果不好。还有一种使用缓存的方式就是将缓存持久化，也就是不设置过期时间，这个就会面临一个数据更新的问题。一般有两种办法，一个是利用时间戳，查询默认以redis为主，每次设置数据的时候放入一个时间戳，每次读取数据的时候用系统当前时间和上次设置的这个时间戳做对比，比如超过5分钟，那么就再查一次数据库。这样可以保证redis里面永远有数据，一般是对DB的一种容错方法。还有一个就是真正的让redis做为DB使用。就是图里面画的通过订阅数据库的binlog通过数据异构系统将数据推送给缓存，同时将将缓存设置为多级。可以通过使用jvmcache作为应用内的一级缓存，一般是体积小，访问频率大的更适合这种jvmcache方式，将一套redis作为二级remote缓存，另外最外层三级redis作为持久化缓存。</p>
<h2 id="10-超时和重试"><a href="#10-超时和重试" class="headerlink" title="10. 超时和重试"></a>10. 超时和重试</h2><p>​超时与重试机制也是容错的一种方法，凡是发生RPC调用的地方，比如读取redis，db，mq等，因为网络故障或者是所依赖的服务故障，长时间不能返回结果，就会导致线程增加，加大cpu负载，甚至导致雪崩。所以对每一个RPC调用都要设置超时时间。对于强依赖RPC调用资源的情况，还要有重试机制，但是重试的次数建议1-2次，另外如果有重试，那么超时时间就要相应的调小，比如重试1次，那么一共是发生2次调用。如果超时时间配置的是2s，那么客户端就要等待4s才能返回。因此重试+超时的方式，超时时间要调小。这里也再谈一下一次PRC调用的时间都消耗在哪些环节，一次正常的调用统计的耗时主要包括： ①调用端RPC框架执行时间 + ②网络发送时间 + ③服务端RPC框架执行时间 + ④服务端业务代码时间。调用方和服务方都有各自的性能监控，比如调用方tp99是500ms，服务方tp99是100ms，找了网络组的同事确认网络没有问题。那么时间都花在什么地方了呢，两种原因，客户端调用方，还有一个原因是网络发生TCP重传。所以要注意这两点。</p>
<h2 id="11-线程池隔离"><a href="#11-线程池隔离" class="headerlink" title="11. 线程池隔离"></a>11. 线程池隔离</h2><p>​在抗量这个环节，Servlet3异步的时候，有提到过线程隔离。线程隔离的之间优势就是防止级联故障，甚至是雪崩。当网关调用N多个接口服务的时候，我们要对每个接口进行线程隔离。比如，我们有调用订单、商品、用户。那么订单的业务不能够影响到商品和用户的请求处理。如果不做线程隔离，当访问订单服务出现网络故障导致延时，线程积压最终导致整个服务CPU负载满。就是我们说的服务全部不可用了，有多少机器都会被此刻的请求塞满。那么有了线程隔离就会使得我们的网关能保证局部问题不会影响全局。</p>
<h2 id="12-降级和限流"><a href="#12-降级和限流" class="headerlink" title="12. 降级和限流"></a>12. 降级和限流</h2><p>​关于降级限流的方法业界都已经有很成熟的方法了，比如FAILBACK机制，限流的方法令牌桶，漏桶，信号量等。这里谈一下我们的一些经验，降级一般都是由统一配置中心的降级开关来实现的，那么当有很多个接口来自同一个提供方，这个提供方的系统或这机器所在机房网络出现了问题，我们就要有一个统一的降级开关，不然就要一个接口一个接口的来降级。也就是要对业务类型有一个大闸刀。还有就是 降级切记暴力降级，什么是暴力降级的，比如把论坛功能降调，结果用户显示一个大白板，我们要实现缓存住一些数据，也就是有托底数据。限流一般分为分布式限流和单机限流，如果实现分布式限流的话就要一个公共的后端存储服务比如redis，在大nginx节点上利用lua读取redis配置信息。我们现在的限流都是单机限流，并没有实施分布式限流。</p>
<h2 id="13-网关监控和统计"><a href="#13-网关监控和统计" class="headerlink" title="13. 网关监控和统计"></a>13. 网关监控和统计</h2><p><img src="https://s2.loli.net/2022/04/24/gmoW8ENrLZB7VyO.jpg"></p>
<p>​ API网关是一个串行的调用，那么每一步发生的异常要记录下来，统一存储到一个地方比如elasticserach中，便于后续对调用异常的分析。鉴于公司docker申请都是统一分配，而且分配之前docker上已经存在3个agent了，不再允许增加。我们自己实现了一个agent程序，来负责采集服务器上面的日志输出，然后发送到kafka集群，再消费到elasticserach中，通过web查询。现在做的追踪功能还比较简单，这块还需要继续丰富。</p>
]]></content>
      <categories>
        <category>微服务</category>
      </categories>
      <tags>
        <tag>微服务</tag>
        <tag>面试宝典</tag>
      </tags>
  </entry>
  <entry>
    <title>SparkSQL</title>
    <url>/2021/12/13/SparkSQL/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="Spark-SQL"><a href="#Spark-SQL" class="headerlink" title="Spark SQL"></a>Spark SQL</h1><h1 id="第1章-SparkSQL概述"><a href="#第1章-SparkSQL概述" class="headerlink" title="第1章 SparkSQL概述"></a>第1章 SparkSQL概述</h1><h2 id="1-1-SparkSQL是什么"><a href="#1-1-SparkSQL是什么" class="headerlink" title="1.1 SparkSQL是什么"></a>1.1 SparkSQL是什么</h2><p>Spark SQL是Spark用于结构化数据(structured data)处理的Spark模块。</p>
<h2 id="1-2-Hive-and-SparkSQL"><a href="#1-2-Hive-and-SparkSQL" class="headerlink" title="1.2 Hive and SparkSQL"></a>1.2 Hive and SparkSQL</h2><ul>
<li><p>SparkSQL的前身是Shark，给熟悉RDBMS但又不理解MapReduce的技术人员提供快速上手的工具。</p>
</li>
<li><p>Hive是早期唯一运行在Hadoop上的SQL-on-Hadoop工具。但是MapReduce计算过程中大量的中间磁盘落地过程消耗了大量的I/O，降低的运行效率，为了提高SQL-on-Hadoop的效率，大量的SQL-on-Hadoop工具开始产生，其中表现较为突出的是：</p>
<ul>
<li>Drill</li>
<li>Impala</li>
<li>Shark</li>
</ul>
</li>
<li><p>其中Shark是伯克利实验室Spark生态环境的组件之一，是基于Hive所开发的工具，它修改了下图所示的右下角的内存管理、物理计划、执行三个模块，并使之能运行在Spark引擎上。<br>  <img src="https://s2.loli.net/2021/12/13/7jzrtMVpEQx9Lcn.jpg"></p>
</li>
<li><p>Shark的出现，使得SQL-on-Hadoop的性能比Hive有了10-100倍的提高。<br>  <img src="https://s2.loli.net/2021/12/13/W7HwSAcQyZvsVGX.jpg"></p>
</li>
<li><p>但是，随着Spark的发展，对于野心勃勃的Spark团队来说，Shark对于Hive的太多依赖（如采用Hive的语法解析器、查询优化器等等），制约了Spark的One Stack Rule Them All的既定方针，制约了Spark各个组件的相互集成，所以提出了SparkSQL项目。SparkSQL抛弃原有Shark的代码，汲取了Shark的一些优点，如内存列存储（In-Memory Columnar Storage）、Hive兼容性等，重新开发了SparkSQL代码；由于摆脱了对Hive的依赖性，SparkSQL无论在数据兼容、性能优化、组件扩展方面都得到了极大的方便，真可谓“退一步，海阔天空”。</p>
<ul>
<li>数据兼容方面 SparkSQL不但兼容Hive，还可以从RDD、parquet文件、JSON文件中获取数据，未来版本甚至支持获取RDBMS数据以及cassandra等NOSQL数据；</li>
<li>性能优化方面 除了采取In-Memory Columnar Storage、byte-code generation等优化技术外、将会引进Cost Model对查询进行动态评估、获取最佳物理计划等等；</li>
<li>组件扩展方面 无论是SQL的语法解析器、分析器还是优化器都可以重新定义，进行扩展。</li>
</ul>
</li>
<li><p>2014年6月1日Shark项目和SparkSQL项目的主持人Reynold Xin宣布：停止对Shark的开发，团队将所有资源放SparkSQL项目上，至此，Shark的发展画上了句话，但也因此发展出两个支线：SparkSQL和Hive on Spark。</p>
</li>
<li><p>其中SparkSQL作为Spark生态的一员继续发展，而不再受限于Hive，只是兼容Hive；而Hive on Spark是一个Hive的发展计划，该计划将Spark作为Hive的底层引擎之一，也就是说，Hive将不再受限于一个引擎，可以采用Map-Reduce、Tez、Spark等引擎。</p>
</li>
<li><p><font color ='blue' >对于开发人员来讲，SparkSQL可以简化RDD的开发，提高开发效率</font>，且执行效率非常快，所以实际工作中，基本上采用的就是SparkSQL。Spark SQL为了简化RDD的开发，提高开发效率，提供了2个编程抽象，类似Spark Core中的RDD</p>
<ul>
<li>DataFrame</li>
<li>DataSet</li>
</ul>
</li>
</ul>
<h2 id="1-3-SparkSQL特点"><a href="#1-3-SparkSQL特点" class="headerlink" title="1.3 SparkSQL特点"></a>1.3 SparkSQL特点</h2><h3 id="1-3-1-易整合"><a href="#1-3-1-易整合" class="headerlink" title="1.3.1 易整合"></a>1.3.1 易整合</h3><p>无缝的整合了 SQL 查询和 Spark 编程<br><img src="https://s2.loli.net/2021/12/13/8sTmDI6NikRp1OV.jpg"></p>
<h3 id="1-3-2-统一的数据访问"><a href="#1-3-2-统一的数据访问" class="headerlink" title="1.3.2 统一的数据访问"></a>1.3.2 统一的数据访问</h3><p>使用相同的方式连接不同的数据源<br><img src="https://s2.loli.net/2021/12/13/EBJyWkd2ipO6stN.jpg"></p>
<h3 id="1-3-3-兼容Hive"><a href="#1-3-3-兼容Hive" class="headerlink" title="1.3.3 兼容Hive"></a>1.3.3 兼容Hive</h3><p>在已有的仓库上直接运行 SQL 或者 HiveQL<br><img src="https://s2.loli.net/2021/12/13/AeYlHzuBoViQvbf.jpg"></p>
<h3 id="1-3-4-标准数据连接"><a href="#1-3-4-标准数据连接" class="headerlink" title="1.3.4 标准数据连接"></a>1.3.4 标准数据连接</h3><p>通过 JDBC 或者 ODBC 来连接<br><img src="https://s2.loli.net/2021/12/13/Trg6ZLADS4laVnI.jpg"></p>
<h2 id="1-4-DataFrame是什么"><a href="#1-4-DataFrame是什么" class="headerlink" title="1.4 DataFrame是什么"></a>1.4 DataFrame是什么</h2><ul>
<li><p>在Spark中，DataFrame是一种以RDD为基础的分布式数据集，类似于传统数据库中的二维表格。DataFrame与RDD的主要区别在于，前者带有schema元信息，即DataFrame所表示的二维表数据集的每一列都带有名称和类型。这使得Spark SQL得以洞察更多的结构信息，从而对藏于DataFrame背后的数据源以及作用于DataFrame之上的变换进行了针对性的优化，最终达到大幅提升运行时效率的目标。反观RDD，由于无从得知所存数据元素的具体内部结构，Spark Core只能在stage层面进行简单、通用的流水线优化。</p>
</li>
<li><p>同时，与Hive类似，DataFrame也支持嵌套数据类型（struct、array和map）。从 API 易用性的角度上看，DataFrame API提供的是一套高层的关系操作，比函数式的RDD API 要更加友好，门槛更低。<br><img src="https://s2.loli.net/2021/12/13/aGlVyieYRHDZhrg.jpg"></p>
</li>
<li><p>上图直观地体现了DataFrame和RDD的区别。</p>
<ul>
<li>左侧的RDD[Person]虽然以Person为类型参数，但Spark框架本身不了解Person类的内部结构。而右侧的DataFrame却提供了详细的结构信息，使得 Spark SQL 可以清楚地知道该数据集中包含哪些列，每列的名称和类型各是什么。</li>
<li>DataFrame是为数据提供了Schema的视图。可以把它当做数据库中的一张表来对待</li>
<li>DataFrame也是懒执行的，但性能上比RDD要高，主要原因：优化的执行计划，即查询计划通过Spark catalyst optimiser进行优化。比如下面一个例子:<br>  <img src="https://s2.loli.net/2021/12/13/6ANB9eFt2KlXIaO.jpg"><br><img src="https://s2.loli.net/2021/12/13/gtdbslanASHGu79.jpg"></li>
</ul>
</li>
<li><p>为了说明查询优化，我们来看上图展示的人口数据分析的示例。图中构造了两个DataFrame，将它们join之后又做了一次filter操作。如果原封不动地执行这个执行计划，最终的执行效率是不高的。因为join是一个代价较大的操作，也可能会产生一个较大的数据集。如果我们能将filter下推到 join下方，先对DataFrame进行过滤，再join过滤后的较小的结果集，便可以有效缩短执行时间。而Spark SQL的查询优化器正是这样做的。简而言之，逻辑查询计划优化就是一个利用基于关系代数的等价变换，将高成本的操作替换为低成本操作的过程。<br><img src="https://s2.loli.net/2021/12/13/NrzleGmhZ6S5PvA.jpg"></p>
</li>
</ul>
<h2 id="1-5-DataSet是什么"><a href="#1-5-DataSet是什么" class="headerlink" title="1.5 DataSet是什么"></a>1.5 DataSet是什么</h2><p>DataSet是分布式数据集合。DataSet是Spark 1.6中添加的一个新抽象，是DataFrame的一个扩展。它提供了RDD的优势（强类型，使用强大的lambda函数的能力）以及Spark SQL优化执行引擎的优点。DataSet也可以使用功能性的转换（操作map，flatMap，filter等等）。</p>
<ul>
<li>DataSet是DataFrame API的一个扩展，是SparkSQL最新的数据抽象</li>
<li>用户友好的API风格，既具有类型安全检查也具有DataFrame的查询优化特性；</li>
<li>用样例类来对DataSet中定义数据的结构信息，样例类中每个属性的名称直接映射到DataSet中的字段名称；</li>
<li>DataSet是强类型的。比如可以有DataSet[Car]，DataSet[Person]。</li>
<li>DataFrame是DataSet的特列，DataFrame=DataSet[Row] ，所以可以通过as方法将DataFrame转换为DataSet。Row是一个类型，跟Car、Person这些的类型一样，所有的表结构信息都用Row来表示。获取数据时需要指定顺序</li>
</ul>
<h1 id="第2章-SparkSQL核心编程"><a href="#第2章-SparkSQL核心编程" class="headerlink" title="第2章 SparkSQL核心编程"></a>第2章 SparkSQL核心编程</h1><p>本节重点学习如何使用 Spark SQL所提供的 DataFrame和DataSet模型进行编程。以及了解它们之间的关系和转换，关于具体的SQL语法不是我们的重点。</p>
<h2 id="2-1-新的起点"><a href="#2-1-新的起点" class="headerlink" title="2.1 新的起点"></a>2.1 新的起点</h2><p>Spark Core中，如果想要执行应用程序，需要首先构建上下文环境对象SparkContext，Spark SQL其实可以理解为对Spark Core的一种封装，不仅仅在模型上进行了封装，上下文环境对象也进行了封装。<br>在老的版本中，SparkSQL提供两种SQL查询起始点：一个叫SQLContext，用于Spark自己提供的SQL查询；一个叫HiveContext，用于连接Hive的查询。<br>SparkSession是Spark最新的SQL查询起始点，实质上是SQLContext和HiveContext的组合，所以在SQLContex和HiveContext上可用的API在SparkSession上同样是可以使用的。SparkSession内部封装了SparkContext，所以计算实际上是由sparkContext完成的。当我们使用 spark-shell 的时候, spark框架会自动的创建一个名称叫做spark的SparkSession对象, 就像我们以前可以自动获取到一个sc来表示SparkContext对象一样<br><img src="https://s2.loli.net/2021/12/13/651jx9Y7BymkDhN.jpg"></p>
<h2 id="2-2-DataFrame"><a href="#2-2-DataFrame" class="headerlink" title="2.2 DataFrame"></a>2.2 DataFrame</h2><p>Spark SQL的DataFrame API 允许我们使用 DataFrame 而不用必须去注册临时表或者生成 SQL 表达式。DataFrame API 既有 transformation操作也有action操作。</p>
<h3 id="2-2-1-创建DataFrame"><a href="#2-2-1-创建DataFrame" class="headerlink" title="2.2.1 创建DataFrame"></a>2.2.1 创建DataFrame</h3><p>在Spark SQL中SparkSession是创建DataFrame和执行SQL的入口，创建DataFrame有三种方式：通过Spark的数据源进行创建；从一个存在的RDD进行转换；还可以从Hive Table进行查询返回。</p>
<ol>
<li>从Spark数据源进行创建<ul>
<li>查看Spark支持创建文件的数据源格式  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; spark.read.</span><br><span class="line">csv   format   jdbc   json   load   option   options   orc   parquet   schema   table   text   textFile</span><br><span class="line"></span><br><span class="line">scala&gt; spark.read.</span><br></pre></td></tr></table></figure></li>
<li>在spark的bin/data目录中创建user.json文件  <figure class="highlight json"><table><tr><td class="code"><pre><span class="line">&#123;&#x27;id&#x27;:<span class="number">1</span>,&#x27;name&#x27;:&#x27;zhangsan&#x27;,&#x27;age&#x27;:<span class="number">20</span>&#125;</span><br><span class="line">&#123;&#x27;id&#x27;:<span class="number">2</span>,&#x27;name&#x27;:&#x27;lisi&#x27;,&#x27;age&#x27;:<span class="number">30</span>&#125;</span><br><span class="line">&#123;&#x27;id&#x27;:<span class="number">3</span>,&#x27;name&#x27;:&#x27;wangwu&#x27;,&#x27;age&#x27;:<span class="number">40</span>&#125;</span><br><span class="line">&#123;&#x27;id&#x27;:<span class="number">4</span>,&#x27;name&#x27;:&#x27;zhaoliu&#x27;,&#x27;age&#x27;:<span class="number">50</span>&#125;</span><br></pre></td></tr></table></figure></li>
<li>读取json文件创建DataFrame  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> df = spark.read.json(<span class="string">&quot;data/user.json&quot;</span>)</span><br><span class="line">df: org.apache.spark.sql.<span class="type">DataFrame</span> = [age: bigint， username: string]</span><br></pre></td></tr></table></figure>
  注意：如果从内存中获取数据，spark可以知道数据类型具体是什么。如果是数字，默认作为Int处理；但是从文件中读取的数字，不能确定是什么类型，所以用bigint接收，可以和Long类型转换，但是和Int不能进行转换</li>
<li>展示结果:show  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">scala&gt; df.show</span><br><span class="line">+---+---+--------+</span><br><span class="line">|age| id|    name|</span><br><span class="line">+---+---+--------+</span><br><span class="line">| 20|  1|zhangsan|</span><br><span class="line">| 30|  2|    lisi|</span><br><span class="line">| 40|  3|  wangwu|</span><br><span class="line">| 50|  4| zhaoliu|</span><br><span class="line">+---+---+--------+</span><br><span class="line">scala&gt;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>从RDD进行转换<br> 在后续章节中讨论</li>
<li>从Hive Table进行查询返回<br> 在后续章节中讨论</li>
</ol>
<h3 id="2-2-2-SQL语法"><a href="#2-2-2-SQL语法" class="headerlink" title="2.2.2 SQL语法"></a>2.2.2 SQL语法</h3><p>SQL语法风格是指我们查询数据的时候使用SQL语句来查询，这种风格的查询必须要有临时视图或者全局视图来辅助</p>
<ol>
<li>读取JSON文件创建DataFrame <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> df = spark.read.json(<span class="string">&quot;data/user.json&quot;</span>)</span><br><span class="line">df: org.apache.spark.sql.<span class="type">DataFrame</span> = [age: bigint， username: string]</span><br></pre></td></tr></table></figure></li>
<li>对DataFrame创建一个临时表(视图) <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; df.createOrReplaceTempView(<span class="string">&quot;user&quot;</span>)</span><br></pre></td></tr></table></figure></li>
<li>通过SQL语句实现查询全表,结果展示 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">scala&gt; spark.sql(<span class="string">&quot;select * from user&quot;</span>).show</span><br><span class="line">+---+---+--------+</span><br><span class="line">|age| id|    name|</span><br><span class="line">+---+---+--------+</span><br><span class="line">| 20|  1|zhangsan|</span><br><span class="line">| 30|  2|    lisi|</span><br><span class="line">| 40|  3|  wangwu|</span><br><span class="line">| 50|  4| zhaoliu|</span><br><span class="line">+---+---+--------+</span><br><span class="line">scala&gt;</span><br></pre></td></tr></table></figure>
 注意：普通临时表是Session范围内的，如果想应用范围内有效，可以使用全局临时表。使用全局临时表时需要全路径访问，如：global_temp.people</li>
<li>对于DataFrame创建一个全局表 <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; df.createGlobalTempView(<span class="string">&quot;user&quot;</span>)</span><br></pre></td></tr></table></figure></li>
<li>通过SQL语句实现查询全表 <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; spark.sql(<span class="string">&quot;SELECT * FROM global_temp.user&quot;</span>).show()</span><br><span class="line">+---+---+--------+</span><br><span class="line">|age| id|    name|</span><br><span class="line">+---+---+--------+</span><br><span class="line">| <span class="number">20</span>|  <span class="number">1</span>|zhangsan|</span><br><span class="line">| <span class="number">30</span>|  <span class="number">2</span>|    lisi|</span><br><span class="line">| <span class="number">40</span>|  <span class="number">3</span>|  wangwu|</span><br><span class="line">| <span class="number">50</span>|  <span class="number">4</span>| zhaoliu|</span><br><span class="line">+---+---+--------+</span><br><span class="line">scala&gt; spark.newSession.sql(<span class="string">&quot;SELECT * FROM global_temp.user&quot;</span>).show()</span><br><span class="line">+---+---+--------+</span><br><span class="line">|age| id|    name|</span><br><span class="line">+---+---+--------+</span><br><span class="line">| <span class="number">20</span>|  <span class="number">1</span>|zhangsan|</span><br><span class="line">| <span class="number">30</span>|  <span class="number">2</span>|    lisi|</span><br><span class="line">| <span class="number">40</span>|  <span class="number">3</span>|  wangwu|</span><br><span class="line">| <span class="number">50</span>|  <span class="number">4</span>| zhaoliu|</span><br><span class="line">+---+---+--------+</span><br><span class="line">scala&gt;</span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="2-2-3-DSL语法"><a href="#2-2-3-DSL语法" class="headerlink" title="2.2.3 DSL语法"></a>2.2.3 DSL语法</h3><p>DataFrame提供一个特定领域语言(domain-specific language, DSL)去管理结构化的数据。可以在 Scala, Java, Python 和 R 中使用 DSL，使用 DSL 语法风格不必去创建临时视图了</p>
<ol>
<li>创建一个DataFrame <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> df = spark.read.json(<span class="string">&quot;data/user.json&quot;</span>)</span><br><span class="line">df: org.apache.spark.sql.<span class="type">DataFrame</span> = [age: bigint， name: string]</span><br></pre></td></tr></table></figure></li>
<li>查看DataFrame的Schema信息 <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; df.printSchema</span><br><span class="line">root</span><br><span class="line"> |-- age: <span class="type">Long</span> (nullable = <span class="literal">true</span>)</span><br><span class="line"> |-- username: string (nullable = <span class="literal">true</span>)</span><br></pre></td></tr></table></figure></li>
<li>只查看”name”列数据， <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; df.select(<span class="string">&quot;name&quot;</span>).show()</span><br><span class="line">+--------+</span><br><span class="line">|    name|</span><br><span class="line">+--------+</span><br><span class="line">|zhangsan|</span><br><span class="line">|    lisi|</span><br><span class="line">|  wangwu|</span><br><span class="line">| zhaoliu|</span><br><span class="line">+--------+</span><br></pre></td></tr></table></figure></li>
<li>查看”name”列数据以及”age+1”数据<br> 注意:涉及到运算的时候, 每列都必须使用$, 或者采用引号表达式：单引号+字段名 <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; df.select($<span class="string">&quot;name&quot;</span>,$<span class="string">&quot;age&quot;</span> + <span class="number">1</span>).show</span><br><span class="line">+--------+---------+</span><br><span class="line">|    name|(age + <span class="number">1</span>)|</span><br><span class="line">+--------+---------+</span><br><span class="line">|zhangsan|       <span class="number">21</span>|</span><br><span class="line">|    lisi|       <span class="number">31</span>|</span><br><span class="line">|  wangwu|       <span class="number">41</span>|</span><br><span class="line">| zhaoliu|       <span class="number">51</span>|</span><br><span class="line">+--------+---------+</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">scala&gt; df.select(<span class="symbol">&#x27;name</span>, <span class="symbol">&#x27;age</span> + <span class="number">1</span>).show()</span><br><span class="line">+--------+---------+</span><br><span class="line">|    name|(age + <span class="number">1</span>)|</span><br><span class="line">+--------+---------+</span><br><span class="line">|zhangsan|       <span class="number">21</span>|</span><br><span class="line">|    lisi|       <span class="number">31</span>|</span><br><span class="line">|  wangwu|       <span class="number">41</span>|</span><br><span class="line">| zhaoliu|       <span class="number">51</span>|</span><br><span class="line">+--------+---------+</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">scala&gt; df.select(<span class="symbol">&#x27;name</span>, <span class="symbol">&#x27;age</span> + <span class="number">1</span> as <span class="string">&quot;newage&quot;</span>).show()</span><br><span class="line">+--------+------+</span><br><span class="line">|    name|newage|</span><br><span class="line">+--------+------+</span><br><span class="line">|zhangsan|    <span class="number">21</span>|</span><br><span class="line">|    lisi|    <span class="number">31</span>|</span><br><span class="line">|  wangwu|    <span class="number">41</span>|</span><br><span class="line">| zhaoliu|    <span class="number">51</span>|</span><br><span class="line">+--------+------+</span><br></pre></td></tr></table></figure></li>
<li>查看”age”大于”30”的数据 <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; df.filter($<span class="string">&quot;age&quot;</span>&gt;<span class="number">30</span>).show</span><br><span class="line">+---+---+-------+</span><br><span class="line">|age| id|   name|</span><br><span class="line">+---+---+-------+</span><br><span class="line">| <span class="number">40</span>|  <span class="number">3</span>| wangwu|</span><br><span class="line">| <span class="number">50</span>|  <span class="number">4</span>|zhaoliu|</span><br><span class="line">+---+---+-------+</span><br></pre></td></tr></table></figure></li>
<li>按照”age”分组，查看数据条数 <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; df.groupBy(<span class="string">&quot;age&quot;</span>).count.show</span><br><span class="line">+---+-----+</span><br><span class="line">|age|count|</span><br><span class="line">+---+-----+</span><br><span class="line">| <span class="number">50</span>|    <span class="number">1</span>|</span><br><span class="line">| <span class="number">30</span>|    <span class="number">1</span>|</span><br><span class="line">| <span class="number">20</span>|    <span class="number">1</span>|</span><br><span class="line">| <span class="number">40</span>|    <span class="number">1</span>|</span><br><span class="line">+---+-----+</span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="2-2-4-RDD转换为DataFrame"><a href="#2-2-4-RDD转换为DataFrame" class="headerlink" title="2.2.4 RDD转换为DataFrame"></a>2.2.4 RDD转换为DataFrame</h3><ul>
<li>在IDEA中开发程序时，如果需要RDD与DF或者DS之间互相操作，那么需要引入 <code>import spark.implicits._</code><ul>
<li>这里的spark不是Scala中的包名，而是创建的sparkSession对象的变量名称，所以必须先创建SparkSession对象再导入。这里的spark对象不能使用var声明，因为Scala只支持val修饰的对象的引入。</li>
</ul>
</li>
<li>spark-shell中无需导入，自动完成此操作。  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> idRDD = sc.textFile(<span class="string">&quot;data/id.txt&quot;</span>)</span><br><span class="line">scala&gt; idRDD.toDF(<span class="string">&quot;id&quot;</span>).show</span><br><span class="line">+---+</span><br><span class="line">| id|</span><br><span class="line">+---+</span><br><span class="line">|  <span class="number">1</span>|</span><br><span class="line">|  <span class="number">2</span>|</span><br><span class="line">|  <span class="number">3</span>|</span><br><span class="line">|  <span class="number">4</span>|</span><br><span class="line">+---+</span><br></pre></td></tr></table></figure></li>
<li>实际开发中，一般通过样例类将RDD转换为DataFrame  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">User</span>(<span class="params">name:<span class="type">String</span>, age:<span class="type">Int</span></span>)</span></span><br><span class="line">defined <span class="class"><span class="keyword">class</span> <span class="title">User</span></span></span><br><span class="line">scala&gt; sc.makeRDD(<span class="type">List</span>((<span class="string">&quot;zhangsan&quot;</span>,<span class="number">30</span>), (<span class="string">&quot;lisi&quot;</span>,<span class="number">40</span>))).map(t=&gt;<span class="type">User</span>(t._1, t._2)).toDF.show</span><br><span class="line">+--------+---+</span><br><span class="line">|     name|age|</span><br><span class="line">+--------+---+</span><br><span class="line">|zhangsan| <span class="number">30</span>|</span><br><span class="line">|    lisi| <span class="number">40</span>|</span><br><span class="line">+--------+---+</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="2-2-5-DataFrame转换为RDD"><a href="#2-2-5-DataFrame转换为RDD" class="headerlink" title="2.2.5 DataFrame转换为RDD"></a>2.2.5 DataFrame转换为RDD</h3><p>DataFrame其实就是对RDD的封装，所以可以直接获取内部的RDD</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> df = sc.makeRDD(<span class="type">List</span>((<span class="string">&quot;zhangsan&quot;</span>,<span class="number">30</span>), (<span class="string">&quot;lisi&quot;</span>,<span class="number">40</span>))).map(t=&gt;<span class="type">User</span>(t._1, t._2)).toDF</span><br><span class="line">df: org.apache.spark.sql.<span class="type">DataFrame</span> = [name: string, age: int]</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> rdd = df.rdd</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[org.apache.spark.sql.<span class="type">Row</span>] = <span class="type">MapPartitionsRDD</span>[<span class="number">46</span>] at rdd at &lt;console&gt;:<span class="number">25</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> array = rdd.collect</span><br><span class="line">array: <span class="type">Array</span>[org.apache.spark.sql.<span class="type">Row</span>] = <span class="type">Array</span>([zhangsan,<span class="number">30</span>], [lisi,<span class="number">40</span>])</span><br></pre></td></tr></table></figure>
<blockquote>
<p>注意：此时得到的RDD存储类型为Row</p>
</blockquote>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; array(<span class="number">0</span>)</span><br><span class="line">res28: org.apache.spark.sql.<span class="type">Row</span> = [zhangsan,<span class="number">30</span>]</span><br><span class="line">scala&gt; array(<span class="number">0</span>)(<span class="number">0</span>)</span><br><span class="line">res29: <span class="type">Any</span> = zhangsan</span><br><span class="line">scala&gt; array(<span class="number">0</span>).getAs[<span class="type">String</span>](<span class="string">&quot;name&quot;</span>)</span><br><span class="line">res30: <span class="type">String</span> = zhangsan</span><br></pre></td></tr></table></figure>

<h2 id="2-3-DataSet"><a href="#2-3-DataSet" class="headerlink" title="2.3 DataSet"></a>2.3 DataSet</h2><p>DataSet是具有强类型的数据集合，需要提供对应的类型信息。</p>
<h3 id="2-3-1-创建DataSet"><a href="#2-3-1-创建DataSet" class="headerlink" title="2.3.1 创建DataSet"></a>2.3.1 创建DataSet</h3><ol>
<li>使用样例类序列创建DataSet <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Person</span>(<span class="params">name: <span class="type">String</span>, age: <span class="type">Long</span></span>)</span></span><br><span class="line">defined <span class="class"><span class="keyword">class</span> <span class="title">Person</span></span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> caseClassDS = <span class="type">Seq</span>(<span class="type">Person</span>(<span class="string">&quot;zhangsan&quot;</span>,<span class="number">2</span>)).toDS()</span><br><span class="line"></span><br><span class="line">caseClassDS: org.apache.spark.sql.<span class="type">Dataset</span>[<span class="type">Person</span>] = [name: string, age: <span class="type">Long</span>]</span><br><span class="line"></span><br><span class="line">scala&gt; caseClassDS.show</span><br><span class="line">+---------+---+</span><br><span class="line">|     name|age|</span><br><span class="line">+---------+---+</span><br><span class="line">| zhangsan|  <span class="number">2</span>|</span><br><span class="line">+---------+---+</span><br></pre></td></tr></table></figure></li>
<li>使用基本类型的序列创建DataSet <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> ds = <span class="type">Seq</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>).toDS</span><br><span class="line">ds: org.apache.spark.sql.<span class="type">Dataset</span>[<span class="type">Int</span>] = [value: int]</span><br><span class="line"></span><br><span class="line">scala&gt; ds.show</span><br><span class="line">+-----+</span><br><span class="line">|value|</span><br><span class="line">+-----+</span><br><span class="line">|    <span class="number">1</span>|</span><br><span class="line">|    <span class="number">2</span>|</span><br><span class="line">|    <span class="number">3</span>|</span><br><span class="line">|    <span class="number">4</span>|</span><br><span class="line">|    <span class="number">5</span>|</span><br><span class="line">+-----+</span><br></pre></td></tr></table></figure>
<blockquote>
<p>注意：在实际使用的时候，很少用到把序列转换成DataSet，更多的是通过RDD来得到DataSet</p>
</blockquote>
</li>
</ol>
<h3 id="2-3-2-RDD转换为DataSet"><a href="#2-3-2-RDD转换为DataSet" class="headerlink" title="2.3.2 RDD转换为DataSet"></a>2.3.2 RDD转换为DataSet</h3><p>SparkSQL能够自动将包含有case类的RDD转换成DataSet，case类定义了table的结构，case类属性通过反射变成了表的列名。Case类可以包含诸如Seq或者Array等复杂的结构。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">User</span>(<span class="params">name:<span class="type">String</span>, age:<span class="type">Int</span></span>)</span></span><br><span class="line">defined <span class="class"><span class="keyword">class</span> <span class="title">User</span></span></span><br><span class="line"></span><br><span class="line">scala&gt; sc.makeRDD(<span class="type">List</span>((<span class="string">&quot;zhangsan&quot;</span>,<span class="number">30</span>), (<span class="string">&quot;lisi&quot;</span>,<span class="number">49</span>))).map(t=&gt;<span class="type">User</span>(t._1, t._2)).toDS</span><br><span class="line">res11: org.apache.spark.sql.<span class="type">Dataset</span>[<span class="type">User</span>] = [name: string, age: int]</span><br></pre></td></tr></table></figure>

<h3 id="2-3-3-DataSet转换为RDD"><a href="#2-3-3-DataSet转换为RDD" class="headerlink" title="2.3.3 DataSet转换为RDD"></a>2.3.3 DataSet转换为RDD</h3><p>DataSet其实也是对RDD的封装，所以可以直接获取内部的RDD</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">User</span>(<span class="params">name:<span class="type">String</span>, age:<span class="type">Int</span></span>)</span></span><br><span class="line">defined <span class="class"><span class="keyword">class</span> <span class="title">User</span></span></span><br><span class="line"></span><br><span class="line">scala&gt; sc.makeRDD(<span class="type">List</span>((<span class="string">&quot;zhangsan&quot;</span>,<span class="number">30</span>), (<span class="string">&quot;lisi&quot;</span>,<span class="number">49</span>))).map(t=&gt;<span class="type">User</span>(t._1, t._2)).toDS</span><br><span class="line">res11: org.apache.spark.sql.<span class="type">Dataset</span>[<span class="type">User</span>] = [name: string, age: int]</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> rdd = res11.rdd</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">User</span>] = <span class="type">MapPartitionsRDD</span>[<span class="number">51</span>] at rdd at &lt;console&gt;:<span class="number">25</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd.collect</span><br><span class="line">res12: <span class="type">Array</span>[<span class="type">User</span>] = <span class="type">Array</span>(<span class="type">User</span>(zhangsan,<span class="number">30</span>), <span class="type">User</span>(lisi,<span class="number">49</span>))</span><br></pre></td></tr></table></figure>

<h2 id="2-4-DataFrame和DataSet转换"><a href="#2-4-DataFrame和DataSet转换" class="headerlink" title="2.4 DataFrame和DataSet转换"></a>2.4 DataFrame和DataSet转换</h2><p>DataFrame其实是DataSet的特例，所以它们之间是可以互相转换的。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">type</span> <span class="title">DataFrame</span> </span>= <span class="type">Dataset</span>[<span class="type">Row</span>]</span><br></pre></td></tr></table></figure>
<ul>
<li>DataFrame转换为DataSet  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">User</span>(<span class="params">name:<span class="type">String</span>, age:<span class="type">Int</span></span>)</span></span><br><span class="line">defined <span class="class"><span class="keyword">class</span> <span class="title">User</span></span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> df = sc.makeRDD(<span class="type">List</span>((<span class="string">&quot;zhangsan&quot;</span>,<span class="number">30</span>), (<span class="string">&quot;lisi&quot;</span>,<span class="number">49</span>))).toDF(<span class="string">&quot;name&quot;</span>,<span class="string">&quot;age&quot;</span>)</span><br><span class="line">df: org.apache.spark.sql.<span class="type">DataFrame</span> = [name: string, age: int]</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> ds = df.as[<span class="type">User</span>]</span><br><span class="line">ds: org.apache.spark.sql.<span class="type">Dataset</span>[<span class="type">User</span>] = [name: string, age: int]</span><br></pre></td></tr></table></figure></li>
<li>DataSet转换为DataFrame  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> ds = df.as[<span class="type">User</span>]</span><br><span class="line">ds: org.apache.spark.sql.<span class="type">Dataset</span>[<span class="type">User</span>] = [name: string, age: int]</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> df = ds.toDF</span><br><span class="line">df: org.apache.spark.sql.<span class="type">DataFrame</span> = [name: string, age: int]</span><br></pre></td></tr></table></figure></li>
</ul>
<h2 id="2-5-RDD、DataFrame、DataSet三者的关系"><a href="#2-5-RDD、DataFrame、DataSet三者的关系" class="headerlink" title="2.5 RDD、DataFrame、DataSet三者的关系"></a>2.5 RDD、DataFrame、DataSet三者的关系</h2><p>在SparkSQL中Spark为我们提供了两个新的抽象，分别是DataFrame和DataSet。他们和RDD有什么区别呢？首先从版本的产生上来看：</p>
<ul>
<li>Spark1.0 =&gt; RDD </li>
<li>Spark1.3 =&gt; DataFrame</li>
<li>Spark1.6 =&gt; Dataset<br>如果同样的数据都给到这三个数据结构，他们分别计算之后，都会给出相同的结果。不同是的他们的执行效率和执行方式。在后期的Spark版本中，DataSet有可能会逐步取代RDD和DataFrame成为唯一的API接口。</li>
</ul>
<h3 id="2-5-1-三者的共性"><a href="#2-5-1-三者的共性" class="headerlink" title="2.5.1 三者的共性"></a>2.5.1 三者的共性</h3><ul>
<li>RDD、DataFrame、DataSet全都是spark平台下的分布式弹性数据集，为处理超大型数据提供便利;</li>
<li>三者都有惰性机制，在进行创建、转换，如map方法时，不会立即执行，只有在遇到Action如foreach时，三者才会开始遍历运算;</li>
<li>三者有许多共同的函数，如filter，排序等;</li>
<li>在对DataFrame和Dataset进行操作许多操作都需要这个包:import spark.implicits._（在创建好SparkSession对象后尽量直接导入）</li>
<li>三者都会根据 Spark 的内存情况自动缓存运算，这样即使数据量很大，也不用担心会内存溢出</li>
<li>三者都有partition的概念</li>
<li>DataFrame和DataSet均可使用模式匹配获取各个字段的值和类型</li>
</ul>
<h3 id="2-5-2-三者的区别"><a href="#2-5-2-三者的区别" class="headerlink" title="2.5.2 三者的区别"></a>2.5.2 三者的区别</h3><ol>
<li>RDD<ul>
<li>RDD一般和spark mllib同时使用</li>
<li>RDD不支持sparksql操作</li>
</ul>
</li>
<li>DataFrame<ul>
<li>与RDD和Dataset不同，DataFrame每一行的类型固定为Row，每一列的值没法直接访问，只有通过解析才能获取各个字段的值</li>
<li>DataFrame与DataSet一般不与 spark mllib 同时使用</li>
<li>DataFrame与DataSet均支持 SparkSQL 的操作，比如select，groupby之类，还能注册临时表/视窗，进行 sql 语句操作</li>
<li>DataFrame与DataSet支持一些特别方便的保存方式，比如保存成csv，可以带上表头，这样每一列的字段名一目了然(后面专门讲解)</li>
</ul>
</li>
<li>DataSet<ul>
<li>Dataset和DataFrame拥有完全相同的成员函数，区别只是每一行的数据类型不同。 DataFrame其实就是DataSet的一个特例  type DataFrame = Dataset[Row]</li>
<li>DataFrame也可以叫Dataset[Row],每一行的类型是Row，不解析，每一行究竟有哪些字段，各个字段又是什么类型都无从得知，只能用上面提到的getAS方法或者共性中的第七条提到的模式匹配拿出特定字段。而Dataset中，每一行是什么类型是不一定的，在自定义了case class之后可以很自由的获得每一行的信息</li>
</ul>
</li>
</ol>
<h3 id="2-5-3-三者的互相转换"><a href="#2-5-3-三者的互相转换" class="headerlink" title="2.5.3 三者的互相转换"></a>2.5.3 三者的互相转换</h3><p><img src="https://s2.loli.net/2021/12/13/Xl9isUamgIWY7MR.jpg"></p>
<h2 id="2-6-IDEA开发SparkSQL"><a href="#2-6-IDEA开发SparkSQL" class="headerlink" title="2.6 IDEA开发SparkSQL"></a>2.6 IDEA开发SparkSQL</h2><p>实际开发中，都是使用IDEA进行开发的。</p>
<h3 id="2-6-1-添加依赖"><a href="#2-6-1-添加依赖" class="headerlink" title="2.6.1 添加依赖"></a>2.6.1 添加依赖</h3><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-sql_2.12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.0.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h3 id="2-6-2-代码实现"><a href="#2-6-2-代码实现" class="headerlink" title="2.6.2 代码实现"></a>2.6.2 代码实现</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SparkSQL01_Demo</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">//创建上下文环境配置对象</span></span><br><span class="line">    <span class="keyword">val</span> conf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">&quot;local[*]&quot;</span>).setAppName(<span class="string">&quot;SparkSQL01_Demo&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//创建SparkSession对象</span></span><br><span class="line">    <span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span>.builder().config(conf).getOrCreate()</span><br><span class="line">    <span class="comment">//RDD=&gt;DataFrame=&gt;DataSet转换需要引入隐式转换规则，否则无法转换</span></span><br><span class="line">    <span class="comment">//spark不是包名，是上下文环境对象名</span></span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line">    <span class="comment">//读取json文件 创建DataFrame  &#123;&quot;username&quot;: &quot;lisi&quot;,&quot;age&quot;: 18&#125;</span></span><br><span class="line">    <span class="keyword">val</span> df: <span class="type">DataFrame</span> = spark.read.json(<span class="string">&quot;input/test.json&quot;</span>)</span><br><span class="line">    <span class="comment">//df.show()</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">//SQL风格语法</span></span><br><span class="line">    df.createOrReplaceTempView(<span class="string">&quot;user&quot;</span>)</span><br><span class="line">    <span class="comment">//spark.sql(&quot;select avg(age) from user&quot;).show</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">//DSL风格语法</span></span><br><span class="line">    <span class="comment">//df.select(&quot;username&quot;,&quot;age&quot;).show()</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">//*****RDD=&gt;DataFrame=&gt;DataSet*****</span></span><br><span class="line">    <span class="comment">//RDD</span></span><br><span class="line">    <span class="keyword">val</span> rdd1: <span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">String</span>, <span class="type">Int</span>)] = spark.sparkContext.makeRDD(<span class="type">List</span>((<span class="number">1</span>,<span class="string">&quot;zhangsan&quot;</span>,<span class="number">30</span>),(<span class="number">2</span>,<span class="string">&quot;lisi&quot;</span>,<span class="number">28</span>),(<span class="number">3</span>,<span class="string">&quot;wangwu&quot;</span>,<span class="number">20</span>)))</span><br><span class="line"></span><br><span class="line">    <span class="comment">//DataFrame</span></span><br><span class="line">    <span class="keyword">val</span> df1: <span class="type">DataFrame</span> = rdd1.toDF(<span class="string">&quot;id&quot;</span>,<span class="string">&quot;name&quot;</span>,<span class="string">&quot;age&quot;</span>)</span><br><span class="line">    <span class="comment">//df1.show()</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">//DateSet</span></span><br><span class="line">    <span class="keyword">val</span> ds1: <span class="type">Dataset</span>[<span class="type">User</span>] = df1.as[<span class="type">User</span>]</span><br><span class="line">    <span class="comment">//ds1.show()</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">//*****DataSet=&gt;DataFrame=&gt;RDD*****</span></span><br><span class="line">    <span class="comment">//DataFrame</span></span><br><span class="line">    <span class="keyword">val</span> df2: <span class="type">DataFrame</span> = ds1.toDF()</span><br><span class="line"></span><br><span class="line">    <span class="comment">//RDD  返回的RDD类型为Row，里面提供的getXXX方法可以获取字段值，类似jdbc处理结果集，但是索引从0开始</span></span><br><span class="line">    <span class="keyword">val</span> rdd2: <span class="type">RDD</span>[<span class="type">Row</span>] = df2.rdd</span><br><span class="line">    <span class="comment">//rdd2.foreach(a=&gt;println(a.getString(1)))</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">//*****RDD=&gt;DataSet*****</span></span><br><span class="line">    rdd1.map&#123;</span><br><span class="line">      <span class="keyword">case</span> (id,name,age)=&gt;<span class="type">User</span>(id,name,age)</span><br><span class="line">    &#125;.toDS()</span><br><span class="line"></span><br><span class="line">    <span class="comment">//*****DataSet=&gt;=&gt;RDD*****</span></span><br><span class="line">    ds1.rdd</span><br><span class="line"></span><br><span class="line">    <span class="comment">//释放资源</span></span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">User</span>(<span class="params">id:<span class="type">Int</span>,name:<span class="type">String</span>,age:<span class="type">Int</span></span>)</span></span><br></pre></td></tr></table></figure>

<h2 id="2-7-用户自定义函数"><a href="#2-7-用户自定义函数" class="headerlink" title="2.7 用户自定义函数"></a>2.7 用户自定义函数</h2><p>用户可以通过spark.udf功能添加自定义函数，实现自定义功能。</p>
<h3 id="2-7-1-UDF"><a href="#2-7-1-UDF" class="headerlink" title="2.7.1 UDF"></a>2.7.1 UDF</h3><ol>
<li>创建DataFrame <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> df = spark.read.json(<span class="string">&quot;data/user.json&quot;</span>)</span><br><span class="line">df: org.apache.spark.sql.<span class="type">DataFrame</span> = [age: bigint， username: string]</span><br></pre></td></tr></table></figure></li>
<li>注册UDF <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; spark.udf.register(<span class="string">&quot;addName&quot;</span>,(x:<span class="type">String</span>)=&gt; <span class="string">&quot;Name:&quot;</span>+x)</span><br><span class="line">res9: org.apache.spark.sql.expressions.<span class="type">UserDefinedFunction</span> = <span class="type">UserDefinedFunction</span>(&lt;function1&gt;,<span class="type">StringType</span>,<span class="type">Some</span>(<span class="type">List</span>(<span class="type">StringType</span>)))</span><br></pre></td></tr></table></figure></li>
<li>创建临时表 <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; df.createOrReplaceTempView(<span class="string">&quot;people&quot;</span>)</span><br></pre></td></tr></table></figure></li>
<li>应用UDF <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; spark.sql(<span class="string">&quot;Select addName(name),age from people&quot;</span>).show()</span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="2-7-2-UDAF"><a href="#2-7-2-UDAF" class="headerlink" title="2.7.2 UDAF"></a>2.7.2 UDAF</h3><p>强类型的Dataset和弱类型的DataFrame都提供了相关的聚合函数， 如 count()，countDistinct()，avg()，max()，min()。除此之外，用户可以设定自己的自定义聚合函数。通过继承UserDefinedAggregateFunction来实现用户自定义弱类型聚合函数。从Spark3.0版本后，UserDefinedAggregateFunction已经不推荐使用了。可以统一采用强类型聚合函数Aggregator<br>需求：计算平均工资<br>一个需求可以采用很多种不同的方法实现需求</p>
<ol>
<li><p>实现方式 - RDD</p>
 <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> conf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">&quot;app&quot;</span>).setMaster(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> sc: <span class="type">SparkContext</span> = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"><span class="keyword">val</span> res: (<span class="type">Int</span>, <span class="type">Int</span>) = sc.makeRDD(<span class="type">List</span>((<span class="string">&quot;zhangsan&quot;</span>, <span class="number">20</span>), (<span class="string">&quot;lisi&quot;</span>, <span class="number">30</span>), (<span class="string">&quot;wangw&quot;</span>, <span class="number">40</span>))).map &#123;</span><br><span class="line">  <span class="keyword">case</span> (name, age) =&gt; &#123;</span><br><span class="line">    (age, <span class="number">1</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;.reduce &#123;</span><br><span class="line">  (t1, t2) =&gt; &#123;</span><br><span class="line">    (t1._1 + t2._1, t1._2 + t2._2)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">println(res._1/res._2)</span><br><span class="line"><span class="comment">// 关闭连接</span></span><br><span class="line">sc.stop()</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
<li><p>实现方式 - 累加器</p>
 <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyAC</span> <span class="keyword">extends</span> <span class="title">AccumulatorV2</span>[<span class="type">Int</span>,<span class="type">Int</span>]</span>&#123;</span><br><span class="line">  <span class="keyword">var</span> sum:<span class="type">Int</span> = <span class="number">0</span></span><br><span class="line">  <span class="keyword">var</span> count:<span class="type">Int</span> = <span class="number">0</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">isZero</span></span>: <span class="type">Boolean</span> = &#123;</span><br><span class="line">    <span class="keyword">return</span> sum ==<span class="number">0</span> &amp;&amp; count == <span class="number">0</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">copy</span></span>(): <span class="type">AccumulatorV2</span>[<span class="type">Int</span>, <span class="type">Int</span>] = &#123;</span><br><span class="line">    <span class="keyword">val</span> newMyAc = <span class="keyword">new</span> <span class="type">MyAC</span></span><br><span class="line">    newMyAc.sum = <span class="keyword">this</span>.sum</span><br><span class="line">    newMyAc.count = <span class="keyword">this</span>.count</span><br><span class="line">    newMyAc</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">reset</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">    sum =<span class="number">0</span></span><br><span class="line">    count = <span class="number">0</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">add</span></span>(v: <span class="type">Int</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    sum += v</span><br><span class="line">    count += <span class="number">1</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">merge</span></span>(other: <span class="type">AccumulatorV2</span>[<span class="type">Int</span>, <span class="type">Int</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    other <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> o:<span class="type">MyAC</span>=&gt;&#123;</span><br><span class="line">        sum += o.sum</span><br><span class="line">        count += o.count</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">case</span> _=&gt;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">value</span></span>: <span class="type">Int</span> = sum/count</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li><p>实现方式 - UDAF - 弱类型</p>
 <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">udaf_avg</span> </span>= &#123;</span><br><span class="line">    println(<span class="string">&quot;*********udf**********&quot;</span>)</span><br><span class="line">    <span class="comment">//</span></span><br><span class="line">    <span class="comment">//val spark = new SparkSession()</span></span><br><span class="line">    <span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span>.builder()</span><br><span class="line">        .master(<span class="string">&quot;local&quot;</span>)</span><br><span class="line">        .appName(<span class="string">&quot;Word Count&quot;</span>)</span><br><span class="line">        .getOrCreate()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> userDF: <span class="type">DataFrame</span> = spark.read.json(<span class="string">&quot;data/user.json&quot;</span>)</span><br><span class="line">    println(<span class="string">&quot;*********userDF.show**********&quot;</span>)</span><br><span class="line">    userDF.show</span><br><span class="line"></span><br><span class="line">    userDF.createOrReplaceTempView(<span class="string">&quot;user&quot;</span>)</span><br><span class="line"></span><br><span class="line">    println(<span class="string">&quot;*********select name from user**********&quot;</span>)</span><br><span class="line">    spark.sql(<span class="string">&quot;select id,age,name from user&quot;</span>).show</span><br><span class="line"></span><br><span class="line">    <span class="comment">//SQL中调用自定义函数 UDAF</span></span><br><span class="line">    spark.udf.register(<span class="string">&quot;udaf_avg&quot;</span>,functions.udaf(<span class="keyword">new</span> <span class="type">MyAveragUDAF</span>))</span><br><span class="line">    spark.sql(<span class="string">&quot;select udaf_avg(age) from user&quot;</span>).show</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    spark.stop()</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">定义类继承UserDefinedAggregateFunction，并重写其中方法</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyAveragUDAF</span> <span class="keyword">extends</span> <span class="title">UserDefinedAggregateFunction</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 聚合函数输入参数的数据类型</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">inputSchema</span></span>: <span class="type">StructType</span> = <span class="type">StructType</span>(<span class="type">Array</span>(<span class="type">StructField</span>(<span class="string">&quot;age&quot;</span>,<span class="type">IntegerType</span>)))</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 聚合函数缓冲区中值的数据类型(age,count)</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">bufferSchema</span></span>: <span class="type">StructType</span> = &#123;</span><br><span class="line">    <span class="type">StructType</span>(<span class="type">Array</span>(<span class="type">StructField</span>(<span class="string">&quot;sum&quot;</span>,<span class="type">LongType</span>),<span class="type">StructField</span>(<span class="string">&quot;count&quot;</span>,<span class="type">LongType</span>)))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 函数返回值的数据类型</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">dataType</span></span>: <span class="type">DataType</span> = <span class="type">DoubleType</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// 稳定性：对于相同的输入是否一直返回相同的输出。</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">deterministic</span></span>: <span class="type">Boolean</span> = <span class="literal">true</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// 函数缓冲区初始化</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">initialize</span></span>(buffer: <span class="type">MutableAggregationBuffer</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// 存年龄的总和</span></span><br><span class="line">    buffer(<span class="number">0</span>) = <span class="number">0</span>L</span><br><span class="line">    <span class="comment">// 存年龄的个数</span></span><br><span class="line">    buffer(<span class="number">1</span>) = <span class="number">0</span>L</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 更新缓冲区中的数据</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">update</span></span>(buffer: <span class="type">MutableAggregationBuffer</span>,input: <span class="type">Row</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">if</span> (!input.isNullAt(<span class="number">0</span>)) &#123;</span><br><span class="line">      buffer(<span class="number">0</span>) = buffer.getLong(<span class="number">0</span>) + input.getInt(<span class="number">0</span>)</span><br><span class="line">      buffer(<span class="number">1</span>) = buffer.getLong(<span class="number">1</span>) + <span class="number">1</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 合并缓冲区</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">merge</span></span>(buffer1: <span class="type">MutableAggregationBuffer</span>,buffer2: <span class="type">Row</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    buffer1(<span class="number">0</span>) = buffer1.getLong(<span class="number">0</span>) + buffer2.getLong(<span class="number">0</span>)</span><br><span class="line">    buffer1(<span class="number">1</span>) = buffer1.getLong(<span class="number">1</span>) + buffer2.getLong(<span class="number">1</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 计算最终结果</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">evaluate</span></span>(buffer: <span class="type">Row</span>): <span class="type">Double</span> = buffer.getLong(<span class="number">0</span>).toDouble / buffer.getLong(<span class="number">1</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li><p>实现方式 - UDAF - 强类型</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">udaf_avg</span> </span>= &#123;</span><br><span class="line">    println(<span class="string">&quot;*********udf**********&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span>.builder()</span><br><span class="line">        .master(<span class="string">&quot;local&quot;</span>)</span><br><span class="line">        .appName(<span class="string">&quot;Word Count&quot;</span>)</span><br><span class="line">        .getOrCreate()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">val</span> df: <span class="type">DataFrame</span> = spark.read.json(<span class="string">&quot;data/user.json&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//封装为DataSet</span></span><br><span class="line">    <span class="keyword">val</span> ds: <span class="type">Dataset</span>[<span class="type">User01</span>] = df.as[<span class="type">User01</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//创建聚合函数</span></span><br><span class="line">    <span class="keyword">var</span> myAgeUdaf1 = <span class="keyword">new</span> <span class="type">MyAveragUDAF1</span></span><br><span class="line">    <span class="comment">//将聚合函数转换为查询的列</span></span><br><span class="line">    <span class="keyword">val</span> col: <span class="type">TypedColumn</span>[<span class="type">User01</span>, <span class="type">Double</span>] = myAgeUdaf1.toColumn</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//查询</span></span><br><span class="line">    ds.select(col).show()</span><br><span class="line">    <span class="type">Spark3</span><span class="number">.0</span>版本可以采用强类型的<span class="type">Aggregate</span>方式代替<span class="type">UserDefinedAggregateFunction</span> </span><br><span class="line">    <span class="comment">// TODO 创建UDAF函数</span></span><br><span class="line">    <span class="keyword">val</span> udaf = <span class="keyword">new</span> <span class="type">MyAvgAgeUDAF</span></span><br><span class="line">    <span class="comment">// TODO 注册到SparkSQL中</span></span><br><span class="line">    spark.udf.register(<span class="string">&quot;avgAge&quot;</span>, functions.udaf(udaf))</span><br><span class="line">    <span class="comment">// TODO 在SQL中使用聚合函数</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 定义用户的自定义聚合函数</span></span><br><span class="line">    spark.sql(<span class="string">&quot;select avgAge(age) from user&quot;</span>).show</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// **************************************************</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Buff</span>(<span class="params"> var sum:<span class="type">Long</span>, var cnt:<span class="type">Long</span> </span>)</span></span><br><span class="line">    <span class="comment">// totalage, count</span></span><br><span class="line">    <span class="class"><span class="keyword">class</span> <span class="title">MyAvgAgeUDAF</span> <span class="keyword">extends</span> <span class="title">Aggregator</span>[<span class="type">Long</span>, <span class="type">Buff</span>, <span class="type">Double</span>]</span>&#123;</span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">zero</span></span>: <span class="type">Buff</span> = <span class="type">Buff</span>(<span class="number">0</span>,<span class="number">0</span>)</span><br><span class="line">    </span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">reduce</span></span>(b: <span class="type">Buff</span>, a: <span class="type">Long</span>): <span class="type">Buff</span> = &#123;</span><br><span class="line">            b.sum += a</span><br><span class="line">            b.cnt += <span class="number">1</span></span><br><span class="line">            b</span><br><span class="line">        &#125;</span><br><span class="line">    </span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">merge</span></span>(b1: <span class="type">Buff</span>, b2: <span class="type">Buff</span>): <span class="type">Buff</span> = &#123;</span><br><span class="line">            b1.sum += b2.sum</span><br><span class="line">            b1.cnt += b2.cnt</span><br><span class="line">            b1</span><br><span class="line">        &#125;</span><br><span class="line">    </span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">finish</span></span>(reduction: <span class="type">Buff</span>): <span class="type">Double</span> = &#123;</span><br><span class="line">            reduction.sum.toDouble/reduction.cnt</span><br><span class="line">        &#125;</span><br><span class="line">    </span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">bufferEncoder</span></span>: <span class="type">Encoder</span>[<span class="type">Buff</span>] = <span class="type">Encoders</span>.product</span><br><span class="line">    </span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">outputEncoder</span></span>: <span class="type">Encoder</span>[<span class="type">Double</span>] = <span class="type">Encoders</span>.scalaDouble</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    spark.stop()</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">//输入数据类型</span></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">User01</span>(<span class="params">username:<span class="type">String</span>,age:<span class="type">Long</span></span>)</span></span><br><span class="line"><span class="comment">//缓存类型</span></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">AgeBuffer</span>(<span class="params">var sum:<span class="type">Long</span>,var count:<span class="type">Long</span></span>)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * 定义类继承org.apache.spark.sql.expressions.Aggregator</span></span><br><span class="line"><span class="comment">  * 重写类中的方法</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyAveragUDAF1</span> <span class="keyword">extends</span> <span class="title">Aggregator</span>[<span class="type">User01</span>,<span class="type">AgeBuffer</span>,<span class="type">Double</span>]</span>&#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">zero</span></span>: <span class="type">AgeBuffer</span> = &#123;</span><br><span class="line">    <span class="type">AgeBuffer</span>(<span class="number">0</span>L,<span class="number">0</span>L)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">reduce</span></span>(b: <span class="type">AgeBuffer</span>, a: <span class="type">User01</span>): <span class="type">AgeBuffer</span> = &#123;</span><br><span class="line">    b.sum = b.sum + a.age</span><br><span class="line">    b.count = b.count + <span class="number">1</span></span><br><span class="line">    b</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">merge</span></span>(b1: <span class="type">AgeBuffer</span>, b2: <span class="type">AgeBuffer</span>): <span class="type">AgeBuffer</span> = &#123;</span><br><span class="line">    b1.sum = b1.sum + b2.sum</span><br><span class="line">    b1.count = b1.count + b2.count</span><br><span class="line">    b1</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">finish</span></span>(buff: <span class="type">AgeBuffer</span>): <span class="type">Double</span> = &#123;</span><br><span class="line">    buff.sum.toDouble/buff.count</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">//DataSet默认额编解码器，用于序列化，固定写法</span></span><br><span class="line">  <span class="comment">//自定义类型就是product自带类型根据类型选择</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">bufferEncoder</span></span>: <span class="type">Encoder</span>[<span class="type">AgeBuffer</span>] = &#123;</span><br><span class="line">    <span class="type">Encoders</span>.product</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">outputEncoder</span></span>: <span class="type">Encoder</span>[<span class="type">Double</span>] = &#123;</span><br><span class="line">    <span class="type">Encoders</span>.scalaDouble</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="2-8-数据的加载和保存"><a href="#2-8-数据的加载和保存" class="headerlink" title="2.8 数据的加载和保存"></a>2.8 数据的加载和保存</h2><h3 id="2-8-1-通用的加载和保存方式"><a href="#2-8-1-通用的加载和保存方式" class="headerlink" title="2.8.1 通用的加载和保存方式"></a>2.8.1 通用的加载和保存方式</h3><p>SparkSQL提供了通用的保存数据和数据加载的方式。这里的通用指的是使用相同的API，根据不同的参数读取和保存不同格式的数据，SparkSQL默认读取和保存的文件格式为parquet</p>
<ol>
<li><p>加载数据</p>
<ul>
<li><p>spark.read.load 是加载数据的通用方法</p>
  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; spark.read.</span><br><span class="line">csv   format   jdbc   json   load   option   options   orc   parquet   schema   table   text   textFile</span><br></pre></td></tr></table></figure></li>
<li><p>如果读取不同格式的数据，可以对不同的数据格式进行设定</p>
  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; spark.read.format(<span class="string">&quot;…&quot;</span>)[.option(<span class="string">&quot;…&quot;</span>)].load(<span class="string">&quot;…&quot;</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>format(“…”)：指定加载的数据类型，包括”csv”、”jdbc”、”json”、”orc”、”parquet”和”textFile”。</li>
<li>load(“…”)：在”csv”、”jdbc”、”json”、”orc”、”parquet”和”textFile”格式下需要传入加载数据的路径。</li>
<li>option(“…”)：在”jdbc”格式下需要传入JDBC相应参数，url、user、password和dbtable</li>
</ul>
</li>
<li><p>前面都是使用read API 先把文件加载到 DataFrame然后再查询，其实，我们也可以直接在文件上进行查询:  文件格式.<code>文件路径</code></p>
  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt;spark.sql(<span class="string">&quot;select * from json.`/opt/module/data/user.json`&quot;</span>).show</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>保存数据</p>
<ul>
<li><p>df.write.save 是保存数据的通用方法</p>
  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt;df.write.</span><br><span class="line">csv  jdbc   json  orc   parquet textFile… …</span><br></pre></td></tr></table></figure></li>
<li><p>如果保存不同格式的数据，可以对不同的数据格式进行设定</p>
  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt;df.write.format(<span class="string">&quot;…&quot;</span>)[.option(<span class="string">&quot;…&quot;</span>)].save(<span class="string">&quot;…&quot;</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>format(“…”)：指定保存的数据类型，包括”csv”、”jdbc”、”json”、”orc”、”parquet”和”textFile”。</li>
<li>save (“…”)：在”csv”、”orc”、”parquet”和”textFile”格式下需要传入保存数据的路径。</li>
<li>option(“…”)：在”jdbc”格式下需要传入JDBC相应参数，url、user、password和dbtable</li>
</ul>
</li>
<li><p>保存操作可以使用 SaveMode, 用来指明如何处理数据，使用mode()方法来设置。</p>
</li>
<li><p>有一点很重要: 这些 SaveMode 都是没有加锁的, 也不是原子操作。</p>
</li>
<li><p>SaveMode是一个枚举类，其中的常量包括：</p>
<table>
<thead>
<tr>
<th>Scala/Java</th>
<th>Any</th>
<th>Language</th>
<th>Meaning</th>
</tr>
</thead>
<tbody><tr>
<td>SaveMode.ErrorIfExists(default)</td>
<td>“error”(default)</td>
<td>如果文件已经存在则抛出异常</td>
<td></td>
</tr>
<tr>
<td>SaveMode.Append</td>
<td>“append”</td>
<td>如果文件已经存在则追加</td>
<td></td>
</tr>
<tr>
<td>SaveMode.Overwrite</td>
<td>“overwrite”</td>
<td>如果文件已经存在则覆盖</td>
<td></td>
</tr>
<tr>
<td>SaveMode.Ignore</td>
<td>“ignore”</td>
<td>如果文件已经存在则忽略</td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">df.write.mode(<span class="string">&quot;append&quot;</span>).json(<span class="string">&quot;/opt/module/data/output&quot;</span>)</span><br></pre></td></tr></table></figure></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody></table>
</li>
</ul>
</li>
</ol>
<h3 id="2-8-2-Parquet"><a href="#2-8-2-Parquet" class="headerlink" title="2.8.2 Parquet"></a>2.8.2 Parquet</h3><p>Spark SQL的默认数据源为Parquet格式。Parquet是一种能够有效存储嵌套数据的列式存储格式。<br>数据源为Parquet文件时，Spark SQL可以方便的执行所有的操作，不需要使用format。修改配置项spark.sql.sources.default，可修改默认数据源格式。</p>
<ol>
<li>加载数据 <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> df = spark.read.load(<span class="string">&quot;examples/src/main/resources/users.parquet&quot;</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; df.show</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
<li>保存数据 <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; <span class="keyword">var</span> df = spark.read.json(<span class="string">&quot;/opt/module/data/input/people.json&quot;</span>)</span><br><span class="line"><span class="comment">//保存为parquet格式</span></span><br><span class="line">scala&gt; df.write.mode(<span class="string">&quot;append&quot;</span>).save(<span class="string">&quot;/opt/module/data/output&quot;</span>)</span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="2-8-3-JSON"><a href="#2-8-3-JSON" class="headerlink" title="2.8.3 JSON"></a>2.8.3 JSON</h3><p>Spark SQL 能够自动推测JSON数据集的结构，并将它加载为一个Dataset[Row]. 可以通过SparkSession.read.json()去加载JSON 文件。</p>
<blockquote>
<p>注意：Spark读取的JSON文件不是传统的JSON文件，每一行都应该是一个JSON串。格式如下：</p>
</blockquote>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line">&#123;<span class="attr">&quot;name&quot;</span>:<span class="string">&quot;Michael&quot;</span>&#125;</span><br><span class="line">&#123;<span class="attr">&quot;name&quot;</span>:<span class="string">&quot;Andy&quot;</span>, <span class="attr">&quot;age&quot;</span>:<span class="number">30</span>&#125;</span><br><span class="line">[&#123;<span class="attr">&quot;name&quot;</span>:<span class="string">&quot;Justin&quot;</span>, <span class="attr">&quot;age&quot;</span>:<span class="number">19</span>&#125;,&#123;<span class="attr">&quot;name&quot;</span>:<span class="string">&quot;Justin&quot;</span>, <span class="attr">&quot;age&quot;</span>:<span class="number">19</span>&#125;]</span><br></pre></td></tr></table></figure>
<ol>
<li>导入隐式转换 <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> spark.implicits._</span><br></pre></td></tr></table></figure></li>
<li>加载JSON文件 <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> path = <span class="string">&quot;/opt/module/spark-local/people.json&quot;</span></span><br><span class="line"><span class="keyword">val</span> peopleDF = spark.read.json(path)</span><br></pre></td></tr></table></figure></li>
<li>创建临时表 <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">peopleDF.createOrReplaceTempView(<span class="string">&quot;people&quot;</span>)</span><br></pre></td></tr></table></figure></li>
<li>数据查询 <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> teenagerNamesDF = spark.sql(<span class="string">&quot;SELECT name FROM people WHERE age BETWEEN 13 AND 19&quot;</span>)</span><br><span class="line">teenagerNamesDF.show()</span><br><span class="line">+------+</span><br><span class="line">|  name|</span><br><span class="line">+------+</span><br><span class="line">|<span class="type">Justin</span>|</span><br><span class="line">+------+</span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="2-8-4-CSV"><a href="#2-8-4-CSV" class="headerlink" title="2.8.4 CSV"></a>2.8.4 CSV</h3><p>Spark SQL可以配置CSV文件的列表信息，读取CSV文件,CSV文件的第一行设置为数据列</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">spark.read</span><br><span class="line">    .format(<span class="string">&quot;csv&quot;</span>)</span><br><span class="line">    .option(<span class="string">&quot;sep&quot;</span>, <span class="string">&quot;;&quot;</span>)</span><br><span class="line">    .option(<span class="string">&quot;inferSchema&quot;</span>, <span class="string">&quot;true&quot;</span>)</span><br><span class="line">    .option(<span class="string">&quot;header&quot;</span>, <span class="string">&quot;true&quot;</span>)</span><br><span class="line">    .load(<span class="string">&quot;data/user.csv&quot;</span>)</span><br></pre></td></tr></table></figure>

<h3 id="2-8-5-MySQL"><a href="#2-8-5-MySQL" class="headerlink" title="2.8.5 MySQL"></a>2.8.5 MySQL</h3><p>Spark SQL可以通过JDBC从关系型数据库中读取数据的方式创建DataFrame，通过对DataFrame一系列的计算后，还可以将数据再写回关系型数据库中。如果使用spark-shell操作，可在启动shell时指定相关的数据库驱动路径或者将相关的数据库驱动放到spark的类路径下。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">bin/spark-shell </span><br><span class="line">--jars mysql-connector-java-5.1.27-bin.jar</span><br></pre></td></tr></table></figure>
<p>我们这里只演示在Idea中通过JDBC对Mysql进行操作</p>
<ol>
<li><p>导入依赖</p>
 <figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>mysql<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>mysql-connector-java<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>5.1.27<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure></li>
<li><p>读取数据</p>
 <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> conf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">&quot;local[*]&quot;</span>).setAppName(<span class="string">&quot;SparkSQL&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">//创建SparkSession对象</span></span><br><span class="line"><span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span>.builder().config(conf).getOrCreate()</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line"><span class="comment">//方式1：通用的load方法读取</span></span><br><span class="line">spark.read.format(<span class="string">&quot;jdbc&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;url&quot;</span>, <span class="string">&quot;jdbc:mysql://linux1:3306/spark-sql&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;driver&quot;</span>, <span class="string">&quot;com.mysql.jdbc.Driver&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;user&quot;</span>, <span class="string">&quot;root&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;password&quot;</span>, <span class="string">&quot;123123&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;dbtable&quot;</span>, <span class="string">&quot;user&quot;</span>)</span><br><span class="line">  .load().show</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">//方式2:通用的load方法读取 参数另一种形式</span></span><br><span class="line">spark.read.format(<span class="string">&quot;jdbc&quot;</span>)</span><br><span class="line">  .options(<span class="type">Map</span>(<span class="string">&quot;url&quot;</span>-&gt;<span class="string">&quot;jdbc:mysql://linux1:3306/spark-sql?user=root&amp;password=123123&quot;</span>,</span><br><span class="line">    <span class="string">&quot;dbtable&quot;</span>-&gt;<span class="string">&quot;user&quot;</span>,<span class="string">&quot;driver&quot;</span>-&gt;<span class="string">&quot;com.mysql.jdbc.Driver&quot;</span>)).load().show</span><br><span class="line"></span><br><span class="line"><span class="comment">//方式3:使用jdbc方法读取</span></span><br><span class="line"><span class="keyword">val</span> props: <span class="type">Properties</span> = <span class="keyword">new</span> <span class="type">Properties</span>()</span><br><span class="line">props.setProperty(<span class="string">&quot;user&quot;</span>, <span class="string">&quot;root&quot;</span>)</span><br><span class="line">props.setProperty(<span class="string">&quot;password&quot;</span>, <span class="string">&quot;123123&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> df: <span class="type">DataFrame</span> = spark.read.jdbc(<span class="string">&quot;jdbc:mysql://linux1:3306/spark-sql&quot;</span>, <span class="string">&quot;user&quot;</span>, props)</span><br><span class="line">df.show</span><br><span class="line"></span><br><span class="line"><span class="comment">//释放资源</span></span><br><span class="line">spark.stop()    </span><br></pre></td></tr></table></figure></li>
<li><p>写入数据</p>
 <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">User2</span>(<span class="params">name: <span class="type">String</span>, age: <span class="type">Long</span></span>)</span></span><br><span class="line">。。。</span><br><span class="line"><span class="keyword">val</span> conf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">&quot;local[*]&quot;</span>).setAppName(<span class="string">&quot;SparkSQL&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">//创建SparkSession对象</span></span><br><span class="line"><span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span>.builder().config(conf).getOrCreate()</span><br><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> rdd: <span class="type">RDD</span>[<span class="type">User2</span>] = spark.sparkContext.makeRDD(<span class="type">List</span>(<span class="type">User2</span>(<span class="string">&quot;lisi&quot;</span>, <span class="number">20</span>), <span class="type">User2</span>(<span class="string">&quot;zs&quot;</span>, <span class="number">30</span>)))</span><br><span class="line"><span class="keyword">val</span> ds: <span class="type">Dataset</span>[<span class="type">User2</span>] = rdd.toDS</span><br><span class="line"><span class="comment">//方式1：通用的方式  format指定写出类型</span></span><br><span class="line"> </span><br><span class="line"><span class="comment">//方式2：通过jdbc方法</span></span><br><span class="line"><span class="keyword">val</span> props: <span class="type">Properties</span> = <span class="keyword">new</span> <span class="type">Properties</span>()</span><br><span class="line">props.setProperty(<span class="string">&quot;user&quot;</span>, <span class="string">&quot;root&quot;</span>)</span><br><span class="line">props.setProperty(<span class="string">&quot;password&quot;</span>, <span class="string">&quot;123123&quot;</span>)</span><br><span class="line">ds.write.mode(<span class="type">SaveMode</span>.<span class="type">Append</span>).jdbc(<span class="string">&quot;jdbc:mysql://linux1:3306/spark-sql&quot;</span>, <span class="string">&quot;user&quot;</span>, props)</span><br><span class="line"></span><br><span class="line"><span class="comment">//释放资源</span></span><br><span class="line">spark.stop()</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="2-8-6-Hive"><a href="#2-8-6-Hive" class="headerlink" title="2.8.6 Hive"></a>2.8.6 Hive</h3><ul>
<li>Apache Hive 是 Hadoop 上的 SQL 引擎，Spark SQL编译时可以包含 Hive 支持，也可以不包含。包含 Hive 支持的 Spark SQL 可以支持 Hive 表访问、UDF (用户自定义函数)以及 Hive 查询语言(HiveQL/HQL)等。需要强调的一点是，如果要在 Spark SQL 中包含Hive 的库，并不需要事先安装 Hive。一般来说，最好还是在编译Spark SQL时引入Hive支持，这样就可以使用这些特性了。如果你下载的是二进制版本的 Spark，它应该已经在编译时添加了 Hive 支持。</li>
<li>若要把 Spark SQL 连接到一个部署好的 Hive 上，你必须把 hive-site.xml 复制到 Spark的配置文件目录中($SPARK_HOME/conf)。即使没有部署好 Hive，Spark SQL 也可以运行。 需要注意的是，如果你没有部署好Hive，Spark SQL 会在当前的工作目录中创建出自己的 Hive 元数据仓库，叫作 metastore_db。此外，如果你尝试使用 HiveQL 中的 CREATE TABLE (并非 CREATE EXTERNAL TABLE)语句来创建表，这些表会被放在你默认的文件系统中的 /user/hive/warehouse 目录中(如果你的 classpath 中有配好的 hdfs-site.xml，默认的文件系统就是 HDFS，否则就是本地文件系统)。</li>
<li>spark-shell默认是Hive支持的；代码中是默认不支持的，需要手动指定（加一个参数即可）。</li>
</ul>
<ol>
<li><p>内嵌的HIVE<br> 如果使用 Spark 内嵌的 Hive, 则什么都不用做, 直接使用即可.<br> Hive 的元数据存储在 derby 中, 默认仓库地址:$SPARK_HOME/spark-warehouse</p>
 <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; spark.sql(<span class="string">&quot;show tables&quot;</span>).show</span><br><span class="line">。。。</span><br><span class="line">+--------+---------+-----------+</span><br><span class="line">|database|tableName|isTemporary|</span><br><span class="line">+--------+---------+-----------+</span><br><span class="line">+--------+---------+-----------+</span><br><span class="line"></span><br><span class="line">scala&gt; spark.sql(<span class="string">&quot;create table aa(id int)&quot;</span>)</span><br><span class="line"></span><br><span class="line">。。。</span><br><span class="line"></span><br><span class="line">scala&gt; spark.sql(<span class="string">&quot;show tables&quot;</span>).show</span><br><span class="line">+--------+---------+-----------+</span><br><span class="line">|database|tableName|isTemporary|</span><br><span class="line">+--------+---------+-----------+</span><br><span class="line">| <span class="keyword">default</span>|       aa|      <span class="literal">false</span>|</span><br><span class="line">+--------+---------+-----------+</span><br></pre></td></tr></table></figure>
<p> 向表加载本地数据</p>
 <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; spark.sql(<span class="string">&quot;load data local inpath &#x27;input/ids.txt&#x27; into table aa&quot;</span>)</span><br><span class="line"></span><br><span class="line">。。。</span><br><span class="line"></span><br><span class="line">scala&gt; spark.sql(<span class="string">&quot;select * from aa&quot;</span>).show</span><br><span class="line">+---+</span><br><span class="line">| id|</span><br><span class="line">+---+</span><br><span class="line">|  <span class="number">1</span>|</span><br><span class="line">|  <span class="number">2</span>|</span><br><span class="line">|  <span class="number">3</span>|</span><br><span class="line">|  <span class="number">4</span>|</span><br><span class="line">+---+</span><br></pre></td></tr></table></figure>
<p> 在实际使用中, 几乎没有任何人会使用内置的 Hive</p>
</li>
<li><p>外部的HIVE<br> 如果想连接外部已经部署好的Hive，需要通过以下几个步骤：</p>
<ul>
<li>Spark要接管Hive需要把hive-site.xml拷贝到conf/目录下</li>
<li>把Mysql的驱动copy到jars/目录下</li>
<li>如果访问不到hdfs，则需要把core-site.xml和hdfs-site.xml拷贝到conf/目录下</li>
<li>重启spark-shell  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; spark.sql(<span class="string">&quot;show tables&quot;</span>).show</span><br><span class="line"><span class="number">20</span>/<span class="number">04</span>/<span class="number">25</span> <span class="number">22</span>:<span class="number">05</span>:<span class="number">14</span> <span class="type">WARN</span> <span class="type">ObjectStore</span>: <span class="type">Failed</span> to get database global_temp, returning <span class="type">NoSuchObjectException</span></span><br><span class="line">+--------+--------------------+-----------+</span><br><span class="line">|database|           tableName|isTemporary|</span><br><span class="line">+--------+--------------------+-----------+</span><br><span class="line">| <span class="keyword">default</span>|                 emp|      <span class="literal">false</span>|</span><br><span class="line">| <span class="keyword">default</span>|hive_hbase_emp_table|      <span class="literal">false</span>|</span><br><span class="line">| <span class="keyword">default</span>| relevance_hbase_emp|      <span class="literal">false</span>|</span><br><span class="line">| <span class="keyword">default</span>|          staff_hive|      <span class="literal">false</span>|</span><br><span class="line">| <span class="keyword">default</span>|                 ttt|      <span class="literal">false</span>|</span><br><span class="line">| <span class="keyword">default</span>|   user_visit_action|      <span class="literal">false</span>|</span><br><span class="line">+--------+--------------------+-----------+</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>运行Spark SQL CLI<br> Spark SQL CLI可以很方便的在本地运行Hive元数据服务以及从命令行执行查询任务。在Spark目录下执行如下命令启动Spark SQL CLI，直接执行SQL语句，类似一Hive窗口bin/spark-sql</p>
</li>
<li><p>运行Spark beeline<br> Spark Thrift Server是Spark社区基于HiveServer2实现的一个Thrift服务。旨在无缝兼容HiveServer2。因为Spark Thrift Server的接口和协议都和HiveServer2完全一致，因此我们部署好Spark Thrift Server后，可以直接使用hive的beeline访问Spark Thrift Server执行相关语句。Spark Thrift Server的目的也只是取代HiveServer2，因此它依旧可以和Hive Metastore进行交互，获取到hive的元数据。<br> 如果想连接Thrift Server，需要通过以下几个步骤：</p>
<ul>
<li>Spark要接管Hive需要把hive-site.xml拷贝到conf/目录下</li>
<li>把Mysql的驱动copy到jars/目录下</li>
<li>如果访问不到hdfs，则需要把core-site.xml和hdfs-site.xml拷贝到conf/目录下</li>
<li>启动Thrift Server  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">sbin/start-thriftserver.sh</span><br></pre></td></tr></table></figure></li>
<li>使用beeline连接Thrift Server  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">bin/beeline -u jdbc:hive2://linux1:10000 -n root</span><br></pre></td></tr></table></figure>
  <img src="https://s2.loli.net/2021/12/13/8Brzyl7RJUVqhba.jpg"></li>
</ul>
<p> 如果连接有问题，可以尝试在hive-site.xml文件中增加如下内容：</p>
 <figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.server2.enable.doAs<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li>
<li><p>代码操作Hive</p>
<ol>
<li>导入依赖 <figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-hive_2.12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.0.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hive<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hive-exec<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.2.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>mysql<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>mysql-connector-java<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>5.1.27<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure></li>
</ol>
</li>
<li><p>将hive-site.xml文件拷贝到项目的resources目录中，代码实现</p>
 <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">//创建SparkSession</span></span><br><span class="line"><span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span></span><br><span class="line">  .builder()</span><br><span class="line">  .enableHiveSupport()</span><br><span class="line">  .master(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">  .appName(<span class="string">&quot;sql&quot;</span>)</span><br><span class="line">  .getOrCreate()</span><br></pre></td></tr></table></figure>
<blockquote>
<p>注意：在开发工具中创建数据库默认是在本地仓库，通过参数修改数据库仓库的地址: </p>
   <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">config(<span class="string">&quot;spark.sql.warehouse.dir&quot;</span>, <span class="string">&quot;hdfs://linux1:8020/user/hive/warehouse&quot;</span>)</span><br></pre></td></tr></table></figure>
<p> 如果在执行操作时，出现如下错误：<br> <img src="https://s2.loli.net/2021/12/13/ZmY8fq3BuMUjN16.jpg"><br> 可以代码最前面增加如下代码解决(此处的root改为hadoop用户名称)：</p>
 <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="type">System</span>.setProperty(<span class="string">&quot;HADOOP_USER_NAME&quot;</span>, <span class="string">&quot;root&quot;</span>)</span><br></pre></td></tr></table></figure></blockquote>
</li>
</ol>
<h1 id="第3章-SparkSQL项目实战"><a href="#第3章-SparkSQL项目实战" class="headerlink" title="第3章 SparkSQL项目实战"></a>第3章 SparkSQL项目实战</h1><h3 id="3-1-数据准备"><a href="#3-1-数据准备" class="headerlink" title="3.1 数据准备"></a>3.1 数据准备</h3><p>我们这次 Spark-sql 操作中所有的数据均来自 Hive，首先在 Hive 中创建表,，并导入数据。<br>一共有3张表： 1张用户行为表，1张城市表，1 张产品表</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 用户行为表</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> `user_visit_action`(</span><br><span class="line">  `<span class="type">date</span>` string,	</span><br><span class="line">  `user_id` <span class="type">bigint</span>,</span><br><span class="line">  `session_id` string,</span><br><span class="line">  `page_id` <span class="type">bigint</span>,</span><br><span class="line">  `action_time` string,</span><br><span class="line">  `search_keyword` string,</span><br><span class="line">  `click_category_id` <span class="type">bigint</span>,</span><br><span class="line">  `click_product_id` <span class="type">bigint</span>,</span><br><span class="line">  `order_category_ids` string,</span><br><span class="line">  `order_product_ids` string,</span><br><span class="line">  `pay_category_ids` string,</span><br><span class="line">  `pay_product_ids` string,</span><br><span class="line">  `city_id` <span class="type">bigint</span>)</span><br><span class="line"><span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span>;</span><br><span class="line">load data <span class="keyword">local</span> inpath <span class="string">&#x27;input/user_visit_action.txt&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span> user_visit_action;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 城市表</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> `product_info`(</span><br><span class="line">  `product_id` <span class="type">bigint</span>,</span><br><span class="line">  `product_name` string,</span><br><span class="line">  `extend_info` string)</span><br><span class="line"><span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span>;</span><br><span class="line">load data <span class="keyword">local</span> inpath <span class="string">&#x27;input/product_info.txt&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span> product_info;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 产品表</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> `city_info`(</span><br><span class="line">  `city_id` <span class="type">bigint</span>,</span><br><span class="line">  `city_name` string,</span><br><span class="line">  `area` string)</span><br><span class="line"><span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span>;</span><br><span class="line">load data <span class="keyword">local</span> inpath <span class="string">&#x27;input/city_info.txt&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span> city_info;</span><br></pre></td></tr></table></figure>

<h2 id="3-2-需求：各区域热门商品-Top3"><a href="#3-2-需求：各区域热门商品-Top3" class="headerlink" title="3.2 需求：各区域热门商品 Top3"></a>3.2 需求：各区域热门商品 Top3</h2><h3 id="3-2-1-需求简介"><a href="#3-2-1-需求简介" class="headerlink" title="3.2.1 需求简介"></a>3.2.1 需求简介</h3><p> 这里的热门商品是从点击量的维度来看的，计算各个区域前三大热门商品，并备注上每个商品在主要城市中的分布比例，超过两个城市用其他显示。例如：<br>|地区        |商品名称      |点击次数      |城市备注       |<br>|———|———–|———–|————–|<br>|华北        |商品A         |100000      |北京21.2%，天津13.2%，其他65.6%    |<br>|华北        |商品P         |80200      |北京63.0%，太原10%，其他27.0%    |<br>|华北        |商品M         |40000         |北京63.0%，太原10%，其他27.0%    |<br>|东北        |商品J         |92000         |大连28%，辽宁17.0%，其他 55.0%   |</p>
<h3 id="3-2-2-需求分析"><a href="#3-2-2-需求分析" class="headerlink" title="3.2.2 需求分析"></a>3.2.2 需求分析</h3><ul>
<li>查询出来所有的点击记录，并与 city_info 表连接，得到每个城市所在的地区，与 Product_info 表连接得到产品名称</li>
<li>按照地区和商品 id 分组，统计出每个商品在每个地区的总点击次数</li>
<li>每个地区内按照点击次数降序排列</li>
<li>只取前三名</li>
<li>城市备注需要自定义 UDAF 函数</li>
</ul>
<h3 id="3-2-3-功能实现"><a href="#3-2-3-功能实现" class="headerlink" title="3.2.3 功能实现"></a>3.2.3 功能实现</h3><ul>
<li>连接三张表的数据，获取完整的数据（只有点击）</li>
<li>将数据根据地区，商品名称分组</li>
<li>统计商品点击次数总和,取Top3</li>
<li>实现自定义聚合函数显示备注<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.bigdata.spark.sql</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.expressions.<span class="type">Aggregator</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.&#123;<span class="type">Encoder</span>, <span class="type">Encoders</span>, <span class="type">SparkSession</span>, functions&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> scala.collection.mutable</span><br><span class="line"><span class="keyword">import</span> scala.collection.mutable.<span class="type">ListBuffer</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SparkSQL_Hive</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="type">System</span>.setProperty(<span class="string">&quot;HADOOP_USER_NAME&quot;</span>, <span class="string">&quot;atguigu&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">//by_sql</span></span><br><span class="line">        by_sql_split</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">by_sql_split</span> </span>= &#123;</span><br><span class="line">        <span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span></span><br><span class="line">            .builder()</span><br><span class="line">            .enableHiveSupport()</span><br><span class="line">            .master(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">            .appName(<span class="string">&quot;sql&quot;</span>)</span><br><span class="line">            .config(<span class="string">&quot;spark.sql.warehouse.dir&quot;</span>, <span class="string">&quot;hdfs://mycluster/user/hive/warehouse&quot;</span>)</span><br><span class="line">            .getOrCreate()</span><br><span class="line"></span><br><span class="line">        spark.sql(<span class="string">&quot;use spark_sql&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">//1. 关联三张表</span></span><br><span class="line">        spark.sql(</span><br><span class="line">            <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">              |select ac.*,</span></span><br><span class="line"><span class="string">              |       pi.product_name,</span></span><br><span class="line"><span class="string">              |       ci.area,</span></span><br><span class="line"><span class="string">              |       ci.city_name</span></span><br><span class="line"><span class="string">              |from user_visit_action ac</span></span><br><span class="line"><span class="string">              |         left join city_info ci on ac.city_id = ci.city_id</span></span><br><span class="line"><span class="string">              |         left join product_info pi on ac.click_product_id = pi.product_id</span></span><br><span class="line"><span class="string">              |where ac.click_product_id != -1</span></span><br><span class="line"><span class="string">              |&quot;&quot;&quot;</span>.stripMargin).createTempView(<span class="string">&quot;t1&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">//2. 商品区域作为整体，统计点击数量（（区域，商品），sum）</span></span><br><span class="line">        spark.udf.register(<span class="string">&quot;cityRemark&quot;</span>, functions.udaf(<span class="keyword">new</span> <span class="type">CityRemarkUDAF</span>()))</span><br><span class="line">        spark.sql(</span><br><span class="line">            <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">              |select area,</span></span><br><span class="line"><span class="string">              |       product_name,</span></span><br><span class="line"><span class="string">              |       count(*) as clickCount,</span></span><br><span class="line"><span class="string">              |       cityRemark(city_name)</span></span><br><span class="line"><span class="string">              |from t1</span></span><br><span class="line"><span class="string">              |group by area, product_name</span></span><br><span class="line"><span class="string">              |&quot;&quot;&quot;</span>.stripMargin).createTempView(<span class="string">&quot;t2&quot;</span>)</span><br><span class="line">        <span class="comment">//3. 将统计结果进行结构的转换（区域，（商品，sum））</span></span><br><span class="line">        <span class="comment">//4. 按区域进行分组（区域，Iter[（商品，sum）,（商品，sum）,（商品，sum）]）</span></span><br><span class="line">        spark.sql(</span><br><span class="line">            <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">              |select *,</span></span><br><span class="line"><span class="string">              |       rank() over (partition by area order by clickCount desc ) as rank</span></span><br><span class="line"><span class="string">              |from t2</span></span><br><span class="line"><span class="string">              |&quot;&quot;&quot;</span>.stripMargin).createTempView(<span class="string">&quot;t3&quot;</span>)</span><br><span class="line">        <span class="comment">//5. 分组后的数据根据点击量排序</span></span><br><span class="line">        spark.sql(</span><br><span class="line">            <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">              |select * from t3 where rank &lt;=3</span></span><br><span class="line"><span class="string">              |&quot;&quot;&quot;</span>.stripMargin)</span><br><span class="line">            .show(<span class="literal">false</span>)</span><br><span class="line"></span><br><span class="line">        spark.stop()</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">CityRemarkBuff</span>(<span class="params">var total: <span class="type">Long</span>, cityMap: mutable.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">Long</span>]</span>)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="class"><span class="keyword">class</span> <span class="title">CityRemarkUDAF</span> <span class="keyword">extends</span> <span class="title">Aggregator</span>[<span class="type">String</span>, <span class="type">CityRemarkBuff</span>, <span class="type">String</span>] </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">zero</span></span>: <span class="type">CityRemarkBuff</span> = &#123;</span><br><span class="line">            <span class="type">CityRemarkBuff</span>(<span class="number">0</span>L, mutable.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">Long</span>]())</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">reduce</span></span>(buff: <span class="type">CityRemarkBuff</span>, city: <span class="type">String</span>): <span class="type">CityRemarkBuff</span> = &#123;</span><br><span class="line">            buff.total += <span class="number">1</span>L</span><br><span class="line">            buff.cityMap.update(city, buff.cityMap.getOrElse(city, <span class="number">0</span>L) + <span class="number">1</span>L)</span><br><span class="line">            buff</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">merge</span></span>(b1: <span class="type">CityRemarkBuff</span>, b2: <span class="type">CityRemarkBuff</span>): <span class="type">CityRemarkBuff</span> = &#123;</span><br><span class="line">            b1.total += b2.total</span><br><span class="line">            b2.cityMap.foreach &#123;</span><br><span class="line">                <span class="keyword">case</span> (city, count) =&gt; &#123;</span><br><span class="line">                    b1.cityMap.update(city, b1.cityMap.getOrElse(city, <span class="number">0</span>L) + count)</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            b1</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">finish</span></span>(reduction: <span class="type">CityRemarkBuff</span>): <span class="type">String</span> = &#123;</span><br><span class="line">            <span class="keyword">val</span> list: <span class="type">ListBuffer</span>[<span class="type">String</span>] = <span class="type">ListBuffer</span>[<span class="type">String</span>]()</span><br><span class="line">            <span class="keyword">val</span> total: <span class="type">Long</span> = reduction.total</span><br><span class="line">            <span class="keyword">val</span> sortCity: <span class="type">List</span>[(<span class="type">String</span>, <span class="type">Long</span>)] = reduction.cityMap.toList.sortBy(_._2)</span><br><span class="line">            <span class="keyword">val</span> top2: <span class="type">List</span>[(<span class="type">String</span>, <span class="type">Long</span>)] = sortCity.take(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">var</span> remain =<span class="number">100</span>L</span><br><span class="line"></span><br><span class="line">            top2.foreach &#123;</span><br><span class="line">                <span class="keyword">case</span> (city, count) =&gt; &#123;</span><br><span class="line">                    <span class="keyword">val</span> percent: <span class="type">Long</span> = count * <span class="number">100</span> / total</span><br><span class="line">                    remain -= percent</span><br><span class="line">                    list.append(<span class="string">s&quot;<span class="subst">$&#123;city&#125;</span> <span class="subst">$percent</span>%&quot;</span>)</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span> (sortCity.length&gt;<span class="number">2</span>) &#123;</span><br><span class="line">                list.append(<span class="string">s&quot;其他 <span class="subst">$&#123;remain&#125;</span>%&quot;</span>)</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            list.mkString(<span class="string">&quot;, &quot;</span>)</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">bufferEncoder</span></span>: <span class="type">Encoder</span>[<span class="type">CityRemarkBuff</span>] = <span class="type">Encoders</span>.product</span><br><span class="line"></span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">outputEncoder</span></span>: <span class="type">Encoder</span>[<span class="type">String</span>] = <span class="type">Encoders</span>.<span class="type">STRING</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">by_sql</span> </span>= &#123;</span><br><span class="line">        <span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span></span><br><span class="line">            .builder()</span><br><span class="line">            .enableHiveSupport()</span><br><span class="line">            .master(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">            .appName(<span class="string">&quot;sql&quot;</span>)</span><br><span class="line">            .config(<span class="string">&quot;spark.sql.warehouse.dir&quot;</span>, <span class="string">&quot;hdfs://mycluster/user/hive/warehouse&quot;</span>)</span><br><span class="line">            .getOrCreate()</span><br><span class="line"></span><br><span class="line">        spark.sql(<span class="string">&quot;use spark_sql&quot;</span>)</span><br><span class="line"></span><br><span class="line">        spark.sql(</span><br><span class="line">            <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">              |select *</span></span><br><span class="line"><span class="string">              |from (</span></span><br><span class="line"><span class="string">              |         select *,</span></span><br><span class="line"><span class="string">              |                rank() over (partition by area order by clickCount desc ) as rank</span></span><br><span class="line"><span class="string">              |         from (</span></span><br><span class="line"><span class="string">              |                  select area,</span></span><br><span class="line"><span class="string">              |                         product_name,</span></span><br><span class="line"><span class="string">              |                         count(*) as clickCount</span></span><br><span class="line"><span class="string">              |                  from (</span></span><br><span class="line"><span class="string">              |                           select ac.*,</span></span><br><span class="line"><span class="string">              |                                  pi.product_name,</span></span><br><span class="line"><span class="string">              |                                  ci.area,</span></span><br><span class="line"><span class="string">              |                                  ci.city_name</span></span><br><span class="line"><span class="string">              |                           from user_visit_action ac</span></span><br><span class="line"><span class="string">              |                                    left join city_info ci on ac.city_id = ci.city_id</span></span><br><span class="line"><span class="string">              |                                    left join product_info pi on ac.click_product_id = pi.product_id</span></span><br><span class="line"><span class="string">              |                           where ac.click_product_id != -1</span></span><br><span class="line"><span class="string">              |                       ) t1</span></span><br><span class="line"><span class="string">              |                  group by area, product_name</span></span><br><span class="line"><span class="string">              |              ) t2</span></span><br><span class="line"><span class="string">              |     ) t3</span></span><br><span class="line"><span class="string">              |where rank &lt;= 3</span></span><br><span class="line"><span class="string">              |&quot;&quot;&quot;</span>.stripMargin).show()</span><br><span class="line"></span><br><span class="line">        spark.stop()</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">by_rdd</span> </span>= &#123;</span><br><span class="line">        <span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span></span><br><span class="line">            .builder()</span><br><span class="line">            .enableHiveSupport()</span><br><span class="line">            .master(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">            .appName(<span class="string">&quot;sql&quot;</span>)</span><br><span class="line">            .config(<span class="string">&quot;spark.sql.warehouse.dir&quot;</span>, <span class="string">&quot;hdfs://mycluster/user/hive/warehouse&quot;</span>)</span><br><span class="line">            .getOrCreate()</span><br><span class="line"></span><br><span class="line">        spark.sql(<span class="string">&quot;use spark_sql&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">//1. 关联三张表</span></span><br><span class="line">        <span class="comment">//2. 商品区域作为整体，统计点击数量（（区域，商品），sum）</span></span><br><span class="line">        <span class="comment">//3. 将统计结果进行结构的转换（区域，（商品，sum））</span></span><br><span class="line">        <span class="comment">//4. 按区域进行分组（区域，Iter[（商品，sum）,（商品，sum）,（商品，sum）]）</span></span><br><span class="line">        <span class="comment">//5. 分组后的数据根据点击量排序</span></span><br><span class="line">        spark.stop()</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 导入数据</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">import_data</span> </span>= &#123;</span><br><span class="line">        <span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span></span><br><span class="line">            .builder()</span><br><span class="line">            .enableHiveSupport()</span><br><span class="line">            .master(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">            .appName(<span class="string">&quot;sql&quot;</span>)</span><br><span class="line">            .config(<span class="string">&quot;spark.sql.warehouse.dir&quot;</span>, <span class="string">&quot;hdfs://mycluster/user/hive/warehouse&quot;</span>)</span><br><span class="line">            .getOrCreate()</span><br><span class="line"></span><br><span class="line">        spark.sql(<span class="string">&quot;use spark_sql&quot;</span>)</span><br><span class="line"></span><br><span class="line">        spark.sql(</span><br><span class="line">            <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">              |CREATE TABLE `user_visit_action`(</span></span><br><span class="line"><span class="string">              |  `date` string,</span></span><br><span class="line"><span class="string">              |  `user_id` bigint,</span></span><br><span class="line"><span class="string">              |  `session_id` string,</span></span><br><span class="line"><span class="string">              |  `page_id` bigint,</span></span><br><span class="line"><span class="string">              |  `action_time` string,</span></span><br><span class="line"><span class="string">              |  `search_keyword` string,</span></span><br><span class="line"><span class="string">              |  `click_category_id` bigint,</span></span><br><span class="line"><span class="string">              |  `click_product_id` bigint,</span></span><br><span class="line"><span class="string">              |  `order_category_ids` string,</span></span><br><span class="line"><span class="string">              |  `order_product_ids` string,</span></span><br><span class="line"><span class="string">              |  `pay_category_ids` string,</span></span><br><span class="line"><span class="string">              |  `pay_product_ids` string,</span></span><br><span class="line"><span class="string">              |  `city_id` bigint)</span></span><br><span class="line"><span class="string">              |row format delimited fields terminated by &#x27;\t&#x27;;</span></span><br><span class="line"><span class="string">              |&quot;&quot;&quot;</span>.stripMargin)</span><br><span class="line">        spark.sql(</span><br><span class="line">            <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">              |CREATE TABLE `city_info`(</span></span><br><span class="line"><span class="string">              |  `city_id` bigint,</span></span><br><span class="line"><span class="string">              |  `city_name` string,</span></span><br><span class="line"><span class="string">              |  `area` string)</span></span><br><span class="line"><span class="string">              |row format delimited fields terminated by &#x27;\t&#x27;;</span></span><br><span class="line"><span class="string">              |&quot;&quot;&quot;</span>.stripMargin)</span><br><span class="line">        spark.sql(</span><br><span class="line">            <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">              |CREATE TABLE `product_info`(</span></span><br><span class="line"><span class="string">              |  `product_id` bigint,</span></span><br><span class="line"><span class="string">              |  `product_name` string,</span></span><br><span class="line"><span class="string">              |  `extend_info` string)</span></span><br><span class="line"><span class="string">              |row format delimited fields terminated by &#x27;\t&#x27;;</span></span><br><span class="line"><span class="string">              |</span></span><br><span class="line"><span class="string">              |&quot;&quot;&quot;</span>.stripMargin)</span><br><span class="line">        spark.sql(<span class="string">&quot;load data local inpath &#x27;data/user_visit_action.txt&#x27; into table user_visit_action;&quot;</span>)</span><br><span class="line">        spark.sql(<span class="string">&quot;load data local inpath &#x27;data/city_info.txt&#x27; into table city_info;&quot;</span>)</span><br><span class="line">        spark.sql(<span class="string">&quot;load data local inpath &#x27;data/product_info.txt&#x27; into table product_info;&quot;</span>)</span><br><span class="line">        spark.sql(<span class="string">&quot;show tables&quot;</span>).show()</span><br><span class="line">        spark.sql(<span class="string">&quot;select * from  city_info&quot;</span>).show()</span><br><span class="line"></span><br><span class="line">        spark.stop()</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">init</span> </span>= &#123;</span><br><span class="line">        <span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span></span><br><span class="line">            .builder()</span><br><span class="line">            .enableHiveSupport()</span><br><span class="line">            .master(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">            .appName(<span class="string">&quot;sql&quot;</span>)</span><br><span class="line">            .getOrCreate()</span><br><span class="line"></span><br><span class="line">        spark.stop()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul>
]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Spark</tag>
        <tag>SparkSQL</tag>
      </tags>
  </entry>
  <entry>
    <title>SparkCore</title>
    <url>/2021/12/11/SparkCore/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="Spark-Core"><a href="#Spark-Core" class="headerlink" title="Spark Core"></a>Spark Core</h1><h1 id="第1章-Spark概述"><a href="#第1章-Spark概述" class="headerlink" title="第1章 Spark概述"></a>第1章 Spark概述</h1><h2 id="1-1-Spark是什么"><a href="#1-1-Spark是什么" class="headerlink" title="1.1 Spark是什么"></a>1.1 Spark是什么</h2><p><img src="https://s2.loli.net/2021/12/11/PRpgQ1WYFbI9StM.jpg"><br><img src="https://s2.loli.net/2021/12/11/WqDcxak3HC26V8Y.jpg"><br>Spark是一种基于内存的快速、通用、可扩展的大数据分析计算引擎。</p>
<h2 id="1-2-Spark-and-Hadoop"><a href="#1-2-Spark-and-Hadoop" class="headerlink" title="1.2 Spark and Hadoop"></a>1.2 Spark and Hadoop</h2><p>在之前的学习中，Hadoop的MapReduce是大家广为熟知的计算框架，那为什么咱们还要学习新的计算框架Spark呢，这里就不得不提到Spark和Hadoop的关系。<br>首先从时间节点上来看:</p>
<ul>
<li>Hadoop<ul>
<li>2006年1月，Doug Cutting加入Yahoo，领导Hadoop的开发</li>
<li>2008年1月，Hadoop成为Apache顶级项目</li>
<li>2011年1.0正式发布</li>
<li>2012年3月稳定版发布</li>
<li>2013年10月发布2.X (Yarn)版本</li>
</ul>
</li>
<li>Spark<ul>
<li>2009年，Spark诞生于伯克利大学的AMPLab实验室</li>
<li>2010年，伯克利大学正式开源了Spark项目</li>
<li>2013年6月，Spark成为了Apache基金会下的项目</li>
<li>2014年2月，Spark以飞快的速度成为了Apache的顶级项目</li>
<li>2015年至今，Spark变得愈发火爆，大量的国内公司开始重点部署或者使用Spark<br>然后我们再从功能上来看:</li>
</ul>
</li>
<li>Hadoop<ul>
<li>Hadoop是由java语言编写的，在分布式服务器集群上存储海量数据并运行分布式分析应用的开源框架</li>
<li>作为Hadoop分布式文件系统，HDFS处于Hadoop生态圈的最下层，存储着所有的数据，支持着Hadoop的所有服务。它的理论基础源于Google的TheGoogleFileSystem这篇论文，它是GFS的开源实现。</li>
<li>MapReduce是一种编程模型，Hadoop根据Google的MapReduce论文将其实现，作为Hadoop的分布式计算模型，是Hadoop的核心。基于这个框架，分布式并行程序的编写变得异常简单。综合了HDFS的分布式存储和MapReduce的分布式计算，Hadoop在处理海量数据时，性能横向扩展变得非常容易。</li>
<li>HBase是对Google的Bigtable的开源实现，但又和Bigtable存在许多不同之处。HBase是一个基于HDFS的分布式数据库，擅长实时地随机读/写超大规模数据集。它也是Hadoop非常重要的组件。</li>
</ul>
</li>
<li>Spark<ul>
<li>Spark是一种由Scala语言开发的快速、通用、可扩展的大数据分析引擎</li>
<li>Spark Core中提供了Spark最基础与最核心的功能</li>
<li>Spark SQL是Spark用来操作结构化数据的组件。通过Spark SQL，用户可以使用SQL或者Apache Hive版本的SQL方言（HQL）来查询数据。</li>
<li>Spark Streaming是Spark平台上针对实时数据进行流式计算的组件，提供了丰富的处理数据流的API。</li>
<li>由上面的信息可以获知，Spark出现的时间相对较晚，并且主要功能主要是用于数据计算，所以其实Spark一直被认为是Hadoop 框架的升级版。</li>
</ul>
</li>
</ul>
<h2 id="1-3-Spark-or-Hadoop"><a href="#1-3-Spark-or-Hadoop" class="headerlink" title="1.3 Spark or Hadoop"></a>1.3 Spark or Hadoop</h2><p>Hadoop的MR框架和Spark框架都是数据处理框架，那么我们在使用时如何选择呢？</p>
<ul>
<li>Hadoop MapReduce由于其设计初衷并不是为了满足循环迭代式数据流处理，因此在多并行运行的数据可复用场景（如：机器学习、图挖掘算法、交互式数据挖掘算法）中存在诸多计算效率等问题。所以Spark应运而生，Spark就是在传统的MapReduce 计算框架的基础上，利用其计算过程的优化，从而大大加快了数据分析、挖掘的运行和读写速度，并将计算单元缩小到更适合并行计算和重复使用的RDD计算模型。</li>
<li>机器学习中ALS、凸优化梯度下降等。这些都需要基于数据集或者数据集的衍生数据反复查询反复操作。MR这种模式不太合适，即使多MR串行处理，性能和时间也是一个问题。数据的共享依赖于磁盘。另外一种是交互式数据挖掘，MR显然不擅长。而Spark所基于的scala语言恰恰擅长函数的处理。</li>
<li>Spark是一个分布式数据快速分析项目。它的核心技术是弹性分布式数据集（Resilient Distributed Datasets），提供了比MapReduce丰富的模型，可以快速在内存中对数据集进行多次迭代，来支持复杂的数据挖掘算法和图形计算算法。</li>
<li>Spark和Hadoop的根本差异是多个作业之间的数据通信问题 : Spark多个作业之间数据通信是基于内存，而Hadoop是基于磁盘。</li>
<li>Spark Task的启动时间快。Spark采用fork线程的方式，而Hadoop采用创建新的进程的方式。</li>
<li>Spark只有在shuffle的时候将数据写入磁盘，而Hadoop中多个MR作业之间的数据交互都要依赖于磁盘交互</li>
<li>Spark的缓存机制比HDFS的缓存机制高效。</li>
</ul>
<p>经过上面的比较，我们可以看出在绝大多数的数据计算场景中，Spark确实会比MapReduce更有优势。但是Spark是基于内存的，所以在实际的生产环境中，由于内存的限制，可能会由于内存资源不够导致Job执行失败，此时，MapReduce其实是一个更好的选择，所以Spark并不能完全替代MR。</p>
<h2 id="1-4-Spark-核心模块"><a href="#1-4-Spark-核心模块" class="headerlink" title="1.4 Spark 核心模块"></a>1.4 Spark 核心模块</h2><p><img src="https://s2.loli.net/2021/12/11/AdkW8MaIlSPyJxh.jpg"></p>
<ul>
<li>Spark Core<br>Spark Core中提供了Spark最基础与最核心的功能，Spark其他的功能如：Spark SQL，Spark Streaming，GraphX, MLlib都是在Spark Core的基础上进行扩展的</li>
<li>Spark SQL<br>Spark SQL是Spark用来操作结构化数据的组件。通过Spark SQL，用户可以使用SQL或者Apache Hive版本的SQL方言（HQL）来查询数据。</li>
<li>Spark Streaming<br>Spark Streaming是Spark平台上针对实时数据进行流式计算的组件，提供了丰富的处理数据流的API。</li>
<li>Spark MLlib<br>MLlib是Spark提供的一个机器学习算法库。MLlib不仅提供了模型评估、数据导入等额外的功能，还提供了一些更底层的机器学习原语。</li>
<li>Spark GraphX<br>GraphX是Spark面向图计算提供的框架与算法库。</li>
</ul>
<h1 id="第3章-Spark运行环境"><a href="#第3章-Spark运行环境" class="headerlink" title="第3章 Spark运行环境"></a>第3章 Spark运行环境</h1><p>Spark作为一个数据处理框架和计算引擎，被设计在所有常见的集群环境中运行, 在国内工作中主流的环境为Yarn，不过逐渐容器式环境也慢慢流行起来。接下来，我们就分别看看不同环境下Spark的运行<br><img src="https://s2.loli.net/2021/12/11/bnH9TZVF6WaSkjt.jpg"></p>
<h2 id="3-1-Local模式"><a href="#3-1-Local模式" class="headerlink" title="3.1  Local模式"></a>3.1  Local模式</h2><p>想啥呢，你之前一直在使用的模式可不是Local模式哟。所谓的Local模式，就是不需要其他任何节点资源就可以在本地执行Spark代码的环境，一般用于教学，调试，演示等，之前在IDEA中运行代码的环境我们称之为开发环境，不太一样。</p>
<h3 id="3-1-1-解压缩文件"><a href="#3-1-1-解压缩文件" class="headerlink" title="3.1.1 解压缩文件"></a>3.1.1 解压缩文件</h3><p>将spark-3.0.0-bin-hadoop3.2.tgz文件上传到Linux并解压缩，放置在指定位置，路径中不要包含中文或空格，课件后续如果涉及到解压缩操作，不再强调。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">tar -zxvf spark-3.0.0-bin-hadoop3.2.tgz -C /opt/module</span><br><span class="line"><span class="built_in">cd</span> /opt/module </span><br><span class="line">mv spark-3.0.0-bin-hadoop3.2 spark-local</span><br></pre></td></tr></table></figure>

<h3 id="3-1-2-启动Local环境"><a href="#3-1-2-启动Local环境" class="headerlink" title="3.1.2 启动Local环境"></a>3.1.2 启动Local环境</h3><ol>
<li>进入解压缩后的路径，执行如下指令 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@bd-server-001 spark-local]$ bin/spark-shell</span><br><span class="line">21/12/01 08:26:41 WARN NativeCodeLoader: Unable to load native-hadoop library <span class="keyword">for</span> your platform... using builtin-java classes <span class="built_in">where</span> applicable</span><br><span class="line">Using Spark<span class="string">&#x27;s default log4j profile: org/apache/spark/log4j-defaults.properties</span></span><br><span class="line"><span class="string">Setting default log level to &quot;WARN&quot;.</span></span><br><span class="line"><span class="string">To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).</span></span><br><span class="line"><span class="string">Spark context Web UI available at http://bd-server-001:4040</span></span><br><span class="line"><span class="string">Spark context available as &#x27;</span>sc<span class="string">&#x27; (master = local[*], app id = local-1638318406888).</span></span><br><span class="line"><span class="string">Spark session available as &#x27;</span>spark<span class="string">&#x27;.</span></span><br><span class="line"><span class="string">Welcome to</span></span><br><span class="line"><span class="string">      ____              __</span></span><br><span class="line"><span class="string">     / __/__  ___ _____/ /__</span></span><br><span class="line"><span class="string">    _\ \/ _ \/ _ `/ __/  &#x27;</span>_/</span><br><span class="line">   /___/ .__/\_,_/_/ /_/\_\   version 3.0.0</span><br><span class="line">      /_/</span><br><span class="line"></span><br><span class="line">Using Scala version 2.12.10 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_212)</span><br><span class="line">Type <span class="keyword">in</span> expressions to have them evaluated.</span><br><span class="line">Type :<span class="built_in">help</span> <span class="keyword">for</span> more information.</span><br><span class="line"></span><br><span class="line">scala&gt;</span><br></pre></td></tr></table></figure></li>
<li>启动成功后，可以输入网址进行Web UI监控页面访问<br> <img src="https://s2.loli.net/2021/12/11/rqa1YJvW2wfQKBP.jpg"></li>
</ol>
<h3 id="3-1-3-命令行工具"><a href="#3-1-3-命令行工具" class="headerlink" title="3.1.3 命令行工具"></a>3.1.3 命令行工具</h3><p>在解压缩文件夹下的data目录中，添加word.txt文件。在命令行工具中执行如下代码指令（和IDEA中代码简化版一致）</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">sc</span><br><span class="line">    .textFile(<span class="string">&quot;data/word.txt&quot;</span>)</span><br><span class="line">    .flatMap(_.split(<span class="string">&quot; &quot;</span>))</span><br><span class="line">    .map((_,<span class="number">1</span>))</span><br><span class="line">    .reduceByKey(_+_)</span><br><span class="line">    .collect</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">scala&gt; sc.textFile(<span class="string">&quot;data/word.txt&quot;</span>).flatMap(_.split(<span class="string">&quot; &quot;</span>)).map((_,1)).reduceByKey(_+_).collect</span><br><span class="line">res0: Array[(String, Int)] = Array((scala,1), (flink,1), (hello,3), (spark,1))</span><br><span class="line"></span><br><span class="line">scala&gt;</span><br></pre></td></tr></table></figure>

<h3 id="3-1-4-退出本地模式"><a href="#3-1-4-退出本地模式" class="headerlink" title="3.1.4 退出本地模式"></a>3.1.4 退出本地模式</h3><p>按键Ctrl+C或输入Scala指令<code>:quit</code></p>
<h3 id="3-1-5-提交应用"><a href="#3-1-5-提交应用" class="headerlink" title="3.1.5 提交应用"></a>3.1.5 提交应用</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">bin/spark-submit \</span><br><span class="line">--class org.apache.spark.examples.SparkPi \</span><br><span class="line">--master <span class="built_in">local</span>[2] \</span><br><span class="line">./examples/jars/spark-examples_2.12-3.0.0.jar \</span><br><span class="line">10</span><br></pre></td></tr></table></figure>
<ol>
<li>–class表示要执行程序的主类，此处可以更换为咱们自己写的应用程序</li>
<li>–master local[2] 部署模式，默认为本地模式，数字表示分配的虚拟CPU核数量</li>
<li>spark-examples_2.12-3.0.0.jar 运行的应用类所在的jar包，实际使用时，可以设定为咱们自己打的jar包</li>
<li>数字10表示程序的入口参数，用于设定当前应用的任务数量</li>
</ol>
<p><img src="https://s2.loli.net/2021/12/11/UleCBVSxPaX8HkA.jpg"></p>
<h2 id="3-2-Standalone模式"><a href="#3-2-Standalone模式" class="headerlink" title="3.2  Standalone模式"></a>3.2  Standalone模式</h2><p>local本地模式毕竟只是用来进行练习演示的，真实工作中还是要将应用提交到对应的集群中去执行，这里我们来看看只使用Spark自身节点运行的集群模式，也就是我们所谓的独立部署（Standalone）模式。Spark的Standalone模式体现了经典的master-slave模式。<br>Spark有自己的资源调度框架：<br><img src="https://s2.loli.net/2021/12/11/9FdAabfSstQUqv4.jpg"></p>
<p>集群规划:<br>|Linux1    |Linux2    |Linux3 |<br>|——–|———|——|<br>|Spark    |Worker Master|Worker    |Worker    |</p>
<h3 id="3-2-1-解压缩文件"><a href="#3-2-1-解压缩文件" class="headerlink" title="3.2.1 解压缩文件"></a>3.2.1 解压缩文件</h3><p>将spark-3.0.0-bin-hadoop3.2.tgz文件上传到Linux并解压缩在指定位置</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">tar -zxvf spark-3.0.0-bin-hadoop3.2.tgz -C /opt/module</span><br><span class="line"><span class="built_in">cd</span> /opt/module </span><br><span class="line">mv spark-3.0.0-bin-hadoop3.2 spark-standalone</span><br></pre></td></tr></table></figure>

<h3 id="3-2-2-修改配置文件"><a href="#3-2-2-修改配置文件" class="headerlink" title="3.2.2 修改配置文件"></a>3.2.2 修改配置文件</h3><ol>
<li>进入解压缩后路径的conf目录，修改slaves.template文件名为slaves <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mv slaves.template slaves</span><br></pre></td></tr></table></figure></li>
<li>修改slaves文件，添加worker节点 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">bd-server-001</span><br><span class="line">bd-server-002</span><br><span class="line">bd-server-003</span><br><span class="line">bd-server-004</span><br><span class="line">bd-server-005</span><br></pre></td></tr></table></figure></li>
<li>修改spark-env.sh.template文件名为spark-env.sh <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mv spark-env.sh.template spark-env.sh</span><br></pre></td></tr></table></figure></li>
<li>修改spark-env.sh文件，添加JAVA_HOME环境变量和集群对应的master节点 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">export</span> JAVA_HOME=/opt/module/jdk1.8.0_212</span><br><span class="line">SPARK_MASTER_HOST=bd-server-001</span><br><span class="line">SPARK_MASTER_PORT=7077</span><br></pre></td></tr></table></figure></li>
<li>分发spark-standalone目录 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">xsync spark-standalone</span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="3-2-3-启动集群"><a href="#3-2-3-启动集群" class="headerlink" title="3.2.3 启动集群"></a>3.2.3 启动集群</h3><ol>
<li>执行脚本命令：<code>sbin/start-all.sh</code> <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@bd-server-001 spark-standalone]$ sbin/start-all.sh</span><br><span class="line">starting org.apache.spark.deploy.master.Master, logging to /opt/module/spark-standalone/logs/spark-atguigu-org.apache.spark.deploy.master.Master-1-bd-server-001.out</span><br><span class="line">bd-server-002: starting org.apache.spark.deploy.worker.Worker, logging to /opt/module/spark-standalone/logs/spark-atguigu-org.apache.spark.deploy.worker.Worker-1-bd-server-002.out</span><br><span class="line">bd-server-001: starting org.apache.spark.deploy.worker.Worker, logging to /opt/module/spark-standalone/logs/spark-atguigu-org.apache.spark.deploy.worker.Worker-1-bd-server-001.out</span><br><span class="line">bd-server-003: starting org.apache.spark.deploy.worker.Worker, logging to /opt/module/spark-standalone/logs/spark-atguigu-org.apache.spark.deploy.worker.Worker-1-bd-server-003.out</span><br><span class="line">bd-server-004: starting org.apache.spark.deploy.worker.Worker, logging to /opt/module/spark-standalone/logs/spark-atguigu-org.apache.spark.deploy.worker.Worker-1-bd-server-004.out</span><br><span class="line">bd-server-005: starting org.apache.spark.deploy.worker.Worker, logging to /opt/module/spark-standalone/logs/spark-atguigu-org.apache.spark.deploy.worker.Worker-1-bd-server-005.out</span><br><span class="line">[atguigu@bd-server-001 spark-standalone]$</span><br></pre></td></tr></table></figure>
</li>
<li>查看三台服务器运行进程 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@bd-server-001 spark-standalone]$ jpsall</span><br><span class="line">&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;  bd-server-001  &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;</span><br><span class="line"> <span class="string">37979 Master</span></span><br><span class="line"><span class="string"> 38070 Worker</span></span><br><span class="line"><span class="string">&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;  bd-server-002  &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;</span></span><br><span class="line"><span class="string"> 27011 Worker</span></span><br><span class="line"><span class="string">&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;  bd-server-003  &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;</span></span><br><span class="line"><span class="string"> 19210 Worker</span></span><br><span class="line"><span class="string">&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;  bd-server-004  &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;</span></span><br><span class="line"><span class="string"> 11267 Worker</span></span><br><span class="line"><span class="string">&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;  bd-server-005  &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;</span></span><br><span class="line"><span class="string"> 10646 Worker</span></span><br><span class="line"><span class="string">[atguigu@bd-server-001 spark-standalone]$</span></span><br></pre></td></tr></table></figure></li>
<li>查看Master资源监控Web UI界面: <a href="http://bd-server-001:8080/">http://bd-server-001:8080/</a><br> <img src="https://s2.loli.net/2021/12/11/GM6EUpD5tJ7FckB.jpg"></li>
</ol>
<h3 id="3-2-4-提交应用"><a href="#3-2-4-提交应用" class="headerlink" title="3.2.4 提交应用"></a>3.2.4 提交应用</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">bin/spark-submit \</span><br><span class="line">--class org.apache.spark.examples.SparkPi \</span><br><span class="line">--master spark://bd-server-001:7077 \</span><br><span class="line">./examples/jars/spark-examples_2.12-3.0.0.jar \</span><br><span class="line">10</span><br></pre></td></tr></table></figure>
<ol>
<li>–class表示要执行程序的主类</li>
<li>–master spark://linux1:7077 独立部署模式，连接到Spark集群</li>
<li>spark-examples_2.12-3.0.0.jar 运行类所在的jar包</li>
<li>数字10表示程序的入口参数，用于设定当前应用的任务数量<br><img src="https://s2.loli.net/2021/12/11/vBFDSG5PC6TrlcR.jpg"></li>
<li>执行任务时，会产生多个Java进程 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@bd-server-001 ~]$ jpsall</span><br><span class="line">&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;  bd-server-001  &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;</span><br><span class="line"> <span class="string">41170 Master</span></span><br><span class="line"><span class="string"> 41262 Worker</span></span><br><span class="line"><span class="string"> 41561 SparkSubmit</span></span><br><span class="line"><span class="string"> 41674 CoarseGrainedExecutorBackend</span></span><br><span class="line"><span class="string">&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;  bd-server-002  &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;</span></span><br><span class="line"><span class="string"> 29924 Worker</span></span><br><span class="line"><span class="string"> 30101 CoarseGrainedExecutorBackend</span></span><br><span class="line"><span class="string">&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;  bd-server-003  &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;</span></span><br><span class="line"><span class="string"> 21171 Worker</span></span><br><span class="line"><span class="string"> 21348 CoarseGrainedExecutorBackend</span></span><br><span class="line"><span class="string">&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;  bd-server-004  &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;</span></span><br><span class="line"><span class="string"> 12192 Worker</span></span><br><span class="line"><span class="string"> 12370 CoarseGrainedExecutorBackend</span></span><br><span class="line"><span class="string">&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;  bd-server-005  &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;</span></span><br><span class="line"><span class="string"> 11571 Worker</span></span><br><span class="line"><span class="string"> 11748 CoarseGrainedExecutorBackend</span></span><br></pre></td></tr></table></figure>
<ul>
<li>SparkSubmit：任务提交进程</li>
<li>CoarseGrainedExecutorBackend：任务执行进程</li>
</ul>
</li>
<li>执行任务时，默认采用服务器集群节点的总核数，每个节点内存1024M。<br> <img src="https://s2.loli.net/2021/12/11/3RpYmK5CPNJ7vda.jpg"></li>
</ol>
<h3 id="3-2-5-提交参数说明"><a href="#3-2-5-提交参数说明" class="headerlink" title="3.2.5 提交参数说明"></a>3.2.5 提交参数说明</h3><p>在提交应用中，一般会同时一些提交参数</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">bin/spark-submit \</span><br><span class="line">--class &lt;main-class&gt;</span><br><span class="line">--master &lt;master-url&gt; \</span><br><span class="line">... <span class="comment"># other options</span></span><br><span class="line">&lt;application-jar&gt; \</span><br><span class="line">[application-arguments]</span><br></pre></td></tr></table></figure>
<table>
<thead>
<tr>
<th>参数</th>
<th>解释</th>
<th>可选值举例</th>
</tr>
</thead>
<tbody><tr>
<td>–class</td>
<td>Spark程序中包含主函数的类</td>
<td></td>
</tr>
<tr>
<td>–master</td>
<td>Spark程序运行的模式(环境)</td>
<td>模式：local[*]、spark://linux1:7077、Yarn</td>
</tr>
<tr>
<td>–executor-memory 1G</td>
<td>指定每个executor可用内存为1G</td>
<td>符合集群内存配置即可，具体情况具体分析。</td>
</tr>
<tr>
<td>–total-executor-cores 2</td>
<td>指定所有executor使用的cpu核数为2个</td>
<td></td>
</tr>
<tr>
<td>–executor-cores</td>
<td>指定每个executor使用的cpu核数</td>
<td></td>
</tr>
<tr>
<td>application-jar</td>
<td>打包好的应用jar，包含依赖。这个URL在集群中全局可见。 比如hdfs:// 共享存储系统，如果是file:// path，那么所有的节点的path都包含同样的jar</td>
<td></td>
</tr>
<tr>
<td>application-arguments</td>
<td>传给main()方法的参数</td>
<td></td>
</tr>
</tbody></table>
<h3 id="3-2-6-配置历史服务"><a href="#3-2-6-配置历史服务" class="headerlink" title="3.2.6 配置历史服务"></a>3.2.6 配置历史服务</h3><p>由于spark-shell停止掉后，集群监控linux1:4040页面就看不到历史任务的运行情况，所以开发时都配置历史服务器记录任务运行情况。</p>
<ol>
<li><p>修改spark-defaults.conf.template文件名为spark-defaults.conf</p>
 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mv spark-defaults.conf.template spark-defaults.conf</span><br></pre></td></tr></table></figure></li>
<li><p>修改spark-default.conf文件，配置日志存储路径</p>
 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">spark.eventLog.enabled          true</span><br><span class="line">spark.eventLog.dir              hdfs://bd-server-002:9092/directory</span><br></pre></td></tr></table></figure>
<ul>
<li>注意：需要启动hadoop集群，HDFS上的directory目录需要提前存在。</li>
</ul>
</li>
<li><p>修改spark-env.sh文件, 添加日志配置</p>
 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">export</span> SPARK_HISTORY_OPTS=<span class="string">&quot;</span></span><br><span class="line"><span class="string">-Dspark.history.ui.port=18080 </span></span><br><span class="line"><span class="string">-Dspark.history.fs.logDirectory=hdfs://bd-server-002:9092/directory </span></span><br><span class="line"><span class="string">-Dspark.history.retainedApplications=30&quot;</span></span><br></pre></td></tr></table></figure>
<ul>
<li>参数1含义：WEB UI访问的端口号为18080</li>
<li>参数2含义：指定历史服务器日志存储路径</li>
<li>参数3含义：指定保存Application历史记录的个数，如果超过这个值，旧的应用程序信息将被删除，这个是内存中的应用数，而不是页面上显示的应用数。</li>
</ul>
</li>
<li><p>分发配置文件</p>
 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">    xsync conf </span><br><span class="line">    ```    </span><br><span class="line">5. 重新启动集群和历史服务</span><br><span class="line">    ```bash</span><br><span class="line">    sbin/stop-all.sh</span><br><span class="line">    sbin/start-all.sh</span><br><span class="line">    sbin/start-history-server.sh</span><br></pre></td></tr></table></figure></li>
<li><p>重新执行任务</p>
 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">bin/spark-submit \</span><br><span class="line">--class org.apache.spark.examples.SparkPi \</span><br><span class="line">--master spark://bd-server-001:7077 \</span><br><span class="line">./examples/jars/spark-examples_2.12-3.0.0.jar \</span><br><span class="line">10</span><br></pre></td></tr></table></figure>
<p> <img src="https://s2.loli.net/2021/12/11/CO3ZQe7rWwEvb1R.jpg"></p>
</li>
<li><p>查看历史服务：<a href="http://bd-server-001:18080/">http://bd-server-001:18080</a><br> <img src="https://s2.loli.net/2021/12/11/j1xLsnyQ6hGpIlt.jpg"></p>
</li>
</ol>
<h3 id="3-2-7-配置高可用（HA）"><a href="#3-2-7-配置高可用（HA）" class="headerlink" title="3.2.7 配置高可用（HA）"></a>3.2.7 配置高可用（HA）</h3><p>所谓的高可用是因为当前集群中的Master节点只有一个，所以会存在单点故障问题。所以为了解决单点故障问题，需要在集群中配置多个Master节点，一旦处于活动状态的Master发生故障时，由备用Master提供服务，保证作业可以继续执行。这里的高可用一般采用Zookeeper设置<br>集群规划:<br>|      |Linux1     |Linux2    |Linux3  |<br>|—-|———|——-|———|<br>|Spark|    <font color ='red' >Master</font><br><font color ='blue' >Zookeeper</font><br>Worker|<font color ='red' >Master</font><br><font color ='blue' >Zookeeper</font><br>Worker|<br><font color ='blue' >Zookeeper</font><br>Worker|</p>
<ol>
<li><p>停止集群</p>
 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sbin/stop-all.sh </span><br></pre></td></tr></table></figure></li>
<li><p>启动Zookeeper</p>
 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">zookeeper_cluster.sh status</span><br></pre></td></tr></table></figure></li>
<li><p>修改spark-env.sh文件添加如下配置</p>
 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">注释如下内容：</span><br><span class="line"><span class="comment">#SPARK_MASTER_HOST=linux1</span></span><br><span class="line"><span class="comment">#SPARK_MASTER_PORT=7077</span></span><br><span class="line"></span><br><span class="line">添加如下内容:</span><br><span class="line"><span class="comment">#Master监控页面默认访问端口为8080，但是可能会和Zookeeper冲突，所以改成8989，也可以自定义，访问UI监控页面时请注意</span></span><br><span class="line">SPARK_MASTER_WEBUI_PORT=8989</span><br><span class="line"></span><br><span class="line"><span class="built_in">export</span> SPARK_DAEMON_JAVA_OPTS=<span class="string">&quot;</span></span><br><span class="line"><span class="string">-Dspark.deploy.recoveryMode=ZOOKEEPER </span></span><br><span class="line"><span class="string">-Dspark.deploy.zookeeper.url=linux1,linux2,linux3 </span></span><br><span class="line"><span class="string">-Dspark.deploy.zookeeper.dir=/spark&quot;</span></span><br></pre></td></tr></table></figure></li>
<li><p>分发配置文件</p>
 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">xsync conf/ </span><br></pre></td></tr></table></figure></li>
<li><p>启动集群</p>
 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sbin/start-all.sh</span><br></pre></td></tr></table></figure>
<p> <img src="https://s2.loli.net/2021/12/11/JjywVYhLs78W3KT.jpg"></p>
</li>
<li><p>启动bd-server-002和bd-server-003的单独Master节点，这两个节点Master状态处于备用状态</p>
 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sbin/start-master.sh</span><br></pre></td></tr></table></figure>
<p> <img src="https://s2.loli.net/2021/12/11/h69zTQAvVymeWZd.jpg"><br> <img src="https://s2.loli.net/2021/12/11/cm9lpOiIdFLvUCQ.jpg"></p>
</li>
<li><p>提交应用到高可用集群</p>
 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">bin/spark-submit \</span><br><span class="line">--class org.apache.spark.examples.SparkPi \</span><br><span class="line">--master spark://bd-server-001:7077,bd-server-002:7077,bd-server-003:7077 \</span><br><span class="line">./examples/jars/spark-examples_2.12-3.0.0.jar \</span><br><span class="line">10</span><br></pre></td></tr></table></figure></li>
<li><p>停止linux1的Master资源监控进程</p>
 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@bd-server-001 spark-standalone]$ jps</span><br><span class="line">48129 JournalNode</span><br><span class="line">48466 ResourceManager</span><br><span class="line">47557 QuorumPeerMain</span><br><span class="line">49861 Worker</span><br><span class="line">49957 HistoryServer</span><br><span class="line">50405 Jps</span><br><span class="line">48569 NodeManager</span><br><span class="line">48378 DFSZKFailoverController</span><br><span class="line">47902 DataNode</span><br><span class="line">49758 Master</span><br><span class="line">47775 NameNode</span><br><span class="line">[atguigu@bd-server-001 spark-standalone]$ <span class="built_in">kill</span> -9 49758</span><br><span class="line">[atguigu@bd-server-001 spark-standalone]$</span><br></pre></td></tr></table></figure></li>
<li><p>查看linux2的Master 资源监控Web UI，稍等一段时间后，linux2节点的Master状态提升为活动状态<br> <img src="https://s2.loli.net/2021/12/11/kslxS6FLeTvny7d.jpg"></p>
</li>
</ol>
<h2 id="3-3-Yarn模式"><a href="#3-3-Yarn模式" class="headerlink" title="3.3  Yarn模式"></a>3.3  Yarn模式</h2><p>独立部署（Standalone）模式由Spark自身提供计算资源，无需其他框架提供资源。这种方式降低了和其他第三方资源框架的耦合性，独立性非常强。但是你也要记住，Spark主要是计算框架，而不是资源调度框架，所以本身提供的资源调度并不是它的强项，所以还是和其他专业的资源调度框架集成会更靠谱一些。所以接下来我们来学习在强大的Yarn环境下Spark是如何工作的（其实是因为在国内工作中，Yarn使用的非常多）。</p>
<h3 id="3-3-1-解压缩文件"><a href="#3-3-1-解压缩文件" class="headerlink" title="3.3.1 解压缩文件"></a>3.3.1 解压缩文件</h3><p>将spark-3.0.0-bin-hadoop3.2.tgz文件上传到linux并解压缩，放置在指定位置。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">tar -zxvf spark-3.0.0-bin-hadoop3.2.tgz -C /opt/module</span><br><span class="line"><span class="built_in">cd</span> /opt/module </span><br><span class="line">mv spark-3.0.0-bin-hadoop3.2 spark-yarn</span><br></pre></td></tr></table></figure>


<h3 id="3-3-2-修改配置文件"><a href="#3-3-2-修改配置文件" class="headerlink" title="3.3.2 修改配置文件"></a>3.3.2 修改配置文件</h3><ol>
<li>修改hadoop配置文件/opt/module/hadoop/etc/hadoop/yarn-site.xml, 并分发 <figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!--是否启动一个线程检查每个任务正使用的物理内存量，如果任务超出分配值，则直接将其杀掉，默认是true --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.pmem-check-enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--是否启动一个线程检查每个任务正使用的虚拟内存量，如果任务超出分配值，则直接将其杀掉，默认是true --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.vmem-check-enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li>
</ol>
<ol start="2">
<li>修改conf/spark-env.sh，添加JAVA_HOME和YARN_CONF_DIR配置 <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">mv spark-env.sh.template spark-env.sh</span><br><span class="line"></span><br><span class="line">export <span class="type">JAVA_HOME</span>=/opt/module/jdk1<span class="number">.8</span><span class="number">.0</span>_144</span><br><span class="line"><span class="type">YARN_CONF_DIR</span>=/opt/module/hadoop/etc/hadoop</span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="3-3-3-启动HDFS以及YARN集群"><a href="#3-3-3-启动HDFS以及YARN集群" class="headerlink" title="3.3.3 启动HDFS以及YARN集群"></a>3.3.3 启动HDFS以及YARN集群</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">zookeeper_cluster.sh start</span><br><span class="line">hadoop_cluster.sh start</span><br></pre></td></tr></table></figure>


<h3 id="3-3-4-提交应用"><a href="#3-3-4-提交应用" class="headerlink" title="3.3.4 提交应用"></a>3.3.4 提交应用</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">bin/spark-submit \</span><br><span class="line">--<span class="class"><span class="keyword">class</span> <span class="title">org</span>.<span class="title">apache</span>.<span class="title">spark</span>.<span class="title">examples</span>.<span class="title">SparkPi</span> <span class="title">\</span></span></span><br><span class="line">--master yarn \</span><br><span class="line">--deploy-mode cluster \</span><br><span class="line">./examples/jars/spark-examples_2<span class="number">.12</span><span class="number">-3.0</span><span class="number">.0</span>.jar \</span><br><span class="line"><span class="number">10</span></span><br></pre></td></tr></table></figure>
<p><img src="https://s2.loli.net/2022/01/22/MkZTrcbmLfIdSj4.jpg"><br><img src="https://s2.loli.net/2022/01/22/4hMIgeWRdzQm3c6.jpg"></p>
<p>查看<a href="http://linux2:8088页面，点击History，查看历史页面">http://linux2:8088页面，点击History，查看历史页面</a><br><img src="https://s2.loli.net/2022/01/22/jesMriWz34o8StZ.jpg"><br><img src="https://s2.loli.net/2022/01/22/YfSAyeEh6XKcgrj.jpg"></p>
<h3 id="3-3-5-配置历史服务器"><a href="#3-3-5-配置历史服务器" class="headerlink" title="3.3.5 配置历史服务器"></a>3.3.5 配置历史服务器</h3><ol>
<li><p>修改spark-defaults.conf.template文件名为spark-defaults.conf</p>
 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mv spark-defaults.conf.template spark-defaults.conf</span><br></pre></td></tr></table></figure></li>
<li><p>修改spark-default.conf文件，配置日志存储路径</p>
 <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">spark.eventLog.enabled          <span class="literal">true</span></span><br><span class="line">spark.eventLog.dir               hdfs:<span class="comment">//linux1:8020/directory</span></span><br></pre></td></tr></table></figure>
<p> 注意：需要启动hadoop集群，HDFS上的目录需要提前存在。</p>
 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@linux1 hadoop]<span class="comment"># sbin/start-dfs.sh</span></span><br><span class="line">[root@linux1 hadoop]<span class="comment"># hadoop fs -mkdir /directory</span></span><br></pre></td></tr></table></figure></li>
<li><p>修改spark-env.sh文件, 添加日志配置</p>
 <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">export <span class="type">SPARK_HISTORY_OPTS</span>=<span class="string">&quot;</span></span><br><span class="line"><span class="string">-Dspark.history.ui.port=18080 </span></span><br><span class="line"><span class="string">-Dspark.history.fs.logDirectory=hdfs://linux1:8020/directory </span></span><br><span class="line"><span class="string">-Dspark.history.retainedApplications=30&quot;</span></span><br></pre></td></tr></table></figure>
<ul>
<li>参数1含义：WEB UI访问的端口号为18080</li>
<li>参数2含义：指定历史服务器日志存储路径</li>
<li>参数3含义：指定保存Application历史记录的个数，如果超过这个值，旧的应用程序信息将被删除，这个是内存中的应用数，而不是页面上显示的应用数。</li>
</ul>
</li>
<li><p>修改spark-defaults.conf</p>
 <figure class="highlight ini"><table><tr><td class="code"><pre><span class="line"><span class="attr">spark.yarn.historyServer.address</span>=linux1:<span class="number">18080</span></span><br><span class="line"><span class="attr">spark.history.ui.port</span>=<span class="number">18080</span></span><br></pre></td></tr></table></figure></li>
<li><p>启动历史服务</p>
 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sbin/start-history-server.sh </span><br></pre></td></tr></table></figure></li>
<li><p>重新提交应用</p>
 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">bin/spark-submit \</span><br><span class="line">--class org.apache.spark.examples.SparkPi \</span><br><span class="line">--master yarn \</span><br><span class="line">--deploy-mode client \</span><br><span class="line">./examples/jars/spark-examples_2.12-3.0.0.jar \</span><br><span class="line">10</span><br></pre></td></tr></table></figure>
<p> <img src="https://s2.loli.net/2022/01/22/YgUQu7FEVz9JTP2.jpg"><br> <img src="https://s2.loli.net/2022/01/22/bihJcluITaWKwA2.jpg"></p>
</li>
<li><p>Web页面查看日志：<a href="http://linux2:8088/">http://linux2:8088</a><br> <img src="https://s2.loli.net/2022/01/22/BDFdKT2ocOu3x71.jpg"><br> <img src="https://s2.loli.net/2022/01/22/MdyXW6Kks5AJTEp.jpg"></p>
</li>
</ol>
<h2 id="3-4-K8S-amp-Mesos模式"><a href="#3-4-K8S-amp-Mesos模式" class="headerlink" title="3.4  K8S &amp; Mesos模式"></a>3.4  K8S &amp; Mesos模式</h2><p>Mesos是Apache下的开源分布式资源管理框架，它被称为是分布式系统的内核,在Twitter得到广泛使用,管理着Twitter超过30,0000台服务器上的应用部署，但是在国内，依然使用着传统的Hadoop大数据框架，所以国内使用Mesos框架的并不多，但是原理都差不多。</p>
<p>容器化部署是目前业界很流行的一项技术，基于Docker镜像运行能够让用户更加方便地对应用进行管理和运维。容器管理工具中最为流行的就是Kubernetes（k8s），而Spark也在最近的版本中支持了k8s部署模式。这里我们也不做过多的讲解。给个链接大家自己感受一下：<a href="https://spark.apache.org/docs/latest/running-on-kubernetes.html">https://spark.apache.org/docs/latest/running-on-kubernetes.html</a></p>
<h2 id="3-5-本地模式"><a href="#3-5-本地模式" class="headerlink" title="3.5  本地模式"></a>3.5  本地模式</h2><p>在同学们自己学习时，每次都需要启动虚拟机，启动集群，这是一个比较繁琐的过程，并且会占大量的系统资源，导致系统执行变慢，不仅仅影响学习效果，也影响学习进度，Spark非常暖心地提供了可以在本地操作系统下启动本地集群的方式，这样，在不使用虚拟机的情况下，也能学习Spark的基本使用</p>
<h2 id="3-5-1-解压缩文件"><a href="#3-5-1-解压缩文件" class="headerlink" title="3.5.1 解压缩文件"></a>3.5.1 解压缩文件</h2><p>将文件spark-3.0.0-bin-hadoop3.2.tgz解压缩到无中文无空格的路径中</p>
<h3 id="3-5-2-启动本地环境"><a href="#3-5-2-启动本地环境" class="headerlink" title="3.5.2 启动本地环境"></a>3.5.2 启动本地环境</h3><ol>
<li>执行解压缩文件路径下bin目录中的spark-shell文件，启动Spark本地环境 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">&gt;bin  ./spark-shell</span><br><span class="line">CMD /Library/Java/JavaVirtualMachines/jdk1.8.0_291.jdk/Contents/Home//bin/java</span><br><span class="line">/Library/Java/JavaVirtualMachines/jdk1.8.0_291.jdk/Contents/Home//bin/java -cp /Users/zhenan/app/spark-3.0.0-bin-hadoop3.2/conf/:/Users/zhenan/app/spark-3.0.0-bin-hadoop3.2/jars/* -Dscala.usejavacp=<span class="literal">true</span> -Xmx1g org.apache.spark.deploy.SparkSubmit --class org.apache.spark.repl.Main --name Spark shell spark-shell</span><br><span class="line">21/12/18 20:44:00 WARN NativeCodeLoader: Unable to load native-hadoop library <span class="keyword">for</span> your platform... using builtin-java classes <span class="built_in">where</span> applicable</span><br><span class="line">Using Spark<span class="string">&#x27;s default log4j profile: org/apache/spark/log4j-defaults.properties</span></span><br><span class="line"><span class="string">Setting default log level to &quot;WARN&quot;.</span></span><br><span class="line"><span class="string">To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).</span></span><br><span class="line"><span class="string">Spark context Web UI available at http://bogon:4040</span></span><br><span class="line"><span class="string">Spark context available as &#x27;</span>sc<span class="string">&#x27; (master = local[*], app id = local-1639831450687).</span></span><br><span class="line"><span class="string">Spark session available as &#x27;</span>spark<span class="string">&#x27;.</span></span><br><span class="line"><span class="string">Welcome to</span></span><br><span class="line"><span class="string">      ____              __</span></span><br><span class="line"><span class="string">     / __/__  ___ _____/ /__</span></span><br><span class="line"><span class="string">    _\ \/ _ \/ _ `/ __/  &#x27;</span>_/</span><br><span class="line">   /___/ .__/\_,_/_/ /_/\_\   version 3.0.0</span><br><span class="line">      /_/</span><br><span class="line"></span><br><span class="line">Using Scala version 2.12.10 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_291)</span><br><span class="line">Type <span class="keyword">in</span> expressions to have them evaluated.</span><br><span class="line">Type :<span class="built_in">help</span> <span class="keyword">for</span> more information.</span><br><span class="line"></span><br><span class="line">scala</span><br></pre></td></tr></table></figure></li>
<li>在bin目录中创建input目录，并添加word.txt文件, 在命令行中输入脚本代码 <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; sc.makeRDD(<span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>)).map(_+<span class="number">1</span>).collect()</span><br><span class="line">res1: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">scala&gt;</span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="3-5-3-命令行提交应用"><a href="#3-5-3-命令行提交应用" class="headerlink" title="3.5.3 命令行提交应用"></a>3.5.3 命令行提交应用</h3><p>在命令行窗口中执行提交指令</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">☁  bin  ./spark-submit --<span class="class"><span class="keyword">class</span> <span class="title">org</span>.<span class="title">apache</span>.<span class="title">spark</span>.<span class="title">examples</span>.<span class="title">SparkPi</span> <span class="title">--master</span> &#x27;<span class="title">local</span>[2]&#x27; ..<span class="title">/examples/jars/spark-examples_2</span>.12<span class="title">-3</span>.0.0.<span class="title">jar</span> 10</span></span><br><span class="line"><span class="type">CMD</span> /<span class="type">Library</span>/<span class="type">Java</span>/<span class="type">JavaVirtualMachines</span>/jdk1<span class="number">.8</span><span class="number">.0</span>_291.jdk/<span class="type">Contents</span>/<span class="type">Home</span><span class="comment">//bin/java</span></span><br><span class="line">/<span class="type">Library</span>/<span class="type">Java</span>/<span class="type">JavaVirtualMachines</span>/jdk1<span class="number">.8</span><span class="number">.0</span>_291.jdk/<span class="type">Contents</span>/<span class="type">Home</span><span class="comment">//bin/java -cp /Users/zhenan/app/spark-3.0.0-bin-hadoop3.2/conf/:/Users/zhenan/app/spark-3.0.0-bin-hadoop3.2/jars/* -Xmx1g org.apache.spark.deploy.SparkSubmit --master local[2] --class org.apache.spark.examples.SparkPi ../examples/jars/spark-examples_2.12-3.0.0.jar 10</span></span><br><span class="line"><span class="number">21</span>/<span class="number">12</span>/<span class="number">18</span> <span class="number">20</span>:<span class="number">47</span>:<span class="number">19</span> <span class="type">WARN</span> <span class="type">NativeCodeLoader</span>: <span class="type">Unable</span> to load native-hadoop library <span class="keyword">for</span> your platform... using builtin-java classes where applicable</span><br><span class="line"><span class="type">Using</span> <span class="type">Spark</span><span class="symbol">&#x27;s</span> <span class="keyword">default</span> log4j profile: org/apache/spark/log4j-defaults.properties</span><br><span class="line"><span class="number">21</span>/<span class="number">12</span>/<span class="number">18</span> <span class="number">20</span>:<span class="number">47</span>:<span class="number">20</span> <span class="type">INFO</span> <span class="type">SparkContext</span>: <span class="type">Running</span> <span class="type">Spark</span> version <span class="number">3.0</span><span class="number">.0</span></span><br><span class="line"><span class="number">21</span>/<span class="number">12</span>/<span class="number">18</span> <span class="number">20</span>:<span class="number">47</span>:<span class="number">20</span> <span class="type">INFO</span> <span class="type">ResourceUtils</span>: ==============================================================</span><br><span class="line"><span class="number">21</span>/<span class="number">12</span>/<span class="number">18</span> <span class="number">20</span>:<span class="number">47</span>:<span class="number">20</span> <span class="type">INFO</span> <span class="type">ResourceUtils</span>: <span class="type">Resources</span> <span class="keyword">for</span> spark.driver:</span><br><span class="line">。。。</span><br><span class="line">。。。</span><br><span class="line">。。。</span><br><span class="line"><span class="type">Pi</span> is roughly <span class="number">3.140919140919141</span></span><br><span class="line"><span class="number">21</span>/<span class="number">12</span>/<span class="number">18</span> <span class="number">20</span>:<span class="number">47</span>:<span class="number">23</span> <span class="type">INFO</span> <span class="type">SparkUI</span>: <span class="type">Stopped</span> <span class="type">Spark</span> web <span class="type">UI</span> at http:<span class="comment">//bogon:4040</span></span><br><span class="line"><span class="number">21</span>/<span class="number">12</span>/<span class="number">18</span> <span class="number">20</span>:<span class="number">47</span>:<span class="number">23</span> <span class="type">INFO</span> <span class="type">MapOutputTrackerMasterEndpoint</span>: <span class="type">MapOutputTrackerMasterEndpoint</span> stopped!</span><br><span class="line"><span class="number">21</span>/<span class="number">12</span>/<span class="number">18</span> <span class="number">20</span>:<span class="number">47</span>:<span class="number">23</span> <span class="type">INFO</span> <span class="type">MemoryStore</span>: <span class="type">MemoryStore</span> cleared</span><br><span class="line"><span class="number">21</span>/<span class="number">12</span>/<span class="number">18</span> <span class="number">20</span>:<span class="number">47</span>:<span class="number">23</span> <span class="type">INFO</span> <span class="type">BlockManager</span>: <span class="type">BlockManager</span> stopped</span><br><span class="line"><span class="number">21</span>/<span class="number">12</span>/<span class="number">18</span> <span class="number">20</span>:<span class="number">47</span>:<span class="number">23</span> <span class="type">INFO</span> <span class="type">BlockManagerMaster</span>: <span class="type">BlockManagerMaster</span> stopped</span><br><span class="line"><span class="number">21</span>/<span class="number">12</span>/<span class="number">18</span> <span class="number">20</span>:<span class="number">47</span>:<span class="number">23</span> <span class="type">INFO</span> <span class="type">OutputCommitCoordinator</span>$<span class="type">OutputCommitCoordinatorEndpoint</span>: <span class="type">OutputCommitCoordinator</span> stopped!</span><br><span class="line"><span class="number">21</span>/<span class="number">12</span>/<span class="number">18</span> <span class="number">20</span>:<span class="number">47</span>:<span class="number">23</span> <span class="type">INFO</span> <span class="type">SparkContext</span>: <span class="type">Successfully</span> stopped <span class="type">SparkContext</span></span><br><span class="line"><span class="number">21</span>/<span class="number">12</span>/<span class="number">18</span> <span class="number">20</span>:<span class="number">47</span>:<span class="number">23</span> <span class="type">INFO</span> <span class="type">ShutdownHookManager</span>: <span class="type">Shutdown</span> hook called</span><br><span class="line"><span class="number">21</span>/<span class="number">12</span>/<span class="number">18</span> <span class="number">20</span>:<span class="number">47</span>:<span class="number">23</span> <span class="type">INFO</span> <span class="type">ShutdownHookManager</span>: <span class="type">Deleting</span> directory /<span class="keyword">private</span>/<span class="keyword">var</span>/folders/__/y015x_9s237711b3mcmf0t1c0000gn/<span class="type">T</span>/spark<span class="number">-49</span>b55761-b06e<span class="number">-43</span>a8<span class="number">-89</span>d3-f6ae77ed8e45</span><br><span class="line"><span class="number">21</span>/<span class="number">12</span>/<span class="number">18</span> <span class="number">20</span>:<span class="number">47</span>:<span class="number">23</span> <span class="type">INFO</span> <span class="type">ShutdownHookManager</span>: <span class="type">Deleting</span> directory /<span class="keyword">private</span>/<span class="keyword">var</span>/folders/__/y015x_9s237711b3mcmf0t1c0000gn/<span class="type">T</span>/spark<span class="number">-3</span>ec7edc0-eca2<span class="number">-4003</span>-ad13<span class="number">-6</span>febbc651e59</span><br></pre></td></tr></table></figure>

<h2 id="3-6-部署模式对比"><a href="#3-6-部署模式对比" class="headerlink" title="3.6  部署模式对比"></a>3.6  部署模式对比</h2><table>
<thead>
<tr>
<th>模式</th>
<th>Spark安装机器数</th>
<th>需启动的进程</th>
<th>所属者</th>
<th>应用场景</th>
</tr>
</thead>
<tbody><tr>
<td>Local</td>
<td>1</td>
<td>无</td>
<td>Spark</td>
<td>测试</td>
</tr>
<tr>
<td>Standalone</td>
<td>3</td>
<td>Master及Worker</td>
<td>Spark</td>
<td>单独部署</td>
</tr>
<tr>
<td>Yarn</td>
<td>1</td>
<td>Yarn及HDFS</td>
<td>Hadoop</td>
<td>混合部署</td>
</tr>
</tbody></table>
<h2 id="3-7-端口号"><a href="#3-7-端口号" class="headerlink" title="3.7  端口号"></a>3.7  端口号</h2><ul>
<li>Spark查看当前Spark-shell运行任务情况端口号：4040（计算）</li>
<li>Spark Master内部通信服务端口号：7077</li>
<li>Standalone模式下，Spark Master Web端口号：8080（资源）</li>
<li>Spark历史服务器端口号：18080</li>
<li>Hadoop YARN任务运行情况查看端口号：8088</li>
</ul>
<h1 id="第4章-Spark运行架构"><a href="#第4章-Spark运行架构" class="headerlink" title="第4章 Spark运行架构"></a>第4章 Spark运行架构</h1><h2 id="4-1-运行架构"><a href="#4-1-运行架构" class="headerlink" title="4.1 运行架构"></a>4.1 运行架构</h2><p>Spark框架的核心是一个计算引擎，整体来说，它采用了标准 master-slave 的结构。<br>如下图所示，它展示了一个 Spark执行时的基本结构。图形中的Driver表示master，负责管理整个集群中的作业任务调度。图形中的Executor 则是 slave，负责实际执行任务。<br><img src="https://s2.loli.net/2021/12/11/BjZFPrvakJHNEhI.jpg"></p>
<h2 id="4-2-核心组件"><a href="#4-2-核心组件" class="headerlink" title="4.2 核心组件"></a>4.2 核心组件</h2><p>由上图可以看出，对于Spark框架有两个核心组件：Driver和Executor</p>
<h3 id="4-2-1-Driver"><a href="#4-2-1-Driver" class="headerlink" title="4.2.1 Driver"></a>4.2.1 Driver</h3><p>Spark驱动器节点，用于执行Spark任务中的main方法，负责实际代码的执行工作。Driver在Spark作业执行时主要负责：</p>
<ul>
<li>将用户程序转化为作业（job）</li>
<li>在Executor之间调度任务(task)</li>
<li>跟踪Executor的执行情况</li>
<li>通过UI展示查询运行情况<br>实际上，我们无法准确地描述Driver的定义，因为在整个的编程过程中没有看到任何有关Driver的字眼。所以简单理解，所谓的Driver就是驱使整个应用运行起来的程序，也称之为Driver类。</li>
</ul>
<h3 id="4-2-2-Executor"><a href="#4-2-2-Executor" class="headerlink" title="4.2.2 Executor"></a>4.2.2 Executor</h3><p>Spark Executor是集群中工作节点（Worker）中的一个JVM进程，负责在 Spark 作业中运行具体任务（Task），任务彼此之间相互独立。Spark 应用启动时，Executor节点被同时启动，并且始终伴随着整个 Spark 应用的生命周期而存在。如果有Executor节点发生了故障或崩溃，Spark 应用也可以继续执行，会将出错节点上的任务调度到其他Executor节点上继续运行。<br>Executor有两个核心功能：</p>
<ul>
<li>负责运行组成Spark应用的任务，并将结果返回给驱动器进程</li>
<li>它们通过自身的块管理器（Block Manager）为用户程序中要求缓存的 RDD 提供内存式存储。RDD 是直接缓存在Executor进程内的，因此任务可以在运行时充分利用缓存数据加速运算。</li>
</ul>
<h3 id="4-2-3-Master-amp-Worker"><a href="#4-2-3-Master-amp-Worker" class="headerlink" title="4.2.3 Master &amp; Worker"></a>4.2.3 Master &amp; Worker</h3><p>Spark集群的独立部署环境中，不需要依赖其他的资源调度框架，自身就实现了资源调度的功能，所以环境中还有其他两个核心组件：Master和Worker，这里的Master是一个进程，主要负责资源的调度和分配，并进行集群的监控等职责，类似于Yarn环境中的RM, 而Worker呢，也是进程，一个Worker运行在集群中的一台服务器上，由Master分配资源对数据进行并行的处理和计算，类似于Yarn环境中NM。</p>
<h3 id="4-2-4-ApplicationMaster"><a href="#4-2-4-ApplicationMaster" class="headerlink" title="4.2.4 ApplicationMaster"></a>4.2.4 ApplicationMaster</h3><p>Hadoop用户向YARN集群提交应用程序时,提交程序中应该包含ApplicationMaster，用于向资源调度器申请执行任务的资源容器Container，运行用户自己的程序任务job，监控整个任务的执行，跟踪整个任务的状态，处理任务失败等异常情况。<br>说的简单点就是，ResourceManager（资源）和Driver（计算）之间的解耦合靠的就是ApplicationMaster。</p>
<h2 id="4-3-核心概念"><a href="#4-3-核心概念" class="headerlink" title="4.3 核心概念"></a>4.3 核心概念</h2><h3 id="4-3-1-Executor与Core（核）"><a href="#4-3-1-Executor与Core（核）" class="headerlink" title="4.3.1 Executor与Core（核）"></a>4.3.1 Executor与Core（核）</h3><p>Spark Executor是集群中运行在工作节点（Worker）中的一个JVM进程，是整个集群中的专门用于计算的节点。在提交应用中，可以提供参数指定计算节点的个数，以及对应的资源。这里的资源一般指的是工作节点Executor的内存大小和使用的虚拟CPU核（Core）数量。</p>
<p>应用程序相关启动参数如下：<br>|名称                  |   说明                      |<br>|——————-|—————————-|<br>|–num-executors      |配置Executor的数量                   |<br>|–executor-memory  |    配置每个Executor的内存大小         |<br>|–executor-cores      |配置每个Executor的虚拟CPU core数量      |</p>
<h3 id="4-3-2-并行度（Parallelism）"><a href="#4-3-2-并行度（Parallelism）" class="headerlink" title="4.3.2 并行度（Parallelism）"></a>4.3.2 并行度（Parallelism）</h3><p>在分布式计算框架中一般都是多个任务同时执行，由于任务分布在不同的计算节点进行计算，所以能够真正地实现多任务并行执行，记住，这里是并行，而不是并发。这里我们将整个集群并行执行任务的数量称之为并行度。那么一个作业到底并行度是多少呢？这个取决于框架的默认配置。应用程序也可以在运行过程中动态修改。</p>
<h3 id="4-3-3-有向无环图（DAG）"><a href="#4-3-3-有向无环图（DAG）" class="headerlink" title="4.3.3 有向无环图（DAG）"></a>4.3.3 有向无环图（DAG）</h3><p>大数据计算引擎框架我们根据使用方式的不同一般会分为四类，其中第一类就是Hadoop所承载的MapReduce,它将计算分为两个阶段，分别为 Map阶段 和 Reduce阶段。对于上层应用来说，就不得不想方设法去拆分算法，甚至于不得不在上层应用实现多个 Job 的串联，以完成一个完整的算法，例如迭代计算。 由于这样的弊端，催生了支持 DAG 框架的产生。因此，支持 DAG 的框架被划分为第二代计算引擎。如 Tez 以及更上层的 Oozie。这里我们不去细究各种 DAG 实现之间的区别，不过对于当时的 Tez 和 Oozie 来说，大多还是批处理的任务。接下来就是以 Spark 为代表的第三代的计算引擎。第三代计算引擎的特点主要是 Job 内部的 DAG 支持（不跨越 Job），以及实时计算。<br>这里所谓的有向无环图，并不是真正意义的图形，而是由Spark程序直接映射成的数据流的高级抽象模型。简单理解就是将整个程序计算的执行过程用图形表示出来,这样更直观，更便于理解，可以用于表示程序的拓扑结构。<br>DAG（Directed Acyclic Graph）有向无环图是由点和线组成的拓扑图形，该图形具有方向，不会闭环。</p>
<h3 id="4-4-提交流程"><a href="#4-4-提交流程" class="headerlink" title="4.4 提交流程"></a>4.4 提交流程</h3><p>所谓的提交流程，其实就是我们开发人员根据需求写的应用程序通过Spark客户端提交给Spark运行环境执行计算的流程。在不同的部署环境中，这个提交过程基本相同，但是又有细微的区别，我们这里不进行详细的比较，但是因为国内工作中，将Spark引用部署到Yarn环境中会更多一些，所以本课程中的提交流程是基于Yarn环境的。</p>
<p>Spark应用程序提交到Yarn环境中执行的时候，一般会有两种部署执行的方式：Client和Cluster。两种模式主要区别在于：Driver程序的运行节点位置。</p>
<h3 id="4-2-1-Yarn-Client模式"><a href="#4-2-1-Yarn-Client模式" class="headerlink" title="4.2.1 Yarn Client模式"></a>4.2.1 Yarn Client模式</h3><p>Client模式将用于监控和调度的Driver模块在客户端执行，而不是在Yarn中，所以一般用于测试。</p>
<ul>
<li>Driver在任务提交的本地机器上运行</li>
<li>Driver启动后会和ResourceManager通讯申请启动ApplicationMaster</li>
<li>ResourceManager分配container，在合适的NodeManager上启动ApplicationMaster，负责向ResourceManager申请Executor内存</li>
<li>ResourceManager接到ApplicationMaster的资源申请后会分配container，然后ApplicationMaster在资源分配指定的NodeManager上启动Executor进程</li>
<li>Executor进程启动后会向Driver反向注册，Executor全部注册完成后Driver开始执行main函数</li>
<li>之后执行到Action算子时，触发一个Job，并根据宽依赖开始划分stage，每个stage生成对应的TaskSet，之后将task分发到各个Executor上执行。</li>
</ul>
<h3 id="4-2-2-Yarn-Cluster模式"><a href="#4-2-2-Yarn-Cluster模式" class="headerlink" title="4.2.2 Yarn Cluster模式"></a>4.2.2 Yarn Cluster模式</h3><p>Cluster模式将用于监控和调度的Driver模块启动在Yarn集群资源中执行。一般应用于实际生产环境。</p>
<ul>
<li>在YARN Cluster模式下，任务提交后会和ResourceManager通讯申请启动ApplicationMaster，</li>
<li>随后ResourceManager分配container，在合适的NodeManager上启动ApplicationMaster，此时的ApplicationMaster就是Driver。</li>
<li>Driver启动后向ResourceManager申请Executor内存，ResourceManager接到ApplicationMaster的资源申请后会分配container，然后在合适的NodeManager上启动Executor进程</li>
<li>Executor进程启动后会向Driver反向注册，Executor全部注册完成后Driver开始执行main函数，</li>
<li>之后执行到Action算子时，触发一个Job，并根据宽依赖开始划分stage，每个stage生成对应的TaskSet，之后将task分发到各个Executor上执行。</li>
</ul>
<h1 id="第5章-Spark核心编程"><a href="#第5章-Spark核心编程" class="headerlink" title="第5章 Spark核心编程"></a>第5章 Spark核心编程</h1><p>Spark计算框架为了能够进行高并发和高吞吐的数据处理，封装了三大数据结构，用于处理不同的应用场景。三大数据结构分别是：</p>
<ul>
<li>RDD : 弹性分布式数据集</li>
<li>累加器：分布式共享只写变量</li>
<li>广播变量：分布式共享只读变量</li>
</ul>
<h2 id="5-1-RDD"><a href="#5-1-RDD" class="headerlink" title="5.1 RDD"></a>5.1 RDD</h2><h3 id="5-1-1-什么是RDD"><a href="#5-1-1-什么是RDD" class="headerlink" title="5.1.1 什么是RDD"></a>5.1.1 什么是RDD</h3><p>RDD（Resilient Distributed Dataset）叫做弹性分布式数据集，是Spark中最基本的数据处理模型。代码中是一个抽象类，它代表一个弹性的、不可变、可分区、里面的元素可并行计算的集合。</p>
<ul>
<li>弹性<ul>
<li>存储的弹性：内存与磁盘的自动切换；</li>
<li>容错的弹性：数据丢失可以自动恢复；</li>
<li>计算的弹性：计算出错重试机制；</li>
<li>分片的弹性：可根据需要重新分片。</li>
</ul>
</li>
<li>分布式：数据存储在大数据集群不同节点上</li>
<li>数据集：RDD封装了计算逻辑，只处理数据，并不保存数据</li>
<li>数据抽象：RDD是一个抽象类，需要子类具体实现</li>
<li>不可变：RDD封装了计算逻辑，是不可以改变的，想要改变，只能产生新的RDD，在新的RDD里面封装计算逻辑</li>
<li>可分区、并行计算</li>
</ul>
<h3 id="5-1-2-核心属性"><a href="#5-1-2-核心属性" class="headerlink" title="5.1.2 核心属性"></a>5.1.2 核心属性</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">* Internally, each RDD is characterized by five main properties:</span><br><span class="line">*</span><br><span class="line">*  分区列表 - A list of partitions</span><br><span class="line">*  分区计算函数 - A function for computing each split  </span><br><span class="line">*  RDD之间的依赖关系 - A list of dependencies on other RDDs  </span><br><span class="line">*  分区器（可选） - Optionally, a Partitioner for key-value RDDs (e.g. to say that the RDD is hash-partitioned)  </span><br><span class="line">*  首选位置（可选） - Optionally, a list of preferred locations to compute each split on (e.g. block locations for an HDFS file)           </span><br></pre></td></tr></table></figure>

<ul>
<li><p>分区列表<br>  RDD数据结构中存在分区列表，用于执行任务时并行计算，是实现分布式计算的重要属性。</p>
  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Implemented by subclasses to return the set of partitions in this RDD. This method will only</span></span><br><span class="line"><span class="comment"> * be called once, so it is safe to implement a time-consuming computation in it.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * The partitions in this array must satisfy the following property:</span></span><br><span class="line"><span class="comment"> *   `rdd.partitions.zipWithIndex.forall &#123; case (partition, index) =&gt; partition.index == index &#125;`</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">getPartitions</span></span>: <span class="type">Array</span>[<span class="type">Partition</span>]</span><br></pre></td></tr></table></figure></li>
<li><p>分区计算函数<br>  Spark在计算时，是使用分区函数对每一个分区进行计算</p>
  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * :: DeveloperApi ::</span></span><br><span class="line"><span class="comment"> * Implemented by subclasses to compute a given partition.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@DeveloperApi</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute</span></span>(split: <span class="type">Partition</span>, context: <span class="type">TaskContext</span>): <span class="type">Iterator</span>[<span class="type">T</span>]</span><br></pre></td></tr></table></figure></li>
<li><p>RDD之间的依赖关系<br>  RDD是计算模型的封装，当需求中需要将多个计算模型进行组合时，就需要将多个RDD建立依赖关系</p>
  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Implemented by subclasses to return how this RDD depends on parent RDDs. This method will only</span></span><br><span class="line"><span class="comment"> * be called once, so it is safe to implement a time-consuming computation in it.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">getDependencies</span></span>: <span class="type">Seq</span>[<span class="type">Dependency</span>[_]] = deps</span><br></pre></td></tr></table></figure></li>
<li><p>分区器（可选）<br>  当数据为KV类型数据时，可以通过设定分区器自定义数据的分区</p>
  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">/** Optionally overridden by subclasses to specify how they are partitioned. */</span></span><br><span class="line"><span class="meta">@transient</span> <span class="keyword">val</span> partitioner: <span class="type">Option</span>[<span class="type">Partitioner</span>] = <span class="type">None</span></span><br></pre></td></tr></table></figure></li>
<li><p>首选位置（可选）</p>
<ul>
<li>计算数据时，可以根据计算节点的状态选择不同的节点位置进行计算  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Optionally overridden by subclasses to specify placement preferences.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">getPreferredLocations</span></span>(split: <span class="type">Partition</span>): <span class="type">Seq</span>[<span class="type">String</span>] = <span class="type">Nil</span></span><br></pre></td></tr></table></figure></li>
<li>数据计算本地化<ul>
<li>进程本地化：数据和计算在同一个进程中</li>
<li>节点本地化：数据和计算在同一个节点中</li>
<li>机架本地化：数据和计算在同一个机架中</li>
<li>任意：数据和计算在不同的任意节点</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="5-1-3-执行原理"><a href="#5-1-3-执行原理" class="headerlink" title="5.1.3 执行原理"></a>5.1.3 执行原理</h3><p>从计算的角度来讲，数据处理过程中需要计算资源（内存 &amp; CPU）和计算模型（逻辑）。执行时，需要将计算资源和计算模型进行协调和整合。<br>Spark框架在执行时，先申请资源，然后将应用程序的数据处理逻辑分解成一个一个的计算任务。然后将任务发到已经分配资源的计算节点上, 按照指定的计算模型进行数据计算。最后得到计算结果。<br>RDD是Spark框架中用于数据处理的核心模型，接下来我们看看，在Yarn环境中，RDD的工作原理:</p>
<ol>
<li>启动Yarn集群环境<pre><code> ![](https://s2.loli.net/2021/12/11/TRiUEOMQtA36CHn.jpg)
</code></pre>
</li>
<li>Spark通过申请资源创建调度节点和计算节点<br> <img src="https://s2.loli.net/2021/12/11/laJOMSj6m1BUeKG.jpg"></li>
<li>Spark框架根据需求将计算逻辑根据分区划分成不同的任务<br> <img src="https://s2.loli.net/2021/12/11/MRKL1gjPIN3ATo2.jpg"></li>
<li>调度节点将任务根据计算节点状态发送到对应的计算节点进行计算<br> <img src="https://s2.loli.net/2021/12/11/PC91pTKyi47RhzA.jpg"></li>
</ol>
<p>从以上流程可以看出RDD在整个流程中主要用于将逻辑进行封装，并生成Task发送给Executor节点执行计算，接下来我们就一起看看Spark框架中RDD是具体是如何进行数据处理的。</p>
<h3 id="5-1-4-基础编程"><a href="#5-1-4-基础编程" class="headerlink" title="5.1.4 基础编程"></a>5.1.4 基础编程</h3><h4 id="5-1-4-1-RDD创建"><a href="#5-1-4-1-RDD创建" class="headerlink" title="5.1.4.1 RDD创建"></a>5.1.4.1 RDD创建</h4><p>在Spark中创建RDD的创建方式可以分为四种：</p>
<ol>
<li>从集合（内存）中创建RDD<ul>
<li>从集合中创建RDD，Spark主要提供了两个方法：parallelize和makeRDD  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">    .setMaster(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">    .setAppName(<span class="string">&quot;spark&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"><span class="keyword">val</span> rdd1 = sparkContext.parallelize(</span><br><span class="line">    <span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line">)</span><br><span class="line"><span class="keyword">val</span> rdd2 = sparkContext.makeRDD(</span><br><span class="line">    <span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line">)</span><br><span class="line">rdd1.collect().foreach(println)</span><br><span class="line">rdd2.collect().foreach(println)</span><br><span class="line">sparkContext.stop()</span><br></pre></td></tr></table></figure></li>
<li>从底层代码实现来讲，makeRDD方法其实就是parallelize方法  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">makeRDD</span></span>[<span class="type">T</span>: <span class="type">ClassTag</span>](</span><br><span class="line">    seq: <span class="type">Seq</span>[<span class="type">T</span>],</span><br><span class="line">    numSlices: <span class="type">Int</span> = defaultParallelism): <span class="type">RDD</span>[<span class="type">T</span>] = withScope &#123;</span><br><span class="line">  parallelize(seq, numSlices)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>从外部存储（文件）创建RDD<ul>
<li>由外部存储系统的数据集创建RDD包括：本地的文件系统，所有Hadoop支持的数据集，比如HDFS、HBase等。  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> sparkConf =<span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">    .setMaster(<span class="string">&quot;local[*]&quot;</span>).setAppName(<span class="string">&quot;spark&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"><span class="keyword">val</span> fileRDD: <span class="type">RDD</span>[<span class="type">String</span>] = sparkContext.textFile(<span class="string">&quot;input&quot;</span>)</span><br><span class="line">fileRDD.collect().foreach(println)</span><br><span class="line">sparkContext.stop()</span><br></pre></td></tr></table></figure></li>
<li>textFile 可以将文件作为数据源<ul>
<li>参数表示文件路径,可以为绝对路径也可以为相对路径<ul>
<li>绝对路径：不可改变的路径<ul>
<li>网络：网络协议://IP:Port//Path</li>
<li>文件系统：file://xx/xx/xx</li>
</ul>
</li>
<li>相对路径：可以改变的路径，一定存在一个基准路径</li>
</ul>
</li>
<li>返回结果为RDD数据模型，其中泛型标识文件中的每一行数据</li>
</ul>
</li>
</ul>
</li>
<li>从其他RDD创建<br> 主要是通过一个RDD运算完后，再产生新的RDD。详情请参考后续章节</li>
<li>直接创建RDD（new）<br> 使用new的方式直接构造RDD，一般由Spark框架自身使用。</li>
</ol>
<h4 id="5-1-4-2-RDD并行度与分区"><a href="#5-1-4-2-RDD并行度与分区" class="headerlink" title="5.1.4.2 RDD并行度与分区"></a>5.1.4.2 RDD并行度与分区</h4><p>默认情况下，Spark可以将一个作业切分多个任务后，发送给Executor节点并行计算，而能够并行计算的任务数量我们称之为并行度。这个数量可以在构建RDD时指定。记住，这里的并行执行的任务数量，并不是指的切分任务的数量，不要混淆了。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">&quot;local[*]&quot;</span>).setAppName(<span class="string">&quot;spark&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"><span class="keyword">val</span> dataRDD: <span class="type">RDD</span>[<span class="type">Int</span>] =</span><br><span class="line">    sparkContext.makeRDD(</span><br><span class="line">        <span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>),</span><br><span class="line">        <span class="number">4</span>)</span><br><span class="line"><span class="keyword">val</span> fileRDD: <span class="type">RDD</span>[<span class="type">String</span>] =</span><br><span class="line">    sparkContext.textFile(</span><br><span class="line">        <span class="string">&quot;input&quot;</span>,</span><br><span class="line">        <span class="number">2</span>)</span><br><span class="line">fileRDD.collect().foreach(println)</span><br><span class="line">sparkContext.stop()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<ul>
<li>读取内存数据时，数据可以按照并行度的设定进行数据的分区操作，数据分区规则的Spark核心源码如下：  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">positions</span></span>(length: <span class="type">Long</span>, numSlices: <span class="type">Int</span>): <span class="type">Iterator</span>[(<span class="type">Int</span>, <span class="type">Int</span>)] = &#123;</span><br><span class="line">  (<span class="number">0</span> until numSlices).iterator.map &#123; i =&gt;</span><br><span class="line">    <span class="keyword">val</span> start = ((i * length) / numSlices).toInt</span><br><span class="line">    <span class="keyword">val</span> end = (((i + <span class="number">1</span>) * length) / numSlices).toInt</span><br><span class="line">    (start, end)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>读取文件数据时，数据是按照Hadoop文件读取的规则进行切片分区，而切片规则和数据读取的规则有些差异，具体Spark核心源码如下  <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> InputSplit[] getSplits(JobConf job, <span class="keyword">int</span> numSplits)</span><br><span class="line">    <span class="keyword">throws</span> IOException &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">long</span> totalSize = <span class="number">0</span>;                           <span class="comment">// compute total size</span></span><br><span class="line">    <span class="keyword">for</span> (FileStatus file: files) &#123;                <span class="comment">// check we have valid files</span></span><br><span class="line">      <span class="keyword">if</span> (file.isDirectory()) &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> IOException(<span class="string">&quot;Not a file: &quot;</span>+ file.getPath());</span><br><span class="line">      &#125;</span><br><span class="line">      totalSize += file.getLen();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">long</span> goalSize = totalSize / (numSplits == <span class="number">0</span> ? <span class="number">1</span> : numSplits);</span><br><span class="line">    <span class="keyword">long</span> minSize = Math.max(job.getLong(org.apache.hadoop.mapreduce.lib.input.</span><br><span class="line">      FileInputFormat.SPLIT_MINSIZE, <span class="number">1</span>), minSplitSize);</span><br><span class="line">      </span><br><span class="line">    ...</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> (FileStatus file: files) &#123;</span><br><span class="line">    </span><br><span class="line">        ...</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> (isSplitable(fs, path)) &#123;</span><br><span class="line">          <span class="keyword">long</span> blockSize = file.getBlockSize();</span><br><span class="line">          <span class="keyword">long</span> splitSize = computeSplitSize(goalSize, minSize, blockSize);</span><br><span class="line"></span><br><span class="line">          ...</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">long</span> <span class="title">computeSplitSize</span><span class="params">(<span class="keyword">long</span> goalSize, <span class="keyword">long</span> minSize,</span></span></span><br><span class="line"><span class="params"><span class="function">                                       <span class="keyword">long</span> blockSize)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> Math.max(minSize, Math.min(goalSize, blockSize));</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>Hadoop计算分区和读取分区数据的方式不一样<ol>
<li>计算分区按照字节来计算</li>
<li>读取分区不是按照字节，是按照行来读取（单次不能跨行）<ul>
<li>读取数据时，按照偏移量来计算，相同的偏移量不能被重复读取</li>
</ul>
</li>
</ol>
</li>
</ul>
</li>
</ul>
<h4 id="5-1-4-3-RDD转换算子"><a href="#5-1-4-3-RDD转换算子" class="headerlink" title="5.1.4.3 RDD转换算子"></a>5.1.4.3 RDD转换算子</h4><p>RDD根据数据处理方式的不同将算子整体上分为Value类型、双Value类型和Key-Value类型</p>
<h5 id="5-1-4-3-1-Value类型"><a href="#5-1-4-3-1-Value类型" class="headerlink" title="5.1.4.3.1 Value类型"></a>5.1.4.3.1 Value类型</h5><ol>
<li><p>map</p>
<ul>
<li>函数签名  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">map</span></span>[<span class="type">U</span>: <span class="type">ClassTag</span>](f: <span class="type">T</span> =&gt; <span class="type">U</span>): <span class="type">RDD</span>[<span class="type">U</span>]</span><br></pre></td></tr></table></figure></li>
<li>函数说明<br>  将处理的数据逐条进行映射转换，这里的转换可以是类型的转换，也可以是值的转换。  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> dataRDD: <span class="type">RDD</span>[<span class="type">Int</span>] = sparkContext.makeRDD(<span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>))</span><br><span class="line"><span class="keyword">val</span> dataRDD1: <span class="type">RDD</span>[<span class="type">Int</span>] = dataRDD.map(</span><br><span class="line">    num =&gt; &#123;</span><br><span class="line">        num * <span class="number">2</span></span><br><span class="line">    &#125;</span><br><span class="line">)</span><br><span class="line"><span class="keyword">val</span> dataRDD2: <span class="type">RDD</span>[<span class="type">String</span>] = dataRDD1.map(</span><br><span class="line">    num =&gt; &#123;</span><br><span class="line">        <span class="string">&quot;&quot;</span> + num</span><br><span class="line">    &#125;</span><br><span class="line">)</span><br></pre></td></tr></table></figure></li>
<li>小功能：从服务器日志数据<a href="media/16382517322435/apache.log">apache.log</a>中获取用户请求URL资源路径  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">apache_log</span> </span>= &#123;</span><br><span class="line">    <span class="comment">//构建环境</span></span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">&quot;WordCount&quot;</span>).setMaster(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">    sparkContext.textFile(<span class="string">&quot;data/apache.log&quot;</span>)</span><br><span class="line">        .map(</span><br><span class="line">            line =&gt; &#123;</span><br><span class="line">                line.split(<span class="string">&quot; &quot;</span>)(<span class="number">6</span>)</span><br><span class="line">            &#125;</span><br><span class="line">        )</span><br><span class="line">        .collect()</span><br><span class="line">        .toList</span><br><span class="line">        .distinct</span><br><span class="line">        .foreach(println)</span><br><span class="line">    </span><br><span class="line">    sparkContext.stop()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>mapPartitions</p>
<ul>
<li>函数签名  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mapPartitions</span></span>[<span class="type">U</span>: <span class="type">ClassTag</span>](</span><br><span class="line">    f: <span class="type">Iterator</span>[<span class="type">T</span>] =&gt; <span class="type">Iterator</span>[<span class="type">U</span>],</span><br><span class="line">    preservesPartitioning: <span class="type">Boolean</span> = <span class="literal">false</span>): <span class="type">RDD</span>[<span class="type">U</span>]</span><br></pre></td></tr></table></figure></li>
<li>函数说明<br>  将待处理的数据以分区为单位发送到计算节点进行处理，这里的处理是指可以进行任意的处理，哪怕是过滤数据。  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> dataRDD1: <span class="type">RDD</span>[<span class="type">Int</span>] = dataRDD.mapPartitions(</span><br><span class="line">    datas =&gt; &#123;</span><br><span class="line">        datas.filter(_==<span class="number">2</span>)</span><br><span class="line">    &#125;</span><br><span class="line">)</span><br></pre></td></tr></table></figure></li>
</ul>
<blockquote>
<p>思考一个问题：map和mapPartitions的区别？</p>
<ul>
<li>数据处理角度<br>  Map算子是分区内一个数据一个数据的执行，类似于串行操作。而mapPartitions算子是以分区为单位进行批处理操作。</li>
<li>功能的角度<br>  Map算子主要目的将数据源中的数据进行转换和改变。但是不会减少或增多数据。MapPartitions算子需要传递一个迭代器，返回一个迭代器，没有要求的元素的个数保持不变，所以可以增加或减少数据</li>
<li>性能的角度<br>  Map算子因为类似于串行操作，所以性能比较低，而是mapPartitions算子类似于批处理，所以性能较高。但是mapPartitions算子会长时间占用内存，那么这样会导致内存可能不够用，出现内存溢出的错误。所以在内存有限的情况下，不推荐使用。使用map操作。</li>
</ul>
</blockquote>
</li>
</ol>
<ol start="3">
<li><p>mapPartitionsWithIndex</p>
<ul>
<li>函数签名  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mapPartitionsWithIndex</span></span>[<span class="type">U</span>: <span class="type">ClassTag</span>](</span><br><span class="line">  f: (<span class="type">Int</span>, <span class="type">Iterator</span>[<span class="type">T</span>]) =&gt; <span class="type">Iterator</span>[<span class="type">U</span>],</span><br><span class="line">  preservesPartitioning: <span class="type">Boolean</span> = <span class="literal">false</span>): <span class="type">RDD</span>[<span class="type">U</span>]</span><br></pre></td></tr></table></figure></li>
<li>函数说明<br>  将待处理的数据以分区为单位发送到计算节点进行处理，这里的处理是指可以进行任意的处理，哪怕是过滤数据，在处理时同时可以获取当前分区索引。  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> dataRDD1 = dataRDD.mapPartitionsWithIndex(</span><br><span class="line">    (index, datas) =&gt; &#123;</span><br><span class="line">         datas.map(index, _)</span><br><span class="line">    &#125;</span><br><span class="line">)</span><br></pre></td></tr></table></figure></li>
<li>小功能：<ul>
<li>获取第二个数据分区的数据</li>
<li>获取每个数据分区的最大值<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    println(<span class="string">&quot;************分区最大值****************&quot;</span>)</span><br><span class="line">    <span class="comment">//构建环境</span></span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">&quot;WordCount&quot;</span>).setMaster(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">    <span class="keyword">val</span> rdd = sparkContext.makeRDD(<span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>), <span class="number">2</span>)</span><br><span class="line">    <span class="keyword">val</span> max = rdd.mapPartitions(iter =&gt; &#123;</span><br><span class="line">        <span class="type">List</span>(iter.max).iterator</span><br><span class="line">    &#125;)</span><br><span class="line">    println(max.collect().toList)</span><br><span class="line">    </span><br><span class="line">    println(<span class="string">&quot;************错误姿势****************&quot;</span>)</span><br><span class="line">    <span class="comment">//分布式环境，无法保证第一个执行的分区就是第一个分区</span></span><br><span class="line">    <span class="comment">//且分布式中没有顺序</span></span><br><span class="line">    <span class="keyword">var</span> partNum = <span class="number">0</span></span><br><span class="line">    <span class="keyword">val</span> partitionMax = rdd.mapPartitions(iter =&gt; &#123;</span><br><span class="line">        <span class="keyword">if</span> (partNum == <span class="number">0</span>) &#123;</span><br><span class="line">            partNum += <span class="number">1</span></span><br><span class="line">            <span class="type">List</span>(iter.max).iterator</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            partNum += <span class="number">1</span></span><br><span class="line">            <span class="type">Nil</span>.iterator</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;)</span><br><span class="line">    println(partitionMax.collect().toList)</span><br><span class="line">    </span><br><span class="line">    println(<span class="string">&quot;************正确姿势****************&quot;</span>)</span><br><span class="line">    <span class="comment">//分布式环境，无法保证第一个执行的分区就是第一个分区</span></span><br><span class="line">    <span class="comment">//且分布式中没有顺序</span></span><br><span class="line">    <span class="keyword">val</span> partitionMaxWithIndex = rdd.mapPartitionsWithIndex(</span><br><span class="line">        (index, iter) =&gt; &#123;</span><br><span class="line">            <span class="keyword">if</span> (index == <span class="number">0</span>) &#123;</span><br><span class="line">                <span class="type">List</span>(iter.max).iterator</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="type">Nil</span>.iterator</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;)</span><br><span class="line">    println(partitionMaxWithIndex.collect().toList)</span><br><span class="line">    </span><br><span class="line">    println(<span class="string">&quot;************第二个分区数据****************&quot;</span>)</span><br><span class="line">    <span class="comment">//分布式环境，无法保证第一个执行的分区就是第一个分区</span></span><br><span class="line">    <span class="comment">//且分布式中没有顺序</span></span><br><span class="line">    <span class="keyword">val</span> secondPartitionMaxWithIndex = rdd.mapPartitionsWithIndex(</span><br><span class="line">        (index, iter) =&gt; &#123;</span><br><span class="line">            <span class="keyword">if</span> (index == <span class="number">1</span>) &#123;</span><br><span class="line">                iter</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="type">Nil</span>.iterator</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;)</span><br><span class="line">    println(secondPartitionMaxWithIndex.collect().toList)</span><br><span class="line">    </span><br><span class="line">    sparkContext.stop()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>
</li>
<li><p>flatMap</p>
<ul>
<li>函数签名  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">flatMap</span></span>[<span class="type">U</span>: <span class="type">ClassTag</span>](f: <span class="type">T</span> =&gt; <span class="type">TraversableOnce</span>[<span class="type">U</span>]): <span class="type">RDD</span>[<span class="type">U</span>]</span><br></pre></td></tr></table></figure></li>
<li>函数说明<br>  将处理的数据进行扁平化后再进行映射处理，所以算子也称之为扁平映射  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> dataRDD = sparkContext.makeRDD(<span class="type">List</span>(</span><br><span class="line">    <span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>),<span class="type">List</span>(<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line">),<span class="number">1</span>)</span><br><span class="line"><span class="keyword">val</span> dataRDD1 = dataRDD.flatMap(</span><br><span class="line">    list =&gt; list</span><br><span class="line">)</span><br></pre></td></tr></table></figure></li>
<li>小功能：将List(List(1,2),3,List(4,5))进行扁平化操作  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">flatMapTest2</span> </span>= &#123;</span><br><span class="line">    println(<span class="string">&quot;************flatMapTest****************&quot;</span>)</span><br><span class="line">    <span class="comment">//构建环境</span></span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">&quot;WordCount&quot;</span>).setMaster(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">    <span class="keyword">val</span> rdd = sparkContext.makeRDD(<span class="type">List</span>(<span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>), <span class="number">4</span>, <span class="type">List</span>(<span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>)), <span class="number">2</span>)</span><br><span class="line">    <span class="keyword">val</span> flatMap = rdd.flatMap &#123;</span><br><span class="line">        <span class="keyword">case</span> seq: <span class="type">List</span>[_] =&gt; seq</span><br><span class="line">        <span class="keyword">case</span> other =&gt; <span class="type">List</span>(other)</span><br><span class="line">    &#125;</span><br><span class="line">    println(flatMap.collect().toList)</span><br><span class="line">    </span><br><span class="line">    sparkContext.stop()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>glom</p>
<ul>
<li>函数签名  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">glom</span></span>(): <span class="type">RDD</span>[<span class="type">Array</span>[<span class="type">T</span>]]</span><br></pre></td></tr></table></figure></li>
<li>函数说明<br>  将同一个分区的数据直接转换为相同类型的内存数组进行处理，分区不变  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> dataRDD : <span class="type">RDD</span>[<span class="type">Int</span>] = sparkContext.makeRDD(<span class="type">List</span>(</span><br><span class="line">    <span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span></span><br><span class="line">),<span class="number">1</span>)</span><br><span class="line"><span class="keyword">val</span> dataRDD1:<span class="type">RDD</span>[<span class="type">Array</span>[<span class="type">Int</span>]] = dataRDD.glom()</span><br></pre></td></tr></table></figure></li>
<li>小功能：计算所有分区最大值求和（分区内取最大值，分区间最大值求和）  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">glomTest3</span> </span>= &#123;</span><br><span class="line">    println(<span class="string">&quot;************glom****************&quot;</span>)</span><br><span class="line">    <span class="comment">//构建环境</span></span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">&quot;WordCount&quot;</span>).setMaster(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">    <span class="keyword">val</span> summ: <span class="type">Int</span> = sparkContext</span><br><span class="line">        .makeRDD(<span class="type">List</span>(<span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>), <span class="type">List</span>(<span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>), <span class="type">List</span>(<span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>)), <span class="number">2</span>)</span><br><span class="line">        .glom()</span><br><span class="line">        .map(</span><br><span class="line">            array =&gt; &#123;</span><br><span class="line">                <span class="keyword">var</span> sum = <span class="number">0</span></span><br><span class="line">                array.foreach(</span><br><span class="line">                    list =&gt; &#123;</span><br><span class="line">                        list.foreach(</span><br><span class="line">                            num =&gt; &#123;</span><br><span class="line">                                sum += num</span><br><span class="line">                            &#125;</span><br><span class="line">                        )</span><br><span class="line">                    &#125;</span><br><span class="line">                )</span><br><span class="line">                sum</span><br><span class="line">            &#125;</span><br><span class="line">        )</span><br><span class="line">        .collect()</span><br><span class="line">        .sum</span><br><span class="line">    println(summ)</span><br><span class="line">    sparkContext.stop()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>groupBy</p>
<ul>
<li>函数签名  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">groupBy</span></span>[<span class="type">K</span>](f: <span class="type">T</span> =&gt; <span class="type">K</span>)(<span class="keyword">implicit</span> kt: <span class="type">ClassTag</span>[<span class="type">K</span>]): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">Iterable</span>[<span class="type">T</span>])]</span><br></pre></td></tr></table></figure></li>
<li>函数说明<br>  将数据根据指定的规则进行分组, 分区默认不变，但是数据会被打乱重新组合，我们将这样的操作称之为shuffle。极限情况下，数据可能被分在同一个分区中<br>  一个组的数据在一个分区中，但是并不是说一个分区中只有一个组  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> dataRDD = sparkContext.makeRDD(<span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>),<span class="number">1</span>)</span><br><span class="line"><span class="keyword">val</span> dataRDD1 = dataRDD.groupBy(</span><br><span class="line">    _%<span class="number">2</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure></li>
<li>小功能：<ul>
<li>将List(“Hello”, “hive”, “hbase”, “Hadoop”)根据单词首写字母进行分组。</li>
<li>从服务器日志数据apache.log中获取每个时间段访问量。</li>
<li>WordCount。<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">groupByTest2</span> </span>= &#123;</span><br><span class="line">    println(<span class="string">&quot;****************************&quot;</span>)</span><br><span class="line">    <span class="comment">//构建环境</span></span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">&quot;WordCount&quot;</span>).setMaster(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">    <span class="keyword">val</span> rdd = sparkContext.makeRDD(<span class="type">List</span>(<span class="string">&quot;Hello&quot;</span>, <span class="string">&quot;hive&quot;</span>, <span class="string">&quot;hbase&quot;</span>, <span class="string">&quot;Hadoop&quot;</span>))</span><br><span class="line">    <span class="keyword">val</span> groupBy: <span class="type">RDD</span>[(<span class="type">Char</span>, <span class="type">Iterable</span>[<span class="type">String</span>])] = rdd.groupBy(str =&gt; str.charAt(<span class="number">0</span>))</span><br><span class="line">    groupBy.collect().foreach(println)</span><br><span class="line">    sparkContext.stop()</span><br><span class="line">&#125;</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">groupByTest_apache_log</span> </span>= &#123;</span><br><span class="line">    println(<span class="string">&quot;****************************&quot;</span>)</span><br><span class="line">    <span class="comment">//构建环境</span></span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">&quot;WordCount&quot;</span>).setMaster(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">    </span><br><span class="line">    sparkContext</span><br><span class="line">        .textFile(<span class="string">&quot;data/apache.log&quot;</span>)</span><br><span class="line">        .map(</span><br><span class="line">            line=&gt;&#123;</span><br><span class="line">                line.split(<span class="string">&quot; &quot;</span>)(<span class="number">3</span>).split(<span class="string">&quot;:&quot;</span>)(<span class="number">1</span>)</span><br><span class="line">            &#125;</span><br><span class="line">        )</span><br><span class="line">        .groupBy(hour=&gt;hour)</span><br><span class="line">        .map&#123;</span><br><span class="line">            <span class="keyword">case</span>(hour,list)=&gt;&#123;</span><br><span class="line">                (hour,list.toList.size)</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        .collect()</span><br><span class="line">        .foreach(println)</span><br><span class="line">    sparkContext.stop()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>
</li>
<li><p>filter</p>
<ul>
<li>函数签名  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">filter</span></span>(f: <span class="type">T</span> =&gt; <span class="type">Boolean</span>): <span class="type">RDD</span>[<span class="type">T</span>]</span><br></pre></td></tr></table></figure></li>
<li>函数说明<br>  将数据根据指定的规则进行筛选过滤，符合规则的数据保留，不符合规则的数据丢弃。<br>  当数据进行筛选过滤后，分区不变，但是分区内的数据可能不均衡，生产环境下，可能会出现数据倾斜。  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> dataRDD = sparkContext.makeRDD(<span class="type">List</span>(</span><br><span class="line">    <span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span></span><br><span class="line">),<span class="number">1</span>)</span><br><span class="line"><span class="keyword">val</span> dataRDD1 = dataRDD.filter(_%<span class="number">2</span> == <span class="number">0</span>)</span><br></pre></td></tr></table></figure></li>
<li>小功能：从服务器日志数据apache.log中获取2015年5月17日的请求路径  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">filterTest_log</span> </span>= &#123;</span><br><span class="line">    println(<span class="string">&quot;****************************&quot;</span>)</span><br><span class="line">    <span class="comment">//构建环境</span></span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">&quot;WordCount&quot;</span>).setMaster(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">    sparkContext</span><br><span class="line">        .textFile(<span class="string">&quot;data/apache.log&quot;</span>)</span><br><span class="line">        .filter(line =&gt; &#123;</span><br><span class="line">            <span class="keyword">val</span> datas = line.split(<span class="string">&quot; &quot;</span>)</span><br><span class="line">            <span class="keyword">val</span> time = datas(<span class="number">3</span>)</span><br><span class="line">            time.startsWith(<span class="string">&quot;17/05/2015&quot;</span>)</span><br><span class="line">        &#125;)</span><br><span class="line">        .map(line =&gt; &#123;</span><br><span class="line">            line.split(<span class="string">&quot; &quot;</span>)(<span class="number">6</span>)</span><br><span class="line">        &#125;)</span><br><span class="line">        .collect()</span><br><span class="line">        .toList</span><br><span class="line">        .distinct</span><br><span class="line">        .foreach(println)</span><br><span class="line">    sparkContext.stop()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>sample</p>
<ul>
<li>函数签名  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sample</span></span>(</span><br><span class="line">  withReplacement: <span class="type">Boolean</span>,</span><br><span class="line">  fraction: <span class="type">Double</span>,</span><br><span class="line">  seed: <span class="type">Long</span> = <span class="type">Utils</span>.random.nextLong): <span class="type">RDD</span>[<span class="type">T</span>]</span><br></pre></td></tr></table></figure></li>
<li>函数说明<br>  根据指定的规则从数据集中抽取数据  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> dataRDD = sparkContext.makeRDD(<span class="type">List</span>(</span><br><span class="line">    <span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span></span><br><span class="line">),<span class="number">1</span>)</span><br><span class="line"><span class="comment">// 抽取数据不放回（伯努利算法）</span></span><br><span class="line"><span class="comment">// 伯努利算法：又叫0、1分布。例如扔硬币，要么正面，要么反面。</span></span><br><span class="line"><span class="comment">// 具体实现：根据种子和随机算法算出一个数和第二个参数设置几率比较，小于第二个参数要，大于不要</span></span><br><span class="line"><span class="comment">// 第一个参数：抽取的数据是否放回，false：不放回</span></span><br><span class="line"><span class="comment">// 第二个参数：抽取的几率，范围在[0,1]之间,0：全不取；1：全取；</span></span><br><span class="line"><span class="comment">// 第三个参数：随机数种子</span></span><br><span class="line"><span class="keyword">val</span> dataRDD1 = dataRDD.sample(<span class="literal">false</span>, <span class="number">0.5</span>)</span><br><span class="line"><span class="comment">// 抽取数据放回（泊松算法）</span></span><br><span class="line"><span class="comment">// 第一个参数：抽取的数据是否放回，true：放回；false：不放回</span></span><br><span class="line"><span class="comment">// 第二个参数：重复数据的几率，范围大于等于0.表示每一个元素被期望抽取到的次数</span></span><br><span class="line"><span class="comment">// 第三个参数：随机数种子</span></span><br><span class="line"><span class="keyword">val</span> dataRDD2 = dataRDD.sample(<span class="literal">true</span>, <span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>思考一个问题：有啥用，抽奖吗？</p>
<ul>
<li>抽样  </li>
</ul>
</blockquote>
</li>
</ul>
</li>
<li><p>distinct</p>
<ul>
<li>函数签名  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">distinct</span></span>()(<span class="keyword">implicit</span> ord: <span class="type">Ordering</span>[<span class="type">T</span>] = <span class="literal">null</span>): <span class="type">RDD</span>[<span class="type">T</span>]</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">distinct</span></span>(numPartitions: <span class="type">Int</span>)(<span class="keyword">implicit</span> ord: <span class="type">Ordering</span>[<span class="type">T</span>] = <span class="literal">null</span>): <span class="type">RDD</span>[<span class="type">T</span>]</span><br></pre></td></tr></table></figure></li>
<li>函数说明<br>  将数据集中重复的数据去重  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> dataRDD = sparkContext.makeRDD(<span class="type">List</span>(</span><br><span class="line">    <span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">1</span>,<span class="number">2</span></span><br><span class="line">),<span class="number">1</span>)</span><br><span class="line"><span class="keyword">val</span> dataRDD1 = dataRDD.distinct()</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> dataRDD2 = dataRDD.distinct(<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>思考一个问题：如果不用该算子，你有什么办法实现数据去重？</p>
<ul>
<li>map</li>
<li>set</li>
</ul>
</blockquote>
</li>
</ul>
</li>
<li><p>coalesce</p>
<ul>
<li>函数签名  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">coalesce</span></span>(numPartitions: <span class="type">Int</span>, shuffle: <span class="type">Boolean</span> = <span class="literal">false</span>,</span><br><span class="line">           partitionCoalescer: <span class="type">Option</span>[<span class="type">PartitionCoalescer</span>] = <span class="type">Option</span>.empty)</span><br><span class="line">          (<span class="keyword">implicit</span> ord: <span class="type">Ordering</span>[<span class="type">T</span>] = <span class="literal">null</span>)</span><br><span class="line">  : <span class="type">RDD</span>[<span class="type">T</span>]</span><br></pre></td></tr></table></figure></li>
<li>函数说明<br>  根据数据量缩减分区，用于大数据集过滤后，提高小数据集的执行效率<br>  当spark程序中，存在过多的小任务的时候，可以通过coalesce方法，收缩合并分区，减少分区的个数，减小任务调度成本  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> dataRDD = sparkContext.makeRDD(<span class="type">List</span>(</span><br><span class="line">    <span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">1</span>,<span class="number">2</span></span><br><span class="line">),<span class="number">6</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> dataRDD1 = dataRDD.coalesce(<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>思考一个问题：我想要扩大分区，怎么办？</p>
<ul>
<li>repartition</li>
</ul>
</blockquote>
</li>
</ul>
</li>
<li><p>repartition</p>
<ul>
<li>函数签名  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">repartition</span></span>(numPartitions: <span class="type">Int</span>)(<span class="keyword">implicit</span> ord: <span class="type">Ordering</span>[<span class="type">T</span>] = <span class="literal">null</span>): <span class="type">RDD</span>[<span class="type">T</span>]</span><br></pre></td></tr></table></figure></li>
<li>函数说明<br>  该操作内部其实执行的是coalesce操作，参数shuffle的默认值为true。无论是将分区数多的RDD转换为分区数少的RDD，还是将分区数少的RDD转换为分区数多的RDD，repartition操作都可以完成，因为无论如何都会经shuffle过程。  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> dataRDD = sparkContext.makeRDD(<span class="type">List</span>(</span><br><span class="line">    <span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">1</span>,<span class="number">2</span></span><br><span class="line">),<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> dataRDD1 = dataRDD.repartition(<span class="number">4</span>)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>思考一个问题：coalesce和repartition区别？</p>
</blockquote>
</li>
</ul>
</li>
<li><p>sortBy</p>
<ul>
<li>函数签名  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sortBy</span></span>[<span class="type">K</span>](</span><br><span class="line">  f: (<span class="type">T</span>) =&gt; <span class="type">K</span>,</span><br><span class="line">  ascending: <span class="type">Boolean</span> = <span class="literal">true</span>,</span><br><span class="line">  numPartitions: <span class="type">Int</span> = <span class="keyword">this</span>.partitions.length)</span><br><span class="line">  (<span class="keyword">implicit</span> ord: <span class="type">Ordering</span>[<span class="type">K</span>], ctag: <span class="type">ClassTag</span>[<span class="type">K</span>]): <span class="type">RDD</span>[<span class="type">T</span>]</span><br></pre></td></tr></table></figure></li>
<li>函数说明<br>  该操作用于排序数据。在排序之前，可以将数据通过f函数进行处理，之后按照f函数处理的结果进行排序，默认为升序排列。排序后新产生的RDD的分区数与原RDD的分区数一致。中间存在shuffle的过程  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> dataRDD = sparkContext.makeRDD(<span class="type">List</span>(</span><br><span class="line">    <span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">1</span>,<span class="number">2</span></span><br><span class="line">),<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> dataRDD1 = dataRDD.sortBy(num=&gt;num, <span class="literal">false</span>, <span class="number">4</span>)</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ol>
<h5 id="5-1-4-3-2-双Value类型"><a href="#5-1-4-3-2-双Value类型" class="headerlink" title="5.1.4.3.2 双Value类型"></a>5.1.4.3.2 双Value类型</h5><ol>
<li>intersection<ul>
<li>函数签名  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">intersection</span></span>(other: <span class="type">RDD</span>[<span class="type">T</span>]): <span class="type">RDD</span>[<span class="type">T</span>]</span><br></pre></td></tr></table></figure></li>
<li>函数说明  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 对源RDD和参数RDD求交集后返回一个新的RDD</span></span><br><span class="line"><span class="keyword">val</span> dataRDD1 = sparkContext.makeRDD(<span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>))</span><br><span class="line"><span class="keyword">val</span> dataRDD2 = sparkContext.makeRDD(<span class="type">List</span>(<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>))</span><br><span class="line"><span class="keyword">val</span> dataRDD = dataRDD1.intersection(dataRDD2)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>思考一个问题：如果两个RDD数据类型不一致怎么办？</p>
<ul>
<li>Cannot resolve overloaded method ‘intersection’</li>
<li>方法声明的泛型，必须和调用方数据类型一致</li>
</ul>
</blockquote>
</li>
</ul>
</li>
<li>union<ul>
<li>函数签名  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">union</span></span>(other: <span class="type">RDD</span>[<span class="type">T</span>]): <span class="type">RDD</span>[<span class="type">T</span>]</span><br></pre></td></tr></table></figure></li>
<li>函数说明  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">//对源RDD和参数RDD求并集后返回一个新的RDD</span></span><br><span class="line"><span class="keyword">val</span> dataRDD1 = sparkContext.makeRDD(<span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>))</span><br><span class="line"><span class="keyword">val</span> dataRDD2 = sparkContext.makeRDD(<span class="type">List</span>(<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>))</span><br><span class="line"><span class="keyword">val</span> dataRDD = dataRDD1.union(dataRDD2)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>思考一个问题：如果两个RDD数据类型不一致怎么办？</p>
<ul>
<li>type mismatch;</li>
<li>方法声明的泛型，必须和调用方数据类型一致</li>
</ul>
</blockquote>
</li>
</ul>
</li>
<li>subtract<ul>
<li>函数签名  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">subtract</span></span>(other: <span class="type">RDD</span>[<span class="type">T</span>]): <span class="type">RDD</span>[<span class="type">T</span>]</span><br></pre></td></tr></table></figure></li>
<li>函数说明  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">//以一个RDD元素为主，去除两个RDD中重复元素，将其他元素保留下来。求差集</span></span><br><span class="line"><span class="keyword">val</span> dataRDD1 = sparkContext.makeRDD(<span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>))</span><br><span class="line"><span class="keyword">val</span> dataRDD2 = sparkContext.makeRDD(<span class="type">List</span>(<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>))</span><br><span class="line"><span class="keyword">val</span> dataRDD = dataRDD1.subtract(dataRDD2)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>思考一个问题：如果两个RDD数据类型不一致怎么办？</p>
<ul>
<li>Cannot resolve overloaded method ‘subtract’</li>
<li>方法声明的泛型，必须和调用方数据类型一致</li>
</ul>
</blockquote>
</li>
</ul>
</li>
<li>zip<ul>
<li>函数签名  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">zip</span></span>[<span class="type">U</span>: <span class="type">ClassTag</span>](other: <span class="type">RDD</span>[<span class="type">U</span>]): <span class="type">RDD</span>[(<span class="type">T</span>, <span class="type">U</span>)]</span><br></pre></td></tr></table></figure></li>
<li>函数说明  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">//将两个RDD中的元素，以键值对的形式进行合并。其中，键值对中的Key为第1个RDD中的元素，Value为第2个RDD中的相同位置的元素。</span></span><br><span class="line"><span class="keyword">val</span> dataRDD1 = sparkContext.makeRDD(<span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>))</span><br><span class="line"><span class="keyword">val</span> dataRDD2 = sparkContext.makeRDD(<span class="type">List</span>(<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>))</span><br><span class="line"><span class="keyword">val</span> dataRDD = dataRDD1.zip(dataRDD2)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>思考一个问题：</p>
<ul>
<li>如果两个RDD数据类型不一致怎么办？<ul>
<li>没影响</li>
</ul>
</li>
<li>如果两个RDD数据分区不一致怎么办？<ul>
<li> Can only zip RDDs with same number of elements in each partition</li>
</ul>
</li>
<li>如果两个RDD分区数据数量不一致怎么办？<ul>
<li>Can’t zip RDDs with unequal numbers of partitions: List(2, 3)</li>
</ul>
</li>
</ul>
</blockquote>
</li>
</ul>
</li>
</ol>
<h5 id="5-1-4-3-3-Key-Value类型"><a href="#5-1-4-3-3-Key-Value类型" class="headerlink" title="5.1.4.3.3 Key-Value类型"></a>5.1.4.3.3 Key-Value类型</h5><ol>
<li><p>partitionBy</p>
<ul>
<li>函数签名  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">partitionBy</span></span>(partitioner: <span class="type">Partitioner</span>): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">V</span>)]</span><br></pre></td></tr></table></figure></li>
<li>函数说明  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">//将数据按照指定Partitioner重新进行分区。Spark默认的分区器是HashPartitioner</span></span><br><span class="line"><span class="keyword">val</span> rdd: <span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">String</span>)] =</span><br><span class="line">    sc.makeRDD(<span class="type">Array</span>((<span class="number">1</span>,<span class="string">&quot;aaa&quot;</span>),(<span class="number">2</span>,<span class="string">&quot;bbb&quot;</span>),(<span class="number">3</span>,<span class="string">&quot;ccc&quot;</span>)),<span class="number">3</span>)</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">HashPartitioner</span></span><br><span class="line"><span class="keyword">val</span> rdd2: <span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">String</span>)] =</span><br><span class="line">    rdd.partitionBy(<span class="keyword">new</span> <span class="type">HashPartitioner</span>(<span class="number">2</span>))</span><br></pre></td></tr></table></figure>
<blockquote>
<p>思考一个问题：</p>
<ul>
<li>如果重分区的分区器和当前RDD的分区器一样怎么办？<ul>
<li>不怎么办</li>
</ul>
</li>
<li>Spark还有其他分区器吗？<ul>
<li>Spark采用Hadoop的分区器</li>
</ul>
</li>
<li>如果想按照自己的方法进行数据分区怎么办？<ul>
<li>同Hadoop自定义分区器</li>
</ul>
</li>
</ul>
</blockquote>
</li>
</ul>
</li>
<li><p>reduceByKey </p>
<ul>
<li>函数签名  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reduceByKey</span></span>(func: (<span class="type">V</span>, <span class="type">V</span>) =&gt; <span class="type">V</span>): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">V</span>)]</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reduceByKey</span></span>(func: (<span class="type">V</span>, <span class="type">V</span>) =&gt; <span class="type">V</span>, numPartitions: <span class="type">Int</span>): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">V</span>)]</span><br></pre></td></tr></table></figure></li>
<li>函数说明<br>  可以将数据按照相同的Key对Value进行聚合  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> dataRDD1 = sparkContext.makeRDD(<span class="type">List</span>((<span class="string">&quot;a&quot;</span>,<span class="number">1</span>),(<span class="string">&quot;b&quot;</span>,<span class="number">2</span>),(<span class="string">&quot;c&quot;</span>,<span class="number">3</span>)))</span><br><span class="line"><span class="keyword">val</span> dataRDD2 = dataRDD1.reduceByKey(_+_)</span><br><span class="line"><span class="keyword">val</span> dataRDD3 = dataRDD1.reduceByKey(_+_, <span class="number">2</span>)</span><br></pre></td></tr></table></figure></li>
<li>小功能：WordCount  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reduceByKey</span> </span>= &#123;</span><br><span class="line">    println(<span class="string">&quot;*************reduceByKey***************&quot;</span>)</span><br><span class="line">    <span class="comment">//构建环境</span></span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">&quot;WordCount&quot;</span>).setMaster(<span class="string">&quot;local&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">val</span> rdd2: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = sparkContext.makeRDD(<span class="type">List</span>(</span><br><span class="line">        (<span class="string">&quot;a&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">2</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">3</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">4</span>)</span><br><span class="line">    ))</span><br><span class="line">    </span><br><span class="line">    rdd2</span><br><span class="line">        .reduceByKey(_ + _)</span><br><span class="line">        .collect()</span><br><span class="line">        .foreach(println)</span><br><span class="line">    sparkContext.stop()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>groupByKey </p>
<ul>
<li>函数签名  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">groupByKey</span></span>(): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">Iterable</span>[<span class="type">V</span>])]</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">groupByKey</span></span>(numPartitions: <span class="type">Int</span>): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">Iterable</span>[<span class="type">V</span>])]</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">groupByKey</span></span>(partitioner: <span class="type">Partitioner</span>): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">Iterable</span>[<span class="type">V</span>])]</span><br></pre></td></tr></table></figure></li>
<li>函数说明<br>  将数据源的数据根据key对value进行分组  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> dataRDD1 =</span><br><span class="line">    sparkContext.makeRDD(<span class="type">List</span>((<span class="string">&quot;a&quot;</span>,<span class="number">1</span>),(<span class="string">&quot;b&quot;</span>,<span class="number">2</span>),(<span class="string">&quot;c&quot;</span>,<span class="number">3</span>)))</span><br><span class="line"><span class="keyword">val</span> dataRDD2 = dataRDD1.groupByKey()</span><br><span class="line"><span class="keyword">val</span> dataRDD3 = dataRDD1.groupByKey(<span class="number">2</span>)</span><br><span class="line"><span class="keyword">val</span> dataRDD4 = dataRDD1.groupByKey(<span class="keyword">new</span> <span class="type">HashPartitioner</span>(<span class="number">2</span>))</span><br></pre></td></tr></table></figure></li>
<li>小功能：WordCount  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">groupByKey</span> </span>= &#123;</span><br><span class="line">    println(<span class="string">&quot;*************groupByKey***************&quot;</span>)</span><br><span class="line">    <span class="comment">//构建环境</span></span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">&quot;WordCount&quot;</span>).setMaster(<span class="string">&quot;local&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> rdd2: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = sparkContext.makeRDD(<span class="type">List</span>(</span><br><span class="line">        (<span class="string">&quot;a&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">2</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">3</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">4</span>)</span><br><span class="line">    ))</span><br><span class="line"></span><br><span class="line">    rdd2</span><br><span class="line">        .groupByKey()</span><br><span class="line">        .mapValues(iter =&gt; iter.sum)</span><br><span class="line">        .collect()</span><br><span class="line">        .foreach(println)</span><br><span class="line">    sparkContext.stop()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>思考一个问题：reduceByKey和groupByKey的区别？</p>
<ul>
<li>从shuffle的角度：reduceByKey和groupByKey都存在shuffle的操作，但是reduceByKey可以在shuffle前对分区内相同key的数据进行预聚合（combine）功能，这样会减少落盘的数据量，而groupByKey只是进行分组，不存在数据量减少的问题，所以reduceByKey性能比较高。</li>
<li>从功能的角度：reduceByKey其实包含分组和聚合的功能。groupByKey只能分组，不能聚合，所以在分组聚合的场合下，推荐使用reduceByKey，如果仅仅是分组而不需要聚合。那么还是只能使用groupByKey</li>
</ul>
</blockquote>
</li>
</ul>
</li>
<li><p>aggregateByKey </p>
<ul>
<li>函数签名  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">aggregateByKey</span></span>[<span class="type">U</span>: <span class="type">ClassTag</span>](zeroValue: <span class="type">U</span>)(seqOp: (<span class="type">U</span>, <span class="type">V</span>) =&gt; <span class="type">U</span>,</span><br><span class="line">    combOp: (<span class="type">U</span>, <span class="type">U</span>) =&gt; <span class="type">U</span>): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">U</span>)]</span><br></pre></td></tr></table></figure></li>
<li>函数说明<br>  将数据根据不同的规则进行分区内计算和分区间计算  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> dataRDD1 =</span><br><span class="line">    sparkContext.makeRDD(<span class="type">List</span>((<span class="string">&quot;a&quot;</span>,<span class="number">1</span>),(<span class="string">&quot;b&quot;</span>,<span class="number">2</span>),(<span class="string">&quot;c&quot;</span>,<span class="number">3</span>)))</span><br><span class="line"><span class="keyword">val</span> dataRDD2 =</span><br><span class="line">    dataRDD1.aggregateByKey(<span class="number">0</span>)(_+_,_+_)</span><br></pre></td></tr></table></figure>
  取出每个分区内相同key的最大值然后分区间相加  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// TODO : 取出每个分区内相同key的最大值然后分区间相加</span></span><br><span class="line"><span class="comment">// aggregateByKey算子是函数柯里化，存在两个参数列表</span></span><br><span class="line"><span class="comment">// 1. 第一个参数列表中的参数表示初始值</span></span><br><span class="line"><span class="comment">// 2. 第二个参数列表中含有两个参数</span></span><br><span class="line"><span class="comment">//    2.1 第一个参数表示分区内的计算规则</span></span><br><span class="line"><span class="comment">//    2.2 第二个参数表示分区间的计算规则</span></span><br><span class="line"><span class="keyword">val</span> rdd =</span><br><span class="line">    sc.makeRDD(<span class="type">List</span>(</span><br><span class="line">        (<span class="string">&quot;a&quot;</span>,<span class="number">1</span>),(<span class="string">&quot;a&quot;</span>,<span class="number">2</span>),(<span class="string">&quot;c&quot;</span>,<span class="number">3</span>),</span><br><span class="line">        (<span class="string">&quot;b&quot;</span>,<span class="number">4</span>),(<span class="string">&quot;c&quot;</span>,<span class="number">5</span>),(<span class="string">&quot;c&quot;</span>,<span class="number">6</span>)</span><br><span class="line">    ),<span class="number">2</span>)</span><br><span class="line"><span class="comment">// 0:(&quot;a&quot;,1),(&quot;a&quot;,2),(&quot;c&quot;,3) =&gt; (a,10)(c,10)</span></span><br><span class="line"><span class="comment">//                                         =&gt; (a,10)(b,10)(c,20)</span></span><br><span class="line"><span class="comment">// 1:(&quot;b&quot;,4),(&quot;c&quot;,5),(&quot;c&quot;,6) =&gt; (b,10)(c,10)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> resultRDD =</span><br><span class="line">    rdd.aggregateByKey(<span class="number">10</span>)(</span><br><span class="line">        (x, y) =&gt; math.max(x,y),</span><br><span class="line">        (x, y) =&gt; x + y</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">resultRDD.collect().foreach(println)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>思考一个问题：分区内计算规则和分区间计算规则相同怎么办？（WordCount）</p>
<ul>
<li>不怎么办，等同于foldByKey</li>
</ul>
</blockquote>
</li>
</ul>
</li>
<li><p>foldByKey </p>
<ul>
<li>函数签名  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">foldByKey</span></span>(zeroValue: <span class="type">V</span>)(func: (<span class="type">V</span>, <span class="type">V</span>) =&gt; <span class="type">V</span>): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">V</span>)]</span><br></pre></td></tr></table></figure></li>
<li>函数说明<br>  当分区内计算规则和分区间计算规则相同时，aggregateByKey就可以简化为foldByKey  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> dataRDD1 = sparkContext.makeRDD(<span class="type">List</span>((<span class="string">&quot;a&quot;</span>,<span class="number">1</span>),(<span class="string">&quot;b&quot;</span>,<span class="number">2</span>),(<span class="string">&quot;c&quot;</span>,<span class="number">3</span>)))</span><br><span class="line"><span class="keyword">val</span> dataRDD2 = dataRDD1.foldByKey(<span class="number">0</span>)(_+_)</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>combineByKey </p>
<ul>
<li>函数签名  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">combineByKey</span></span>[<span class="type">C</span>](</span><br><span class="line">  createCombiner: <span class="type">V</span> =&gt; <span class="type">C</span>,</span><br><span class="line">  mergeValue: (<span class="type">C</span>, <span class="type">V</span>) =&gt; <span class="type">C</span>,</span><br><span class="line">  mergeCombiners: (<span class="type">C</span>, <span class="type">C</span>) =&gt; <span class="type">C</span>): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">C</span>)]</span><br></pre></td></tr></table></figure></li>
<li>函数说明<br>  最通用的对key-value型rdd进行聚集操作的聚集函数（aggregation function）。类似于aggregate()，combineByKey()允许用户返回值的类型与输入不一致。</li>
<li>小功能：数据List((“a”, 88), (“b”, 95), (“a”, 91), (“b”, 93), (“a”, 95), (“b”, 98))求每个key的平均值  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> list: <span class="type">List</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">List</span>((<span class="string">&quot;a&quot;</span>, <span class="number">88</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">95</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">91</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">93</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">95</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">98</span>))</span><br><span class="line"><span class="keyword">val</span> input: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = sc.makeRDD(list, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> combineRdd: <span class="type">RDD</span>[(<span class="type">String</span>, (<span class="type">Int</span>, <span class="type">Int</span>))] = input.combineByKey(</span><br><span class="line">    (_, <span class="number">1</span>),</span><br><span class="line">    (acc: (<span class="type">Int</span>, <span class="type">Int</span>), v) =&gt; (acc._1 + v, acc._2 + <span class="number">1</span>),</span><br><span class="line">    (acc1: (<span class="type">Int</span>, <span class="type">Int</span>), acc2: (<span class="type">Int</span>, <span class="type">Int</span>)) =&gt; (acc1._1 + acc2._1, acc1._2 + acc2._2)</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>思考一个问题：reduceByKey、foldByKey、aggregateByKey、combineByKey的区别？</p>
<ul>
<li>reduceByKey: 相同key的第一个数据不进行任何计算，分区内和分区间计算规则相同</li>
<li>foldByKey: 相同key的第一个数据和初始值进行分区内计算，分区内和分区间计算规则相同</li>
<li>aggregateByKey：相同key的第一个数据和初始值进行分区内计算，分区内和分区间计算规则可以不相同</li>
<li>combineByKey:当计算时，发现数据结构不满足要求时，可以让第一个数据转换结构。分区内和分区间计算规则不相同。</li>
</ul>
</blockquote>
</li>
</ul>
</li>
<li><p>sortByKey</p>
<ul>
<li>函数签名  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sortByKey</span></span>(ascending: <span class="type">Boolean</span> = <span class="literal">true</span>, numPartitions: <span class="type">Int</span> = self.partitions.length)</span><br><span class="line">  : <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">V</span>)]</span><br></pre></td></tr></table></figure></li>
<li>函数说明<br>  在一个(K,V)的RDD上调用，K必须实现Ordered接口(特质)，返回一个按照key进行排序的  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> dataRDD1 = sparkContext.makeRDD(<span class="type">List</span>((<span class="string">&quot;a&quot;</span>,<span class="number">1</span>),(<span class="string">&quot;b&quot;</span>,<span class="number">2</span>),(<span class="string">&quot;c&quot;</span>,<span class="number">3</span>)))</span><br><span class="line"><span class="keyword">val</span> sortRDD1: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = dataRDD1.sortByKey(<span class="literal">true</span>)</span><br><span class="line"><span class="keyword">val</span> sortRDD1: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = dataRDD1.sortByKey(<span class="literal">false</span>)</span><br></pre></td></tr></table></figure></li>
<li>小功能：设置key为自定义类User  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sortByKey_user</span> </span>= &#123;</span><br><span class="line">    println(<span class="string">&quot;*************sortByKey***************&quot;</span>)</span><br><span class="line">    <span class="comment">//构建环境</span></span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">&quot;WordCount&quot;</span>).setMaster(<span class="string">&quot;local&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> rdd: <span class="type">RDD</span>[(<span class="type">User</span>, <span class="type">Int</span>)] = sparkContext.makeRDD(<span class="type">List</span>(</span><br><span class="line">        (<span class="keyword">new</span> <span class="type">User</span>(), <span class="number">1</span>), (<span class="keyword">new</span> <span class="type">User</span>(), <span class="number">2</span>), (<span class="keyword">new</span> <span class="type">User</span>(), <span class="number">3</span>), (<span class="keyword">new</span> <span class="type">User</span>(), <span class="number">4</span>)</span><br><span class="line">    ))</span><br><span class="line">    rdd</span><br><span class="line">        .sortByKey()</span><br><span class="line">        .collect()</span><br><span class="line">        .foreach(println)</span><br><span class="line">    sparkContext.stop()</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">User</span> <span class="keyword">extends</span> <span class="title">Ordered</span>[<span class="type">User</span>] </span>&#123;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">compare</span></span>(that: <span class="type">User</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">        <span class="number">0</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>join</p>
<ul>
<li>函数签名  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">join</span></span>[<span class="type">W</span>](other: <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">W</span>)]): <span class="type">RDD</span>[(<span class="type">K</span>, (<span class="type">V</span>, <span class="type">W</span>))]</span><br></pre></td></tr></table></figure></li>
<li>函数说明<br>  在类型为(K,V)和(K,W)的RDD上调用，返回一个相同key对应的所有元素连接在一起的(K,(V,W))的RDD  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd: <span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">String</span>)] = sc.makeRDD(<span class="type">Array</span>((<span class="number">1</span>, <span class="string">&quot;a&quot;</span>), (<span class="number">2</span>, <span class="string">&quot;b&quot;</span>), (<span class="number">3</span>, <span class="string">&quot;c&quot;</span>)))</span><br><span class="line"><span class="keyword">val</span> rdd1: <span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">Int</span>)] = sc.makeRDD(<span class="type">Array</span>((<span class="number">1</span>, <span class="number">4</span>), (<span class="number">2</span>, <span class="number">5</span>), (<span class="number">3</span>, <span class="number">6</span>)))</span><br><span class="line">rdd.join(rdd1).collect().foreach(println)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>思考一个问题：如果key存在不相等呢？</p>
<ul>
<li>不相等的关联不到</li>
</ul>
</blockquote>
</li>
</ul>
</li>
<li><p>leftOuterJoin</p>
<ul>
<li>函数签名  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">leftOuterJoin</span></span>[<span class="type">W</span>](other: <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">W</span>)]): <span class="type">RDD</span>[(<span class="type">K</span>, (<span class="type">V</span>, <span class="type">Option</span>[<span class="type">W</span>]))]</span><br></pre></td></tr></table></figure></li>
<li>函数说明<br>  类似于SQL语句的左外连接  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> dataRDD1 = sparkContext.makeRDD(<span class="type">List</span>((<span class="string">&quot;a&quot;</span>,<span class="number">1</span>),(<span class="string">&quot;b&quot;</span>,<span class="number">2</span>),(<span class="string">&quot;c&quot;</span>,<span class="number">3</span>)))</span><br><span class="line"><span class="keyword">val</span> dataRDD2 = sparkContext.makeRDD(<span class="type">List</span>((<span class="string">&quot;a&quot;</span>,<span class="number">1</span>),(<span class="string">&quot;b&quot;</span>,<span class="number">2</span>),(<span class="string">&quot;c&quot;</span>,<span class="number">3</span>)))</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> rdd: <span class="type">RDD</span>[(<span class="type">String</span>, (<span class="type">Int</span>, <span class="type">Option</span>[<span class="type">Int</span>]))] = dataRDD1.leftOuterJoin(dataRDD2)</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>cogroup</p>
<ul>
<li>函数签名  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cogroup</span></span>[<span class="type">W</span>](other: <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">W</span>)]): <span class="type">RDD</span>[(<span class="type">K</span>, (<span class="type">Iterable</span>[<span class="type">V</span>], <span class="type">Iterable</span>[<span class="type">W</span>]))]</span><br></pre></td></tr></table></figure></li>
<li>函数说明<br>  在类型为(K,V)和(K,W)的RDD上调用，返回一个(K,(Iterable<V>,Iterable<W>))类型的RDD  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> dataRDD1 = sparkContext.makeRDD(<span class="type">List</span>((<span class="string">&quot;a&quot;</span>,<span class="number">1</span>),(<span class="string">&quot;a&quot;</span>,<span class="number">2</span>),(<span class="string">&quot;c&quot;</span>,<span class="number">3</span>)))</span><br><span class="line"><span class="keyword">val</span> dataRDD2 = sparkContext.makeRDD(<span class="type">List</span>((<span class="string">&quot;a&quot;</span>,<span class="number">1</span>),(<span class="string">&quot;c&quot;</span>,<span class="number">2</span>),(<span class="string">&quot;c&quot;</span>,<span class="number">3</span>)))</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> value: <span class="type">RDD</span>[(<span class="type">String</span>, (<span class="type">Iterable</span>[<span class="type">Int</span>], <span class="type">Iterable</span>[<span class="type">Int</span>]))] = </span><br><span class="line"></span><br><span class="line">dataRDD1.cogroup(dataRDD2)</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ol>
<h4 id="5-1-4-4-案例实操"><a href="#5-1-4-4-案例实操" class="headerlink" title="5.1.4.4 案例实操"></a>5.1.4.4 案例实操</h4><ol>
<li>数据准备<ul>
<li><a href="media/16382517322435/agent.log">agent.log</a></li>
<li>时间戳，省份，城市，用户，广告，中间字段使用空格分隔。</li>
</ul>
</li>
<li>需求描述<ul>
<li>统计出每一个省份每个广告被点击数量排行的Top3</li>
</ul>
</li>
<li>需求分析<ul>
<li>转换数据结构line =&gt;（（省份，广告），1） </li>
<li>聚合数据（（省份，广告），1） =&gt; （（省份，广告），count）</li>
<li>转换数据结构（（省份，广告），count）=&gt;（省份，（广告，count））</li>
<li>分组（省份，（广告，count））=&gt;（省份，List（（广告，count），（广告，count），·····））</li>
<li>排序取前三</li>
</ul>
</li>
<li>功能实现 <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">agent_log</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    println(<span class="string">&quot;*************agent.log***************&quot;</span>)</span><br><span class="line">    <span class="comment">//构建环境</span></span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">&quot;WordCount&quot;</span>).setMaster(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"></span><br><span class="line">    sparkContext</span><br><span class="line">        .textFile(<span class="string">&quot;data/agent.log&quot;</span>)</span><br><span class="line">        .map(line =&gt; &#123;</span><br><span class="line">            <span class="keyword">val</span> datas: <span class="type">Array</span>[<span class="type">String</span>] = line.split(<span class="string">&quot; &quot;</span>)</span><br><span class="line">            ((datas(<span class="number">1</span>), datas(<span class="number">4</span>)), <span class="number">1</span>)</span><br><span class="line">        &#125;)</span><br><span class="line">        .reduceByKey(_ + _)</span><br><span class="line">        .map &#123;</span><br><span class="line">            <span class="keyword">case</span> ((p, a), s) =&gt; (p, (a, s))</span><br><span class="line">        &#125;</span><br><span class="line">        .groupByKey()</span><br><span class="line">        .mapValues(</span><br><span class="line">            iter =&gt; &#123;</span><br><span class="line">                iter.toList.sortBy(_._2)(<span class="type">Ordering</span>.<span class="type">Int</span>.reverse).take(<span class="number">3</span>)</span><br><span class="line">            &#125;</span><br><span class="line">        )</span><br><span class="line">        .collect()</span><br><span class="line">        .foreach(println)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    sparkContext.stop()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ol>
<h4 id="5-1-4-5-RDD行动算子"><a href="#5-1-4-5-RDD行动算子" class="headerlink" title="5.1.4.5 RDD行动算子"></a>5.1.4.5 RDD行动算子</h4><ul>
<li>所谓行动算子就是通过调用RDD的方法，让RDD的计算开始执行，执行时会让整个RDD模型的逻辑全部执行；如果不调用行动算子，那么逻辑不会启动</li>
<li>转换算子是将旧的RDD转换为新的RDD，返回结果一定为RDD</li>
<li>行动算子是让RDD逻辑开始执行，返回结果为具体的值<ul>
<li>一个Spark程序中，可以生成作业，然后通过算子执行作业（Job），触发行动算子，一定会产生一个新的作业（Job）</li>
</ul>
</li>
</ul>
<ol>
<li><p>reduce</p>
<ul>
<li>函数签名  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reduce</span></span>(f: (<span class="type">T</span>, <span class="type">T</span>) =&gt; <span class="type">T</span>): <span class="type">T</span></span><br></pre></td></tr></table></figure></li>
<li>函数说明<br>  聚集RDD中的所有元素，先聚合分区内数据，再聚合分区间数据<br>  简化，规约，聚合  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd: <span class="type">RDD</span>[<span class="type">Int</span>] = sc.makeRDD(<span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>))</span><br><span class="line">    </span><br><span class="line"><span class="comment">// 聚合数据</span></span><br><span class="line"><span class="keyword">val</span> reduceResult: <span class="type">Int</span> = rdd.reduce(_+_)</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>collect</p>
<ul>
<li>函数签名  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">collect</span></span>(): <span class="type">Array</span>[<span class="type">T</span>]</span><br></pre></td></tr></table></figure></li>
<li>函数说明<br>  在驱动程序（Driver）中，以数组Array的形式返回数据集的所有元素  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd: <span class="type">RDD</span>[<span class="type">Int</span>] = sc.makeRDD(<span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>))</span><br><span class="line">    </span><br><span class="line"><span class="comment">// 收集数据到Driver</span></span><br><span class="line">rdd.collect().foreach(println)</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>count</p>
<ul>
<li>函数签名  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">count</span></span>(): <span class="type">Long</span></span><br></pre></td></tr></table></figure></li>
<li>函数说明<br>  返回RDD中元素的个数  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd: <span class="type">RDD</span>[<span class="type">Int</span>] = sc.makeRDD(<span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>))</span><br><span class="line">    </span><br><span class="line"><span class="comment">// 返回RDD中元素的个数</span></span><br><span class="line"><span class="keyword">val</span> countResult: <span class="type">Long</span> = rdd.count()</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>first</p>
<ul>
<li>函数签名  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">first</span></span>(): <span class="type">T</span></span><br></pre></td></tr></table></figure></li>
<li>函数说明<br>  返回RDD中的第一个元素  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd: <span class="type">RDD</span>[<span class="type">Int</span>] = sc.makeRDD(<span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">// 返回RDD中元素的个数</span></span><br><span class="line"><span class="keyword">val</span> firstResult: <span class="type">Int</span> = rdd.first()</span><br><span class="line">println(firstResult)</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>take</p>
<ul>
<li>函数签名  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">take</span></span>(num: <span class="type">Int</span>): <span class="type">Array</span>[<span class="type">T</span>]</span><br></pre></td></tr></table></figure></li>
<li>函数说明<br>  返回一个由RDD的前n个元素组成的数组  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">vval rdd: <span class="type">RDD</span>[<span class="type">Int</span>] = sc.makeRDD(<span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">// 返回RDD中元素的个数</span></span><br><span class="line"><span class="keyword">val</span> takeResult: <span class="type">Array</span>[<span class="type">Int</span>] = rdd.take(<span class="number">2</span>)</span><br><span class="line">println(takeResult.mkString(<span class="string">&quot;,&quot;</span>))</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>takeOrdered</p>
<ul>
<li>函数签名  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">takeOrdered</span></span>(num: <span class="type">Int</span>)(<span class="keyword">implicit</span> ord: <span class="type">Ordering</span>[<span class="type">T</span>]): <span class="type">Array</span>[<span class="type">T</span>]</span><br></pre></td></tr></table></figure></li>
<li>函数说明<br>  返回该RDD排序后的前n个元素组成的数组  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd: <span class="type">RDD</span>[<span class="type">Int</span>] = sc.makeRDD(<span class="type">List</span>(<span class="number">1</span>,<span class="number">3</span>,<span class="number">2</span>,<span class="number">4</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">// 返回RDD中元素的个数</span></span><br><span class="line"><span class="keyword">val</span> result: <span class="type">Array</span>[<span class="type">Int</span>] = rdd.takeOrdered(<span class="number">2</span>)</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>aggregate</p>
<ul>
<li>函数签名  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">aggregate</span></span>[<span class="type">U</span>: <span class="type">ClassTag</span>](zeroValue: <span class="type">U</span>)(seqOp: (<span class="type">U</span>, <span class="type">T</span>) =&gt; <span class="type">U</span>, combOp: (<span class="type">U</span>, <span class="type">U</span>) =&gt; <span class="type">U</span>): <span class="type">U</span></span><br></pre></td></tr></table></figure></li>
<li>函数说明<br>  分区的数据通过初始值和分区内的数据进行聚合，然后再和初始值进行分区间的数据聚合  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd: <span class="type">RDD</span>[<span class="type">Int</span>] = sc.makeRDD(<span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>), <span class="number">8</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 将该RDD所有元素相加得到结果</span></span><br><span class="line"><span class="comment">//val result: Int = rdd.aggregate(0)(_ + _, _ + _)</span></span><br><span class="line"><span class="keyword">val</span> result: <span class="type">Int</span> = rdd.aggregate(<span class="number">10</span>)(_ + _, _ + _)</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>fold</p>
<ul>
<li>函数签名  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fold</span></span>(zeroValue: <span class="type">T</span>)(op: (<span class="type">T</span>, <span class="type">T</span>) =&gt; <span class="type">T</span>): <span class="type">T</span></span><br></pre></td></tr></table></figure></li>
<li>函数说明<br>  折叠操作，aggregate的简化版操作  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd: <span class="type">RDD</span>[<span class="type">Int</span>] = sc.makeRDD(<span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> foldResult: <span class="type">Int</span> = rdd.fold(<span class="number">0</span>)(_+_)</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>countByKey</p>
<ul>
<li>函数签名  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">countByKey</span></span>(): <span class="type">Map</span>[<span class="type">K</span>, <span class="type">Long</span>]</span><br></pre></td></tr></table></figure></li>
<li>函数说明<br>  统计每种key的个数  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd: <span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">String</span>)] = sc.makeRDD(<span class="type">List</span>((<span class="number">1</span>, <span class="string">&quot;a&quot;</span>), (<span class="number">1</span>, <span class="string">&quot;a&quot;</span>), (<span class="number">1</span>, <span class="string">&quot;a&quot;</span>), (<span class="number">2</span>, <span class="string">&quot;b&quot;</span>), (<span class="number">3</span>, <span class="string">&quot;c&quot;</span>), (<span class="number">3</span>, <span class="string">&quot;c&quot;</span>)))</span><br><span class="line"></span><br><span class="line"><span class="comment">// 统计每种key的个数</span></span><br><span class="line"><span class="keyword">val</span> result: collection.<span class="type">Map</span>[<span class="type">Int</span>, <span class="type">Long</span>] = rdd.countByKey()</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ol>
<ol start="10">
<li><p>save相关算子</p>
<ul>
<li>函数签名  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">saveAsTextFile</span></span>(path: <span class="type">String</span>): <span class="type">Unit</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">saveAsObjectFile</span></span>(path: <span class="type">String</span>): <span class="type">Unit</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">saveAsSequenceFile</span></span>(</span><br><span class="line">  path: <span class="type">String</span>,</span><br><span class="line">  codec: <span class="type">Option</span>[<span class="type">Class</span>[_ &lt;: <span class="type">CompressionCodec</span>]] = <span class="type">None</span>): <span class="type">Unit</span></span><br></pre></td></tr></table></figure></li>
<li>函数说明<br>  将数据保存到不同格式的文件中  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 保存成Text文件</span></span><br><span class="line">rdd.saveAsTextFile(<span class="string">&quot;output&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 序列化成对象保存到文件</span></span><br><span class="line">rdd.saveAsObjectFile(<span class="string">&quot;output1&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 保存成Sequencefile文件</span></span><br><span class="line">rdd.map((_,<span class="number">1</span>)).saveAsSequenceFile(<span class="string">&quot;output2&quot;</span>)</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>foreach</p>
<ul>
<li>函数签名  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">foreach</span></span>(f: <span class="type">T</span> =&gt; <span class="type">Unit</span>): <span class="type">Unit</span> = withScope &#123;</span><br><span class="line">    <span class="keyword">val</span> cleanF = sc.clean(f)</span><br><span class="line">    sc.runJob(<span class="keyword">this</span>, (iter: <span class="type">Iterator</span>[<span class="type">T</span>]) =&gt; iter.foreach(cleanF))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>函数说明<br>  分布式遍历RDD中的每一个元素，调用指定函数  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd: <span class="type">RDD</span>[<span class="type">Int</span>] = sc.makeRDD(<span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">// 收集后打印</span></span><br><span class="line">rdd.map(num=&gt;num).collect().foreach(println)</span><br><span class="line"></span><br><span class="line">println(<span class="string">&quot;****************&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 分布式打印</span></span><br><span class="line">rdd.foreach(println)</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ol>
<h4 id="5-1-4-6-RDD序列化"><a href="#5-1-4-6-RDD序列化" class="headerlink" title="5.1.4.6 RDD序列化"></a>5.1.4.6 RDD序列化</h4><ol>
<li>闭包检查<br> 从计算的角度, 算子以外的代码都是在Driver端执行, 算子里面的代码都是在Executor端执行。那么在scala的函数式编程中，就会导致算子内经常会用到算子外的数据，这样就形成了闭包的效果，如果使用的算子外的数据无法序列化，就意味着无法传值给Executor端执行，就会发生错误，所以需要在执行任务计算前，检测闭包内的对象是否可以进行序列化，这个操作我们称之为闭包检测。Scala2.12版本后闭包编译方式发生了改变</li>
<li>序列化方法和属性<br> 从计算的角度, 算子以外的代码都是在Driver端执行, 算子里面的代码都是在Executor端执行，看如下代码： <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">serializable02_function</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="comment">//1.创建SparkConf并设置App名称</span></span><br><span class="line">        <span class="keyword">val</span> conf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">&quot;SparkCoreTest&quot;</span>).setMaster(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">//2.创建SparkContext，该对象是提交Spark App的入口</span></span><br><span class="line">        <span class="keyword">val</span> sc: <span class="type">SparkContext</span> = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"></span><br><span class="line">        <span class="comment">//3.创建一个RDD</span></span><br><span class="line">        <span class="keyword">val</span> rdd: <span class="type">RDD</span>[<span class="type">String</span>] = sc.makeRDD(<span class="type">Array</span>(<span class="string">&quot;hello world&quot;</span>, <span class="string">&quot;hello spark&quot;</span>, <span class="string">&quot;hive&quot;</span>, <span class="string">&quot;atguigu&quot;</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment">//3.1创建一个Search对象</span></span><br><span class="line">        <span class="keyword">val</span> search = <span class="keyword">new</span> <span class="type">Search</span>(<span class="string">&quot;hello&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">//3.2 函数传递，打印：ERROR Task not serializable</span></span><br><span class="line">        search.getMatch1(rdd).collect().foreach(println)</span><br><span class="line"></span><br><span class="line">        <span class="comment">//3.3 属性传递，打印：ERROR Task not serializable</span></span><br><span class="line">        search.getMatch2(rdd).collect().foreach(println)</span><br><span class="line"></span><br><span class="line">        <span class="comment">//4.关闭连接</span></span><br><span class="line">        sc.stop()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Search</span>(<span class="params">query:<span class="type">String</span></span>) <span class="keyword">extends</span> <span class="title">Serializable</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">isMatch</span></span>(s: <span class="type">String</span>): <span class="type">Boolean</span> = &#123;</span><br><span class="line">        s.contains(query)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 函数序列化案例</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">getMatch1</span> </span>(rdd: <span class="type">RDD</span>[<span class="type">String</span>]): <span class="type">RDD</span>[<span class="type">String</span>] = &#123;</span><br><span class="line">        <span class="comment">//rdd.filter(this.isMatch)</span></span><br><span class="line">        rdd.filter(isMatch)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 属性序列化案例</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">getMatch2</span></span>(rdd: <span class="type">RDD</span>[<span class="type">String</span>]): <span class="type">RDD</span>[<span class="type">String</span>] = &#123;</span><br><span class="line">        <span class="comment">//rdd.filter(x =&gt; x.contains(this.query))</span></span><br><span class="line">        rdd.filter(x =&gt; x.contains(query))</span><br><span class="line">        <span class="comment">//val q = query</span></span><br><span class="line">        <span class="comment">//rdd.filter(x =&gt; x.contains(q))</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>Kryo序列化框架<br> 参考地址: <a href="https://github.com/EsotericSoftware/kryo">https://github.com/EsotericSoftware/kryo</a><br> Java的序列化能够序列化任何的类。但是比较重（字节多），序列化后，对象的提交也比较大。Spark出于性能的考虑，Spark2.0开始支持另外一种Kryo序列化机制。Kryo速度是Serializable的10倍。当RDD在Shuffle数据的时候，简单数据类型、数组和字符串类型已经在Spark内部使用Kryo来序列化。<br> 注意：即使使用Kryo序列化，也要继承Serializable接口。 <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">serializable_Kryo</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> conf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">                .setAppName(<span class="string">&quot;SerDemo&quot;</span>)</span><br><span class="line">                .setMaster(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">                <span class="comment">// 替换默认的序列化机制</span></span><br><span class="line">                .set(<span class="string">&quot;spark.serializer&quot;</span>, <span class="string">&quot;org.apache.spark.serializer.KryoSerializer&quot;</span>)</span><br><span class="line">                <span class="comment">// 注册需要使用 kryo 序列化的自定义类</span></span><br><span class="line">                .registerKryoClasses(<span class="type">Array</span>(classOf[<span class="type">Searcher</span>]))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> rdd: <span class="type">RDD</span>[<span class="type">String</span>] = sc.makeRDD(<span class="type">Array</span>(<span class="string">&quot;hello world&quot;</span>, <span class="string">&quot;hello atguigu&quot;</span>, <span class="string">&quot;atguigu&quot;</span>, <span class="string">&quot;hahah&quot;</span>), <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> searcher = <span class="keyword">new</span> <span class="type">Searcher</span>(<span class="string">&quot;hello&quot;</span>)</span><br><span class="line">        <span class="keyword">val</span> result: <span class="type">RDD</span>[<span class="type">String</span>] = searcher.getMatchedRDD1(rdd)</span><br><span class="line"></span><br><span class="line">        result.collect.foreach(println)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Searcher</span>(<span class="params">val query: <span class="type">String</span></span>) </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">isMatch</span></span>(s: <span class="type">String</span>) = &#123;</span><br><span class="line">        s.contains(query)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">getMatchedRDD1</span></span>(rdd: <span class="type">RDD</span>[<span class="type">String</span>]) = &#123;</span><br><span class="line">        rdd.filter(isMatch) </span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">getMatchedRDD2</span></span>(rdd: <span class="type">RDD</span>[<span class="type">String</span>]) = &#123;</span><br><span class="line">        <span class="keyword">val</span> q = query</span><br><span class="line">        rdd.filter(_.contains(q))</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ol>
<h4 id="5-1-4-7-RDD依赖关系"><a href="#5-1-4-7-RDD依赖关系" class="headerlink" title="5.1.4.7 RDD依赖关系"></a>5.1.4.7 RDD依赖关系</h4><ol>
<li><p>RDD 血缘关系<br> RDD只支持粗粒度转换，即在大量记录上执行的单个操作。将创建RDD的一系列Lineage（血统）记录下来，以便恢复丢失的分区。RDD的Lineage会记录RDD的元数据信息和转换行为，当该RDD的部分分区数据丢失时，它可以根据这些信息来重新运算和恢复丢失的数据分区。</p>
 <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> fileRDD: <span class="type">RDD</span>[<span class="type">String</span>] = sc.textFile(<span class="string">&quot;input/1.txt&quot;</span>)</span><br><span class="line">println(fileRDD.toDebugString)</span><br><span class="line">println(<span class="string">&quot;----------------------&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> wordRDD: <span class="type">RDD</span>[<span class="type">String</span>] = fileRDD.flatMap(_.split(<span class="string">&quot; &quot;</span>))</span><br><span class="line">println(wordRDD.toDebugString)</span><br><span class="line">println(<span class="string">&quot;----------------------&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> mapRDD: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = wordRDD.map((_,<span class="number">1</span>))</span><br><span class="line">println(mapRDD.toDebugString)</span><br><span class="line">println(<span class="string">&quot;----------------------&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> resultRDD: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = mapRDD.reduceByKey(_+_)</span><br><span class="line">println(resultRDD.toDebugString)</span><br><span class="line"></span><br><span class="line">resultRDD.collect()</span><br></pre></td></tr></table></figure></li>
<li><p>RDD 依赖关系<br> 这里所谓的依赖关系，其实就是两个相邻RDD之间的关系</p>
 <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> sc: <span class="type">SparkContext</span> = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> fileRDD: <span class="type">RDD</span>[<span class="type">String</span>] = sc.textFile(<span class="string">&quot;input/1.txt&quot;</span>)</span><br><span class="line">println(fileRDD.dependencies)</span><br><span class="line">println(<span class="string">&quot;----------------------&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> wordRDD: <span class="type">RDD</span>[<span class="type">String</span>] = fileRDD.flatMap(_.split(<span class="string">&quot; &quot;</span>))</span><br><span class="line">println(wordRDD.dependencies)</span><br><span class="line">println(<span class="string">&quot;----------------------&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> mapRDD: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = wordRDD.map((_,<span class="number">1</span>))</span><br><span class="line">println(mapRDD.dependencies)</span><br><span class="line">println(<span class="string">&quot;----------------------&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> resultRDD: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = mapRDD.reduceByKey(_+_)</span><br><span class="line">println(resultRDD.dependencies)</span><br><span class="line"></span><br><span class="line">resultRDD.collect()</span><br></pre></td></tr></table></figure></li>
<li><p>RDD 窄依赖<br> 窄依赖表示每一个父(上游)RDD的Partition最多被子（下游）RDD的一个Partition使用，窄依赖我们形象的比喻为独生子女。</p>
 <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">OneToOneDependency</span>[<span class="type">T</span>](<span class="params">rdd: <span class="type">RDD</span>[<span class="type">T</span>]</span>) <span class="keyword">extends</span> <span class="title">NarrowDependency</span>[<span class="type">T</span>](<span class="params">rdd</span>) </span></span><br></pre></td></tr></table></figure></li>
<li><p>RDD 宽依赖<br> 宽依赖表示同一个父（上游）RDD的Partition被多个子（下游）RDD的Partition依赖，会引起Shuffle，总结：宽依赖我们形象的比喻为多生。</p>
 <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ShuffleDependency</span>[<span class="type">K</span>: <span class="type">ClassTag</span>, <span class="type">V</span>: <span class="type">ClassTag</span>, <span class="type">C</span>: <span class="type">ClassTag</span>](<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="class">    @transient private val _rdd: <span class="type">RDD</span>[_ &lt;: <span class="type">Product2</span>[<span class="type">K</span>, <span class="type">V</span>]],</span></span></span><br><span class="line"><span class="params"><span class="class">    val partitioner: <span class="type">Partitioner</span>,</span></span></span><br><span class="line"><span class="params"><span class="class">    val serializer: <span class="type">Serializer</span> = <span class="type">SparkEnv</span>.get.serializer,</span></span></span><br><span class="line"><span class="params"><span class="class">    val keyOrdering: <span class="type">Option</span>[<span class="type">Ordering</span>[<span class="type">K</span>]] = <span class="type">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="class">    val aggregator: <span class="type">Option</span>[<span class="type">Aggregator</span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">C</span>]] = <span class="type">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="class">    val mapSideCombine: <span class="type">Boolean</span> = false</span>)</span></span><br><span class="line">  <span class="keyword">extends</span> <span class="type">Dependency</span>[<span class="type">Product2</span>[<span class="type">K</span>, <span class="type">V</span>]] </span><br></pre></td></tr></table></figure></li>
<li><p>RDD 阶段划分<br> DAG（Directed Acyclic Graph）有向无环图是由点和线组成的拓扑图形，该图形具有方向，不会闭环。例如，DAG记录了RDD的转换过程和任务的阶段。<br> <img src="https://s2.loli.net/2021/12/11/ki2LpfesH4xmbhW.jpg"></p>
</li>
<li><p>RDD 阶段划分源码</p>
<ul>
<li>SparkContext对象包含有一个私有属性DAGScheduler阶段调度器，主要用于阶段的划分。在一个应用程序中，任务的提交都是从行动算子触发的。行动算子的方法内部会调用一个runJob方法，其中就有DAG调度器发挥运行Job的作用：  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">dagScheduler.runJob(rdd, cleanedFunc, partitions, callSite, resultHandler, localProperties.get)</span><br></pre></td></tr></table></figure></li>
<li>runJob方法中，会执行submitJob方法：  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> waiter = submitJob(rdd, func, partitions, callSite, resultHandler, properties)</span><br></pre></td></tr></table></figure></li>
<li>继续查看这个方法的源码，其内部的重点代码区域如下：  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> func2 = func.asInstanceOf[(<span class="type">TaskContext</span>, <span class="type">Iterator</span>[_]) =&gt; _]</span><br><span class="line"><span class="keyword">val</span> waiter = <span class="keyword">new</span> <span class="type">JobWaiter</span>[<span class="type">U</span>](<span class="keyword">this</span>, jobId, partitions.size, resultHandler)</span><br><span class="line">eventProcessLoop.post(<span class="type">JobSubmitted</span>(</span><br><span class="line">    jobId, rdd, func2, partitions.toArray, callSite, waiter,</span><br><span class="line">    <span class="type">Utils</span>.cloneProperties(properties)))</span><br><span class="line">waiter</span><br></pre></td></tr></table></figure></li>
<li>此处有一个JobSubmitted事件，这个事件作为post方法的参数，该post方法主要用于将事件放入到一个队列中，然后等待事件线程执行队列中的事件：  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">post</span></span>(event: <span class="type">E</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="keyword">if</span> (!stopped.get) &#123;</span><br><span class="line">    <span class="keyword">if</span> (eventThread.isAlive) &#123;</span><br><span class="line">      eventQueue.put(event)</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      onError(<span class="keyword">new</span> <span class="type">IllegalStateException</span>(<span class="string">s&quot;<span class="subst">$name</span> has already been stopped accidentally.&quot;</span>))</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>查看这个事件线程eventThread，当这个事件线程执行的时候，会运行run方法，在方法的内部会取出事件队列中的事件。  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span>[spark] <span class="keyword">val</span> eventThread = <span class="keyword">new</span> <span class="type">Thread</span>(name) &#123;</span><br><span class="line">  setDaemon(<span class="literal">true</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="keyword">while</span> (!stopped.get) &#123;</span><br><span class="line">        <span class="keyword">val</span> event = eventQueue.take()</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">          onReceive(event)</span><br><span class="line">        &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">          <span class="keyword">case</span> <span class="type">NonFatal</span>(e) =&gt;</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">              onError(e)</span><br><span class="line">            &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">              <span class="keyword">case</span> <span class="type">NonFatal</span>(e) =&gt; logError(<span class="string">&quot;Unexpected error in &quot;</span> + name, e)</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> ie: <span class="type">InterruptedException</span> =&gt; <span class="comment">// exit even if eventQueue is not empty</span></span><br><span class="line">      <span class="keyword">case</span> <span class="type">NonFatal</span>(e) =&gt; logError(<span class="string">&quot;Unexpected error in &quot;</span> + name, e)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>事件取出之后，会将事件传给一个onReceive方法，继续查看该方法的源码，直接点进去会看到显示的是一个抽象的方法，这个抽象方法是位于EventLoop这个抽象类中的，真正执行的onReceive方法是实现这个抽象类的DAGSchedulerEventProcessLoop类中的onReceive方法。  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onReceive</span></span>(event: <span class="type">DAGSchedulerEvent</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> timerContext = timer.time()</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    doOnReceive(event)</span><br><span class="line">  &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">    timerContext.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>这是阶段调度器的主要事件循环。该方法又将事件传给了doOnReceive方法，  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">doOnReceive</span></span>(event: <span class="type">DAGSchedulerEvent</span>): <span class="type">Unit</span> = event <span class="keyword">match</span> &#123;</span><br><span class="line">  <span class="keyword">case</span> <span class="type">JobSubmitted</span>(jobId, rdd, func, partitions, callSite, listener, properties) =&gt;</span><br><span class="line">    dagScheduler.handleJobSubmitted(jobId, rdd, func, partitions, callSite, listener, properties)</span><br><span class="line"></span><br><span class="line">	……</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>该方法中包含模式匹配，JobSubmitted事件正好可以匹配到第一项，说白了就是DAGScheduler类会向事件队列发送一个消息，消息中包含的是事件，然后事件线程收到消息之后会对消息进行匹配。此处会执行handleJobSubmitted方法，查看其源码，其中  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span>[scheduler] <span class="function"><span class="keyword">def</span> <span class="title">handleJobSubmitted</span></span>(jobId: <span class="type">Int</span>,</span><br><span class="line">    finalRDD: <span class="type">RDD</span>[_],</span><br><span class="line">    func: (<span class="type">TaskContext</span>, <span class="type">Iterator</span>[_]) =&gt; _,</span><br><span class="line">    partitions: <span class="type">Array</span>[<span class="type">Int</span>],</span><br><span class="line">    callSite: <span class="type">CallSite</span>,</span><br><span class="line">    listener: <span class="type">JobListener</span>,</span><br><span class="line">    properties: <span class="type">Properties</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="keyword">var</span> finalStage: <span class="type">ResultStage</span> = <span class="literal">null</span></span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="comment">// New stage creation may throw an exception if, for example, jobs are run on a</span></span><br><span class="line">    <span class="comment">// HadoopRDD whose underlying HDFS files have been deleted.</span></span><br><span class="line">    finalStage = createResultStage(finalRDD, func, partitions, jobId, callSite)</span><br><span class="line">  &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">	……</span><br></pre></td></tr></table></figure></li>
<li>这部分区域是对阶段进行划分。createResultStage方法用于ResultStage阶段。  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">createResultStage</span></span>(</span><br><span class="line">    rdd: <span class="type">RDD</span>[_],</span><br><span class="line">    func: (<span class="type">TaskContext</span>, <span class="type">Iterator</span>[_]) =&gt; _,</span><br><span class="line">    partitions: <span class="type">Array</span>[<span class="type">Int</span>],</span><br><span class="line">    jobId: <span class="type">Int</span>,</span><br><span class="line">    callSite: <span class="type">CallSite</span>): <span class="type">ResultStage</span> = &#123;</span><br><span class="line">  checkBarrierStageWithDynamicAllocation(rdd)</span><br><span class="line">  checkBarrierStageWithNumSlots(rdd)</span><br><span class="line">  checkBarrierStageWithRDDChainPattern(rdd, partitions.toSet.size)</span><br><span class="line">  <span class="keyword">val</span> parents = getOrCreateParentStages(rdd, jobId)</span><br><span class="line">  <span class="keyword">val</span> id = nextStageId.getAndIncrement()</span><br><span class="line">  <span class="keyword">val</span> stage = <span class="keyword">new</span> <span class="type">ResultStage</span>(id, rdd, func, partitions, parents, jobId, callSite)</span><br><span class="line">  stageIdToStage(id) = stage</span><br><span class="line">  updateJobIdStageIdMaps(jobId, stage)</span><br><span class="line">  stage</span><br><span class="line">&#125;</span><br><span class="line">​ <span class="type">ResultStage</span>中包含的rdd就是执行行动算子的那个rdd（下图中黄色表示的那个），也就是最后的那个rdd（下图中黄色图表示的rdd）。parents是<span class="type">ResultStage</span>的上一级阶段，parents是getOrCreateParentStages方法的返回值。getOrCreateParentStages用于获取或者创建给定<span class="type">RDD</span>的父阶段列表。</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">getOrCreateParentStages</span></span>(rdd: <span class="type">RDD</span>[_], firstJobId: <span class="type">Int</span>): <span class="type">List</span>[<span class="type">Stage</span>] = &#123;</span><br><span class="line">  getShuffleDependencies(rdd).map &#123; shuffleDep =&gt;</span><br><span class="line">    getOrCreateShuffleMapStage(shuffleDep, firstJobId)</span><br><span class="line">  &#125;.toList</span><br><span class="line">&#125;</span><br><span class="line">​ getShuffleDependencies方法用于获取给定rdd的shuffle依赖，其核心代码如下：</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span>[scheduler] <span class="function"><span class="keyword">def</span> <span class="title">getShuffleDependencies</span></span>(</span><br><span class="line">    rdd: <span class="type">RDD</span>[_]): <span class="type">HashSet</span>[<span class="type">ShuffleDependency</span>[_, _, _]] = &#123;</span><br><span class="line"> 		……</span><br><span class="line">  <span class="keyword">while</span> (waitingForVisit.nonEmpty) &#123;</span><br><span class="line">    <span class="keyword">val</span> toVisit = waitingForVisit.remove(<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">if</span> (!visited(toVisit)) &#123;</span><br><span class="line">      visited += toVisit</span><br><span class="line">      toVisit.dependencies.foreach &#123;</span><br><span class="line">        <span class="keyword">case</span> shuffleDep: <span class="type">ShuffleDependency</span>[_, _, _] =&gt;</span><br><span class="line">          parents += shuffleDep</span><br><span class="line">        <span class="keyword">case</span> dependency =&gt;</span><br><span class="line">          waitingForVisit.prepend(dependency.rdd)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  parents</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>核心代码用于判断给定rdd的依赖关系是不是shuffle依赖，如果是则加入结果列表。最终返回的结果列表，会通过map方法将列表中的每一个元素执行getOrCreateShuffleMapStage方法，该方法用于获取或者创建ShuffleMap阶段（写磁盘之前的阶段）。  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">getOrCreateShuffleMapStage(shuffleDep, firstJobId) =&gt; createShuffleMapStage(shuffleDep, firstJobId)</span><br></pre></td></tr></table></figure></li>
<li>createShuffleMapStage方法中会创建ShuffleMapStage对象，并当前rdd（调用行动算子的那个）依赖的rdd（下图紫色那个rdd）传给这个对象。  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createShuffleMapStage</span></span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">C</span>](</span><br><span class="line">    shuffleDep: <span class="type">ShuffleDependency</span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">C</span>], jobId: <span class="type">Int</span>): <span class="type">ShuffleMapStage</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> rdd = shuffleDep.rdd</span><br><span class="line">	……</span><br><span class="line">  <span class="keyword">val</span> numTasks = rdd.partitions.length</span><br><span class="line">  <span class="keyword">val</span> parents = getOrCreateParentStages(rdd, jobId)</span><br><span class="line">  <span class="keyword">val</span> id = nextStageId.getAndIncrement()</span><br><span class="line">  <span class="keyword">val</span> stage = <span class="keyword">new</span> <span class="type">ShuffleMapStage</span>(</span><br><span class="line">    id, rdd, numTasks, parents, jobId, rdd.creationSite, shuffleDep, mapOutputTracker)</span><br><span class="line"></span><br><span class="line">	……</span><br></pre></td></tr></table></figure></li>
<li>此时，ShuffleMap阶段（下图红色区域）就是Result阶段（蓝色区域）的上一级阶段。在上面的代码中，我们还可以看到，如果当前ShuffleMap阶段还有上一级阶段，那么getOrCreateParentStages(rdd, jobId)方法还是会获取它的上一级阶段的，此时这个方法中的rdd就不再是最后一个rdd，而是最后一个rdd的前一个rdd，也就是紫色表示的那个rdd。也就是说，阶段的寻找是一个不断往前的过程，只要含有shuffle过程，那么就会有新的阶段。<br>  <img src="https://s2.loli.net/2021/12/11/rvgSydiUjZf3pst.jpg"></li>
</ul>
</li>
<li><p>RDD 任务划分<br>RDD任务切分中间分为：Application、Job、Stage和Task</p>
</li>
</ol>
<ul>
<li>Application：初始化一个SparkContext即生成一个Application；</li>
<li>Job：一个Action算子就会生成一个Job；</li>
<li>Stage：Stage等于宽依赖(ShuffleDependency)的个数加1；</li>
<li>Task：一个Stage阶段中，最后一个RDD的分区个数就是Task的个数。<br><font color ='red' >注意：Application-&gt;Job-&gt;Stage-&gt;Task每一层都是1对n的关系。 </font></li>
</ul>
<ol start="8">
<li>RDD 任务划分源码<ul>
<li>DAGScheduler类的handleJobSubmitted方法中，有一个提交阶段的的方法：  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">var</span> finalStage: <span class="type">ResultStage</span> = <span class="literal">null</span></span><br><span class="line">	……</span><br><span class="line">finalStage = createResultStage(finalRDD, func, partitions, jobId, callSite)</span><br><span class="line">	……</span><br><span class="line">submitStage(finalStage)</span><br></pre></td></tr></table></figure></li>
<li>submitStage方法用于提交最终的ResultStage阶段，由于在最终的ResultStage可能包含了多个上级阶段，所以此处就相当于是提交整个应用程序的全部阶段。查看一下该方法的源码：  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">submitStage</span></span>(stage: <span class="type">Stage</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> jobId = activeJobForStage(stage)</span><br><span class="line">  <span class="keyword">if</span> (jobId.isDefined) &#123;</span><br><span class="line">    logDebug(<span class="string">s&quot;submitStage(<span class="subst">$stage</span> (name=<span class="subst">$&#123;stage.name&#125;</span>;&quot;</span> +</span><br><span class="line">      <span class="string">s&quot;jobs=<span class="subst">$&#123;stage.jobIds.toSeq.sorted.mkString(&quot;,&quot;)&#125;</span>))&quot;</span>)</span><br><span class="line">    <span class="keyword">if</span> (!waitingStages(stage) &amp;&amp; !runningStages(stage) &amp;&amp; !failedStages(stage)) &#123;</span><br><span class="line">      <span class="keyword">val</span> missing = getMissingParentStages(stage).sortBy(_.id)</span><br><span class="line">      logDebug(<span class="string">&quot;missing: &quot;</span> + missing)</span><br><span class="line">      <span class="keyword">if</span> (missing.isEmpty) &#123;</span><br><span class="line">        logInfo(<span class="string">&quot;Submitting &quot;</span> + stage + <span class="string">&quot; (&quot;</span> + stage.rdd + <span class="string">&quot;), which has no missing parents&quot;</span>)</span><br><span class="line">        submitMissingTasks(stage, jobId.get)</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">for</span> (parent &lt;- missing) &#123;</span><br><span class="line">          submitStage(parent)</span><br><span class="line">        &#125;</span><br><span class="line">        waitingStages += stage</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    abortStage(stage, <span class="string">&quot;No active job for stage &quot;</span> + stage.id, <span class="type">None</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>该方法的内部核心逻辑是先获取当前阶段的的所有父级阶段，如果其父级阶段为空那么直接执行submitMissingTasks方法，如果不为空，那么递归执行submitStage方法，只不过传入的参数是当前阶段的父级阶段，一直递归直到找到没有上级阶段的阶段，最终没有上级阶段的那个阶段会执行submitMissingTasks方法。下面查看一下该方法的核心源码部分：  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">submitMissingTasks</span></span>(stage: <span class="type">Stage</span>, jobId: <span class="type">Int</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    	 ……</span><br><span class="line">  <span class="keyword">val</span> partitionsToCompute: <span class="type">Seq</span>[<span class="type">Int</span>] = stage.findMissingPartitions()</span><br><span class="line">		 ……</span><br><span class="line">  <span class="keyword">val</span> tasks: <span class="type">Seq</span>[<span class="type">Task</span>[_]] = <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="keyword">val</span> serializedTaskMetrics = closureSerializer.serialize(stage.latestInfo.taskMetrics).array()</span><br><span class="line">    stage <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> stage: <span class="type">ShuffleMapStage</span> =&gt;</span><br><span class="line">        stage.pendingPartitions.clear()</span><br><span class="line">        partitionsToCompute.map &#123; id =&gt;</span><br><span class="line">          <span class="keyword">val</span> locs = taskIdToLocations(id)</span><br><span class="line">          <span class="keyword">val</span> part = partitions(id)</span><br><span class="line">          stage.pendingPartitions += id</span><br><span class="line">          <span class="keyword">new</span> <span class="type">ShuffleMapTask</span>(stage.id, stage.latestInfo.attemptNumber,</span><br><span class="line">            taskBinary, part, locs, properties, serializedTaskMetrics, <span class="type">Option</span>(jobId),</span><br><span class="line">            <span class="type">Option</span>(sc.applicationId), sc.applicationAttemptId, stage.rdd.isBarrier())</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">      <span class="keyword">case</span> stage: <span class="type">ResultStage</span> =&gt;</span><br><span class="line">        partitionsToCompute.map &#123; id =&gt;</span><br><span class="line">          <span class="keyword">val</span> p: <span class="type">Int</span> = stage.partitions(id)</span><br><span class="line">          <span class="keyword">val</span> part = partitions(p)</span><br><span class="line">          <span class="keyword">val</span> locs = taskIdToLocations(id)</span><br><span class="line">          <span class="keyword">new</span> <span class="type">ResultTask</span>(stage.id, stage.latestInfo.attemptNumber,</span><br><span class="line">            taskBinary, part, locs, id, properties, serializedTaskMetrics,</span><br><span class="line">            <span class="type">Option</span>(jobId), <span class="type">Option</span>(sc.applicationId), sc.applicationAttemptId,</span><br><span class="line">            stage.rdd.isBarrier())</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="type">NonFatal</span>(e) =&gt;</span><br><span class="line">      abortStage(stage, <span class="string">s&quot;Task creation failed: <span class="subst">$e</span>\n<span class="subst">$&#123;Utils.exceptionString(e)&#125;</span>&quot;</span>, <span class="type">Some</span>(e))</span><br><span class="line">      runningStages -= stage</span><br><span class="line">      <span class="keyword">return</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"> 	……</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>核心代码的逻辑在于根据传入的stage进行模式匹配，会根据不同类型的Satge创建的不同的Task，那么首先会计算分区得到分区索引集合，然后使用map方法将根据分区id创建xxxMapTask对象，有几个分区id就创建几个xxxMapTask对象。partitionsToCompute是stage.findMissingPartitions()的返回值，那么查看其源码，stage是一个抽象类的引用，调用的这个方法具体的实现在具体的xxxMapStage类中。分别查看一下在resultstage和中的源码：<ul>
<li>ResultStage：  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">findMissingPartitions</span></span>(): <span class="type">Seq</span>[<span class="type">Int</span>] = &#123;</span><br><span class="line">  <span class="keyword">val</span> job = activeJob.get</span><br><span class="line">  (<span class="number">0</span> until job.numPartitions).filter(id =&gt; !job.finished(id))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>ShuffleMapStage：  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">findMissingPartitions</span></span>(): <span class="type">Seq</span>[<span class="type">Int</span>] = &#123;</span><br><span class="line">  mapOutputTrackerMaster</span><br><span class="line">    .findMissingPartitions(shuffleDep.shuffleId)</span><br><span class="line">    .getOrElse(<span class="number">0</span> until numPartitions)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>所以可以看出，partitionsToCompute就是一个分区索引的集合。ResultStage和ShuffleMapStage的numPartitions的值计算方式一样，都是来自于它们所处阶段的最后一个rdd的分区数量值：<ul>
<li>job.numPartitions值：  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> numPartitions = finalStage <span class="keyword">match</span> &#123;</span><br><span class="line">  <span class="keyword">case</span> r: <span class="type">ResultStage</span> =&gt; r.partitions.length</span><br><span class="line">  <span class="keyword">case</span> m: <span class="type">ShuffleMapStage</span> =&gt; m.rdd.partitions.length</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>numPartitions：  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> numPartitions = rdd.partitions.length</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>所以总结一下，应用程序的总任务数量等于每个阶段的最后一个rdd的分区数量之和。</li>
</ul>
</li>
</ul>
</li>
</ol>
<h4 id="5-1-4-8-RDD持久化"><a href="#5-1-4-8-RDD持久化" class="headerlink" title="5.1.4.8 RDD持久化"></a>5.1.4.8 RDD持久化</h4><ol>
<li><p>RDD Cache缓存<br> RDD通过Cache或者Persist方法将前面的计算结果缓存，默认情况下会把数据以缓存在JVM的堆内存中。但是并不是这两个方法被调用时立即缓存，而是触发后面的action算子时，该RDD将会被缓存在计算节点的内存中，并供后面重用。</p>
 <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// cache操作会增加血缘关系，不改变原有的血缘关系</span></span><br><span class="line">println(wordToOneRdd.toDebugString)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 数据缓存。</span></span><br><span class="line">wordToOneRdd.cache()</span><br><span class="line"></span><br><span class="line"><span class="comment">// 可以更改存储级别</span></span><br><span class="line"><span class="comment">//mapRdd.persist(StorageLevel.MEMORY_AND_DISK_2)</span></span><br><span class="line">存储级别</span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">StorageLevel</span> </span>&#123;</span><br><span class="line">  <span class="keyword">val</span> <span class="type">NONE</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">false</span>, <span class="literal">false</span>, <span class="literal">false</span>, <span class="literal">false</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">DISK_ONLY</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">false</span>, <span class="literal">false</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">DISK_ONLY_2</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">false</span>, <span class="literal">false</span>, <span class="number">2</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">MEMORY_ONLY</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">false</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">true</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">MEMORY_ONLY_2</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">false</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">true</span>, <span class="number">2</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">MEMORY_ONLY_SER</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">false</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">false</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">MEMORY_ONLY_SER_2</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">false</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">false</span>, <span class="number">2</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">MEMORY_AND_DISK</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">true</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">MEMORY_AND_DISK_2</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">true</span>, <span class="number">2</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">MEMORY_AND_DISK_SER</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">false</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">MEMORY_AND_DISK_SER_2</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">false</span>, <span class="number">2</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">OFF_HEAP</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">true</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p> <img src="https://s2.loli.net/2021/12/11/lf4FvbAzEqIcYTn.jpg"><br> 缓存有可能丢失，或者存储于内存的数据由于内存不足而被删除，RDD的缓存容错机制保证了即使缓存丢失也能保证计算的正确执行。通过基于RDD的一系列转换，丢失的数据会被重算，由于RDD的各个Partition是相对独立的，因此只需要计算丢失的部分即可，并不需要重算全部Partition。<br> Spark会自动对一些Shuffle操作的中间数据做持久化操作(比如：reduceByKey)。这样做的目的是为了当一个节点Shuffle失败了避免重新计算整个输入。但是，在实际使用的时候，如果想重用数据，仍然建议调用persist或cache。</p>
</li>
<li><p>RDD CheckPoint检查点<br> 所谓的检查点其实就是将RDD中间结果写入磁盘<br> 由于血缘依赖过长会造成容错成本过高，这样就不如在中间阶段做检查点容错，如果检查点之后有节点出现问题，可以从检查点开始重做血缘，减少了开销。<br> 对RDD进行checkpoint操作并不会马上被执行，必须执行Action操作才能触发。</p>
 <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 设置检查点路径</span></span><br><span class="line">sc.setCheckpointDir(<span class="string">&quot;./checkpoint1&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 创建一个RDD，读取指定位置文件:hello atguigu atguigu</span></span><br><span class="line"><span class="keyword">val</span> lineRdd: <span class="type">RDD</span>[<span class="type">String</span>] = sc.textFile(<span class="string">&quot;input/1.txt&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 业务逻辑</span></span><br><span class="line"><span class="keyword">val</span> wordRdd: <span class="type">RDD</span>[<span class="type">String</span>] = lineRdd.flatMap(line =&gt; line.split(<span class="string">&quot; &quot;</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> wordToOneRdd: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Long</span>)] = wordRdd.map &#123;</span><br><span class="line">    word =&gt; &#123;</span><br><span class="line">        (word, <span class="type">System</span>.currentTimeMillis())</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 增加缓存,避免再重新跑一个job做checkpoint</span></span><br><span class="line">wordToOneRdd.cache()</span><br><span class="line"><span class="comment">// 数据检查点：针对wordToOneRdd做检查点计算</span></span><br><span class="line">wordToOneRdd.checkpoint()</span><br><span class="line"></span><br><span class="line"><span class="comment">// 触发执行逻辑</span></span><br><span class="line">wordToOneRdd.collect().foreach(println)</span><br></pre></td></tr></table></figure></li>
<li><p>缓存和检查点区别</p>
<ul>
<li>Cache缓存只是将数据保存起来，不切断血缘依赖。Checkpoint检查点切断血缘依赖。</li>
<li>Cache缓存的数据通常存储在磁盘、内存等地方，可靠性低。Checkpoint的数据通常存储在HDFS等容错、高可用的文件系统，可靠性高。</li>
<li>建议对checkpoint()的RDD使用Cache缓存，这样checkpoint的job只需从Cache缓存中读取数据即可，否则需要再从头计算一次RDD。</li>
</ul>
</li>
</ol>
<h4 id="5-1-4-9-RDD分区器"><a href="#5-1-4-9-RDD分区器" class="headerlink" title="5.1.4.9 RDD分区器"></a>5.1.4.9 RDD分区器</h4><p>Spark目前支持Hash分区和Range分区，和用户自定义分区。Hash分区为当前的默认分区。分区器直接决定了RDD中分区的个数、RDD中每条数据经过Shuffle后进入哪个分区，进而决定了Reduce的个数。</p>
<ul>
<li>只有Key-Value类型的RDD才有分区器，非Key-Value类型的RDD分区的值是None</li>
<li>每个RDD的分区ID范围：0 ~ (numPartitions - 1)，决定这个值是属于那个分区的。</li>
</ul>
<ol>
<li>Hash分区：对于给定的key，计算其hashCode,并除以分区个数取余 <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">HashPartitioner</span>(<span class="params">partitions: <span class="type">Int</span></span>) <span class="keyword">extends</span> <span class="title">Partitioner</span> </span>&#123;</span><br><span class="line">  require(partitions &gt;= <span class="number">0</span>, <span class="string">s&quot;Number of partitions (<span class="subst">$partitions</span>) cannot be negative.&quot;</span>)</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">numPartitions</span></span>: <span class="type">Int</span> = partitions</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getPartition</span></span>(key: <span class="type">Any</span>): <span class="type">Int</span> = key <span class="keyword">match</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="literal">null</span> =&gt; <span class="number">0</span></span><br><span class="line">    <span class="keyword">case</span> _ =&gt; <span class="type">Utils</span>.nonNegativeMod(key.hashCode, numPartitions)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">equals</span></span>(other: <span class="type">Any</span>): <span class="type">Boolean</span> = other <span class="keyword">match</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> h: <span class="type">HashPartitioner</span> =&gt;</span><br><span class="line">      h.numPartitions == numPartitions</span><br><span class="line">    <span class="keyword">case</span> _ =&gt;</span><br><span class="line">      <span class="literal">false</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">hashCode</span></span>: <span class="type">Int</span> = numPartitions</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>Range分区：将一定范围内的数据映射到一个分区中，尽量保证每个分区数据均匀，而且分区间有序 <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RangePartitioner</span>[<span class="type">K</span> : <span class="type">Ordering</span> : <span class="type">ClassTag</span>, <span class="type">V</span>](<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="class">    partitions: <span class="type">Int</span>,</span></span></span><br><span class="line"><span class="params"><span class="class">    rdd: <span class="type">RDD</span>[_ &lt;: <span class="type">Product2</span>[<span class="type">K</span>, <span class="type">V</span>]],</span></span></span><br><span class="line"><span class="params"><span class="class">    private var ascending: <span class="type">Boolean</span> = true</span>)</span></span><br><span class="line">  <span class="keyword">extends</span> <span class="type">Partitioner</span> &#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// We allow partitions = 0, which happens when sorting an empty RDD under the default settings.</span></span><br><span class="line">  require(partitions &gt;= <span class="number">0</span>, <span class="string">s&quot;Number of partitions cannot be negative but found <span class="subst">$partitions</span>.&quot;</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> ordering = implicitly[<span class="type">Ordering</span>[<span class="type">K</span>]]</span><br><span class="line"></span><br><span class="line">  <span class="comment">// An array of upper bounds for the first (partitions - 1) partitions</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> rangeBounds: <span class="type">Array</span>[<span class="type">K</span>] = &#123;</span><br><span class="line">  ...</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">numPartitions</span></span>: <span class="type">Int</span> = rangeBounds.length + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> binarySearch: ((<span class="type">Array</span>[<span class="type">K</span>], <span class="type">K</span>) =&gt; <span class="type">Int</span>) = <span class="type">CollectionsUtils</span>.makeBinarySearch[<span class="type">K</span>]</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getPartition</span></span>(key: <span class="type">Any</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> k = key.asInstanceOf[<span class="type">K</span>]</span><br><span class="line">    <span class="keyword">var</span> partition = <span class="number">0</span></span><br><span class="line">    <span class="keyword">if</span> (rangeBounds.length &lt;= <span class="number">128</span>) &#123;</span><br><span class="line">      <span class="comment">// If we have less than 128 partitions naive search</span></span><br><span class="line">      <span class="keyword">while</span> (partition &lt; rangeBounds.length &amp;&amp; ordering.gt(k, rangeBounds(partition))) &#123;</span><br><span class="line">        partition += <span class="number">1</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="comment">// Determine which binary search method to use only once.</span></span><br><span class="line">      partition = binarySearch(rangeBounds, k)</span><br><span class="line">      <span class="comment">// binarySearch either returns the match location or -[insertion point]-1</span></span><br><span class="line">      <span class="keyword">if</span> (partition &lt; <span class="number">0</span>) &#123;</span><br><span class="line">        partition = -partition<span class="number">-1</span></span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">if</span> (partition &gt; rangeBounds.length) &#123;</span><br><span class="line">        partition = rangeBounds.length</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (ascending) &#123;</span><br><span class="line">      partition</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      rangeBounds.length - partition</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">equals</span></span>(other: <span class="type">Any</span>): <span class="type">Boolean</span> = other <span class="keyword">match</span> &#123;</span><br><span class="line">  ...</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">hashCode</span></span>(): <span class="type">Int</span> = &#123;</span><br><span class="line">  ...</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@throws</span>(classOf[<span class="type">IOException</span>])</span><br><span class="line">  <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">writeObject</span></span>(out: <span class="type">ObjectOutputStream</span>): <span class="type">Unit</span> = <span class="type">Utils</span>.tryOrIOException &#123;</span><br><span class="line">  ...</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@throws</span>(classOf[<span class="type">IOException</span>])</span><br><span class="line">  <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">readObject</span></span>(in: <span class="type">ObjectInputStream</span>): <span class="type">Unit</span> = <span class="type">Utils</span>.tryOrIOException &#123;</span><br><span class="line">  ...</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ol>
<h4 id="5-1-4-10-RDD文件读取与保存"><a href="#5-1-4-10-RDD文件读取与保存" class="headerlink" title="5.1.4.10 RDD文件读取与保存"></a>5.1.4.10 RDD文件读取与保存</h4><p>Spark的数据读取及数据保存可以从两个维度来作区分：文件格式以及文件系统。</p>
<ul>
<li>文件格式分为：text文件、csv文件、sequence文件以及Object文件；</li>
<li>文件系统分为：本地文件系统、HDFS、HBASE以及数据库。<ul>
<li>text文件  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 读取输入文件</span></span><br><span class="line"><span class="keyword">val</span> inputRDD: <span class="type">RDD</span>[<span class="type">String</span>] = sc.textFile(<span class="string">&quot;input/1.txt&quot;</span>)</span><br><span class="line"><span class="comment">// 保存数据</span></span><br><span class="line">inputRDD.saveAsTextFile(<span class="string">&quot;output&quot;</span>)</span><br></pre></td></tr></table></figure></li>
<li>sequence文件<br>  SequenceFile文件是Hadoop用来存储二进制形式的key-value对而设计的一种平面文件(Flat File)。在SparkContext中，可以调用sequenceFile<a href="path">keyClass, valueClass</a>。  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 保存数据为SequenceFile</span></span><br><span class="line">dataRDD.saveAsSequenceFile(<span class="string">&quot;output&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 读取SequenceFile文件</span></span><br><span class="line">sc.sequenceFile[<span class="type">Int</span>,<span class="type">Int</span>](<span class="string">&quot;output&quot;</span>).collect().foreach(println)</span><br></pre></td></tr></table></figure></li>
<li>object对象文件<br>  对象文件是将对象序列化后保存的文件，采用Java的序列化机制。可以通过objectFile<a href="path">T: ClassTag</a>函数接收一个路径，读取对象文件，返回对应的RDD，也可以通过调用saveAsObjectFile()实现对对象文件的输出。因为是序列化所以要指定类型。  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 保存数据</span></span><br><span class="line">dataRDD.saveAsObjectFile(<span class="string">&quot;output&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 读取数据</span></span><br><span class="line">sc.objectFile[<span class="type">Int</span>](<span class="string">&quot;output&quot;</span>).collect().foreach(println)</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>
<h2 id="5-2-累加器"><a href="#5-2-累加器" class="headerlink" title="5.2 累加器"></a>5.2 累加器</h2><h3 id="5-2-1-实现原理"><a href="#5-2-1-实现原理" class="headerlink" title="5.2.1 实现原理"></a>5.2.1 实现原理</h3><p>累加器用来把Executor端变量信息聚合到Driver端。在Driver程序中定义的变量，在Executor端的每个Task都会得到这个变量的一份新的副本，每个task更新这些副本的值后，传回Driver端进行merge。</p>
<h3 id="5-2-2-基础编程"><a href="#5-2-2-基础编程" class="headerlink" title="5.2.2 基础编程"></a>5.2.2 基础编程</h3><h4 id="5-2-2-1-系统累加器"><a href="#5-2-2-1-系统累加器" class="headerlink" title="5.2.2.1 系统累加器"></a>5.2.2.1 系统累加器</h4><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd = sc.makeRDD(<span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>))</span><br><span class="line"><span class="comment">// 声明累加器</span></span><br><span class="line"><span class="keyword">var</span> sum = sc.longAccumulator(<span class="string">&quot;sum&quot;</span>);</span><br><span class="line">rdd.foreach(</span><br><span class="line">  num =&gt; &#123;</span><br><span class="line">    <span class="comment">// 使用累加器</span></span><br><span class="line">    sum.add(num)</span><br><span class="line">  &#125;</span><br><span class="line">)</span><br><span class="line"><span class="comment">// 获取累加器的值</span></span><br><span class="line">println(<span class="string">&quot;sum = &quot;</span> + sum.value)</span><br></pre></td></tr></table></figure>

<h4 id="5-2-2-2-自定义累加器"><a href="#5-2-2-2-自定义累加器" class="headerlink" title="5.2.2.2 自定义累加器"></a>5.2.2.2 自定义累加器</h4><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 自定义累加器</span></span><br><span class="line"><span class="comment">// 1. 继承AccumulatorV2，并设定泛型</span></span><br><span class="line"><span class="comment">// 2. 重写累加器的抽象方法</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">WordCountAccumulator</span> <span class="keyword">extends</span> <span class="title">AccumulatorV2</span>[<span class="type">String</span>, mutable.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">Long</span>]]</span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">var</span> map : mutable.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">Long</span>] = mutable.<span class="type">Map</span>()</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 累加器是否为初始状态</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">isZero</span></span>: <span class="type">Boolean</span> = &#123;</span><br><span class="line">      map.isEmpty</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 复制累加器</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">copy</span></span>(): <span class="type">AccumulatorV2</span>[<span class="type">String</span>, mutable.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">Long</span>]] = &#123;</span><br><span class="line">      <span class="keyword">new</span> <span class="type">WordCountAccumulator</span></span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 重置累加器</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">reset</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">      map.clear()</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 向累加器中增加数据 (In)</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">add</span></span>(word: <span class="type">String</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="comment">// 查询map中是否存在相同的单词</span></span><br><span class="line">        <span class="comment">// 如果有相同的单词，那么单词的数量加1</span></span><br><span class="line">        <span class="comment">// 如果没有相同的单词，那么在map中增加这个单词</span></span><br><span class="line">        map(word) = map.getOrElse(word, <span class="number">0</span>L) + <span class="number">1</span>L</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 合并累加器</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">merge</span></span>(other: <span class="type">AccumulatorV2</span>[<span class="type">String</span>, mutable.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">Long</span>]]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    </span><br><span class="line">      <span class="keyword">val</span> map1 = map</span><br><span class="line">      <span class="keyword">val</span> map2 = other.value</span><br><span class="line">    </span><br><span class="line">      <span class="comment">// 两个Map的合并</span></span><br><span class="line">      map = map1.foldLeft(map2)(</span><br><span class="line">        ( innerMap, kv ) =&gt; &#123;</span><br><span class="line">          innerMap(kv._1) = innerMap.getOrElse(kv._1, <span class="number">0</span>L) + kv._2</span><br><span class="line">          innerMap</span><br><span class="line">        &#125;</span><br><span class="line">      )</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 返回累加器的结果 （Out）</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">value</span></span>: mutable.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">Long</span>] = map</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="5-3-广播变量"><a href="#5-3-广播变量" class="headerlink" title="5.3 广播变量"></a>5.3 广播变量</h2><h3 id="5-3-1-实现原理"><a href="#5-3-1-实现原理" class="headerlink" title="5.3.1 实现原理"></a>5.3.1 实现原理</h3><p>广播变量用来高效分发较大的对象。向所有工作节点发送一个较大的只读值，以供一个或多个Spark操作使用。比如，如果你的应用需要向所有节点发送一个较大的只读查询表，广播变量用起来都很顺手。在多个并行操作中使用同一个变量，但是 Spark会为每个任务分别发送。</p>
<h3 id="5-3-2-基础编程"><a href="#5-3-2-基础编程" class="headerlink" title="5.3.2 基础编程"></a>5.3.2 基础编程</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd1 = sc.makeRDD(<span class="type">List</span>( (<span class="string">&quot;a&quot;</span>,<span class="number">1</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">2</span>), (<span class="string">&quot;c&quot;</span>, <span class="number">3</span>), (<span class="string">&quot;d&quot;</span>, <span class="number">4</span>) ),<span class="number">4</span>)</span><br><span class="line"><span class="keyword">val</span> list = <span class="type">List</span>( (<span class="string">&quot;a&quot;</span>,<span class="number">4</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">5</span>), (<span class="string">&quot;c&quot;</span>, <span class="number">6</span>), (<span class="string">&quot;d&quot;</span>, <span class="number">7</span>) )</span><br><span class="line"><span class="comment">// 声明广播变量</span></span><br><span class="line"><span class="keyword">val</span> broadcast: <span class="type">Broadcast</span>[<span class="type">List</span>[(<span class="type">String</span>, <span class="type">Int</span>)]] = sc.broadcast(list)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> resultRDD: <span class="type">RDD</span>[(<span class="type">String</span>, (<span class="type">Int</span>, <span class="type">Int</span>))] = rdd1.map &#123;</span><br><span class="line">  <span class="keyword">case</span> (key, num) =&gt; &#123;</span><br><span class="line">    <span class="keyword">var</span> num2 = <span class="number">0</span></span><br><span class="line">    <span class="comment">// 使用广播变量</span></span><br><span class="line">    <span class="keyword">for</span> ((k, v) &lt;- broadcast.value) &#123;</span><br><span class="line">      <span class="keyword">if</span> (k == key) &#123;</span><br><span class="line">        num2 = v</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    (key, (num, num2))</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


<h1 id="第6章-Spark案例实操"><a href="#第6章-Spark案例实操" class="headerlink" title="第6章 Spark案例实操"></a>第6章 Spark案例实操</h1><p>在实际的工作中如何使用这些API实现具体的需求。这些需求是电商网站的真实需求，所以在实现功能前，必须先将数据准备好。<br><img src="https://s2.loli.net/2021/12/11/TLAMWjC5Ybc4a19.jpg"></p>
<p>上面的数据图是从数据文件中截取的一部分内容，表示为电商网站的用户行为数据，主要包含用户的4种行为：搜索，点击，下单，支付。数据规则如下：</p>
<ul>
<li>数据文件中每行数据采用下划线分隔数据</li>
<li>每一行数据表示用户的一次行为，这个行为只能是4种行为的一种</li>
<li>如果搜索关键字为null,表示数据不是搜索数据</li>
<li>如果点击的品类ID和产品ID为-1，表示数据不是点击数据</li>
<li>针对于下单行为，一次可以下单多个商品，所以品类ID和产品ID可以是多个，id之间采用逗号分隔，如果本次不是下单行为，则数据采用null表示</li>
<li>支付行为和下单行为类似</li>
</ul>
<p>详细字段说明：<br>|编号        |字段名称      |字段类型      |字段含义       |<br>|——– |———–|———–|————-|<br>|1|  date| String|  用户点击行为的日期|<br>|2|  user_id|  Long| 用户的ID|<br>|3|  session_id|   String|    Session的ID|<br>|4|  page_id|  Long| 某个页面的ID|<br>|5|  action_time|  String|   动作的时间点|<br>|6|  search_keyword|   String|    用户搜索的关键词|<br>|7|  click_category_id|    Long|   某一个商品品类的ID|<br>|8|  click_product_id| Long|    某一个商品的ID|<br>|9|  order_category_ids|   String|    一次订单中所有品类的ID集合|<br>|10| order_product_ids|   String|    一次订单中所有商品的ID集合|<br>|11| pay_category_ids|    String| 一次支付中所有品类的ID集合|<br>|12| pay_product_ids| String|  一次支付中所有商品的ID集合|<br>|13| city_id| Long|    城市 id|</p>
<p>样例类：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">//用户访问动作表</span></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">UserVisitAction</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="class">    date: <span class="type">String</span>,//用户点击行为的日期</span></span></span><br><span class="line"><span class="params"><span class="class">    user_id: <span class="type">Long</span>,//用户的<span class="type">ID</span></span></span></span><br><span class="line"><span class="params"><span class="class">    session_id: <span class="type">String</span>,//<span class="type">Session</span>的<span class="type">ID</span></span></span></span><br><span class="line"><span class="params"><span class="class">    page_id: <span class="type">Long</span>,//某个页面的<span class="type">ID</span></span></span></span><br><span class="line"><span class="params"><span class="class">    action_time: <span class="type">String</span>,//动作的时间点</span></span></span><br><span class="line"><span class="params"><span class="class">    search_keyword: <span class="type">String</span>,//用户搜索的关键词</span></span></span><br><span class="line"><span class="params"><span class="class">    click_category_id: <span class="type">Long</span>,//某一个商品品类的<span class="type">ID</span></span></span></span><br><span class="line"><span class="params"><span class="class">    click_product_id: <span class="type">Long</span>,//某一个商品的<span class="type">ID</span></span></span></span><br><span class="line"><span class="params"><span class="class">    order_category_ids: <span class="type">String</span>,//一次订单中所有品类的<span class="type">ID</span>集合</span></span></span><br><span class="line"><span class="params"><span class="class">    order_product_ids: <span class="type">String</span>,//一次订单中所有商品的<span class="type">ID</span>集合</span></span></span><br><span class="line"><span class="params"><span class="class">    pay_category_ids: <span class="type">String</span>,//一次支付中所有品类的<span class="type">ID</span>集合</span></span></span><br><span class="line"><span class="params"><span class="class">    pay_product_ids: <span class="type">String</span>,//一次支付中所有商品的<span class="type">ID</span>集合</span></span></span><br><span class="line"><span class="params"><span class="class">    city_id: <span class="type">Long</span>//城市 id</span></span></span><br><span class="line"><span class="params"><span class="class"></span>)</span></span><br></pre></td></tr></table></figure>

<h2 id="6-1-需求1：Top10热门品类"><a href="#6-1-需求1：Top10热门品类" class="headerlink" title="6.1 需求1：Top10热门品类"></a>6.1 需求1：Top10热门品类</h2><p><img src="https://s2.loli.net/2021/12/11/NHbitm2oXMDEnqZ.jpg"></p>
<h3 id="6-1-1-需求说明"><a href="#6-1-1-需求说明" class="headerlink" title="6.1.1 需求说明"></a>6.1.1 需求说明</h3><p>品类是指产品的分类，大型电商网站品类分多级，咱们的项目中品类只有一级，不同的公司可能对热门的定义不一样。我们按照每个品类的点击、下单、支付的量来统计热门品类。</p>
<p>鞋            点击数 下单数  支付数<br>衣服        点击数 下单数  支付数<br>电脑        点击数 下单数  支付数</p>
<p>例如，综合排名 = 点击数<em>20%+下单数</em>30%+支付数*50%</p>
<p>本项目需求优化为：<font color ='red' >先按照点击数排名，靠前的就排名高；如果点击数相同，再比较下单数；下单数再相同，就比较支付数。</font></p>
<h3 id="6-1-2-实现方案一"><a href="#6-1-2-实现方案一" class="headerlink" title="6.1.2 实现方案一"></a>6.1.2 实现方案一</h3><h4 id="6-1-2-1-需求分析"><a href="#6-1-2-1-需求分析" class="headerlink" title="6.1.2.1 需求分析"></a>6.1.2.1 需求分析</h4><p>分别统计每个品类点击的次数，下单的次数和支付的次数：<br>（品类，点击总数）（品类，下单总数）（品类，支付总数）</p>
<h4 id="6-1-2-2-需求实现"><a href="#6-1-2-2-需求实现" class="headerlink" title="6.1.2.2 需求实现"></a>6.1.2.2 需求实现</h4><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">top10_1</span> </span>= &#123;</span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">&quot;WordCount&quot;</span>).setMaster(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//读取文件</span></span><br><span class="line">    <span class="keyword">val</span> userVisitRDD: <span class="type">RDD</span>[<span class="type">String</span>] = sparkContext.textFile(<span class="string">&quot;data/user_visit_action.txt&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//统计品类的点击数量</span></span><br><span class="line">    <span class="keyword">val</span> clickCountRDD: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = userVisitRDD.filter(</span><br><span class="line">        line =&gt; &#123;</span><br><span class="line">            (line.split(<span class="string">&quot;_&quot;</span>)(<span class="number">6</span>) != <span class="string">&quot;-1&quot;</span>) &amp;&amp; (line.split(<span class="string">&quot;_&quot;</span>)(<span class="number">7</span>) != <span class="string">&quot;-1&quot;</span>)</span><br><span class="line">        &#125;</span><br><span class="line">    ).map(</span><br><span class="line">        line =&gt; &#123;</span><br><span class="line">            <span class="keyword">val</span> datas: <span class="type">Array</span>[<span class="type">String</span>] = line.split(<span class="string">&quot;_&quot;</span>)</span><br><span class="line">            (datas(<span class="number">6</span>), <span class="number">1</span>)</span><br><span class="line">        &#125;</span><br><span class="line">    ).reduceByKey(_ + _)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//统计品类的下单数量</span></span><br><span class="line">    <span class="keyword">val</span> orderCountRDD: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = userVisitRDD.filter(</span><br><span class="line">        line =&gt; &#123;</span><br><span class="line">            <span class="keyword">val</span> datas = line.split(<span class="string">&quot;_&quot;</span>)</span><br><span class="line">            datas(<span class="number">8</span>) != <span class="string">&quot;null&quot;</span></span><br><span class="line">        &#125;</span><br><span class="line">    ).flatMap(</span><br><span class="line">        line =&gt; &#123;</span><br><span class="line">            <span class="keyword">val</span> datas: <span class="type">Array</span>[<span class="type">String</span>] = line.split(<span class="string">&quot;_&quot;</span>)</span><br><span class="line">            <span class="keyword">val</span> ids: <span class="type">Array</span>[<span class="type">String</span>] = datas(<span class="number">8</span>).split(<span class="string">&quot;,&quot;</span>)</span><br><span class="line">            ids.map((_, <span class="number">1</span>))</span><br><span class="line">        &#125;</span><br><span class="line">    ).reduceByKey(_ + _)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//统计品类的支付数量</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> payCountRDD: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = userVisitRDD.filter(</span><br><span class="line">        line =&gt; &#123;</span><br><span class="line">            <span class="keyword">val</span> datas = line.split(<span class="string">&quot;_&quot;</span>)</span><br><span class="line">            datas(<span class="number">10</span>) != <span class="string">&quot;null&quot;</span></span><br><span class="line">        &#125;</span><br><span class="line">    ).flatMap(</span><br><span class="line">        line =&gt; &#123;</span><br><span class="line">            <span class="keyword">val</span> datas = line.split(<span class="string">&quot;_&quot;</span>)</span><br><span class="line">            <span class="keyword">val</span> ids = datas(<span class="number">10</span>).split(<span class="string">&quot;,&quot;</span>)</span><br><span class="line">            ids.map((_, <span class="number">1</span>))</span><br><span class="line">        &#125;</span><br><span class="line">    ).reduceByKey(_ + _)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> mergeRDD: <span class="type">RDD</span>[(<span class="type">String</span>, (<span class="type">Iterable</span>[<span class="type">Int</span>], <span class="type">Iterable</span>[<span class="type">Int</span>], <span class="type">Iterable</span>[<span class="type">Int</span>]))] = clickCountRDD.cogroup(orderCountRDD, payCountRDD)</span><br><span class="line">    <span class="keyword">val</span> resultRDD: <span class="type">RDD</span>[(<span class="type">String</span>, (<span class="type">Int</span>, <span class="type">Int</span>, <span class="type">Int</span>))] = mergeRDD.mapValues &#123;</span><br><span class="line">        <span class="keyword">case</span> (click, order, pay) =&gt; &#123;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">var</span> clickCnt = <span class="number">0</span></span><br><span class="line">            <span class="keyword">var</span> orderCnt = <span class="number">0</span></span><br><span class="line">            <span class="keyword">var</span> payCnt = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">val</span> iterator1: <span class="type">Iterator</span>[<span class="type">Int</span>] = click.iterator</span><br><span class="line">            <span class="keyword">if</span> (iterator1.hasNext) &#123;</span><br><span class="line">                clickCnt = iterator1.next()</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">val</span> iterator2: <span class="type">Iterator</span>[<span class="type">Int</span>] = order.iterator</span><br><span class="line">            <span class="keyword">if</span> (iterator2.hasNext) &#123;</span><br><span class="line">                orderCnt = iterator2.next()</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">val</span> iterator3: <span class="type">Iterator</span>[<span class="type">Int</span>] = pay.iterator</span><br><span class="line">            <span class="keyword">if</span> (iterator3.hasNext) &#123;</span><br><span class="line">                payCnt = iterator3.next()</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            (clickCnt, orderCnt, payCnt)</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> top10 = resultRDD.sortBy(_._2, <span class="literal">false</span>).take(<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    top10.foreach(println)</span><br><span class="line"></span><br><span class="line">    sparkContext.stop()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="6-1-3-实现方案二"><a href="#6-1-3-实现方案二" class="headerlink" title="6.1.3 实现方案二"></a>6.1.3 实现方案二</h3><h4 id="6-1-3-1-需求分析"><a href="#6-1-3-1-需求分析" class="headerlink" title="6.1.3.1 需求分析"></a>6.1.3.1 需求分析</h4><p>一次性统计每个品类点击的次数，下单的次数和支付的次数：<br>（品类，（点击总数，下单总数，支付总数））</p>
<h4 id="6-1-3-2-需求实现"><a href="#6-1-3-2-需求实现" class="headerlink" title="6.1.3.2 需求实现"></a>6.1.3.2 需求实现</h4><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">//方式一</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">top10_2</span> </span>= &#123;</span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">&quot;WordCount&quot;</span>).setMaster(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//读取文件</span></span><br><span class="line">    <span class="keyword">val</span> userVisitRDD: <span class="type">RDD</span>[<span class="type">String</span>] = sparkContext.textFile(<span class="string">&quot;data/user_visit_action.txt&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//统计品类的点击数量</span></span><br><span class="line">    <span class="keyword">val</span> clickCountRDD: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = userVisitRDD.filter(</span><br><span class="line">        line =&gt; &#123;</span><br><span class="line">            (line.split(<span class="string">&quot;_&quot;</span>)(<span class="number">6</span>) != <span class="string">&quot;-1&quot;</span>) &amp;&amp; (line.split(<span class="string">&quot;_&quot;</span>)(<span class="number">7</span>) != <span class="string">&quot;-1&quot;</span>)</span><br><span class="line">        &#125;</span><br><span class="line">    ).map(</span><br><span class="line">        line =&gt; &#123;</span><br><span class="line">            <span class="keyword">val</span> datas: <span class="type">Array</span>[<span class="type">String</span>] = line.split(<span class="string">&quot;_&quot;</span>)</span><br><span class="line">            (datas(<span class="number">6</span>), <span class="number">1</span>)</span><br><span class="line">        &#125;</span><br><span class="line">    ).reduceByKey(_ + _)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//统计品类的下单数量</span></span><br><span class="line">    <span class="keyword">val</span> orderCountRDD: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = userVisitRDD.filter(</span><br><span class="line">        line =&gt; &#123;</span><br><span class="line">            <span class="keyword">val</span> datas = line.split(<span class="string">&quot;_&quot;</span>)</span><br><span class="line">            datas(<span class="number">8</span>) != <span class="string">&quot;null&quot;</span></span><br><span class="line">        &#125;</span><br><span class="line">    ).flatMap(</span><br><span class="line">        line =&gt; &#123;</span><br><span class="line">            <span class="keyword">val</span> datas: <span class="type">Array</span>[<span class="type">String</span>] = line.split(<span class="string">&quot;_&quot;</span>)</span><br><span class="line">            <span class="keyword">val</span> ids: <span class="type">Array</span>[<span class="type">String</span>] = datas(<span class="number">8</span>).split(<span class="string">&quot;,&quot;</span>)</span><br><span class="line">            ids.map((_, <span class="number">1</span>))</span><br><span class="line">        &#125;</span><br><span class="line">    ).reduceByKey(_ + _)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//统计品类的支付数量</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> payCountRDD: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = userVisitRDD.filter(</span><br><span class="line">        line =&gt; &#123;</span><br><span class="line">            <span class="keyword">val</span> datas = line.split(<span class="string">&quot;_&quot;</span>)</span><br><span class="line">            datas(<span class="number">10</span>) != <span class="string">&quot;null&quot;</span></span><br><span class="line">        &#125;</span><br><span class="line">    ).flatMap(</span><br><span class="line">        line =&gt; &#123;</span><br><span class="line">            <span class="keyword">val</span> datas = line.split(<span class="string">&quot;_&quot;</span>)</span><br><span class="line">            <span class="keyword">val</span> ids = datas(<span class="number">10</span>).split(<span class="string">&quot;,&quot;</span>)</span><br><span class="line">            ids.map((_, <span class="number">1</span>))</span><br><span class="line">        &#125;</span><br><span class="line">    ).reduceByKey(_ + _)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> unionRDD: <span class="type">RDD</span>[(<span class="type">String</span>, (<span class="type">Int</span>, <span class="type">Int</span>, <span class="type">Int</span>))] = clickCountRDD.map &#123;</span><br><span class="line">        <span class="keyword">case</span> (cid, clickCount) =&gt; &#123;</span><br><span class="line">            (cid, (clickCount, <span class="number">0</span>, <span class="number">0</span>))</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;.union(</span><br><span class="line">        orderCountRDD.map &#123;</span><br><span class="line">            <span class="keyword">case</span> (cid, orderCount) =&gt; &#123;</span><br><span class="line">                (cid, (<span class="number">0</span>, orderCount, <span class="number">0</span>))</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    ).union(</span><br><span class="line">        payCountRDD.map &#123;</span><br><span class="line">            <span class="keyword">case</span> (cid, payCount) =&gt; &#123;</span><br><span class="line">                (cid, (<span class="number">0</span>, <span class="number">0</span>, payCount))</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">val</span> top10: <span class="type">Array</span>[(<span class="type">String</span>, (<span class="type">Int</span>, <span class="type">Int</span>, <span class="type">Int</span>))] = unionRDD.reduceByKey(</span><br><span class="line">        (t1, t2) =&gt; &#123;</span><br><span class="line">            (t1._1 + t2._1, t1._2 + t2._2, t1._3 + t2._3)</span><br><span class="line">        &#125;</span><br><span class="line">    ).sortBy(_._2, <span class="literal">false</span>).take(<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    top10.foreach(println)</span><br><span class="line"></span><br><span class="line">    sparkContext.stop()</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//方式二</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">top10_3</span> </span>= &#123;</span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">&quot;WordCount&quot;</span>).setMaster(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">    <span class="comment">//读取文件</span></span><br><span class="line">    <span class="keyword">val</span> userVisitRDD: <span class="type">RDD</span>[<span class="type">String</span>] = sparkContext.textFile(<span class="string">&quot;data/user_visit_action.txt&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> top10: <span class="type">Array</span>[(<span class="type">String</span>, (<span class="type">Int</span>, <span class="type">Int</span>, <span class="type">Int</span>))] = userVisitRDD.flatMap(</span><br><span class="line">        line =&gt; &#123;</span><br><span class="line">            <span class="keyword">val</span> datas = line.split(<span class="string">&quot;_&quot;</span>)</span><br><span class="line">            <span class="keyword">if</span> ((line.split(<span class="string">&quot;_&quot;</span>)(<span class="number">6</span>) != <span class="string">&quot;-1&quot;</span>) &amp;&amp; (line.split(<span class="string">&quot;_&quot;</span>)(<span class="number">7</span>) != <span class="string">&quot;-1&quot;</span>)) &#123;</span><br><span class="line">                <span class="type">List</span>((datas(<span class="number">6</span>), (<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>)))</span><br><span class="line">            &#125; <span class="keyword">else</span> <span class="keyword">if</span> (datas(<span class="number">8</span>) != <span class="string">&quot;null&quot;</span>) &#123;</span><br><span class="line">                <span class="keyword">val</span> ids: <span class="type">Array</span>[<span class="type">String</span>] = datas(<span class="number">8</span>).split(<span class="string">&quot;,&quot;</span>)</span><br><span class="line">                ids.map((_, (<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>)))</span><br><span class="line">            &#125; <span class="keyword">else</span> <span class="keyword">if</span> (datas(<span class="number">10</span>) != <span class="string">&quot;null&quot;</span>) &#123;</span><br><span class="line">                <span class="keyword">val</span> ids: <span class="type">Array</span>[<span class="type">String</span>] = datas(<span class="number">10</span>).split(<span class="string">&quot;,&quot;</span>)</span><br><span class="line">                ids.map((_, (<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>)))</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="type">Nil</span></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    ).reduceByKey(</span><br><span class="line">        (t1, t2) =&gt; &#123;</span><br><span class="line">            (t1._1 + t2._1, t1._2 + t2._2, t1._3 + t2._3)</span><br><span class="line">        &#125;</span><br><span class="line">    ).sortBy(_._2, <span class="literal">false</span>).take(<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    top10.foreach(println)</span><br><span class="line"></span><br><span class="line">    sparkContext.stop()</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="6-1-4-实现方案三"><a href="#6-1-4-实现方案三" class="headerlink" title="6.1.4 实现方案三"></a>6.1.4 实现方案三</h3><h4 id="6-1-4-1-需求分析"><a href="#6-1-4-1-需求分析" class="headerlink" title="6.1.4.1 需求分析"></a>6.1.4.1 需求分析</h4><p>使用累加器的方式聚合数据</p>
<h4 id="6-1-4-2-需求实现"><a href="#6-1-4-2-需求实现" class="headerlink" title="6.1.4.2 需求实现"></a>6.1.4.2 需求实现</h4><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">top10_4</span> </span>= &#123;</span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">&quot;WordCount&quot;</span>).setMaster(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">    <span class="comment">//创建累加器</span></span><br><span class="line">    <span class="keyword">val</span> accumulator = <span class="keyword">new</span> <span class="type">HotCategoryAccumulator</span>()</span><br><span class="line">    <span class="comment">//注册累加器</span></span><br><span class="line">    sparkContext.register(accumulator, <span class="string">&quot;HotCategoryAccumulator&quot;</span>)</span><br><span class="line">    <span class="comment">//读取文件</span></span><br><span class="line">    <span class="keyword">val</span> userVisitRDD: <span class="type">RDD</span>[<span class="type">String</span>] = sparkContext.textFile(<span class="string">&quot;data/user_visit_action.txt&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//遍历数据</span></span><br><span class="line">    userVisitRDD.foreach(</span><br><span class="line">        line =&gt; &#123;</span><br><span class="line">            <span class="keyword">val</span> datas = line.split(<span class="string">&quot;_&quot;</span>)</span><br><span class="line">            <span class="keyword">if</span> ((datas(<span class="number">6</span>) != <span class="string">&quot;-1&quot;</span>) &amp;&amp; (datas(<span class="number">7</span>) != <span class="string">&quot;-1&quot;</span>)) &#123;</span><br><span class="line">                accumulator.add(datas(<span class="number">6</span>), <span class="string">&quot;click&quot;</span>)</span><br><span class="line">            &#125; <span class="keyword">else</span> <span class="keyword">if</span> (datas(<span class="number">8</span>) != <span class="string">&quot;null&quot;</span>) &#123;</span><br><span class="line">                <span class="keyword">val</span> ids: <span class="type">Array</span>[<span class="type">String</span>] = datas(<span class="number">8</span>).split(<span class="string">&quot;,&quot;</span>)</span><br><span class="line">                ids.foreach(</span><br><span class="line">                    id =&gt; &#123;</span><br><span class="line">                        accumulator.add(id, <span class="string">&quot;order&quot;</span>)</span><br><span class="line">                    &#125;</span><br><span class="line">                )</span><br><span class="line">            &#125; <span class="keyword">else</span> <span class="keyword">if</span> (datas(<span class="number">10</span>) != <span class="string">&quot;null&quot;</span>) &#123;</span><br><span class="line">                <span class="keyword">val</span> ids: <span class="type">Array</span>[<span class="type">String</span>] = datas(<span class="number">10</span>).split(<span class="string">&quot;,&quot;</span>)</span><br><span class="line">                ids.foreach(</span><br><span class="line">                    id =&gt; &#123;</span><br><span class="line">                        accumulator.add(id, <span class="string">&quot;pay&quot;</span>)</span><br><span class="line">                    &#125;</span><br><span class="line">                )</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="type">Nil</span></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    )</span><br><span class="line">    <span class="comment">//获取累加器的值</span></span><br><span class="line">    <span class="keyword">val</span> accResult: mutable.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">HotCategory</span>] = accumulator.value</span><br><span class="line">    <span class="keyword">val</span> top10: <span class="type">List</span>[<span class="type">HotCategory</span>] = accResult.toList.map(_._2).sortBy(</span><br><span class="line">        obj =&gt; &#123;</span><br><span class="line">            (obj.clickCnt, obj.orderCnt, obj.payCnt)</span><br><span class="line">        &#125;</span><br><span class="line">    )(<span class="type">Ordering</span>.<span class="type">Tuple3</span>(<span class="type">Ordering</span>.<span class="type">Int</span>.reverse, <span class="type">Ordering</span>.<span class="type">Int</span>.reverse, <span class="type">Ordering</span>.<span class="type">Int</span>.reverse)).take(<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    top10.foreach(println)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    sparkContext.stop()</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//累加器</span></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">HotCategory</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="class">                          var cid: <span class="type">String</span>,</span></span></span><br><span class="line"><span class="params"><span class="class">                          var clickCnt: <span class="type">Int</span> = 0,</span></span></span><br><span class="line"><span class="params"><span class="class">                          var orderCnt: <span class="type">Int</span> = 0,</span></span></span><br><span class="line"><span class="params"><span class="class">                          var payCnt: <span class="type">Int</span> = 0</span></span></span><br><span class="line"><span class="params"><span class="class">                      </span>)</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">HotCategoryAccumulator</span> <span class="keyword">extends</span> <span class="title">AccumulatorV2</span>[(<span class="type">String</span>, <span class="type">String</span>), mutable.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">HotCategory</span>]] </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">val</span> hcMap = mutable.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">HotCategory</span>]()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">isZero</span></span>: <span class="type">Boolean</span> = &#123;</span><br><span class="line">        hcMap.isEmpty</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">copy</span></span>(): <span class="type">AccumulatorV2</span>[(<span class="type">String</span>, <span class="type">String</span>), mutable.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">HotCategory</span>]] = &#123;</span><br><span class="line">        <span class="keyword">new</span> <span class="type">HotCategoryAccumulator</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">reset</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">        hcMap.clear()</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">add</span></span>(v: (<span class="type">String</span>, <span class="type">String</span>)): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> (cid, actionType) = v</span><br><span class="line">        <span class="keyword">val</span> hotCategory: <span class="type">HotCategory</span> = hcMap.getOrElse(cid, <span class="type">HotCategory</span>(cid))</span><br><span class="line">        actionType <span class="keyword">match</span> &#123;</span><br><span class="line">            <span class="keyword">case</span> <span class="string">&quot;click&quot;</span> =&gt; &#123;</span><br><span class="line">                hotCategory.clickCnt += <span class="number">1</span></span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">case</span> <span class="string">&quot;order&quot;</span> =&gt; &#123;</span><br><span class="line">                hotCategory.orderCnt += <span class="number">1</span></span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">case</span> <span class="string">&quot;pay&quot;</span> =&gt; &#123;</span><br><span class="line">                hotCategory.payCnt += <span class="number">1</span></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        hcMap.update(cid, hotCategory)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">merge</span></span>(other: <span class="type">AccumulatorV2</span>[(<span class="type">String</span>, <span class="type">String</span>), mutable.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">HotCategory</span>]]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> otherMap: mutable.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">HotCategory</span>] = other.value</span><br><span class="line">        otherMap.foreach &#123;</span><br><span class="line">            <span class="keyword">case</span> (cid, accumulator) =&gt; &#123;</span><br><span class="line">                <span class="keyword">val</span> hotCategory: <span class="type">HotCategory</span> = hcMap.getOrElse(cid, <span class="type">HotCategory</span>(cid))</span><br><span class="line">                hotCategory.clickCnt += accumulator.clickCnt</span><br><span class="line">                hotCategory.orderCnt += accumulator.orderCnt</span><br><span class="line">                hotCategory.payCnt += accumulator.payCnt</span><br><span class="line">                hcMap.update(cid, hotCategory)</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">value</span></span>: mutable.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">HotCategory</span>] = &#123;</span><br><span class="line">        hcMap</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="6-2-需求2：Top10热门品类中每个品类的Top10活跃Session统计"><a href="#6-2-需求2：Top10热门品类中每个品类的Top10活跃Session统计" class="headerlink" title="6.2 需求2：Top10热门品类中每个品类的Top10活跃Session统计"></a>6.2 需求2：Top10热门品类中每个品类的Top10活跃Session统计</h2><h3 id="6-2-1-需求说明"><a href="#6-2-1-需求说明" class="headerlink" title="6.2.1 需求说明"></a>6.2.1 需求说明</h3><p>在需求一的基础上，增加每个品类用户session的点击统计</p>
<h3 id="6-2-2-需求分析"><a href="#6-2-2-需求分析" class="headerlink" title="6.2.2 需求分析"></a>6.2.2 需求分析</h3><ol>
<li>按照需求一的逻辑获取Top10品类id</li>
<li>过滤Top10品类相关数据</li>
<li>结构转换line =&gt; ((品类, session), 1) =&gt; ((品类, session), sum)</li>
<li>结构转换(品类, (session, sum)) =&gt; (品类, Iter[(session1, sum), (session2, sum), (session3, sum)])</li>
<li>将分组后的数据进行排序(降序)，取 Top10</li>
</ol>
<h3 id="6-2-3-功能实现"><a href="#6-2-3-功能实现" class="headerlink" title="6.2.3 功能实现"></a>6.2.3 功能实现</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.rdd.<span class="type">RDD</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">HotCategoryTop10Analysis_feature2</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">&quot;WordCount&quot;</span>).setMaster(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">        <span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">        <span class="comment">//读取文件</span></span><br><span class="line">        <span class="keyword">val</span> userVisitRDD: <span class="type">RDD</span>[<span class="type">String</span>] = sparkContext.textFile(<span class="string">&quot;data/user_visit_action.txt&quot;</span>)</span><br><span class="line"></span><br><span class="line">        userVisitRDD.cache()</span><br><span class="line"></span><br><span class="line">        <span class="comment">//需求1</span></span><br><span class="line">        <span class="keyword">val</span> top10: <span class="type">Array</span>[(<span class="type">String</span>, (<span class="type">Int</span>, <span class="type">Int</span>, <span class="type">Int</span>))] = userVisitRDD.flatMap(</span><br><span class="line">            line =&gt; &#123;</span><br><span class="line">                <span class="keyword">val</span> datas: <span class="type">Array</span>[<span class="type">String</span>] = line.split(<span class="string">&quot;_&quot;</span>)</span><br><span class="line">                <span class="keyword">if</span> ((line.split(<span class="string">&quot;_&quot;</span>)(<span class="number">6</span>) != <span class="string">&quot;-1&quot;</span>) &amp;&amp; (line.split(<span class="string">&quot;_&quot;</span>)(<span class="number">7</span>) != <span class="string">&quot;-1&quot;</span>)) &#123;</span><br><span class="line">                    <span class="type">List</span>((datas(<span class="number">6</span>), (<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>)))</span><br><span class="line">                &#125; <span class="keyword">else</span> <span class="keyword">if</span> (datas(<span class="number">8</span>) != <span class="string">&quot;null&quot;</span>) &#123;</span><br><span class="line">                    <span class="keyword">val</span> ids: <span class="type">Array</span>[<span class="type">String</span>] = datas(<span class="number">8</span>).split(<span class="string">&quot;,&quot;</span>)</span><br><span class="line">                    ids.map((_, (<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>)))</span><br><span class="line">                &#125; <span class="keyword">else</span> <span class="keyword">if</span> (datas(<span class="number">10</span>) != <span class="string">&quot;null&quot;</span>) &#123;</span><br><span class="line">                    <span class="keyword">val</span> ids: <span class="type">Array</span>[<span class="type">String</span>] = datas(<span class="number">10</span>).split(<span class="string">&quot;,&quot;</span>)</span><br><span class="line">                    ids.map((_, (<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>)))</span><br><span class="line">                &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                    <span class="type">Nil</span></span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        ).reduceByKey(</span><br><span class="line">            (t1, t2) =&gt; &#123;</span><br><span class="line">                (t1._1 + t2._1, t1._2 + t2._2, t1._3 + t2._3)</span><br><span class="line">            &#125;</span><br><span class="line">        ).sortBy(_._2, <span class="literal">false</span>).take(<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">//top10.foreach(println)</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> top10Ids: <span class="type">Array</span>[<span class="type">String</span>] = top10.map(_._1)</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1. 将数据进行筛选过滤，只保留 Top10 的热门品类的数据</span></span><br><span class="line">        <span class="comment">// 1.1 将数据转换为样例类</span></span><br><span class="line">        <span class="keyword">val</span> actionRDD: <span class="type">RDD</span>[<span class="type">UserVisitAction</span>] = userVisitRDD.map(</span><br><span class="line">            line =&gt; &#123;</span><br><span class="line">                <span class="keyword">val</span> datas: <span class="type">Array</span>[<span class="type">String</span>] = line.split(<span class="string">&quot;_&quot;</span>)</span><br><span class="line">                <span class="type">UserVisitAction</span>(</span><br><span class="line">                    datas(<span class="number">0</span>),</span><br><span class="line">                    datas(<span class="number">1</span>).toLong,</span><br><span class="line">                    datas(<span class="number">2</span>),</span><br><span class="line">                    datas(<span class="number">3</span>).toLong,</span><br><span class="line">                    datas(<span class="number">4</span>),</span><br><span class="line">                    datas(<span class="number">5</span>),</span><br><span class="line">                    datas(<span class="number">6</span>).toLong,</span><br><span class="line">                    datas(<span class="number">7</span>).toLong,</span><br><span class="line">                    datas(<span class="number">8</span>),</span><br><span class="line">                    datas(<span class="number">9</span>),</span><br><span class="line">                    datas(<span class="number">10</span>),</span><br><span class="line">                    datas(<span class="number">11</span>),</span><br><span class="line">                    datas(<span class="number">12</span>).toLong)</span><br><span class="line">            &#125;</span><br><span class="line">        )</span><br><span class="line">        <span class="comment">// 1.2 将数据进行过滤</span></span><br><span class="line">        <span class="keyword">val</span> filterRDD: <span class="type">RDD</span>[<span class="type">UserVisitAction</span>] = actionRDD.filter(</span><br><span class="line">            action =&gt; &#123;</span><br><span class="line">                <span class="keyword">if</span> (action.click_category_id != <span class="number">-1</span>) &#123;</span><br><span class="line">                    top10Ids.contains(action.click_category_id.toString)</span><br><span class="line">                &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                    <span class="literal">false</span></span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        )</span><br><span class="line">        <span class="comment">// 2. 将数据进行结构的转换</span></span><br><span class="line">        <span class="comment">//    line =&gt; ((品类, session), 1)</span></span><br><span class="line">        <span class="comment">// 3. 将转换结构后的数据进行统计</span></span><br><span class="line">        <span class="comment">//    line =&gt; ((品类, session), 1) =&gt; ((品类, session), sum)</span></span><br><span class="line">        <span class="keyword">val</span> reduceByKeyRDD: <span class="type">RDD</span>[((<span class="type">Long</span>, <span class="type">String</span>), <span class="type">Int</span>)] = filterRDD.map(</span><br><span class="line">            action =&gt; &#123;</span><br><span class="line">                ((action.click_category_id, action.session_id), <span class="number">1</span>)</span><br><span class="line">            &#125;</span><br><span class="line">        ).reduceByKey(_ + _)</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 4. 将统计结果进行结构的转换</span></span><br><span class="line">        <span class="comment">// ((品类, session), sum) =&gt; (品类, (session, sum))</span></span><br><span class="line">        <span class="comment">// 5. 将转换结构后的数据按照品类进行分组</span></span><br><span class="line">        <span class="comment">// (品类, (session, sum)) =&gt; (品类, Iter[(session1, sum), (session2, sum), (session3, sum)])</span></span><br><span class="line">        <span class="keyword">val</span> groupRDD: <span class="type">RDD</span>[(<span class="type">Long</span>, <span class="type">Iterable</span>[(<span class="type">String</span>, <span class="type">Int</span>)])] = reduceByKeyRDD.map &#123;</span><br><span class="line">            <span class="keyword">case</span> ((cid, sessionid), sum) =&gt; (cid, (sessionid, sum))</span><br><span class="line">        &#125;.groupByKey()</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 6. 将分组后的数据进行排序(降序)，取 Top10</span></span><br><span class="line">        <span class="keyword">val</span> result: <span class="type">RDD</span>[(<span class="type">Long</span>, <span class="type">List</span>[(<span class="type">String</span>, <span class="type">Int</span>)])] = groupRDD.mapValues(</span><br><span class="line">            iter =&gt; &#123;</span><br><span class="line">                iter.toList.sortBy(_._2)(<span class="type">Ordering</span>.<span class="type">Int</span>.reverse).take(<span class="number">10</span>)</span><br><span class="line">            &#125;</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 7. 将结果采集后打印在控制台</span></span><br><span class="line">        result.collect().sortBy(_._1).foreach(println)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        sparkContext.stop()</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//用户访问动作表</span></span><br><span class="line">    <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">UserVisitAction</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="class">                                  date: <span class="type">String</span>, //用户点击行为的日期</span></span></span><br><span class="line"><span class="params"><span class="class">                                  user_id: <span class="type">Long</span>, //用户的<span class="type">ID</span></span></span></span><br><span class="line"><span class="params"><span class="class">                                  session_id: <span class="type">String</span>, //<span class="type">Session</span>的<span class="type">ID</span></span></span></span><br><span class="line"><span class="params"><span class="class">                                  page_id: <span class="type">Long</span>, //某个页面的<span class="type">ID</span></span></span></span><br><span class="line"><span class="params"><span class="class">                                  action_time: <span class="type">String</span>, //动作的时间点</span></span></span><br><span class="line"><span class="params"><span class="class">                                  search_keyword: <span class="type">String</span>, //用户搜索的关键词</span></span></span><br><span class="line"><span class="params"><span class="class">                                  click_category_id: <span class="type">Long</span>, //某一个商品品类的<span class="type">ID</span></span></span></span><br><span class="line"><span class="params"><span class="class">                                  click_product_id: <span class="type">Long</span>, //某一个商品的<span class="type">ID</span></span></span></span><br><span class="line"><span class="params"><span class="class">                                  order_category_ids: <span class="type">String</span>, //一次订单中所有品类的<span class="type">ID</span>集合</span></span></span><br><span class="line"><span class="params"><span class="class">                                  order_product_ids: <span class="type">String</span>, //一次订单中所有商品的<span class="type">ID</span>集合</span></span></span><br><span class="line"><span class="params"><span class="class">                                  pay_category_ids: <span class="type">String</span>, //一次支付中所有品类的<span class="type">ID</span>集合</span></span></span><br><span class="line"><span class="params"><span class="class">                                  pay_product_ids: <span class="type">String</span>, //一次支付中所有商品的<span class="type">ID</span>集合</span></span></span><br><span class="line"><span class="params"><span class="class">                                  city_id: <span class="type">Long</span> //城市 id</span></span></span><br><span class="line"><span class="params"><span class="class">                              </span>)</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="6-3-需求3：页面单跳转换率统计"><a href="#6-3-需求3：页面单跳转换率统计" class="headerlink" title="6.3 需求3：页面单跳转换率统计"></a>6.3 需求3：页面单跳转换率统计</h2><h3 id="6-3-1-需求说明"><a href="#6-3-1-需求说明" class="headerlink" title="6.3.1 需求说明"></a>6.3.1 需求说明</h3><ol>
<li><p>页面单跳转化率<br> 计算页面单跳转化率，什么是页面单跳转换率，比如一个用户在一次 Session 过程中访问的页面路径 3,5,7,9,10,21，那么页面 3 跳到页面 5 叫一次单跳，7-9 也叫一次单跳，那么单跳转化率就是要统计页面点击的概率。<br> 比如：计算 3-5 的单跳转化率，先获取符合条件的 Session 对于页面 3 的访问次数（PV）为 A，然后获取符合条件的 Session 中访问了页面 3 又紧接着访问了页面 5 的次数为 B，那么 B/A 就是 3-5 的页面单跳转化率。</p>
</li>
<li><p>统计页面单跳转化率意义<br> 产品经理和运营总监，可以根据这个指标，去尝试分析，整个网站，产品，各个页面的表现怎么样，是不是需要去优化产品的布局；吸引用户最终可以进入最后的支付页面。<br> 数据分析师，可以此数据做更深一步的计算和分析。<br> 企业管理层，可以看到整个公司的网站，各个页面的之间的跳转的表现如何，可以适当调整公司的经营战略或策略。</p>
</li>
</ol>
<h3 id="6-3-2-需求分析"><a href="#6-3-2-需求分析" class="headerlink" title="6.3.2 需求分析"></a>6.3.2 需求分析</h3><ol>
<li>计算页面累计点击数量</li>
<li>根据session对访问数据分组</li>
<li>排序后出去所有单跳页面的组合</li>
<li>对组合执行wordCount统计</li>
<li>计算</li>
</ol>
<h3 id="6-3-3-功能实现"><a href="#6-3-3-功能实现" class="headerlink" title="6.3.3 功能实现"></a>6.3.3 功能实现</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.rdd.<span class="type">RDD</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">HotCategoryTop10Analysis_feature3</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">&quot;WordCount&quot;</span>).setMaster(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">        <span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">        <span class="comment">//读取文件</span></span><br><span class="line">        <span class="keyword">val</span> userVisitRDD: <span class="type">RDD</span>[<span class="type">String</span>] = sparkContext.textFile(<span class="string">&quot;data/user_visit_action.txt&quot;</span>)</span><br><span class="line"></span><br><span class="line">        userVisitRDD.cache()</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1. 将数据进行筛选过滤，只保留 Top10 的热门品类的数据</span></span><br><span class="line">        <span class="comment">// 1.1 将数据转换为样例类</span></span><br><span class="line">        <span class="keyword">val</span> actionRDD: <span class="type">RDD</span>[<span class="type">UserVisitAction</span>] = userVisitRDD.map(</span><br><span class="line">            line =&gt; &#123;</span><br><span class="line">                <span class="keyword">val</span> datas: <span class="type">Array</span>[<span class="type">String</span>] = line.split(<span class="string">&quot;_&quot;</span>)</span><br><span class="line">                <span class="type">UserVisitAction</span>(</span><br><span class="line">                    datas(<span class="number">0</span>),</span><br><span class="line">                    datas(<span class="number">1</span>).toLong,</span><br><span class="line">                    datas(<span class="number">2</span>),</span><br><span class="line">                    datas(<span class="number">3</span>).toLong,</span><br><span class="line">                    datas(<span class="number">4</span>),</span><br><span class="line">                    datas(<span class="number">5</span>),</span><br><span class="line">                    datas(<span class="number">6</span>).toLong,</span><br><span class="line">                    datas(<span class="number">7</span>).toLong,</span><br><span class="line">                    datas(<span class="number">8</span>),</span><br><span class="line">                    datas(<span class="number">9</span>),</span><br><span class="line">                    datas(<span class="number">10</span>),</span><br><span class="line">                    datas(<span class="number">11</span>),</span><br><span class="line">                    datas(<span class="number">12</span>).toLong)</span><br><span class="line">            &#125;</span><br><span class="line">        )</span><br><span class="line">        actionRDD.cache()</span><br><span class="line">        <span class="keyword">val</span> pageIdDistinct: <span class="type">Array</span>[<span class="type">Long</span>] = actionRDD.map(_.page_id).distinct().collect()</span><br><span class="line"></span><br><span class="line">        <span class="comment">//0. 计算分母(word(页面),count(数量))</span></span><br><span class="line">        <span class="keyword">val</span> pageCount: <span class="type">Array</span>[(<span class="type">Long</span>, <span class="type">Int</span>)] = actionRDD</span><br><span class="line">            .filter(</span><br><span class="line">                action =&gt; &#123;</span><br><span class="line">                    pageIdDistinct.contains(action.page_id)</span><br><span class="line">                &#125;</span><br><span class="line">            )</span><br><span class="line">            .map(</span><br><span class="line">                action =&gt; (action.page_id, <span class="number">1</span>)</span><br><span class="line">            ).reduceByKey(_ + _).collect()</span><br><span class="line">        <span class="keyword">val</span> pageCountMap: <span class="type">Map</span>[<span class="type">Long</span>, <span class="type">Int</span>] = pageCount.toMap</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">//1. 按照Session分组</span></span><br><span class="line">        <span class="keyword">val</span> sessionGroupRDD: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Iterable</span>[<span class="type">UserVisitAction</span>])] = actionRDD.groupBy(_.session_id)</span><br><span class="line">        <span class="comment">//2. 按时间排序</span></span><br><span class="line">        <span class="keyword">val</span> mapRDD: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">List</span>[(<span class="type">Long</span>, <span class="type">Long</span>)])] = sessionGroupRDD.mapValues(</span><br><span class="line">            iter =&gt; &#123;</span><br><span class="line">                <span class="keyword">val</span> sortAction: <span class="type">List</span>[<span class="type">UserVisitAction</span>] = iter.toList.sortBy(_.action_time)</span><br><span class="line">                <span class="keyword">val</span> pageIdList: <span class="type">List</span>[<span class="type">Long</span>] = sortAction.map(_.page_id)</span><br><span class="line">                <span class="comment">//val sliding: Iterator[List[Long]] = pageIdList.sliding(2)</span></span><br><span class="line">                <span class="comment">//sliding.map(</span></span><br><span class="line">                <span class="comment">//    list=&gt;&#123;</span></span><br><span class="line">                <span class="comment">//        list(0)+&quot;_&quot;+list(1)</span></span><br><span class="line">                <span class="comment">//    &#125;</span></span><br><span class="line">                <span class="comment">//)</span></span><br><span class="line">                <span class="keyword">val</span> zipList: <span class="type">List</span>[(<span class="type">Long</span>, <span class="type">Long</span>)] = pageIdList.zip(pageIdList.tail)</span><br><span class="line">                zipList</span><br><span class="line">            &#125;</span><br><span class="line">        )</span><br><span class="line">        <span class="comment">//3. 计算分子</span></span><br><span class="line">        <span class="comment">//  转换数据格式</span></span><br><span class="line">        <span class="comment">//  转换数据结构 3,5 =&gt; 3-5</span></span><br><span class="line">        <span class="comment">//  将连续页面跳转统计数量</span></span><br><span class="line">        <span class="keyword">val</span> pageListRDD: <span class="type">RDD</span>[<span class="type">List</span>[(<span class="type">Long</span>, <span class="type">Long</span>)]] = mapRDD.map(_._2)</span><br><span class="line">        <span class="keyword">val</span> pageRedirect: <span class="type">RDD</span>[(<span class="type">Long</span>, <span class="type">Long</span>)] = pageListRDD.flatMap(list =&gt; list)</span><br><span class="line">        <span class="keyword">val</span> pageRedirectCount: <span class="type">RDD</span>[((<span class="type">Long</span>, <span class="type">Long</span>), <span class="type">Int</span>)] = pageRedirect.map((_, <span class="number">1</span>)).reduceByKey(_ + _)</span><br><span class="line">        <span class="keyword">val</span> pageCountResult: <span class="type">Array</span>[((<span class="type">Long</span>, <span class="type">Long</span>), <span class="type">Int</span>)] = pageRedirectCount.collect()</span><br><span class="line"></span><br><span class="line">        <span class="comment">//4 计算</span></span><br><span class="line">        pageCountResult.foreach&#123;</span><br><span class="line">            <span class="keyword">case</span> ((page1,page2), count) =&gt;&#123;</span><br><span class="line">                println(<span class="string">s&quot;页面id:<span class="subst">$&#123;page1&#125;</span>-&gt;<span class="subst">$&#123;page2&#125;</span> 单跳转换率:&quot;</span>+(count.toDouble/pageCountMap.getOrElse(page1,<span class="number">0</span>)))</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        sparkContext.stop()</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//用户访问动作表</span></span><br><span class="line">    <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">UserVisitAction</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="class">          date: <span class="type">String</span>, //用户点击行为的日期</span></span></span><br><span class="line"><span class="params"><span class="class">          user_id: <span class="type">Long</span>, //用户的<span class="type">ID</span></span></span></span><br><span class="line"><span class="params"><span class="class">          session_id: <span class="type">String</span>, //<span class="type">Session</span>的<span class="type">ID</span></span></span></span><br><span class="line"><span class="params"><span class="class">          page_id: <span class="type">Long</span>, //某个页面的<span class="type">ID</span></span></span></span><br><span class="line"><span class="params"><span class="class">          action_time: <span class="type">String</span>, //动作的时间点</span></span></span><br><span class="line"><span class="params"><span class="class">          search_keyword: <span class="type">String</span>, //用户搜索的关键词</span></span></span><br><span class="line"><span class="params"><span class="class">          click_category_id: <span class="type">Long</span>, //某一个商品品类的<span class="type">ID</span></span></span></span><br><span class="line"><span class="params"><span class="class">          click_product_id: <span class="type">Long</span>, //某一个商品的<span class="type">ID</span></span></span></span><br><span class="line"><span class="params"><span class="class">          order_category_ids: <span class="type">String</span>, //一次订单中所有品类的<span class="type">ID</span>集合</span></span></span><br><span class="line"><span class="params"><span class="class">          order_product_ids: <span class="type">String</span>, //一次订单中所有商品的<span class="type">ID</span>集合</span></span></span><br><span class="line"><span class="params"><span class="class">          pay_category_ids: <span class="type">String</span>, //一次支付中所有品类的<span class="type">ID</span>集合</span></span></span><br><span class="line"><span class="params"><span class="class">          pay_product_ids: <span class="type">String</span>, //一次支付中所有商品的<span class="type">ID</span>集合</span></span></span><br><span class="line"><span class="params"><span class="class">          city_id: <span class="type">Long</span> //城市 id</span></span></span><br><span class="line"><span class="params"><span class="class">      </span>)</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h1 id="第7章-Spark-Core-源码解析"><a href="#第7章-Spark-Core-源码解析" class="headerlink" title="第7章 Spark Core 源码解析"></a>第7章 Spark Core 源码解析</h1><p><img src="https://s2.loli.net/2022/01/22/E5xmZGhjOsIqJ4M.png" alt="源码图解"></p>
<h2 id="7-1-提交任务源码"><a href="#7-1-提交任务源码" class="headerlink" title="7.1 提交任务源码"></a>7.1 提交任务源码</h2><ul>
<li>提交任务入口  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">bin/spark-submit \</span><br><span class="line">--class org.apache.spark.examples.SparkPi \</span><br><span class="line">--master <span class="built_in">local</span>[2] \</span><br><span class="line">./examples/jars/spark-examples_2.12-3.0.0.jar \</span><br><span class="line">10</span><br></pre></td></tr></table></figure>
<ul>
<li>跟进spark-submit脚本  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">。。。</span><br><span class="line">。。。</span><br><span class="line"><span class="built_in">exec</span> <span class="string">&quot;<span class="variable">$&#123;SPARK_HOME&#125;</span>&quot;</span>/bin/spark-class org.apache.spark.deploy.SparkSubmit <span class="string">&quot;<span class="variable">$@</span>&quot;</span></span><br></pre></td></tr></table></figure>
<ul>
<li>关注 类名为<code>org.apache.spark.deploy.SparkSubmit</code>  </li>
</ul>
</li>
<li>跟进spark-class脚本  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">。。。</span><br><span class="line">。。。</span><br><span class="line">CMD=(<span class="string">&quot;<span class="variable">$&#123;CMD[@]:0:$LAST&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;<span class="variable">$&#123;CMD[@]&#125;</span>&quot;</span></span><br><span class="line"><span class="built_in">exec</span> <span class="string">&quot;<span class="variable">$&#123;CMD[@]&#125;</span>&quot;</span></span><br><span class="line">。。。</span><br><span class="line">。。。</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<ul>
<li>添加echo打印执行命令为  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="variable">$JAVA_HOME</span>/bin/java -cp <span class="variable">$SPARK_HOME</span>/conf/:/Users/zhenan/app/spark-3.0.0-bin-hadoop3.2/jars/* -Xmx1g org.apache.spark.deploy.SparkSubmit --master <span class="built_in">local</span>[2] --class org.apache.spark.examples.SparkPi ./examples/jars/spark-examples_2.12-3.0.0.jar 10</span><br></pre></td></tr></table></figure></li>
<li>确定执行类为<code>org.apache.spark.deploy.SparkSubmit</code>  </li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="7-1-1-SparkSubmit源码"><a href="#7-1-1-SparkSubmit源码" class="headerlink" title="7.1.1 SparkSubmit源码"></a>7.1.1 SparkSubmit源码</h3><ul>
<li>跟进<code>org.apache.spark.deploy.SparkSubmit</code>  </li>
<li>进入伴生对象的main方法<code>org.apache.spark.deploy.SparkSubmit#main</code>  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> submit = <span class="keyword">new</span> <span class="type">SparkSubmit</span>() &#123;</span><br><span class="line">    self =&gt;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">override</span> <span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">parseArguments</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">SparkSubmitArguments</span> = &#123;</span><br><span class="line">      <span class="keyword">new</span> <span class="type">SparkSubmitArguments</span>(args) &#123;</span><br><span class="line">        <span class="keyword">override</span> <span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">logInfo</span></span>(msg: =&gt; <span class="type">String</span>): <span class="type">Unit</span> = self.logInfo(msg)</span><br><span class="line">    </span><br><span class="line">        <span class="keyword">override</span> <span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">logWarning</span></span>(msg: =&gt; <span class="type">String</span>): <span class="type">Unit</span> = self.logWarning(msg)</span><br><span class="line">    </span><br><span class="line">        <span class="keyword">override</span> <span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">logError</span></span>(msg: =&gt; <span class="type">String</span>): <span class="type">Unit</span> = self.logError(msg)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">override</span> <span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">logInfo</span></span>(msg: =&gt; <span class="type">String</span>): <span class="type">Unit</span> = printMessage(msg)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">override</span> <span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">logWarning</span></span>(msg: =&gt; <span class="type">String</span>): <span class="type">Unit</span> = printMessage(<span class="string">s&quot;Warning: <span class="subst">$msg</span>&quot;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">override</span> <span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">logError</span></span>(msg: =&gt; <span class="type">String</span>): <span class="type">Unit</span> = printMessage(<span class="string">s&quot;Error: <span class="subst">$msg</span>&quot;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">doSubmit</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="keyword">super</span>.doSubmit(args)</span><br><span class="line">      &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">        <span class="keyword">case</span> e: <span class="type">SparkUserAppException</span> =&gt;</span><br><span class="line">          exitFn(e.exitCode)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">  &#125;</span><br><span class="line">    </span><br><span class="line">  submit.doSubmit(args)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>跟进<code>super.doSubmit(args)</code> </li>
<li>-&gt; 解析参数：<code>val appArgs = parseArguments(args)</code> <ul>
<li>-&gt; <code>new SparkSubmitArguments(args)</code></li>
<li>-&gt; <code>parse(args.asJava)</code>  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Set parameters from command line arguments</span></span><br><span class="line">parse(args.asJava)</span><br></pre></td></tr></table></figure></li>
<li>-&gt; 参数校验：由具体实现类提供逻辑  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> (!handle(name, value)) &#123;</span><br><span class="line">  <span class="keyword">break</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
  <img src="https://s2.loli.net/2022/01/22/UOB4kK3TzWZSvtp.jpg"><ul>
<li>-&gt;实现类: <code>org.apache.spark.deploy.SparkSubmitArguments#handle</code></li>
<li>获取到   <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">--master  =&gt; master    =&gt; yarn</span><br><span class="line">--class   =&gt; mainClass =&gt; SparkPI(WordCount)</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>
</li>
<li>-&gt; 执行action动作  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">appArgs.action <span class="keyword">match</span> &#123;</span><br><span class="line">  <span class="keyword">case</span> <span class="type">SparkSubmitAction</span>.<span class="type">SUBMIT</span> =&gt; submit(appArgs, uninitLog)</span><br><span class="line">  <span class="keyword">case</span> <span class="type">SparkSubmitAction</span>.<span class="type">KILL</span> =&gt; kill(appArgs)</span><br><span class="line">  <span class="keyword">case</span> <span class="type">SparkSubmitAction</span>.<span class="type">REQUEST_STATUS</span> =&gt; requestStatus(appArgs)</span><br><span class="line">  <span class="keyword">case</span> <span class="type">SparkSubmitAction</span>.<span class="type">PRINT_VERSION</span> =&gt; printVersion()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>-&gt; 跟进appArgs.action赋值  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Action should be SUBMIT unless otherwise specified</span></span><br><span class="line">action = <span class="type">Option</span>(action).getOrElse(<span class="type">SUBMIT</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>默认值为<code>SUBMIT</code></li>
</ul>
</li>
<li>-&gt; 跟进任务提交方法<code>case SparkSubmitAction.SUBMIT =&gt; submit(appArgs, uninitLog)</code><ul>
<li>-&gt; <code>doRunMain</code></li>
<li>-&gt; <code>runMain</code><ul>
<li>-&gt; 准备提交环境的参数，跟进<code>val (childArgs, childClasspath, sparkConf, childMainClass) = prepareSubmitEnvironment(args)</code><ul>
<li>-&gt; 跟进childMainClass取值,查找赋值的位置</li>
<li>-&gt; <code>val isYarnCluster = clusterManager == YARN &amp;&amp; deployMode == CLUSTER</code></li>
<li>-&gt; 确定类型childMainClass  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> (isYarnCluster) &#123;</span><br><span class="line">  childMainClass = <span class="type">YARN_CLUSTER_SUBMIT_CLASS</span></span><br><span class="line">  。。。</span><br><span class="line">  。。。</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>-&gt; <code>private[deploy] val YARN_CLUSTER_SUBMIT_CLASS = &quot;org.apache.spark.deploy.yarn.YarnClusterApplication&quot;</code></li>
</ul>
</li>
<li>-&gt; 添加依赖jar  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> loader = getSubmitClassLoader(sparkConf)</span><br><span class="line"><span class="keyword">for</span> (jar &lt;- childClasspath) &#123;</span><br><span class="line">  addJarToClasspath(jar, loader)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>-&gt; <code>mainClass = Utils.classForName(childMainClass)</code></li>
<li>-&gt; 判断类型，创建Spark应用程序，<font color ='red' >SparkApplication</font>  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> app: <span class="type">SparkApplication</span> = <span class="keyword">if</span> (classOf[<span class="type">SparkApplication</span>].isAssignableFrom(mainClass)) &#123;</span><br><span class="line">  mainClass.getConstructor().newInstance().asInstanceOf[<span class="type">SparkApplication</span>]</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">  <span class="keyword">new</span> <span class="type">JavaMainApplication</span>(mainClass)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>-&gt; 启动应用程序<code>app.start</code>,具体逻辑由childMainClass实现，Yarn模式为<code>org.apache.spark.deploy.yarn.YarnClusterApplication</code></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="7-1-2-YarnClusterApplication-源码"><a href="#7-1-2-YarnClusterApplication-源码" class="headerlink" title="7.1.2 YarnClusterApplication 源码"></a>7.1.2 YarnClusterApplication 源码</h3><ul>
<li>跟进YarnClusterApplication  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span>[spark] <span class="class"><span class="keyword">class</span> <span class="title">YarnClusterApplication</span> <span class="keyword">extends</span> <span class="title">SparkApplication</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">start</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>], conf: <span class="type">SparkConf</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// SparkSubmit would use yarn cache to distribute files &amp; jars in yarn mode,</span></span><br><span class="line">    <span class="comment">// so remove them from sparkConf here for yarn mode.</span></span><br><span class="line">    conf.remove(<span class="type">JARS</span>)</span><br><span class="line">    conf.remove(<span class="type">FILES</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">new</span> <span class="type">Client</span>(<span class="keyword">new</span> <span class="type">ClientArguments</span>(args), conf, <span class="literal">null</span>).run()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>-&gt; 定位start方法  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">new</span> <span class="type">Client</span>(<span class="keyword">new</span> <span class="type">ClientArguments</span>(args), conf, <span class="literal">null</span>).run()</span><br></pre></td></tr></table></figure>
<ul>
<li>-&gt; 跟进ClientArguments：<code>--class =&gt; userClass =&gt; SparkPi(WordCount)</code>  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">case</span> (<span class="string">&quot;--class&quot;</span>) :: value :: tail =&gt;</span><br><span class="line"> userClass = value</span><br><span class="line"> args = tail</span><br></pre></td></tr></table></figure></li>
<li>-&gt; 跟进<code>new Client()</code><ul>
<li>-&gt;创建YarnClient  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">val</span> yarnClient = <span class="type">YarnClient</span>.createYarnClient</span><br></pre></td></tr></table></figure>
<ul>
<li>-&gt; 跟进 <code>createYarnClient</code>  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">public static <span class="type">YarnClient</span> createYarnClient() &#123;</span><br><span class="line">  <span class="type">YarnClient</span> client = <span class="keyword">new</span> <span class="type">YarnClientImpl</span>();</span><br><span class="line">  <span class="keyword">return</span> client;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>-&gt; 查找rmClient<code>org.apache.hadoop.yarn.client.api.impl.YarnClientImpl#rmClient</code>  <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">protected</span> ApplicationClientProtocol rmClient;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>
</li>
<li>-&gt; 跟进<code>org.apache.spark.deploy.yarn.Client#run</code><ul>
<li>-&gt; 跟进<code>org.apache.spark.deploy.yarn.Client#submitApplication</code><ul>
<li>创建Yran客户端,建立连接<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">submitApplication</span></span>(): <span class="type">ApplicationId</span> = &#123;</span><br><span class="line"><span class="type">ResourceRequestHelper</span>.validateResources(sparkConf)</span><br><span class="line">  。。。</span><br><span class="line">  。。。</span><br><span class="line">  launcherBackend.connect()</span><br><span class="line">  yarnClient.init(hadoopConf)</span><br><span class="line">  yarnClient.start()</span><br><span class="line">  。。。</span><br><span class="line">  。。。</span><br><span class="line">  <span class="comment">// Set up the appropriate contexts to launch our AM</span></span><br><span class="line">  <span class="keyword">val</span> containerContext = createContainerLaunchContext(newAppResponse)</span><br><span class="line">  <span class="keyword">val</span> appContext = createApplicationSubmissionContext(newApp, containerContext)</span><br><span class="line">  。。。</span><br><span class="line">  。。。</span><br><span class="line">  logInfo(<span class="string">s&quot;Submitting application <span class="subst">$appId</span> to ResourceManager&quot;</span>)</span><br><span class="line">  yarnClient.submitApplication(appContext)</span><br><span class="line">  。。。</span><br><span class="line">  。。。</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>-&gt; 跟进 <code>createContainerLaunchContext</code> <code>Set up a ContainerLaunchContext to launch our ApplicationMaster container. This sets up the launch environment, java options, and the command for launching the AM.</code><ul>
<li>-&gt; java虚拟机配置  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">javaOpts += <span class="string">&quot;-Xmx&quot;</span> + amMemory + <span class="string">&quot;m&quot;</span></span><br></pre></td></tr></table></figure></li>
<li>-&gt; 垃圾回收  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> (useConcurrentAndIncrementalGC) &#123;</span><br><span class="line">  <span class="comment">// In our expts, using (default) throughput collector has severe perf ramifications in</span></span><br><span class="line">  <span class="comment">// multi-tenant machines</span></span><br><span class="line">  javaOpts += <span class="string">&quot;-XX:+UseConcMarkSweepGC&quot;</span></span><br><span class="line">  javaOpts += <span class="string">&quot;-XX:MaxTenuringThreshold=31&quot;</span></span><br><span class="line">  javaOpts += <span class="string">&quot;-XX:SurvivorRatio=8&quot;</span></span><br><span class="line">  javaOpts += <span class="string">&quot;-XX:+CMSIncrementalMode&quot;</span></span><br><span class="line">  javaOpts += <span class="string">&quot;-XX:+CMSIncrementalPacing&quot;</span></span><br><span class="line">  javaOpts += <span class="string">&quot;-XX:CMSIncrementalDutyCycleMin=0&quot;</span></span><br><span class="line">  javaOpts += <span class="string">&quot;-XX:CMSIncrementalDutyCycle=10&quot;</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>-&gt; 拼接容器运行命令<ul>
<li>-&gt; 确定amClass  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> amClass =</span><br><span class="line">  <span class="keyword">if</span> (isClusterMode) &#123;</span><br><span class="line">    <span class="type">Utils</span>.classForName(<span class="string">&quot;org.apache.spark.deploy.yarn.ApplicationMaster&quot;</span>).getName</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="type">Utils</span>.classForName(<span class="string">&quot;org.apache.spark.deploy.yarn.ExecutorLauncher&quot;</span>).getName</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>Cluster:org.apache.spark.deploy.yarn.ApplicationMaster</li>
<li>Client:org.apache.spark.deploy.yarn.ExecutorLauncher</li>
</ul>
</li>
<li>-&gt; 拼接amArgs  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> amArgs =</span><br><span class="line">  <span class="type">Seq</span>(amClass) ++ userClass ++ userJar ++ primaryPyFile ++ primaryRFile ++ userArgs ++</span><br><span class="line">  <span class="type">Seq</span>(<span class="string">&quot;--properties-file&quot;</span>,</span><br><span class="line">    buildPath(<span class="type">Environment</span>.<span class="type">PWD</span>.$$(), <span class="type">LOCALIZED_CONF_DIR</span>, <span class="type">SPARK_CONF_FILE</span>)) ++</span><br><span class="line">  <span class="type">Seq</span>(<span class="string">&quot;--dist-cache-conf&quot;</span>,</span><br><span class="line">    buildPath(<span class="type">Environment</span>.<span class="type">PWD</span>.$$(), <span class="type">LOCALIZED_CONF_DIR</span>, <span class="type">DIST_CACHE_CONF_FILE</span>))</span><br></pre></td></tr></table></figure></li>
<li>-&gt; 拼接Command 最终结果为<code>bin/java org.apache.spark.deploy.yarn.ApplicationMaster</code><ul>
<li>此时启动的是AM(<code>ApplicationMaster</code>),位于NodeManager<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Command for the ApplicationMaster</span></span><br><span class="line"><span class="keyword">val</span> commands = prefixEnv ++</span><br><span class="line">  <span class="type">Seq</span>(<span class="type">Environment</span>.<span class="type">JAVA_HOME</span>.$$() + <span class="string">&quot;/bin/java&quot;</span>, <span class="string">&quot;-server&quot;</span>) ++</span><br><span class="line">  javaOpts ++ amArgs ++</span><br><span class="line">  <span class="type">Seq</span>(</span><br><span class="line">    <span class="string">&quot;1&gt;&quot;</span>, <span class="type">ApplicationConstants</span>.<span class="type">LOG_DIR_EXPANSION_VAR</span> + <span class="string">&quot;/stdout&quot;</span>,</span><br><span class="line">    <span class="string">&quot;2&gt;&quot;</span>, <span class="type">ApplicationConstants</span>.<span class="type">LOG_DIR_EXPANSION_VAR</span> + <span class="string">&quot;/stderr&quot;</span>)</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="7-1-3-ApplicationMaster-源码"><a href="#7-1-3-ApplicationMaster-源码" class="headerlink" title="7.1.3 ApplicationMaster 源码"></a>7.1.3 ApplicationMaster 源码</h3><ul>
<li>跟进伴生对象<code>org.apache.spark.deploy.yarn.ApplicationMaster</code></li>
<li>定位main方法<ul>
<li>-&gt; 跟进参数封装 <code>val amArgs = new ApplicationMasterArguments(args)</code><ul>
<li>-&gt; 定位<code>parseArgs</code></li>
<li>-&gt; <code>--class =&gt; userClass =&gt; SparkPi/WordCount</code>  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">case</span> (<span class="string">&quot;--class&quot;</span>) :: value :: tail =&gt;</span><br><span class="line">  userClass = value</span><br><span class="line">  args = tail</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>-&gt; 跟进创建ApplicationMaster  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">master = <span class="keyword">new</span> <span class="type">ApplicationMaster</span>(amArgs, sparkConf, yarnConf)</span><br></pre></td></tr></table></figure>
<ul>
<li>-&gt; 创建YarnRMClient  <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span> val client = <span class="keyword">new</span> YarnRMClient()</span><br></pre></td></tr></table></figure></li>
<li>-&gt; 定位 run方法<code>org.apache.spark.deploy.yarn.ApplicationMaster#run</code>  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">。。。</span><br><span class="line">。。。</span><br><span class="line"><span class="keyword">if</span> (isClusterMode) &#123;</span><br><span class="line">    runDriver()</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    runExecutorLauncher()</span><br><span class="line">&#125;</span><br><span class="line">。。。</span><br><span class="line">。。。</span><br></pre></td></tr></table></figure>
<ul>
<li>Cluster:runDriver</li>
<li>Client:runExecutorLauncher</li>
</ul>
</li>
<li>-&gt;跟进 <code>org.apache.spark.deploy.yarn.ApplicationMaster#runDriver</code></li>
<li>-&gt; 定位<code>userClassThread = startUserApplication()</code></li>
<li>-&gt; 跟进 <code>startUserApplication</code></li>
<li>-&gt; 获取main方法(–class =&gt; userClass =&gt; SparkPi/WordCount 的main)  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> mainMethod = userClassLoader.loadClass(args.userClass)</span><br><span class="line">    .getMethod(<span class="string">&quot;main&quot;</span>, classOf[<span class="type">Array</span>[<span class="type">String</span>]])</span><br></pre></td></tr></table></figure></li>
<li>启动线程执行main(<font color ='red' >启动Driver</font>)  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> userThread = <span class="keyword">new</span> <span class="type">Thread</span> &#123;</span><br><span class="line">      <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">          。。。</span><br><span class="line">          。。。  </span><br><span class="line">          mainMethod.invoke(<span class="literal">null</span>, userArgs.toArray)</span><br><span class="line">          。。。</span><br><span class="line">          。。。</span><br><span class="line">      &#125;</span><br><span class="line">    userThread.setContextClassLoader(userClassLoader)</span><br><span class="line">    userThread.setName(<span class="string">&quot;Driver&quot;</span>)</span><br><span class="line">    userThread.start()</span><br><span class="line">    userThread</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>此处启动的线程就是Driver</li>
<li>run方法调动的内容为应用程序的main方法(SparkPi/WordCount 的main)</li>
</ul>
</li>
<li>-&gt; ApplicationMaster中<font color ='red' >资源线程阻塞等待</font>  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> sc = <span class="type">ThreadUtils</span>.awaitResult(sparkContextPromise.future,</span><br><span class="line"><span class="type">Duration</span>(totalWaitTime, <span class="type">TimeUnit</span>.<span class="type">MILLISECONDS</span>))</span><br></pre></td></tr></table></figure></li>
<li>-&gt; 计算线程初始化SparkContext完成后退出阻塞状态继续执行<ul>
<li><code>mainMethod.invoke(null, userArgs.toArray)</code>启动SparkPi/WordCount 的main<ul>
<li>new SparkContext()</li>
<li><code>org.apache.spark.scheduler.cluster.YarnClusterScheduler#postStartHook</code>  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">postStartHook</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="type">ApplicationMaster</span>.sparkContextInitialized(sc)</span><br><span class="line">  <span class="keyword">super</span>.postStartHook()</span><br><span class="line">  logInfo(<span class="string">&quot;YarnClusterScheduler.postStartHook done&quot;</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li><font color ='red' >计算线程进入阻塞状态</font>  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">sparkContextInitialized</span></span>(sc: <span class="type">SparkContext</span>) = &#123;</span><br><span class="line">  sparkContextPromise.synchronized &#123;</span><br><span class="line">    <span class="comment">// Notify runDriver function that SparkContext is available</span></span><br><span class="line">    sparkContextPromise.success(sc)</span><br><span class="line">    <span class="comment">// Pause the user class thread in order to make proper initialization in runDriver function.</span></span><br><span class="line">     sparkContextPromise.wait()</span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>此时资源线程条件满足继续执行</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>-&gt; 注册AM 定位 <code>registerAM(host, port, userConf, sc.ui.map(_.webUrl), appAttemptId)</code><ul>
<li>由Client反馈ResourceManager，注册AM的信息</li>
</ul>
</li>
<li>-&gt; 跟进<code>createAllocator(driverRef, userConf, rpcEnv, appAttemptId, distCacheConf)</code><ul>
<li>申请计算资源   <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">。。。</span><br><span class="line">。。。</span><br><span class="line">。。。</span><br><span class="line">allocator = client.createAllocator(</span><br><span class="line">yarnConf,</span><br><span class="line">_sparkConf,</span><br><span class="line">appAttemptId,</span><br><span class="line">driverUrl,</span><br><span class="line">driverRef,</span><br><span class="line">securityMgr,</span><br><span class="line">localResources)</span><br><span class="line">。。。</span><br><span class="line">。。。</span><br><span class="line">。。。</span><br></pre></td></tr></table></figure></li>
<li>分配资源   <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">allocator.allocateResources()</span><br></pre></td></tr></table></figure>
<ul>
<li>获取运行容器  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> allocatedContainers = allocateResponse.getAllocatedContainers()</span><br></pre></td></tr></table></figure></li>
<li>处理容器 跟进<code>handleAllocatedContainers(allocatedContainers.asScala)</code></li>
<li>分配容器  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Assign remaining that are neither node-local nor rack-local</span></span><br><span class="line"><span class="keyword">val</span> remainingAfterOffRackMatches = <span class="keyword">new</span> <span class="type">ArrayBuffer</span>[<span class="type">Container</span>]</span><br><span class="line"><span class="keyword">for</span> (allocatedContainer &lt;- remainingAfterRackMatches) &#123;</span><br><span class="line">  matchContainerToRequest(allocatedContainer, <span class="type">ANY_HOST</span>, containersToUse,</span><br><span class="line">    remainingAfterOffRackMatches)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>启动容器 跟进  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">runAllocatedContainers(containersToUse)</span><br></pre></td></tr></table></figure></li>
<li><font color ='red' >启动Executors</font>  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">    <span class="keyword">new</span> <span class="type">ExecutorRunnable</span>(</span><br><span class="line">    <span class="type">Some</span>(container),</span><br><span class="line">    conf,</span><br><span class="line">    sparkConf,</span><br><span class="line">    driverUrl,</span><br><span class="line">    executorId,</span><br><span class="line">    executorHostname,</span><br><span class="line">    executorMemory,</span><br><span class="line">    executorCores,</span><br><span class="line">    appAttemptId.getApplicationId.toString,</span><br><span class="line">    securityMgr,</span><br><span class="line">    localResources,</span><br><span class="line">    <span class="type">ResourceProfile</span>.<span class="type">DEFAULT_RESOURCE_PROFILE_ID</span> <span class="comment">// use until fully supported</span></span><br><span class="line">  ).run()</span><br><span class="line">    ```  </span><br><span class="line">- -&gt; 跟进 run方法 &lt;font color =<span class="symbol">&#x27;re</span>d&#x27; &gt;nmClient启动&lt;/font&gt;</span><br><span class="line">    - `org.apache.spark.deploy.yarn.<span class="type">ExecutorRunnable</span>#run`</span><br><span class="line">    - -&gt; nmClient启动容器 `org.apache.spark.deploy.yarn.<span class="type">ExecutorRunnable</span>#startContainer` </span><br><span class="line">        ```scala</span><br><span class="line">        <span class="comment">// Send the start request to the ContainerManager</span></span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">          nmClient.startContainer(container.get, ctx)</span><br><span class="line">        &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">          <span class="keyword">case</span> ex: <span class="type">Exception</span> =&gt;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">SparkException</span>(<span class="string">s&quot;Exception while starting container <span class="subst">$&#123;container.get.getId&#125;</span>&quot;</span> +</span><br><span class="line">              <span class="string">s&quot; on host <span class="subst">$hostname</span>&quot;</span>, ex)</span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>-&gt; 准备运行命令<code>org.apache.spark.deploy.yarn.ExecutorRunnable#prepareCommand</code></li>
<li>确定执行命令  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">  <span class="keyword">val</span> commands = prefixEnv ++</span><br><span class="line"><span class="type">Seq</span>(<span class="type">Environment</span>.<span class="type">JAVA_HOME</span>.$$() + <span class="string">&quot;/bin/java&quot;</span>, <span class="string">&quot;-server&quot;</span>) ++</span><br><span class="line">javaOpts ++</span><br><span class="line"><span class="type">Seq</span>(<span class="string">&quot;org.apache.spark.executor.YarnCoarseGrainedExecutorBackend&quot;</span>,</span><br><span class="line">  <span class="string">&quot;--driver-url&quot;</span>, masterAddress,</span><br><span class="line">  <span class="string">&quot;--executor-id&quot;</span>, executorId,</span><br><span class="line">  <span class="string">&quot;--hostname&quot;</span>, hostname,</span><br><span class="line">  <span class="string">&quot;--cores&quot;</span>, executorCores.toString,</span><br><span class="line">  <span class="string">&quot;--app-id&quot;</span>, appId,</span><br><span class="line">  <span class="string">&quot;--resourceProfileId&quot;</span>, resourceProfileId.toString) ++</span><br><span class="line">userClassPath ++</span><br><span class="line"><span class="type">Seq</span>(</span><br><span class="line">  <span class="string">s&quot;1&gt;<span class="subst">$&#123;ApplicationConstants.LOG_DIR_EXPANSION_VAR&#125;</span>/stdout&quot;</span>,</span><br><span class="line">  <span class="string">s&quot;2&gt;<span class="subst">$&#123;ApplicationConstants.LOG_DIR_EXPANSION_VAR&#125;</span>/stderr&quot;</span>)</span><br></pre></td></tr></table></figure></li>
<li><font color ='red' >容器执行类</font><code>org.apache.spark.executor.YarnCoarseGrainedExecutorBackend</code></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>Driver中mainMethod即为用户程序的main方法入口，workCount的main方法<ul>
<li>-&gt; 跟进<code>new SparkContext(conf)</code></li>
<li>-&gt; 定位<code>Post init</code>  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">_taskScheduler.postStartHook()</span><br></pre></td></tr></table></figure></li>
<li>-&gt; 跟进 <code>org.apache.spark.scheduler.cluster.YarnClusterScheduler#postStartHook</code></li>
<li>-&gt; 跟进 (注意：这里的方法为半生对象的静态方法)<code>org.apache.spark.deploy.yarn.ApplicationMaster#sparkContextInitialized</code></li>
<li>-&gt; 跟进 (注意：这里的方法为类的成员方法)<code>org.apache.spark.deploy.yarn.ApplicationMaster#sparkContextInitialized</code>  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">  <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">sparkContextInitialized</span></span>(sc: <span class="type">SparkContext</span>) = &#123;</span><br><span class="line">      sparkContextPromise.synchronized &#123;</span><br><span class="line">      <span class="comment">// Notify runDriver function that SparkContext is available</span></span><br><span class="line">      sparkContextPromise.success(sc)</span><br><span class="line">      <span class="comment">// Pause the user class thread in order to make proper initialization in runDriver function.</span></span><br><span class="line">      sparkContextPromise.wait()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>通知runDriver方法SparkContext可用，退出阻塞状态</li>
<li>当前计算线程进入阻塞状态，Driver执行准备工作</li>
</ul>
</li>
</ul>
</li>
<li> 资源申请结束 恢复Driver <code>resumeDriver()</code></li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="7-1-4-ExecutorBackend-源码"><a href="#7-1-4-ExecutorBackend-源码" class="headerlink" title="7.1.4 ExecutorBackend 源码"></a>7.1.4 ExecutorBackend 源码</h3><ul>
<li>入口<code>org.apache.spark.executor.YarnCoarseGrainedExecutorBackend</code></li>
<li>伴生对象的main方法<code>org.apache.spark.executor.YarnCoarseGrainedExecutorBackend#main</code></li>
<li>跟进run方法  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="type">CoarseGrainedExecutorBackend</span>.run(backendArgs, createFn)</span><br></pre></td></tr></table></figure></li>
<li>定位 Executor  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">env.rpcEnv.setupEndpoint(<span class="string">&quot;Executor&quot;</span>,</span><br><span class="line">    backendCreateFn(env.rpcEnv, arguments, env, cfg.resourceProfile))</span><br></pre></td></tr></table></figure>
<ul>
<li>Backend：后台</li>
<li>Endpoint：终端</li>
<li>RpcEnv：rpc通信环境</li>
<li>RpcEndpoint：rpc通信终端  <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">The life-cycle of an endpoint is:</span><br><span class="line">constructor -&gt; onStart -&gt; receive* -&gt; onStop</span><br></pre></td></tr></table></figure>
<ul>
<li>YarnCoarseGrainedExecutorBackend父类 -&gt; CoarseGrainedExecutorBackend  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onStart</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">  logInfo(<span class="string">&quot;Connecting to driver: &quot;</span> + driverUrl)</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    _resources = parseOrFindResources(resourcesFileOpt)</span><br><span class="line">  &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="type">NonFatal</span>(e) =&gt;</span><br><span class="line">      exitExecutor(<span class="number">1</span>, <span class="string">&quot;Unable to create executor due to &quot;</span> + e.getMessage, e)</span><br><span class="line">  &#125;</span><br><span class="line">  rpcEnv.asyncSetupEndpointRefByURI(driverUrl).flatMap &#123; ref =&gt;</span><br><span class="line">    <span class="comment">// This is a very fast action so we can use &quot;ThreadUtils.sameThread&quot;</span></span><br><span class="line">    driver = <span class="type">Some</span>(ref)</span><br><span class="line">    ref.ask[<span class="type">Boolean</span>](<span class="type">RegisterExecutor</span>(executorId, self, hostname, cores, extractLogUrls,</span><br><span class="line">      extractAttributes, _resources, resourceProfile.id))</span><br><span class="line">  &#125;(<span class="type">ThreadUtils</span>.sameThread).onComplete &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="type">Success</span>(_) =&gt;</span><br><span class="line">      self.send(<span class="type">RegisteredExecutor</span>)</span><br><span class="line">    <span class="keyword">case</span> <span class="type">Failure</span>(e) =&gt;</span><br><span class="line">      exitExecutor(<span class="number">1</span>, <span class="string">s&quot;Cannot register with driver: <span class="subst">$driverUrl</span>&quot;</span>, e, notifyDriver = <span class="literal">false</span>)</span><br><span class="line">  &#125;(<span class="type">ThreadUtils</span>.sameThread)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>Backend和Driver通讯<code>rpcEnv.asyncSetupEndpointRefByURI</code><ul>
<li>注册Executor  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">ref.ask[<span class="type">Boolean</span>](<span class="type">RegisterExecutor</span>(executorId, self, hostname, cores, extractLogUrls,</span><br><span class="line">    extractAttributes, _resources, resourceProfile.id))</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>Driver应答<code>org.apache.spark.SparkContext#_schedulerBackend</code><ul>
<li><code>org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend.DriverEndpoint#receiveAndReply</code>  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">  listenerBus.post(</span><br><span class="line">  <span class="type">SparkListenerExecutorAdded</span>(<span class="type">System</span>.currentTimeMillis(), executorId, data))</span><br><span class="line"><span class="comment">// Note: some tests expect the reply to come after we put the executor in the map</span></span><br><span class="line">context.reply(<span class="literal">true</span>)</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Spark</tag>
        <tag>SparkCore</tag>
      </tags>
  </entry>
  <entry>
    <title>Scala入门</title>
    <url>/2021/12/04/Scala%E5%85%A5%E9%97%A8/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="Scala"><a href="#Scala" class="headerlink" title="Scala"></a>Scala</h1><h1 id="第1章-Scala入门"><a href="#第1章-Scala入门" class="headerlink" title="第1章 Scala入门"></a>第1章 Scala入门</h1><h2 id="1-1-概述"><a href="#1-1-概述" class="headerlink" title="1.1 概述"></a>1.1 概述</h2><p><img src="https://s2.loli.net/2021/12/04/snWwTjmo2i7ZCFt.jpg"></p>
<h3 id="1-1-1-什么是Scala"><a href="#1-1-1-什么是Scala" class="headerlink" title="1.1.1 什么是Scala"></a>1.1.1 什么是Scala</h3><ul>
<li>从英文的角度来讲，Scala并不是一个单词，而是Scalable Language两个单词的缩写，表示可伸缩语言的意思。从计算机的角度来讲，Scala是一门完整的软件编程语言，那么连在一起就表示Scala是一门可伸缩的软件编程语言。之所以说它是可伸缩，是因为这门语言体现了面向对象，函数式编程等多种不同的语言范式，且融合了不同语言新的特性。</li>
<li>Scala编程语言是由联邦理工学院洛桑（EPFL）的Martin Odersky于2001年基于Funnel的工作开始设计并开发的。由于Martin Odersky之前的工作是开发通用Java和Javac（Sun公司的Java编译器），所以基于Java平台的Scala语言于2003年底/2004年初发布。</li>
<li>截至到2020年8月，Scala最新版本为2.13.3，支持JVM和JavaScript</li>
<li>Scala官网：<a href="https://www.scala-lang.org/">https://www.scala-lang.org/</a></li>
</ul>
<h3 id="1-1-2-为什么学习Scala"><a href="#1-1-2-为什么学习Scala" class="headerlink" title="1.1.2 为什么学习Scala"></a>1.1.2 为什么学习Scala</h3><p>在之前的学习中，我们已经学习了很长时间的Java语言，为什么此时要学习一门新的语言呢？主要基于以下几个原因：</p>
<ol>
<li>大数据主要的批处理计算引擎框架Spark是基于Scala语言开发的</li>
<li>大数据主要的流式计算引擎框架Flink也提供了Scala相应的API</li>
<li>大数据领域中函数式编程的开发效率更高，更直观，更容易理解</li>
</ol>
<h3 id="1-1-3-Java-and-Scala"><a href="#1-1-3-Java-and-Scala" class="headerlink" title="1.1.3 Java and Scala"></a>1.1.3 Java and Scala</h3><p>Martin Odersky是狂热的编译器爱好者，长时间的编程后，希望开发一种语言，能够让写程序的过程变得简单，高效，所以当接触到Java语言后，感受到了这门语言的魅力，决定将函数式编程语言的特性融合到Java语言中，由此产生了2门语言（Pizza &amp; Scala）,这两种语言极大地推动了Java语言的发展</p>
<ul>
<li>JDK1.5的泛型，增强for循环，自动类型转换等都是从Pizza语言引入的新特性</li>
<li>JDK1.8的类型推断，λ（lambda）表达式是从Scala语言引入的新特性</li>
</ul>
<p>由上可知，Scala语言是基于Java开发的，所以其编译后的文件也是字节码文件，并可以运行在JVM中。</p>
<h2 id="1-2-快速上手"><a href="#1-2-快速上手" class="headerlink" title="1.2 快速上手"></a>1.2 快速上手</h2><h3 id="1-2-1-Scala环境安装"><a href="#1-2-1-Scala环境安装" class="headerlink" title="1.2.1 Scala环境安装"></a>1.2.1 Scala环境安装</h3><ol>
<li>安装JDK 1.8（略）</li>
<li>安装Scala2.12<ol>
<li>下载scala-2.12.11.zip</li>
<li>配置环境变量 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">#SCALA_HOME</span></span><br><span class="line"><span class="built_in">export</span> SCALA_HOME=/usr/<span class="built_in">local</span>/etc/scala-2.11.12</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$SCALA_HOME</span>/bin</span><br></pre></td></tr></table></figure></li>
<li>环境测试 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">➜  ~ scala</span><br><span class="line">Welcome to Scala 2.11.12 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_291).</span><br><span class="line">Type <span class="keyword">in</span> expressions <span class="keyword">for</span> evaluation. Or try :<span class="built_in">help</span>.</span><br><span class="line"></span><br><span class="line">scala&gt; 1+1</span><br><span class="line">res0: Int = 2</span><br><span class="line"></span><br><span class="line">scala&gt;</span><br></pre></td></tr></table></figure></li>
</ol>
</li>
</ol>
<h3 id="1-2-2-Scala插件安装"><a href="#1-2-2-Scala插件安装" class="headerlink" title="1.2.2 Scala插件安装"></a>1.2.2 Scala插件安装</h3><p>默认情况下IDEA不支持Scala的开发，需要安装Scala插件。<br><img src="https://s2.loli.net/2021/12/04/2pyYvDoeamgQH4w.jpg"></p>
<h3 id="1-2-3-Hello-Scala案例"><a href="#1-2-3-Hello-Scala案例" class="headerlink" title="1.2.3 Hello Scala案例"></a>1.2.3 Hello Scala案例</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.bigdata.scala</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">HelloScala</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="type">System</span>.out.println(<span class="string">&quot;Hello Scala&quot;</span>)</span><br><span class="line">       println(<span class="string">&quot;Hello Scala&quot;</span>)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ol>
<li>代码解析<ul>
<li>object</li>
<li>def</li>
<li>args : Array[String]</li>
<li>Unit</li>
<li>System.out.println</li>
<li>println</li>
</ul>
</li>
</ol>
<p>如果只是通过代码来进行语法的解析，并不能了解其真正的实现原理。scala语言是基于Java语言开发的，所以也会编译为class文件，那么我们可以通过反编译指令javap</p>
<ul>
<li>字节码：javap -c -l 类名  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">➜  scala git:(master) ✗ javap -c -l HelloScala\$ </span><br><span class="line">警告: 二进制文件HelloScala$包含com.bigdata.scala.HelloScala$</span><br><span class="line">Compiled from <span class="string">&quot;HelloScala.scala&quot;</span></span><br><span class="line">public final class com.bigdata.scala.HelloScala$ &#123;</span><br><span class="line">  public static final com.bigdata.scala.HelloScala$ MODULE$;</span><br><span class="line"></span><br><span class="line">  public static &#123;&#125;;</span><br><span class="line">    Code:</span><br><span class="line">       0: new           <span class="comment">#2                  // class com/bigdata/scala/HelloScala$</span></span><br><span class="line">       3: invokespecial <span class="comment">#12                 // Method &quot;&lt;init&gt;&quot;:()V</span></span><br><span class="line">       6: <span class="built_in">return</span></span><br><span class="line"></span><br><span class="line">  public void main(java.lang.String[]);</span><br><span class="line">    Code:</span><br><span class="line">       0: getstatic     <span class="comment">#20                 // Field java/lang/System.out:Ljava/io/PrintStream;</span></span><br><span class="line">       3: ldc           <span class="comment">#22                 // String Hello Scala</span></span><br><span class="line">       5: invokevirtual <span class="comment">#28                 // Method java/io/PrintStream.println:(Ljava/lang/String;)V</span></span><br><span class="line">       8: getstatic     <span class="comment">#33                 // Field scala/Predef$.MODULE$:Lscala/Predef$;</span></span><br><span class="line">      11: ldc           <span class="comment">#22                 // String Hello Scala</span></span><br><span class="line">      13: invokevirtual <span class="comment">#36                 // Method scala/Predef$.println:(Ljava/lang/Object;)V</span></span><br><span class="line">      16: <span class="built_in">return</span></span><br><span class="line">    LineNumberTable:</span><br><span class="line">      line 5: 0</span><br><span class="line">      line 6: 8</span><br><span class="line">    LocalVariableTable:</span><br><span class="line">      Start  Length  Slot  Name   Signature</span><br><span class="line">          0      17     0  this   Lcom/bigdata/scala/HelloScala$;</span><br><span class="line">          0      17     1  args   [Ljava/lang/String;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
<li>反编译  <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//decompiled from HelloScala$.class</span></span><br><span class="line"><span class="keyword">package</span> com.bigdata.scala;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> scala.Predef.;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">HelloScala</span>$ </span>&#123;</span><br><span class="line">   <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> HelloScala$ MODULE$;</span><br><span class="line"></span><br><span class="line">   <span class="keyword">static</span> &#123;</span><br><span class="line">      <span class="keyword">new</span> HelloScala$();</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">      System.out.println(<span class="string">&quot;Hello Scala&quot;</span>);</span><br><span class="line">      .MODULE$.println(<span class="string">&quot;Hello Scala&quot;</span>);</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line">   <span class="keyword">private</span> HelloScala$() &#123;</span><br><span class="line">      MODULE$ = <span class="keyword">this</span>;</span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//decompiled from HelloScala.class</span></span><br><span class="line"><span class="keyword">package</span> com.bigdata.scala;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> scala.reflect.ScalaSignature;</span><br><span class="line"></span><br><span class="line"><span class="meta">@ScalaSignature(</span></span><br><span class="line"><span class="meta">   bytes = &quot;\u0006\u0001!:Q!\u0001\u0002\t\u0002%\t!\u0002S3mY&gt;\u001c6-\u00197b\u0015\t\u0019A!A\u0003tG\u0006d\u0017M\u0003\u0002\u0006\r\u00059!-[4eCR\f&#x27;\&quot;A\u0004\u0002\u0007\r|Wn\u0001\u0001\u0011\u0005)</span>YQ\<span class="string">&quot;\u0001\u0002\u0007\u000b1\u0011\u0001\u0012A\u0007\u0003\u0015!+G\u000e\\8TG\u0006d\u0017m\u0005\u0002\f\u001dA\u0011q\&quot;E\u0007\u0002!)\t1!\u0003\u0002\u0013!\t1\u0011I\\=SK\u001aDQ\u0001F\u0006\u0005\u0002U\ta\u0001P5oSRtD#A\u0005\t\u000b]YA\u0011\u0001\r\u0002\t5\f\u0017N\u001c\u000b\u00033q\u0001\&quot;a\u0004\u000e\n\u0005m\u0001\&quot;\u0001B+oSRDQ!\b\fA\u0002y\tA!\u0019:hgB\u0019qbH\u0011\n\u0005\u0001\u0002\&quot;!B!se\u0006L\bC\u0001\u0012&amp;\u001d\ty1%\u0003\u0002%!\u00051\u0001K]3eK\u001aL!AJ\u0014\u0003\rM#(/\u001b8h\u0015\t!\u0003\u0003&quot;</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">HelloScala</span> </span>&#123;</span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] var0)</span> </span>&#123;</span><br><span class="line">      HelloScala$.MODULE$.main(var0);</span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul>
<h1 id="第2章-变量和数据类型"><a href="#第2章-变量和数据类型" class="headerlink" title="第2章 变量和数据类型"></a>第2章 变量和数据类型</h1><h2 id="2-1-注释"><a href="#2-1-注释" class="headerlink" title="2.1 注释"></a>2.1 注释</h2><p>Scala注释使用和Java完全一样。注释是一个程序员必须要具有的良好编程习惯。将自己的思想通过注释先整理出来，再用代码去体现。</p>
<h3 id="2-1-1-单行注释"><a href="#2-1-1-单行注释" class="headerlink" title="2.1.1 单行注释"></a>2.1.1 单行注释</h3><pre><code><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.atguigu.bigdata.scala</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaComment</span></span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="comment">// 单行注释</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</code></pre>
<h3 id="2-1-2-多行注释"><a href="#2-1-2-多行注释" class="headerlink" title="2.1.2 多行注释"></a>2.1.2 多行注释</h3><pre><code><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.atguigu.bigdata.scala</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaComment</span></span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">           多行注释</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</code></pre>
<h3 id="2-1-3-文档注释"><a href="#2-1-3-文档注释" class="headerlink" title="2.1.3 文档注释"></a>2.1.3 文档注释</h3><pre><code><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">package com.atguigu.bigdata.scala</span><br><span class="line">/**</span><br><span class="line">  * doc注释</span><br><span class="line">  */</span><br><span class="line">object ScalaComment&#123;</span><br><span class="line">    def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</code></pre>
<h2 id="2-2-变量"><a href="#2-2-变量" class="headerlink" title="2.2 变量"></a>2.2 变量</h2><p>变量是一种使用方便的占位符，用于引用计算机内存地址，变量创建后会占用一定的内存空间。基于变量的数据类型，操作系统会进行内存分配并且决定什么将被储存在保留内存中。因此，通过给变量分配不同的数据类型，你可以在这些变量中存储整数，小数或者字母。</p>
<h3 id="2-2-1-语法声明"><a href="#2-2-1-语法声明" class="headerlink" title="2.2.1 语法声明"></a>2.2.1 语法声明</h3><ul>
<li>变量的类型在变量名之后等号之前声明。  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaVariable</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="comment">// var | val 变量名 ：变量类型 = 变量值</span></span><br><span class="line">        <span class="comment">// 用户名称</span></span><br><span class="line">        <span class="keyword">var</span> username : <span class="type">String</span> = <span class="string">&quot;zhangsan&quot;</span></span><br><span class="line">        <span class="comment">// 用户密码</span></span><br><span class="line">        <span class="keyword">val</span> userpswd : <span class="type">String</span> = <span class="string">&quot;000000&quot;</span> </span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>变量的类型如果能够通过变量值推断出来，那么可以省略类型声明，这里的省略，并不是不声明，而是由Scala编译器在编译时自动声明编译的。  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaVariable</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="comment">// 因为变量值为字符串，又因为Scala是静态类型语言，所以即使不声明类型</span></span><br><span class="line">        <span class="comment">// Scala也能在编译时正确的判断出变量的类型，这体现了Scala语言的简洁特性。</span></span><br><span class="line">        <span class="keyword">var</span> username = <span class="string">&quot;zhangsan&quot;</span></span><br><span class="line">        <span class="keyword">val</span> userpswd = <span class="string">&quot;000000&quot;</span> </span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="2-2-2-变量初始化"><a href="#2-2-2-变量初始化" class="headerlink" title="2.2.2 变量初始化"></a>2.2.2 变量初始化</h3><ul>
<li>Java语法中变量在使用前进行初始化就可以，但是Scala语法中是不允许的，必须显示进行初始化操作。  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaVariable</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">var</span> username <span class="comment">// Error</span></span><br><span class="line">        <span class="keyword">val</span> username = <span class="string">&quot;zhangsan&quot;</span> <span class="comment">// OK</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="2-2-3-可变变量"><a href="#2-2-3-可变变量" class="headerlink" title="2.2.3 可变变量"></a>2.2.3 可变变量</h3><ul>
<li>值可以改变的变量，称之为可变变量，但是变量类型无法发生改变, Scala中可变变量使用关键字var进行声明  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaVariable</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="comment">// 用户名称</span></span><br><span class="line">        <span class="keyword">var</span> username : <span class="type">String</span> = <span class="string">&quot;zhangsan&quot;</span></span><br><span class="line">        username = <span class="string">&quot;lisi&quot;</span> <span class="comment">// OK</span></span><br><span class="line">        username = <span class="literal">true</span> <span class="comment">// Error</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="2-2-4-不可变变量"><a href="#2-2-4-不可变变量" class="headerlink" title="2.2.4 不可变变量"></a>2.2.4 不可变变量</h3><ul>
<li>值一旦初始化后无法改变的变量，称之为不可变变量。Scala中不可变变量使用关键字val进行声明, 类似于Java语言中的final关键字</li>
<li>变量申明后修改的可能性比较低  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaVariable</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="comment">// 用户名称</span></span><br><span class="line">        <span class="keyword">val</span> username : <span class="type">String</span> = <span class="string">&quot;zhangsan&quot;</span></span><br><span class="line">        username = <span class="string">&quot;lisi&quot;</span> <span class="comment">// Error</span></span><br><span class="line">        username = <span class="literal">true</span> <span class="comment">// Error</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="2-2-5-强类型、弱类型"><a href="#2-2-5-强类型、弱类型" class="headerlink" title="2.2.5 强类型、弱类型"></a>2.2.5 强类型、弱类型</h3><ul>
<li>强类型：在执行之前，必须明确变量类型，而且类型确定后无法发生改变，取值的类型也不能发生变化；变量的类型和取值是统一的</li>
<li>弱类型：语言是在运行时决定数据类型，可以根据数据本身的类型动态的调整变量的类型</li>
<li>思考两个问题：<ul>
<li>val和var两个修饰符，哪一个会推荐使用？<ul>
<li>val</li>
</ul>
</li>
<li>Java中的字符串为何称之为不可变字符串？<ul>
<li>声明之后无法更改</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="2-3-标识符"><a href="#2-3-标识符" class="headerlink" title="2.3 标识符"></a>2.3 标识符</h2><p>Scala 可以使用两种形式的标志符，字符数字和符号。</p>
<ul>
<li>字符数字使用字母或是下划线开头，后面可以接字母或是数字，符号”$”在 Scala 中也看作为字母。然而以”$”开头的标识符为保留的 Scala 编译器产生的标志符使用，应用程序应该避免使用”$”开始的标识符，以免造成冲突。</li>
<li>Scala 的命名规范采用和 Java 类似的 camel 命名规范，首字符小写，比如 toString。类名的首字符还是使用大写。此外也应该避免使用以下划线结尾的标志符以避免冲突。</li>
<li>Scala 内部实现时会使用转义的标志符，比如:-&gt; 使用 $colon$minus$greater 来表示这个符号。</li>
<li>Scala 中的标识符也不能是关键字或保留字，那么Scala中有多少关键字或保留字呢？<br>   <img src="https://s2.loli.net/2021/12/04/GnNkOwfJSloHX2q.jpg"></li>
<li>注意<ol>
<li>可以使用字母、数字、美元符号$、下划线_</li>
<li>数字不能开头</li>
<li>不能使用关键字，保留字</li>
<li>长度没有限制，最好见名知意（驼峰规范）</li>
<li>区分大小写</li>
<li>基础标识符命名，Scala和Java一致</li>
<li>Java是Unicode编码，所以能使用Unicode转换的内容都可以作为标识符（中文）<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 和Java一样的标识符命名规则</span></span><br><span class="line"><span class="keyword">val</span> name = <span class="string">&quot;zhangsan&quot;</span> <span class="comment">// OK</span></span><br><span class="line"><span class="keyword">val</span> name1 = <span class="string">&quot;zhangsan0&quot;</span>   <span class="comment">// OK</span></span><br><span class="line"><span class="comment">//val 1name = &quot;zhangsan0&quot; // Error</span></span><br><span class="line"><span class="keyword">val</span> name$ = <span class="string">&quot;zhangsan1&quot;</span> <span class="comment">// OK</span></span><br><span class="line"><span class="keyword">val</span> $name = <span class="string">&quot;zhangsan2&quot;</span> <span class="comment">// OK</span></span><br><span class="line"><span class="keyword">val</span> name_ = <span class="string">&quot;zhangsan3&quot;</span> <span class="comment">// OK</span></span><br><span class="line"><span class="keyword">val</span> _name = <span class="string">&quot;zhangsan4&quot;</span> <span class="comment">// OK</span></span><br><span class="line"><span class="keyword">val</span> $ = <span class="string">&quot;zhangsan5&quot;</span>     <span class="comment">// OK</span></span><br><span class="line"><span class="keyword">val</span> _ = <span class="string">&quot;zhangsan6&quot;</span>     <span class="comment">// OK</span></span><br><span class="line"><span class="comment">//val 1 = &quot;zhangsan6&quot;     // Error</span></span><br><span class="line"><span class="comment">//val true = &quot;zhangsan6&quot;  // Error</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 和Java不一样的标识符命名规则</span></span><br><span class="line"><span class="keyword">val</span> + = <span class="string">&quot;lisi&quot;</span> <span class="comment">// OK</span></span><br><span class="line"><span class="keyword">val</span> - = <span class="string">&quot;lisi&quot;</span> <span class="comment">// OK</span></span><br><span class="line"><span class="keyword">val</span> * = <span class="string">&quot;lisi&quot;</span> <span class="comment">// OK</span></span><br><span class="line"><span class="keyword">val</span> / = <span class="string">&quot;lisi&quot;</span> <span class="comment">// OK</span></span><br><span class="line"><span class="keyword">val</span> ! = <span class="string">&quot;lisi&quot;</span> <span class="comment">// OK</span></span><br><span class="line"><span class="comment">//val @ = &quot;lisi&quot; // Error</span></span><br><span class="line"><span class="keyword">val</span> @@ = <span class="string">&quot;lisi&quot;</span> <span class="comment">// OK</span></span><br><span class="line"><span class="comment">//val # = &quot;lisi&quot; // Error</span></span><br><span class="line"><span class="keyword">val</span> ## = <span class="string">&quot;lisi&quot;</span> <span class="comment">// OK</span></span><br><span class="line"><span class="keyword">val</span> % = <span class="string">&quot;lisi&quot;</span> <span class="comment">// OK</span></span><br><span class="line"><span class="keyword">val</span> ^ = <span class="string">&quot;lisi&quot;</span> <span class="comment">// OK</span></span><br><span class="line"><span class="keyword">val</span> &amp; = <span class="string">&quot;lisi&quot;</span> <span class="comment">// OK</span></span><br><span class="line"><span class="comment">//val ( = &quot;lisi&quot; // Error</span></span><br><span class="line"><span class="comment">//val ( = &quot;lisi&quot; // Error</span></span><br><span class="line"><span class="comment">//val ) = &quot;lisi&quot; // Error</span></span><br><span class="line"><span class="comment">//val = = &quot;lisi&quot; // Error</span></span><br><span class="line"><span class="keyword">val</span> == = <span class="string">&quot;lisi&quot;</span> <span class="comment">// OK</span></span><br><span class="line"><span class="comment">//val [ = &quot;lisi&quot; // Error</span></span><br><span class="line"><span class="comment">//val ] = &quot;lisi&quot; // Error</span></span><br><span class="line"><span class="comment">//val : = &quot;lisi&quot; // Error</span></span><br><span class="line"><span class="keyword">val</span> :: = <span class="string">&quot;lisi&quot;</span> <span class="comment">// OK</span></span><br><span class="line"><span class="comment">//val ; = &quot;lisi&quot; // Error</span></span><br><span class="line"><span class="comment">//val &#x27; = &quot;lisi&quot; // Error</span></span><br><span class="line"><span class="comment">//val &quot; = &quot;lisi&quot; // Error</span></span><br><span class="line"><span class="keyword">val</span> <span class="string">&quot;&quot;</span> = <span class="string">&quot;lisi&quot;</span> <span class="comment">// OK</span></span><br><span class="line"><span class="keyword">val</span> &lt; = <span class="string">&quot;lisi&quot;</span> <span class="comment">// OK</span></span><br><span class="line"><span class="keyword">val</span> &gt; = <span class="string">&quot;lisi&quot;</span> <span class="comment">// OK</span></span><br><span class="line"><span class="keyword">val</span> ? = <span class="string">&quot;lisi&quot;</span> <span class="comment">// OK</span></span><br><span class="line"><span class="keyword">val</span> | = <span class="string">&quot;lisi&quot;</span> <span class="comment">// OK</span></span><br><span class="line"><span class="keyword">val</span> \ = <span class="string">&quot;lisi&quot;</span> <span class="comment">// OK</span></span><br><span class="line"><span class="comment">//val ` = &quot;lisi&quot; // Error</span></span><br><span class="line"><span class="keyword">val</span> ~ = <span class="string">&quot;lisi&quot;</span> <span class="comment">// OK</span></span><br><span class="line"><span class="keyword">val</span> :-&gt; = <span class="string">&quot;wangwu&quot;</span> <span class="comment">// OK</span></span><br><span class="line"><span class="keyword">val</span> :-&lt; = <span class="string">&quot;wangwu&quot;</span> <span class="comment">// OK</span></span><br><span class="line"><span class="comment">// 切记，能声明和能使用是两回事</span></span><br><span class="line"><span class="comment">// 能用</span></span><br><span class="line"><span class="keyword">val</span> 国家 = <span class="string">&quot;中国&quot;</span></span><br></pre></td></tr></table></figure></li>
</ol>
</li>
<li>Scala语言不仅是面向对象的语言，本身也融合了函数式编程，函数的命名也是用标识符<ul>
<li>函数式编程更注重功能</li>
<li>面向对象的变成语言更注重关系<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> name</span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaMethodName</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> a = <span class="number">1</span></span><br><span class="line">        <span class="keyword">val</span> b = <span class="number">2</span></span><br><span class="line">        <span class="keyword">val</span> sum = a + b</span><br><span class="line">        print(sum)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">+</span></span>(a: <span class="type">Int</span>, b: <span class="type">Int</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">        <span class="keyword">return</span> a + b</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>
<h2 id="2-4-字符串"><a href="#2-4-字符串" class="headerlink" title="2.4 字符串"></a>2.4 字符串</h2><ul>
<li>在 Scala 中，字符串的类型实际上就是 Java中的 String类，它本身是没有 String 类的。</li>
<li>在 Scala 中，String 是一个不可变的字符串对象，所以该对象不可被修改。这就意味着你如果修改字符串就会产生一个新的字符串对象。  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaString</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> name : <span class="type">String</span> = <span class="string">&quot;scala&quot;</span></span><br><span class="line">        <span class="keyword">val</span> subname : <span class="type">String</span> = name.substring(<span class="number">0</span>,<span class="number">2</span>)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="2-4-1-字符串连接"><a href="#2-4-1-字符串连接" class="headerlink" title="2.4.1 字符串连接"></a>2.4.1 字符串连接</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaString</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="comment">// 字符串连接</span></span><br><span class="line">        println(<span class="string">&quot;Hello &quot;</span> + name)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="2-4-2-传值字符串"><a href="#2-4-2-传值字符串" class="headerlink" title="2.4.2 传值字符串"></a>2.4.2 传值字符串</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaString</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="comment">// 传值字符串(格式化字符串)</span></span><br><span class="line">        printf(<span class="string">&quot;name=%s\n&quot;</span>, name)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="2-4-3-插值字符串"><a href="#2-4-3-插值字符串" class="headerlink" title="2.4.3 插值字符串"></a>2.4.3 插值字符串</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaString</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="comment">// 插值字符串</span></span><br><span class="line">        <span class="comment">// 将变量值插入到字符串</span></span><br><span class="line">        println(<span class="string">s&quot;name=<span class="subst">$&#123;name&#125;</span>&quot;</span>)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="2-4-4-多行字符串"><a href="#2-4-4-多行字符串" class="headerlink" title="2.4.4 多行字符串"></a>2.4.4 多行字符串</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaString</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="comment">// 多行格式化字符串</span></span><br><span class="line">        <span class="comment">// 在封装JSON或SQL时比较常用</span></span><br><span class="line">        <span class="comment">// | 默认顶格符</span></span><br><span class="line">        println(</span><br><span class="line">                    <span class="string">s&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">                      | Hello</span></span><br><span class="line"><span class="string">                      | $&#123;name&#125;</span></span><br><span class="line"><span class="string">        &quot;</span><span class="string">&quot;&quot;</span>.stripMargin)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="2-5-输入输出"><a href="#2-5-输入输出" class="headerlink" title="2.5 输入输出"></a>2.5 输入输出</h2><h3 id="2-5-1-输入"><a href="#2-5-1-输入" class="headerlink" title="2.5.1 输入"></a>2.5.1 输入</h3><ul>
<li>从屏幕（控制台）中获取输入  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaIn</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="comment">// 标准化屏幕输入</span></span><br><span class="line">        <span class="keyword">val</span> age : <span class="type">Int</span> = scala.io.<span class="type">StdIn</span>.readInt()</span><br><span class="line">        println(age)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>从文件中获取输入  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaIn</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="comment">// 请注意文件路径的位置</span></span><br><span class="line">        scala.io.<span class="type">Source</span>.fromFile(<span class="string">&quot;input/user.json&quot;</span>).foreach(</span><br><span class="line">            line =&gt; &#123;</span><br><span class="line">                print(line)</span><br><span class="line">            &#125;</span><br><span class="line">        )</span><br><span class="line">        scala.io.<span class="type">Source</span>.fromFile(<span class="string">&quot;input/user.json&quot;</span>).getLines()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="2-5-2-输出"><a href="#2-5-2-输出" class="headerlink" title="2.5.2 输出"></a>2.5.2 输出</h3><ul>
<li>Scala进行文件写操作，用的都是 java中的I/O类  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaOut</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">      <span class="keyword">val</span> writer = <span class="keyword">new</span> <span class="type">PrintWriter</span>(<span class="keyword">new</span> <span class="type">File</span>(<span class="string">&quot;output/test.txt&quot;</span> ))</span><br><span class="line">      writer.write(<span class="string">&quot;Hello Scala&quot;</span>)</span><br><span class="line">      writer.close()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="2-5-3-网络"><a href="#2-5-3-网络" class="headerlink" title="2.5.3 网络"></a>2.5.3 网络</h3><ul>
<li>Scala进行网络数据交互时，采用的也依然是 java中的I/O类</li>
<li>服务端  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">TestServer</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> server = <span class="keyword">new</span> <span class="type">ServerSocket</span>(<span class="number">9999</span>)</span><br><span class="line">        <span class="keyword">while</span> ( <span class="literal">true</span> ) &#123;</span><br><span class="line">            <span class="keyword">val</span> socket: <span class="type">Socket</span> = server.accept()</span><br><span class="line">            <span class="keyword">val</span> reader = <span class="keyword">new</span> <span class="type">BufferedReader</span>(</span><br><span class="line">                <span class="keyword">new</span> <span class="type">InputStreamReader</span>(</span><br><span class="line">                    socket.getInputStream,</span><br><span class="line">                    <span class="string">&quot;UTF-8&quot;</span></span><br><span class="line">                )</span><br><span class="line">            )</span><br><span class="line">            <span class="keyword">var</span> s : <span class="type">String</span> = <span class="string">&quot;&quot;</span></span><br><span class="line">            <span class="keyword">var</span> flg = <span class="literal">true</span></span><br><span class="line">            <span class="keyword">while</span> ( flg  ) &#123;</span><br><span class="line">                s = reader.readLine()</span><br><span class="line">                <span class="keyword">if</span> ( s != <span class="literal">null</span> ) &#123;</span><br><span class="line">                    println(s)</span><br><span class="line">                &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                    flg = <span class="literal">false</span></span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>客户端  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">TestClient</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> client = <span class="keyword">new</span> <span class="type">Socket</span>(<span class="string">&quot;localhost&quot;</span>, <span class="number">9999</span>)</span><br><span class="line">        <span class="keyword">val</span> out = <span class="keyword">new</span> <span class="type">PrintWriter</span>(</span><br><span class="line">            <span class="keyword">new</span> <span class="type">OutputStreamWriter</span>(</span><br><span class="line">                client.getOutputStream,</span><br><span class="line">                <span class="string">&quot;UTF-8&quot;</span></span><br><span class="line">            )</span><br><span class="line">        )</span><br><span class="line">        out.print(<span class="string">&quot;hello Scala&quot;</span>)</span><br><span class="line">        out.flush()</span><br><span class="line">        out.close()</span><br><span class="line">        client.close()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>思考：java的序列化怎么回事？<ol>
<li>java将内存中的对象存储到磁盘文件中，要求对象必须实现可序列化接口</li>
<li>在网络中想要传递对象，这个对象需要序列化。</li>
</ol>
</li>
</ul>
<h2 id="2-6-数据类型"><a href="#2-6-数据类型" class="headerlink" title="2.6 数据类型"></a>2.6 数据类型</h2><p>Scala与Java有着相同的数据类型，但是又有不一样的地方</p>
<h3 id="2-6-1-Java数据类型"><a href="#2-6-1-Java数据类型" class="headerlink" title="2.6.1 Java数据类型"></a>2.6.1 Java数据类型</h3><p>Java的数据类型包含基本类型和引用类型</p>
<ul>
<li>基本类型：byte,short,char,int,long,float,double,boolean</li>
<li>引用类型：Object，数组，字符串，包装类，集合，POJO对象等</li>
</ul>
<h3 id="2-6-2-Scala数据类型"><a href="#2-6-2-Scala数据类型" class="headerlink" title="2.6.2 Scala数据类型"></a>2.6.2 Scala数据类型</h3><ol>
<li>基本数据类型（byte short int long float double char boolean）<ul>
<li>基本数据类型使用时，直接分配在栈内存空间</li>
</ul>
</li>
<li>引用数据类型 Object<ul>
<li>引用数据类型分配在堆内存中，引用在栈内存中，所以栈内存引用堆内存 </li>
</ul>
</li>
</ol>
<p>Scala是完全面向对象的语言，所以不存在基本数据类型的概念，有的只是任意值对象类型（AnyVal）和任意引用对象类型(AnyRef)</p>
<ul>
<li>任意值对象类型AnyVal: 将基本数据类型用面向对象的方式进行转换处理</li>
<li>任意引用对象类型AnyRef: Scala集合，Java类，Scala类</li>
<li><img src="https://s2.loli.net/2021/12/04/KxEZ1sJcnFoPQt9.jpg"><br><img src="https://s2.loli.net/2021/12/04/sPpcDVK6B5l92Mg.jpg"></li>
</ul>
<h2 id="2-7-类型转换"><a href="#2-7-类型转换" class="headerlink" title="2.7 类型转换"></a>2.7 类型转换</h2><h3 id="2-7-1-自动类型转化（隐式转换）"><a href="#2-7-1-自动类型转化（隐式转换）" class="headerlink" title="2.7.1 自动类型转化（隐式转换）"></a>2.7.1 自动类型转化（隐式转换）</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaDataType</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> b : <span class="type">Byte</span> = <span class="number">10</span></span><br><span class="line">        <span class="keyword">val</span> s : <span class="type">Short</span> = b</span><br><span class="line">        <span class="keyword">val</span> i : <span class="type">Int</span> = s</span><br><span class="line">        <span class="keyword">val</span> lon : <span class="type">Long</span> = i</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>思考：如下代码是否正确？  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> c : <span class="type">Char</span> = &#x27;<span class="type">A</span>&#x27; + <span class="number">1</span></span><br><span class="line">println(c)</span><br></pre></td></tr></table></figure>
<ul>
<li>可以，ASCII运行</li>
</ul>
</li>
</ul>
<h3 id="2-7-2-强制类型转化"><a href="#2-7-2-强制类型转化" class="headerlink" title="2.7.2 强制类型转化"></a>2.7.2 强制类型转化</h3><ol>
<li>Java语言<ul>
<li>基本数据类型转换-&gt;精度的提升或者截取</li>
<li>引用数据类型转换-&gt;参与运算的两个类型必须有关联，子类等 <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> a = <span class="number">10</span></span><br><span class="line"><span class="keyword">byte</span> b = (<span class="keyword">byte</span>)a</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>Scala语言 <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">var</span> a : Int = <span class="number">10</span></span><br><span class="line"><span class="keyword">var</span> b : Byte = a.toByte</span><br><span class="line"><span class="comment">// 基本上Scala的AnyVal类型之间都提供了相应转换的方法。</span></span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="2-7-3-字符串类型转化"><a href="#2-7-3-字符串类型转化" class="headerlink" title="2.7.3 字符串类型转化"></a>2.7.3 字符串类型转化</h3><ul>
<li>Scala是完全面向对象的语言，所有的类型都提供了toString方法，可以直接转换为字符串lon.toString</li>
<li>任意类型都提供了和字符串进行拼接的方法  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> i = <span class="number">10</span></span><br><span class="line"><span class="keyword">val</span> s = <span class="string">&quot;hello &quot;</span> + i</span><br></pre></td></tr></table></figure></li>
</ul>
<h1 id="第3章-运算符"><a href="#第3章-运算符" class="headerlink" title="第3章 运算符"></a>第3章 运算符</h1><p>scala运算符的使用和Java运算符的使用基本相同，只有个别细节上不同。</p>
<h2 id="3-1-算数运算符"><a href="#3-1-算数运算符" class="headerlink" title="3.1 算数运算符"></a>3.1 算数运算符</h2><ul>
<li>加+，字符串拼接</li>
<li>减-</li>
<li>乘*</li>
<li>除/</li>
<li>取模%</li>
</ul>
<h2 id="3-2-关系运算符"><a href="#3-2-关系运算符" class="headerlink" title="3.2 关系运算符"></a>3.2 关系运算符</h2><ul>
<li>等于： ==</li>
<li>不等于：！=</li>
<li>大于：&gt;</li>
<li>小于：&lt;</li>
<li>大于等于：&gt;=</li>
<li>小于等于：&lt;=</li>
</ul>
<h2 id="思考：如下代码执行结果如何？"><a href="#思考：如下代码执行结果如何？" class="headerlink" title="思考：如下代码执行结果如何？"></a>思考：如下代码执行结果如何？</h2><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> a = <span class="keyword">new</span> <span class="type">String</span>(<span class="string">&quot;abc&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> b = <span class="keyword">new</span> <span class="type">String</span>(<span class="string">&quot;abc&quot;</span>)</span><br><span class="line"></span><br><span class="line">println(a == b)      <span class="comment">//true 非空的equals</span></span><br><span class="line">println(a.equals(b)) <span class="comment">//true </span></span><br><span class="line">println(a.eq(b))     <span class="comment">//false等价于java的==</span></span><br></pre></td></tr></table></figure>

<h2 id="3-3-赋值运算符"><a href="#3-3-赋值运算符" class="headerlink" title="3.3 赋值运算符"></a>3.3 赋值运算符</h2><ul>
<li>简单赋值：=</li>
<li>相加后赋值：+=</li>
<li>相减后赋值：-=</li>
<li>相乘后赋值：*=</li>
<li>相除后赋值：/=</li>
<li>取模后赋值：%=</li>
<li>按位左移后赋值：&lt;&lt;=</li>
<li>按位右移后赋值：&gt;&gt;=</li>
<li>按位与运算后复制：$=</li>
<li>按位异运算后复制：^=</li>
<li>按位或运算后复制：|=</li>
<li>思考一个问题：为什么在上面的运算符中没有看到 ++， –？<ul>
<li>++运算有歧义，容易理解出现错误，所以scala中没有这样的语法，所以采用 +=的方式来代替。</li>
<li>++运算符放置在数据后面，标识先赋值（临时变量），再加一</li>
<li>++运算符放置在数据前面，标识先加一，再赋值</li>
<li>类似i=i++（i=(temp=i=0)(i=i+1)）含义模糊</li>
</ul>
</li>
</ul>
<h2 id="3-4-逻辑运算符"><a href="#3-4-逻辑运算符" class="headerlink" title="3.4 逻辑运算符"></a>3.4 逻辑运算符</h2><ul>
<li>逻辑与：&amp;&amp;</li>
<li>逻辑或：||</li>
<li>逻辑非：！</li>
</ul>
<h2 id="3-5-位运算符"><a href="#3-5-位运算符" class="headerlink" title="3.5 位运算符"></a>3.5 位运算符</h2><p>如果指定 A = 60; 及 B = 13; 两个变量对应的二进制为<br>A = 0011 1100<br>B = 0000 1101</p>
<ul>
<li>按位与</li>
<li>按位或</li>
<li>按位异或</li>
<li>按位取反</li>
<li>左移运算</li>
<li>右移运算</li>
<li>无符号右移</li>
</ul>
<ol>
<li>JDK1.8 HashMap 数据如何存储（定位）<ul>
<li>hash(key.hashCode()) &amp; length</li>
</ul>
</li>
<li>JDK1.8 HashMap 极限情况放置多少数据会转换成红黑二叉树（在链表上放多少条数据会转换成红黑二叉树）<ul>
<li>11条</li>
<li></li>
</ul>
</li>
<li>JDK1.8 HashMap 扩容为什么是2倍</li>
</ol>
<h2 id="3-6-运算符本质"><a href="#3-6-运算符本质" class="headerlink" title="3.6 运算符本质"></a>3.6 运算符本质</h2><p>在Scala中其实是没有运算符的，所有运算符都是方法。</p>
<ul>
<li>scala是完全面向对象的语言，所以数字其实也是对象</li>
<li>当调用对象的方法时，点.可以省略</li>
<li>如果函数参数只有一个，或者没有参数，()可以省略  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaOper</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> i : <span class="type">Int</span> = <span class="number">10</span></span><br><span class="line">        <span class="keyword">val</span> j : <span class="type">Int</span> = i.+(<span class="number">10</span>)</span><br><span class="line">        <span class="keyword">val</span> k : <span class="type">Int</span> = j +(<span class="number">20</span>)</span><br><span class="line">        <span class="keyword">val</span> m : <span class="type">Int</span> = k + <span class="number">30</span></span><br><span class="line">        println(m)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h1 id="第4章-流程控制"><a href="#第4章-流程控制" class="headerlink" title="第4章 流程控制"></a>第4章 流程控制</h1>Scala程序代码和所有编程语言代码一样，都会有特定的执行流程顺序，默认情况下是顺序执行，上一条逻辑执行完成后才会执行下一条逻辑，执行期间也可以根据某些条件执行不同的分支逻辑代码。</li>
</ul>
<h2 id="4-1-分支控制"><a href="#4-1-分支控制" class="headerlink" title="4.1 分支控制"></a>4.1 分支控制</h2><p>让程序有选择的的执行，分支控制有三种：单分支、双分支、多分支</p>
<h2 id="4-1-1-单分支"><a href="#4-1-1-单分支" class="headerlink" title="4.1.1 单分支"></a>4.1.1 单分支</h2><p>IF…ELSE 语句是通过一条或多条语句的执行结果（true或者false）来决定执行的代码块</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">if(布尔表达式) &#123;</span><br><span class="line">   // 如果布尔表达式为 true 则执行该语句块</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>如果布尔表达式为 true 则执行大括号内的语句块，否则跳过大括号内的语句块，执行大括号之后的语句块。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaBranch</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> b = <span class="literal">true</span></span><br><span class="line">        <span class="keyword">if</span> ( b ) &#123;</span><br><span class="line">            println(<span class="string">&quot;true&quot;</span>)</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="4-1-2-双分支"><a href="#4-1-2-双分支" class="headerlink" title="4.1.2 双分支"></a>4.1.2 双分支</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">if(布尔表达式) &#123;</span><br><span class="line">   // 如果布尔表达式为 true 则执行该语句块</span><br><span class="line">&#125; else &#123;</span><br><span class="line">   // 如果布尔表达式为 false 则执行该语句块</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>如果布尔表达式为 true 则执行接着的大括号内的语句块，否则执行else后的大括号内的语句块。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaBranch</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> b = <span class="literal">true</span></span><br><span class="line">        <span class="keyword">if</span> ( b ) &#123;</span><br><span class="line">            println(<span class="string">&quot;true&quot;</span>)</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            println(<span class="string">&quot;false&quot;</span>)</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="4-1-3-多分支"><a href="#4-1-3-多分支" class="headerlink" title="4.1.3 多分支"></a>4.1.3 多分支</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">if(布尔表达式1) &#123;</span><br><span class="line">   // 如果布尔表达式1为 true，则执行该语句块</span><br><span class="line">&#125; else if ( 布尔表达式2 ) &#123;</span><br><span class="line">   // 如果布尔表达式2为 true，则执行该语句块</span><br><span class="line">&#125;</span><br><span class="line">...</span><br><span class="line">else &#123;</span><br><span class="line">   // 上面条件都不满足的场合，则执行该语句块</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>实现一个小功能：输入年龄，如果年龄小于18岁，则输出“童年”。如果年龄大于等于18且小于等于30，则输出“青年”，如果年龄大于30小于等于50，则输出”中年”，否则，输出“老年”。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaBranch</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> age = <span class="number">30</span></span><br><span class="line">        <span class="keyword">if</span> ( age &lt; <span class="number">18</span> ) &#123;</span><br><span class="line">            println(<span class="string">&quot;童年&quot;</span>)</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> ( age &lt;= <span class="number">30</span> ) &#123;</span><br><span class="line">            println(<span class="string">&quot;青年&quot;</span>)</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> ( age &lt;= <span class="number">50</span> ) &#123;</span><br><span class="line">            println(<span class="string">&quot;中年&quot;</span>)</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            println(<span class="string">&quot;老年&quot;</span>)</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>实际上，Scala中的表达式都是有返回值的，所以上面的小功能还有其他的实现方式</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaBranch</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> age = <span class="number">30</span></span><br><span class="line">        <span class="keyword">val</span> result = <span class="keyword">if</span> ( age &lt; <span class="number">18</span> ) &#123;</span><br><span class="line">            <span class="string">&quot;童年&quot;</span></span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> ( age &lt;= <span class="number">30</span> ) &#123;</span><br><span class="line">            <span class="string">&quot;青年&quot;</span></span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> ( age &lt;= <span class="number">50</span> ) &#123;</span><br><span class="line">            <span class="string">&quot;中年&quot;</span></span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="string">&quot;老年&quot;</span></span><br><span class="line">        &#125;</span><br><span class="line">        println(result)</span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<blockquote>
<p>思考一个问题: 怎么没有讲三元运算符？<br>Scala语言中没有三元运算符的，使用if分支判断来代替三元运算符</p>
</blockquote>
<h2 id="4-2-循环控制"><a href="#4-2-循环控制" class="headerlink" title="4.2 循环控制"></a>4.2 循环控制</h2><p>有的时候，我们可能需要多次执行同一块代码。一般情况下，语句是按顺序执行的：函数中的第一个语句先执行，接着是第二个语句，依此类推。编程语言提供了更为复杂执行路径的多种控制结构。循环语句允许我们多次执行一个语句或语句组<br>Scala语言提供了以下几种循环类型</p>
<ul>
<li><code>while</code>循环</li>
<li><code>do···while</code>循环</li>
<li><code>for</code>循环</li>
</ul>
<h3 id="4-2-1-for循环"><a href="#4-2-1-for循环" class="headerlink" title="4.2.1 for循环"></a>4.2.1 for循环</h3><ol>
<li><p>基本语法</p>
 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">for ( 循环变量 &lt;- 数据集 ) &#123;</span><br><span class="line">    循环体</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p> 这里的数据集可以是任意类型的数据集合，如字符串，集合，数组等。</p>
 <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaLoop</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">for</span> ( i &lt;- <span class="type">Range</span>(<span class="number">1</span>,<span class="number">5</span>) ) &#123; <span class="comment">// 范围集合</span></span><br><span class="line">            println(<span class="string">&quot;i = &quot;</span> + i )</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">for</span> ( i &lt;- <span class="number">1</span> to <span class="number">5</span> ) &#123; <span class="comment">// 包含5</span></span><br><span class="line">            println(<span class="string">&quot;i = &quot;</span> + i )</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">for</span> ( i &lt;- <span class="number">1</span> until <span class="number">5</span> ) &#123; <span class="comment">// 不包含5</span></span><br><span class="line">            println(<span class="string">&quot;i = &quot;</span> + i )</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li><p>循环守卫<br> 循环时可以增加条件来决定是否继续循环体的执行,这里的判断条件我们称之为循环守卫</p>
 <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaLoop</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">for</span> ( i &lt;- <span class="type">Range</span>(<span class="number">1</span>,<span class="number">5</span>) <span class="keyword">if</span> i != <span class="number">3</span>  ) &#123;</span><br><span class="line">            println(<span class="string">&quot;i = &quot;</span> + i )</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li><p>循环步长<br> scala的集合也可以设定循环的增长幅度，也就是所谓的步长step</p>
 <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaLoop</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">for</span> ( i &lt;- <span class="type">Range</span>(<span class="number">1</span>,<span class="number">5</span>,<span class="number">2</span>) ) &#123;</span><br><span class="line">            println(<span class="string">&quot;i = &quot;</span> + i )</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">for</span> ( i &lt;- <span class="number">1</span> to <span class="number">5</span> by <span class="number">2</span> ) &#123;</span><br><span class="line">            println(<span class="string">&quot;i = &quot;</span> + i )</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li><p>循环嵌套</p>
 <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaLoop</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">for</span> ( i &lt;- <span class="type">Range</span>(<span class="number">1</span>,<span class="number">5</span>); j &lt;- <span class="type">Range</span>(<span class="number">1</span>,<span class="number">4</span>) ) &#123;</span><br><span class="line">            println(<span class="string">&quot;i = &quot;</span> + i + <span class="string">&quot;,j = &quot;</span> + j )</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">for</span> ( i &lt;- <span class="type">Range</span>(<span class="number">1</span>,<span class="number">5</span>) ) &#123;</span><br><span class="line">            <span class="keyword">for</span> ( j &lt;- <span class="type">Range</span>(<span class="number">1</span>,<span class="number">4</span>) ) &#123;</span><br><span class="line">                println(<span class="string">&quot;i = &quot;</span> + i + <span class="string">&quot;,j = &quot;</span> + j )</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li><p>引入变量</p>
 <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaLoop</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">for</span> ( i &lt;- <span class="type">Range</span>(<span class="number">1</span>,<span class="number">5</span>); j = i - <span class="number">1</span> ) &#123;</span><br><span class="line">            println(<span class="string">&quot;j = &quot;</span> + j )</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>思考一个问题: 那如何只使用一次for循环实现九层妖塔</p>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">        *</span><br><span class="line">       ***</span><br><span class="line">      *****</span><br><span class="line">     *******</span><br><span class="line">    *********</span><br><span class="line">   ***********</span><br><span class="line">  *************</span><br><span class="line"> ***************</span><br><span class="line">*****************</span><br></pre></td></tr></table></figure>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> total = <span class="number">9</span></span><br><span class="line"><span class="keyword">for</span> (level &lt;- <span class="number">1</span> to total) &#123;</span><br><span class="line">    println((<span class="string">&quot; &quot;</span> * (total - level)) + (<span class="string">&quot;*&quot;</span> * (level * <span class="number">2</span> - <span class="number">1</span>)))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li><p>循环返回值<br>scala所有的表达式都是有返回值的。但是这里的返回值并不一定都是有值的。<br>如果希望for循环表达式的返回值有具体的值，需要使用关键字yield</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaLoop</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> result = <span class="keyword">for</span> ( i &lt;- <span class="type">Range</span>(<span class="number">1</span>,<span class="number">5</span>) ) <span class="keyword">yield</span> &#123;</span><br><span class="line">            i * <span class="number">2</span></span><br><span class="line">        &#125;</span><br><span class="line">        println(result)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<blockquote>
<p>思考:<br>Java中的线程有yield方法，Scala中该如何调用？<br>  线程的yield调用 <code>Thread.`yield`() </code></p>
</blockquote>
</li>
</ol>
<h3 id="4-2-2-while循环"><a href="#4-2-2-while循环" class="headerlink" title="4.2.2 while循环"></a>4.2.2 while循环</h3><ol>
<li>基本语法<br>当循环条件表达式返回值为true时，执行循环体代码<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">while( 循环条件表达式 ) &#123;</span><br><span class="line">    循环体</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
一种特殊的while循环就是，先执行循环体，再判断循环条件是否成立<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">do &#123;</span><br><span class="line">    循环体</span><br><span class="line">&#125; while ( 循环条件表达式 )</span><br></pre></td></tr></table></figure></li>
<li>while循环<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaLoop</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">var</span> i = <span class="number">0</span></span><br><span class="line">        <span class="keyword">while</span> ( i &lt; <span class="number">5</span> ) &#123;</span><br><span class="line">            println(i)</span><br><span class="line">            i += <span class="number">1</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>do…while循环<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaLoop</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">var</span> i = <span class="number">5</span></span><br><span class="line">        do &#123;</span><br><span class="line">            println(i)</span><br><span class="line">        &#125; <span class="keyword">while</span> ( i &lt; <span class="number">5</span> )</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="4-2-3-循环中断"><a href="#4-2-3-循环中断" class="headerlink" title="4.2.3 循环中断"></a>4.2.3 循环中断</h3><p>scala是完全面向对象的语言，所以无法使用break，continue关键字这样的方式来中断，或继续循环逻辑，而是采用了函数式编程的方式代替了循环语法中的break和continue</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaLoop</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        scala.util.control.<span class="type">Breaks</span>.breakable &#123;</span><br><span class="line">            <span class="keyword">for</span> ( i &lt;- <span class="number">1</span> to <span class="number">5</span> ) &#123;</span><br><span class="line">                <span class="keyword">if</span> ( i == <span class="number">3</span> ) &#123;</span><br><span class="line">                    scala.util.control.<span class="type">Breaks</span>.<span class="keyword">break</span></span><br><span class="line">                &#125;</span><br><span class="line">                println(i)</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>Breaks.break原理是抛出异常</li>
<li>Breaks.breakable原理是try-catch</li>
<li>使用静态导入简化代码  <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import scala.util.control.Breaks._</span><br><span class="line"></span><br><span class="line">breakable&#123;</span><br><span class="line">    for (i&lt;-1 to 5) &#123;</span><br><span class="line">        if (i==3)&#123;</span><br><span class="line">            break</span><br><span class="line">        &#125;</span><br><span class="line">        println(&quot;break &quot; + i)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="4-2-4-嵌套循环"><a href="#4-2-4-嵌套循环" class="headerlink" title="4.2.4 嵌套循环"></a>4.2.4 嵌套循环</h3><p>循环中有循环，就是嵌套循环。通过嵌套循环可以实现特殊的功能，比如说九九乘法表</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> (x &lt;- <span class="number">1</span> to <span class="number">9</span>; y &lt;- <span class="number">1</span> to x) &#123;</span><br><span class="line">    print(y + <span class="string">&quot; * &quot;</span> + x + <span class="string">&quot; = &quot;</span> + (x * y) + <span class="string">&quot;\t&quot;</span>)</span><br><span class="line">    <span class="keyword">if</span> (x == y) &#123;</span><br><span class="line">        println()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h1 id="第5章-函数式编程"><a href="#第5章-函数式编程" class="headerlink" title="第5章 函数式编程"></a>第5章 函数式编程</h1><p>在Java中，采用面向对象编程，所以解决问题都是按照面向对象的方式来处理的。比如用户登陆等业务功能，但是Scala采用函数式编程，使用函数式编程的思路来解决问题。scala编程语言将函数式编程和面向对象编程完美地融合在一起了。</p>
<ul>
<li>面向对象编程<br>  分解对象，行为，属性，然后通过对象的关系以及行为的调用来解决问题</li>
<li>函数式编程<br>  将问题分解成一个一个的步骤，将每个步骤进行封装（函数），通过调用这些封装好的功能按照指定的步骤，解决问题。</li>
</ul>
<h2 id="5-1-基础函数编程"><a href="#5-1-基础函数编程" class="headerlink" title="5.1 基础函数编程"></a>5.1 基础函数编程</h2><h3 id="5-1-1-基本语法"><a href="#5-1-1-基本语法" class="headerlink" title="5.1.1 基本语法"></a>5.1.1 基本语法</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[修饰符] def 函数名 (参数名称: 参数类型 ···) [:返回值类型] = &#123;</span><br><span class="line">    函数体</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>示例：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">test</span></span>( s : <span class="type">String</span> ) : <span class="type">Unit</span> = &#123;</span><br><span class="line">    println(s)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="5-1-2-函数-amp-方法"><a href="#5-1-2-函数-amp-方法" class="headerlink" title="5.1.2 函数&amp;方法"></a>5.1.2 函数&amp;方法</h3><ul>
<li>scala 中存在方法与函数两个不同的概念，二者在语义上的区别很小。scala 方法是类的一部分，而函数是一个对象，可以赋值给一个变量。换句话来说在类中定义的函数即是方法。scala 中的方法跟 Java 的类似，方法是组成类的一部分。scala 中的函数则是一个完整的对象。</li>
<li>Scala中的方法和函数从语法概念上来讲，一般不好区分，所以简单的理解就是：方法也是函数。只不过类中声明的函数称之为方法，其他场合声明的就是函数了。类中的方法是有重载和重写的。而函数可就没有重载和重写的概念了，但是函数可以嵌套声明使用，方法就没有这个能力了。</li>
</ul>
<h3 id="5-1-3-函数定义"><a href="#5-1-3-函数定义" class="headerlink" title="5.1.3 函数定义"></a>5.1.3 函数定义</h3><ol>
<li>无参，无返回值 <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaFunction</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">fun1</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">            println(<span class="string">&quot;函数体&quot;</span>)</span><br><span class="line">        &#125;</span><br><span class="line">        fun1()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>无参，有返回值 <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaFunction</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">fun2</span></span>(): <span class="type">String</span> = &#123;</span><br><span class="line">            <span class="string">&quot;zhangsan&quot;</span></span><br><span class="line">        &#125;</span><br><span class="line">        println( fun2() )</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>有参，无返回值 <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaFunction</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">fun3</span></span>( name:<span class="type">String</span> ): <span class="type">Unit</span> = &#123;</span><br><span class="line">            println( name )</span><br><span class="line">        &#125;</span><br><span class="line">        fun3(<span class="string">&quot;zhangsan&quot;</span>)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>有参，有返回值 <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaFunction</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">fun4</span></span>(name:<span class="type">String</span>): <span class="type">String</span> = &#123;</span><br><span class="line">            <span class="string">&quot;Hello &quot;</span> + name</span><br><span class="line">        &#125;</span><br><span class="line">        println( fun4(<span class="string">&quot;zhangsan&quot;</span>) )</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>多参，无返回值 <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaFunction</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">fun5</span></span>(hello:<span class="type">String</span>, name:<span class="type">String</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">            println( hello + <span class="string">&quot; &quot;</span> + name )</span><br><span class="line">        &#125;</span><br><span class="line">        fun5(<span class="string">&quot;Hello&quot;</span>, <span class="string">&quot;zhangsan&quot;</span>)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>多参，有返回值 <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaFunction</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">fun6</span></span>(hello:<span class="type">String</span>, name:<span class="type">String</span>): <span class="type">String</span> = &#123;</span><br><span class="line">            hello + <span class="string">&quot; &quot;</span> + name</span><br><span class="line">        &#125;</span><br><span class="line">        println( fun6(<span class="string">&quot;Hello&quot;</span>, <span class="string">&quot;zhangsan&quot;</span>))</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="5-1-4-函数参数"><a href="#5-1-4-函数参数" class="headerlink" title="5.1.4 函数参数"></a>5.1.4 函数参数</h3><ol>
<li>可变参数 <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaFunction</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">fun7</span></span>(names:<span class="type">String</span>*): <span class="type">Unit</span> = &#123;</span><br><span class="line">            println(names)</span><br><span class="line">        &#125;</span><br><span class="line">        fun7()</span><br><span class="line">        fun7( <span class="string">&quot;zhangsan&quot;</span> )</span><br><span class="line">        fun7( <span class="string">&quot;zhangsan&quot;</span>, <span class="string">&quot;lisi&quot;</span> )</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
 可变参数不能放置在参数列表的前面，一般放置在参数列表的最后 <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">oobject <span class="type">ScalaFunction</span> &#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="comment">// Error</span></span><br><span class="line">        <span class="comment">//def fun77(names:String*, name:String): Unit = &#123;</span></span><br><span class="line">            </span><br><span class="line">        <span class="comment">//&#125;</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">fun777</span></span>( name:<span class="type">String</span>, names:<span class="type">String</span>* ): <span class="type">Unit</span> = &#123;</span><br><span class="line">            println( name )</span><br><span class="line">            println( names )</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>参数默认值 <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaFunction</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">fun8</span></span>( name:<span class="type">String</span>, password:<span class="type">String</span> = <span class="string">&quot;000000&quot;</span> ): <span class="type">Unit</span> = &#123;</span><br><span class="line">            println( name + <span class="string">&quot;,&quot;</span> + password )</span><br><span class="line">        &#125;</span><br><span class="line">        fun8(<span class="string">&quot;zhangsan&quot;</span>, <span class="string">&quot;123123&quot;</span>)</span><br><span class="line">        fun8(<span class="string">&quot;zhangsan&quot;</span>)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
<li>带名参数 <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaFunction</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">fun9</span></span>( password:<span class="type">String</span> = <span class="string">&quot;000000&quot;</span>, name:<span class="type">String</span> ): <span class="type">Unit</span> = &#123;</span><br><span class="line">            println( name + <span class="string">&quot;,&quot;</span> + password )</span><br><span class="line">        &#125;</span><br><span class="line">        fun9(<span class="string">&quot;123123&quot;</span>, <span class="string">&quot;zhangsan&quot;</span> )</span><br><span class="line">        fun9(name=<span class="string">&quot;zhangsan&quot;</span>)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="5-1-5-函数至简原则"><a href="#5-1-5-函数至简原则" class="headerlink" title="5.1.5 函数至简原则"></a>5.1.5 函数至简原则</h3><p>所谓的至简原则，其实就是Scala的作者为了开发人员能够大幅度提高开发效率。通过编译器的动态判定功能，帮助我们将函数声明中能简化的地方全部都进行了简化。也就是说将函数声明中那些能省的地方全部都省掉。所以这里的至简原则，简单来说就是：能省则省。</p>
<ol>
<li>省略return关键字 <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaFunction</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">fun1</span></span>(): <span class="type">String</span> = &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="string">&quot;zhangsan&quot;</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">fun11</span></span>(): <span class="type">String</span> = &#123;</span><br><span class="line">            <span class="string">&quot;zhangsan&quot;</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>省略花括号 <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaFunction</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">fun2</span></span>(): <span class="type">String</span> = <span class="string">&quot;zhangsan&quot;</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>省略返回值类型 <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaFunction</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">fun3</span></span>() = <span class="string">&quot;zhangsan&quot;</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>省略参数列表 <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaFunction</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">fun4</span> </span>= <span class="string">&quot;zhangsan&quot;</span></span><br><span class="line">        fun4<span class="comment">// OK</span></span><br><span class="line">        fun4()<span class="comment">//(ERROR)</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
<li>省略等号</li>
</ol>
<ul>
<li>如果函数体中有明确的return语句，那么返回值类型不能省略  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaFunction</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">fun5</span></span>(): <span class="type">String</span> = &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="string">&quot;zhangsan&quot;</span></span><br><span class="line">        &#125;</span><br><span class="line">        println(fun5())</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>如果函数体返回值类型明确为Unit, 那么函数体中即使有return关键字也不起作用  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaFunction</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">fun5</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="string">&quot;zhangsan&quot;</span></span><br><span class="line">        &#125;</span><br><span class="line">        println(fun5())</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>如果函数体返回值类型声明为Unit, 但是又想省略，那么此时就必须连同等号一起省略，同时，return不起作用  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaFunction</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">fun5</span></span>() &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="string">&quot;zhangsan&quot;</span></span><br><span class="line">        &#125;</span><br><span class="line">        println(fun5())</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul>
<ol start="6">
<li>省略名称和关键字<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaFunction</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        () =&gt; &#123;</span><br><span class="line">            println(<span class="string">&quot;zhangsan&quot;</span>)</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="5-2-高阶函数编程"><a href="#5-2-高阶函数编程" class="headerlink" title="5.2 高阶函数编程"></a>5.2 高阶函数编程</h2><p>所谓的高阶函数，其实就是将函数当成一个类型来使用，而不是当成特定的语法结构。</p>
<h3 id="5-2-1-函数作为值"><a href="#5-2-1-函数作为值" class="headerlink" title="5.2.1 函数作为值"></a>5.2.1 函数作为值</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaFunction</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">fun1</span></span>(): <span class="type">String</span> = &#123;</span><br><span class="line">            <span class="string">&quot;zhangsan&quot;</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">val</span> a = fun1</span><br><span class="line">        <span class="keyword">val</span> b = fun1 _</span><br><span class="line">        <span class="keyword">val</span> c : ()=&gt;<span class="type">Unit</span> = fun1</span><br><span class="line">        println(a)</span><br><span class="line">        println(b)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>函数类型：默认情况下，函数类型为FunctionX,X表示参数的个数，最多22个</p>
<h3 id="5-2-2-函数作为参数"><a href="#5-2-2-函数作为参数" class="headerlink" title="5.2.2 函数作为参数"></a>5.2.2 函数作为参数</h3><p>函数作为参数使用的目的，是为了灵活改变实现逻辑，而不是固定写死</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaFunction</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">fun2</span></span>( i:<span class="type">Int</span> ): <span class="type">Int</span> = &#123;</span><br><span class="line">            i * <span class="number">2</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">fun22</span></span>( f : <span class="type">Int</span> =&gt; <span class="type">Int</span> ): <span class="type">Int</span> = &#123;</span><br><span class="line">            f(<span class="number">10</span>)</span><br><span class="line">        &#125;</span><br><span class="line">        println(fun22(fun2))</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="5-2-3-函数作为返回值"><a href="#5-2-3-函数作为返回值" class="headerlink" title="5.2.3 函数作为返回值"></a>5.2.3 函数作为返回值</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaFunction</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">fun3</span></span>( i:<span class="type">Int</span> ): <span class="type">Int</span> = &#123;</span><br><span class="line">            i * <span class="number">2</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">fun33</span></span>( ) = &#123;</span><br><span class="line">            fun3 _</span><br><span class="line">        &#125;</span><br><span class="line">        println(fun33()(<span class="number">10</span>))</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="5-2-4-匿名函数"><a href="#5-2-4-匿名函数" class="headerlink" title="5.2.4 匿名函数"></a>5.2.4 匿名函数</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaFunction</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">fun4</span></span>( f:<span class="type">Int</span> =&gt; <span class="type">Int</span> ): <span class="type">Int</span> = &#123;</span><br><span class="line">            f(<span class="number">10</span>)</span><br><span class="line">        &#125;</span><br><span class="line">        println(fun4((x:<span class="type">Int</span>)=&gt;&#123;x * <span class="number">20</span>&#125;))</span><br><span class="line">        println(fun4((x)=&gt;&#123;x * <span class="number">20</span>&#125;))</span><br><span class="line">        println(fun4((x)=&gt;x * <span class="number">20</span>))</span><br><span class="line">        println(fun4(x=&gt;x * <span class="number">20</span>))</span><br><span class="line">        println(fun4(_ * <span class="number">20</span>))</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="5-2-7-控制抽象"><a href="#5-2-7-控制抽象" class="headerlink" title="5.2.7 控制抽象"></a>5.2.7 控制抽象</h3><ul>
<li>所谓控制抽象, 就意味着直接传递代码</li>
<li>一般传递代码都是多行传递，所以小括号会改为花括号</li>
<li><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaFunction</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">fun7</span></span>(op: =&gt; <span class="type">Unit</span>) = &#123;</span><br><span class="line">            op</span><br><span class="line">        &#125;</span><br><span class="line">        fun7&#123;</span><br><span class="line">            println(<span class="string">&quot;xx&quot;</span>)</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="5-2-5-闭包"><a href="#5-2-5-闭包" class="headerlink" title="5.2.5 闭包"></a>5.2.5 闭包</h3><ul>
<li>一个函数使用到函数之外的变量，但是考虑变量的生命周期问题，需要改变（延长）变量的生命周期，那么需要将变量包含到函数的内部，形成闭合的环境，这个环境成称为闭包环境，简称闭包（closu）</li>
<li>将函数作为对象使用的时候都会有闭包</li>
<li>匿名函数使用会有闭包<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaFunction</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">fun5</span></span>() = &#123;</span><br><span class="line">            <span class="keyword">val</span> i = <span class="number">20</span></span><br><span class="line">            <span class="function"><span class="keyword">def</span> <span class="title">fun55</span></span>() = &#123;</span><br><span class="line">                i * <span class="number">2</span></span><br><span class="line">            &#125;</span><br><span class="line">            fun55 _</span><br><span class="line">        &#125;</span><br><span class="line">        fun5()()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul>
<blockquote>
<p>思考一个问题: 没有使用外部变量还能称之为闭包吗？ </p>
</blockquote>
<h3 id="5-2-6-函数柯里化"><a href="#5-2-6-函数柯里化" class="headerlink" title="5.2.6 函数柯里化"></a>5.2.6 函数柯里化</h3><ul>
<li>将无关参数分离开，降低参数和方法的耦合性<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaFunction</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">fun6</span></span>(i:<span class="type">Int</span>)(j:<span class="type">Int</span>) = &#123;</span><br><span class="line">            i * j</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="5-2-7-递归函数"><a href="#5-2-7-递归函数" class="headerlink" title="5.2.7 递归函数"></a>5.2.7 递归函数</h3><ul>
<li>自己调用自己就是递归</li>
<li>递归需要包含跳出条件</li>
<li>递归传递的参数应该有规律</li>
<li>尾递归<ul>
<li>最后实现方法的递归操作</li>
<li>不能影响输出和返回（不受到其他逻辑代码的限制）</li>
<li>Scala中的尾递归会被优化成while死循环，所以称为伪递归<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaFunction</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">fun8</span></span>(j:<span class="type">Int</span>):<span class="type">Int</span> = &#123;</span><br><span class="line">            <span class="keyword">if</span> ( j &lt;= <span class="number">1</span> ) &#123;</span><br><span class="line">                <span class="number">1</span></span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                j * fun8(j<span class="number">-1</span>)</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        println(fun8(<span class="number">5</span>))</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>
<h3 id="5-2-9-惰性函数"><a href="#5-2-9-惰性函数" class="headerlink" title="5.2.9 惰性函数"></a>5.2.9 惰性函数</h3><p>当函数返回值被声明为lazy时，函数的执行将被推迟，直到我们首次对此取值，该函数才会执行。这种函数我们称之为惰性函数。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaFunction</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">fun9</span></span>(): <span class="type">String</span> = &#123;</span><br><span class="line">            println(<span class="string">&quot;function...&quot;</span>)</span><br><span class="line">            <span class="string">&quot;zhangsan&quot;</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">lazy</span> <span class="keyword">val</span> a = fun9()</span><br><span class="line">        println(<span class="string">&quot;----------&quot;</span>)</span><br><span class="line">        println(a)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h1 id="第6章-面向对象编程"><a href="#第6章-面向对象编程" class="headerlink" title="第6章 面向对象编程"></a>第6章 面向对象编程</h1><p>Scala是一门完全面向对象的语言，摒弃了Java中很多不是面向对象的语法。虽然如此，但其面向对象思想和Java的面向对象思想还是一致的</p>
<h2 id="6-1-基础面向对象编程"><a href="#6-1-基础面向对象编程" class="headerlink" title="6.1 基础面向对象编程"></a>6.1 基础面向对象编程</h2><h3 id="6-1-1-包"><a href="#6-1-1-包" class="headerlink" title="6.1.1 包"></a>6.1.1 包</h3><ol>
<li>基本语法<br>Scala中基本的package包语法和Java完全一致<br>package com.atguigu.bigdata.scala</li>
<li>扩展语法<br>Java中package包的语法比较单一，Scala对此进行扩展<ul>
<li>Scala中的包和类的物理路径没有关系</li>
<li>package关键字可以嵌套声明使用  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com</span><br><span class="line"><span class="keyword">package</span> atguigu &#123;</span><br><span class="line">    <span class="keyword">package</span> bigdata &#123;</span><br><span class="line">        <span class="keyword">package</span> scala &#123;</span><br><span class="line">            <span class="class"><span class="keyword">object</span> <span class="title">ScalaPackage</span> </span>&#123;</span><br><span class="line">                <span class="function"><span class="keyword">def</span> <span class="title">test</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">                    println(<span class="string">&quot;test...&quot;</span>)</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>同一个源码文件中子包可以直接访问父包中的内容，而无需import  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com</span><br><span class="line"><span class="keyword">package</span> atguigu &#123;</span><br><span class="line">    <span class="keyword">package</span> bigdata &#123;</span><br><span class="line">        <span class="class"><span class="keyword">class</span> <span class="title">Test</span> </span>&#123;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">package</span> scala &#123;</span><br><span class="line">            <span class="class"><span class="keyword">object</span> <span class="title">ScalaPackage</span> </span>&#123;</span><br><span class="line">                <span class="function"><span class="keyword">def</span> <span class="title">test</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">                    <span class="keyword">new</span> <span class="type">Test</span>()</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>Scala中package也可以看作对象，并声明属性和函数  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com</span><br><span class="line"><span class="keyword">package</span> <span class="class"><span class="keyword">object</span> <span class="title">atguigu</span> </span>&#123;</span><br><span class="line">    <span class="keyword">val</span> name : <span class="type">String</span> = <span class="string">&quot;zhangsan&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">test</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">        println( name )</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">package</span> atguigu &#123;</span><br><span class="line">    <span class="keyword">package</span> bigdata &#123;</span><br><span class="line">        <span class="keyword">package</span> scala &#123;</span><br><span class="line">            <span class="class"><span class="keyword">object</span> <span class="title">ScalaPackage</span> </span>&#123;</span><br><span class="line">                <span class="function"><span class="keyword">def</span> <span class="title">test</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ol>
<h3 id="6-1-2-导入"><a href="#6-1-2-导入" class="headerlink" title="6.1.2 导入"></a>6.1.2 导入</h3><ol>
<li>基本语法<br>Scala中基本的import导入语法和Java完全一致<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.<span class="type">List</span></span><br><span class="line"><span class="keyword">import</span> java.util._ <span class="comment">// Scala中使用下划线代替Java中的星号</span></span><br></pre></td></tr></table></figure></li>
<li>扩展语法<br>Java中import导入的语法比较单一，Scala对此进行扩展<ul>
<li>Scala中的import语法可以在任意位置使用  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaImport</span></span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">import</span> java.util.<span class="type">ArrayList</span></span><br><span class="line">        <span class="keyword">new</span>  <span class="type">ArrayList</span>()   </span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>Scala中可以导包，而不是导类  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaImport</span></span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">import</span> java.util</span><br><span class="line">        <span class="keyword">new</span> util.<span class="type">ArrayList</span>()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>Scala中可以在同一行中导入相同包中的多个类，简化代码  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.&#123;<span class="type">List</span>, <span class="type">ArrayList</span>&#125;</span><br></pre></td></tr></table></figure></li>
<li>Scala中可以屏蔽某个包中的类  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util._</span><br><span class="line"><span class="keyword">import</span> java.sql.&#123; <span class="type">Date</span>=&gt;_, <span class="type">Array</span>=&gt;_, _ &#125;</span><br></pre></td></tr></table></figure></li>
<li>Scala中可以给类起别名，简化使用  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.&#123;<span class="type">ArrayList</span>=&gt;<span class="type">AList</span>&#125;</span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaImport</span></span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">new</span> <span class="type">AList</span>()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>Scala中可以使用类的绝对路径而不是相对路径  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> _root_.java.util.<span class="type">ArrayList</span></span><br></pre></td></tr></table></figure></li>
<li>默认情况下，Scala中会导入如下包和对象  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.lang._</span><br><span class="line"><span class="keyword">import</span> scala._</span><br><span class="line"><span class="keyword">import</span> scala.<span class="type">Predef</span>._</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ol>
<h3 id="6-1-3-类"><a href="#6-1-3-类" class="headerlink" title="6.1.3 类"></a>6.1.3 类</h3><p>面向对象编程中类可以看成一个模板，而对象可以看成是根据模板所创建的具体事物</p>
<ol>
<li>基本语法 <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 声明类：访问权限 class 类名 &#123; 类主体内容 &#125; </span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">User</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 类的主体内容</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 对象：new 类名(参数列表)</span></span><br><span class="line"><span class="keyword">new</span> <span class="type">User</span>()</span><br></pre></td></tr></table></figure></li>
<li>扩展语法<ul>
<li>Scala中一个源文件中可以声明多个公共类</li>
</ul>
</li>
</ol>
<h3 id="6-1-4-属性"><a href="#6-1-4-属性" class="headerlink" title="6.1.4 属性"></a>6.1.4 属性</h3><ul>
<li>属性和变量的声明和初始化必须放置在一起</li>
<li>Scala使用下划线<code>_</code>实现先声明后初始化的功能<ul>
<li>必须使用var声明才生效</li>
</ul>
</li>
</ul>
<ol>
<li>基本语法 <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">User</span> </span>&#123;</span><br><span class="line">    <span class="keyword">var</span> name : <span class="type">String</span> = _ <span class="comment">// 类属性其实就是类变量</span></span><br><span class="line">    <span class="keyword">var</span> age : <span class="type">Int</span> = _ <span class="comment">// 下划线表示类的属性默认初始化</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>扩展语法<ul>
<li>Scala中的属性其实在编译后也会生成对应的set(赋值)，get(取值)方法</li>
<li>访问属性其实是访问对应属性的方法，而不是访问属性本身</li>
<li>使用@BeanProperty 生成和Java统一的getter setter方法</li>
<li>属性使用val声明，编译时生成的属性是final修饰<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">User</span> </span>&#123;</span><br><span class="line">    <span class="keyword">var</span> name : <span class="type">String</span> = _</span><br><span class="line">    <span class="keyword">val</span> age : <span class="type">Int</span> = <span class="number">30</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">val</span> email : <span class="type">String</span> = _</span><br><span class="line">    <span class="meta">@BeanProperty</span> <span class="keyword">var</span> address : <span class="type">String</span> = _</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ol>
<h3 id="6-1-5-访问权限"><a href="#6-1-5-访问权限" class="headerlink" title="6.1.5 访问权限"></a>6.1.5 访问权限</h3><p>Scala中的访问权限和Java中的访问权限类似，但是又有区别：</p>
<ul>
<li>private : 私有访问权限</li>
<li>private[包名]: 包访问权限</li>
<li>protected : 受保护权限，不能同包</li>
<li>（default,不指定访问权限） : 公共访问权限</li>
</ul>
<h3 id="6-1-6-方法"><a href="#6-1-6-方法" class="headerlink" title="6.1.6 方法"></a>6.1.6 方法</h3><ul>
<li>Scala中的类的方法其实就是函数，所以声明方式完全一样，但是必须通过使用对象进行调用</li>
<li>方法能否使用取决于声明类型而不是对象</li>
<li>方法具体如何执行，由对象确定<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaMethod</span></span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> user = <span class="keyword">new</span> <span class="type">User</span></span><br><span class="line">        user.login(<span class="string">&quot;zhangsan&quot;</span>, <span class="string">&quot;000000&quot;</span>)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">User</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">login</span></span>( name:<span class="type">String</span>, password:<span class="type">String</span> ): <span class="type">Boolean</span> = &#123;</span><br><span class="line">        <span class="literal">false</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>思考两个问题: 还记得方法的重写和重载吗？ 你真的明白吗？</p>
<ol>
<li>双亲委派机制：JDK中的类和自定义类冲突，使用哪一个</li>
<li>动态绑定机制：JVM调用对象的成员方法，会将方法和实际内存对象进行绑定，然后调用<br>属性不存在动态绑定，在哪声明，在哪使用</li>
</ol>
</blockquote>
</li>
</ul>
<h3 id="6-1-7-对象"><a href="#6-1-7-对象" class="headerlink" title="6.1.7 对象"></a>6.1.7 对象</h3><p>Scala中的对象和Java是类似的</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> | <span class="keyword">var</span> 对象名 [：类型]  = <span class="keyword">new</span> 类型()</span><br><span class="line"><span class="keyword">var</span> user : <span class="type">User</span> = <span class="keyword">new</span> <span class="type">User</span>()</span><br></pre></td></tr></table></figure>

<h3 id="6-1-8-构造方法"><a href="#6-1-8-构造方法" class="headerlink" title="6.1.8 构造方法"></a>6.1.8 构造方法</h3><ul>
<li>和Java一样，Scala中构造对象也需要调用类的构造方法来创建。并且一个类中可以有任意多个不相同的构造方法。这些构造方法可以分为2大类：主构造函数和辅助构造函数。<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">User</span>(<span class="params"></span>) </span>&#123; <span class="comment">// 主构造函数</span></span><br><span class="line">    <span class="keyword">var</span> username : <span class="type">String</span> = _ </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">this</span></span>( name:<span class="type">String</span> ) &#123; <span class="comment">// 辅助构造函数，使用this关键字声明</span></span><br><span class="line">        <span class="keyword">this</span>() <span class="comment">// 辅助构造函数应该直接或间接调用主构造函数</span></span><br><span class="line">        username = name</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">this</span></span>( name:<span class="type">String</span>, password:<span class="type">String</span> ) &#123;</span><br><span class="line">        <span class="keyword">this</span>(name) <span class="comment">// 构造器调用其他另外的构造器，要求被调用构造器必须提前声明</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul>
<h2 id="6-2-高阶面向对象编程"><a href="#6-2-高阶面向对象编程" class="headerlink" title="6.2 高阶面向对象编程"></a>6.2 高阶面向对象编程</h2><h3 id="6-2-1-继承"><a href="#6-2-1-继承" class="headerlink" title="6.2.1 继承"></a>6.2.1 继承</h3><ol>
<li>和Java一样，Scala中的继承也是单继承，且使用extends关键字。 <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Person</span> </span>&#123;</span><br><span class="line">&#125;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">User</span> <span class="keyword">extends</span> <span class="title">Person</span> </span>&#123;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>构造对象时需要考虑构造方法的执行顺序</p>
</blockquote>
</li>
<li>使用overwrite关键字重写方法 <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">say</span></span>(): <span class="type">Unit</span> =&#123;</span><br><span class="line">    println(<span class="string">&quot;Person.say。。。。。。&quot;</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>构建子类对象时，父类对象会先构建出来，所有父类构造方法先执行，并先初始化</li>
<li>如果继承父类后，父类的构造方法有参数，那么需要在继承的时候，传递参数；一般不会写死，一般是从子类传递过来 <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Person</span> <span class="title">private</span>(<span class="params"></span>) </span>&#123;</span><br><span class="line">    println(<span class="string">&quot;Person init......&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">this</span></span>(name: <span class="type">String</span>) &#123;</span><br><span class="line">        <span class="keyword">this</span>()</span><br><span class="line">        println(<span class="string">&quot;Person this&quot;</span>)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">say</span></span>(): <span class="type">Unit</span> =&#123;</span><br><span class="line">        println(<span class="string">&quot;Person.say。。。。。。&quot;</span>)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">User</span>(<span class="params">var name: <span class="type">String</span></span>) <span class="keyword">extends</span> <span class="title">Person</span>(<span class="params">name</span>) </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">this</span></span>() &#123;</span><br><span class="line">        <span class="keyword">this</span>(<span class="string">&quot;母鸡&quot;</span>)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">say</span></span>(): <span class="type">Unit</span> =&#123;</span><br><span class="line">        println(<span class="string">&quot;Person.say。。。。。。&quot;</span>)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    println(<span class="string">&quot;User init......&quot;</span>)</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="6-2-2-封装"><a href="#6-2-2-封装" class="headerlink" title="6.2.2 封装"></a>6.2.2 封装</h3><p>封装就是把抽象出的数据和对数据的操作封装在一起，数据被保护在内部，程序的其它部分只有通过被授权的操作（成员方法），才能对数据进行访问。</p>
<ul>
<li>将属性进行私有化</li>
<li>提供一个公共的set方法，用于对属性赋值</li>
<li>提供一个公共的get方法，用于获取属性的值</li>
</ul>
<h3 id="6-2-3-抽象"><a href="#6-2-3-抽象" class="headerlink" title="6.2.3 抽象"></a>6.2.3 抽象</h3><ul>
<li>Scala将一个不完整的类称之为抽象类。  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">Person</span> </span>&#123;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>Scala中如果一个方法<font color ='red' >只有声明而没有实现</font>，那么是抽象方法，因为它不完整。<ul>
<li>Scala中抽象方法不需要使用关键字abstract<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaAbstractClass</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> user:<span class="type">Person</span> = <span class="keyword">new</span> <span class="type">User</span>()</span><br><span class="line">        user.eat()</span><br><span class="line">        <span class="keyword">val</span> person = <span class="keyword">new</span> <span class="type">Person</span> &#123;</span><br><span class="line">            <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">eat</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">                println(<span class="string">&quot;anonymous Person.eat&quot;</span>)</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        person.eat()</span><br><span class="line">        user.drink()</span><br><span class="line">        person.drink()</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">Person</span> </span>&#123;</span><br><span class="line">        <span class="comment">//抽象方法</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">eat</span></span>(): <span class="type">Unit</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">drink</span></span>(): <span class="type">Unit</span> =&#123;</span><br><span class="line">            println(<span class="string">&quot;Person.drink&quot;</span>)</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="class"><span class="keyword">class</span> <span class="title">User</span> <span class="keyword">extends</span> <span class="title">Person</span></span>&#123;</span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">eat</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">            println(<span class="string">&quot;User.eat&quot;</span>)</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">drink</span></span>(): <span class="type">Unit</span> =&#123;</span><br><span class="line">            println(<span class="string">&quot;User.drink&quot;</span>)</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>Scala中如果一个属性只有声明没有初始化，那么是抽象属性，因为它不完整。<ul>
<li>抽象属性编译后并不会生成java类的属性，而是生成属性对应的set,get方法，且是抽象的</li>
<li>子类通过override补全父类抽象属性</li>
<li>可变变量不能被冲重写，不可变变量可以被重写，Scala中访问一个对象的属性，并不是访问这个属性本身，而是访问这个属性的get方法，而get方法是成员方法，所以会遵循动态绑定机制<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaAbstractField</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> user = <span class="keyword">new</span> <span class="type">User</span>()</span><br><span class="line">        user.sex = <span class="string">&quot;男&quot;</span></span><br><span class="line">        println(user.sex)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">Person</span> </span>&#123;</span><br><span class="line">        <span class="keyword">var</span> sex: <span class="type">String</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="class"><span class="keyword">class</span> <span class="title">User</span> <span class="keyword">extends</span> <span class="title">Person</span> </span>&#123;</span><br><span class="line">        <span class="keyword">override</span> <span class="keyword">var</span> sex: <span class="type">String</span> = _</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>子类如果继承抽象类，必须实现抽象方法或补全抽象属性，否则也必须声明为抽象的，因为依然不完整。  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">Person</span> </span>&#123;</span><br><span class="line">    <span class="keyword">var</span> name:<span class="type">String</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">User</span> <span class="keyword">extends</span> <span class="title">Person</span> </span>&#123;</span><br><span class="line">    <span class="keyword">var</span> name : <span class="type">String</span> = <span class="string">&quot;zhangsan&quot;</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="6-2-4-单例对象"><a href="#6-2-4-单例对象" class="headerlink" title="6.2.4 单例对象"></a>6.2.4 单例对象</h3><ul>
<li>所谓的单例对象，就是在程序运行过程中，指定类的对象只能创建一个，而不能创建多个。这样的对象可以由特殊的设计方式获得，也可以由语言本身设计得到，比如object伴生对象</li>
<li>Scala语言是完全面向对象的语言，所以并没有静态的操作（即在Scala中没有静态的概念）。但是为了能够和Java语言交互（因为Java中有静态概念），就产生了一种特殊的对象来模拟类对象，该对象为单例对象。若单例对象名与类名一致，则称该单例对象这个类的伴生对象，这个类的所有“静态”内容都可以放置在它的伴生对象中声明，然后通过伴生对象名称直接调用</li>
<li>如果类名和伴生对象名称保持一致，那么这个类称之为伴生类。Scala编译器可以通过伴生对象的apply方法创建伴生类对象。apply方法可以重载，并传递参数，且可由Scala编译器自动识别。所以在使用时，其实是可以省略的。  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">User</span> </span>&#123; <span class="comment">// 伴生类</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">User</span> </span>&#123; <span class="comment">// 伴生对象</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">apply</span></span>() = <span class="keyword">new</span> <span class="type">User</span>() <span class="comment">// 构造伴生类对象</span></span><br><span class="line">&#125;</span><br><span class="line">...</span><br><span class="line"><span class="keyword">val</span> user1 = <span class="keyword">new</span> <span class="type">User</span>()<span class="comment">// 通过构造方法创建对象</span></span><br><span class="line"><span class="type">Val</span> user2 = <span class="type">User</span>.apply() <span class="comment">// 通过伴生对象的apply方法构造伴生类对象 </span></span><br><span class="line"><span class="keyword">val</span> user3 = <span class="type">User</span>() <span class="comment">// scala编译器省略apply方法，自动完成调用</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>思考一个问题: Thread线程中wait方法和sleep方法的区别？</p>
<ol>
<li>sleep是Thread类的静态方法</li>
</ol>
<ul>
<li>静态方法和对象无关，所以跟锁无关</li>
</ul>
<ol start="2">
<li>wait是Object类的成员方法</li>
</ol>
</blockquote>
</li>
</ul>
<h3 id="6-2-5-特质"><a href="#6-2-5-特质" class="headerlink" title="6.2.5 特质"></a>6.2.5 特质</h3><p>Scala将多个类的相同特征从类中剥离出来，形成一个独立的语法结构，称之为“特质”（特征）。这种方式在Java中称之为接口，但是Scala中没有接口的概念。所以scala中没有interface关键字，而是采用特殊的关键字trait来声明特质, 如果一个类符合某一个特征（特质），那么就可以将这个特征（特质）“混入”到类中。这种混入的操作可以在声明类时使用，也可以在创建类对象时动态使用。</p>
<ol>
<li>基本语法 <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">特质名称</span></span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">类名</span> <span class="keyword">extends</span> <span class="title">父类（特质1）</span> <span class="keyword">with</span> <span class="title">特质2</span> <span class="keyword">with</span><span class="title">特质3</span></span></span><br><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">Operator</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">DB</span></span>&#123;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MySQL</span> <span class="keyword">extends</span> <span class="title">Operator</span> <span class="keyword">with</span> <span class="title">DB</span></span>&#123;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>思考一个问题: 特质到底是什么？为什么又可以使用extends，又可以使用with？</p>
<ol>
<li>如果一个类没有父类，但是符合特征，trait相当于抽象类</li>
<li>如果一个类有父类，且符合特征，trait相当于接口</li>
</ol>
</blockquote>
</li>
<li>动态混入：<code>with trait</code> <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaTrait</span></span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> mysql = <span class="keyword">new</span> <span class="type">MySQL</span> <span class="keyword">with</span> <span class="type">Operator</span></span><br><span class="line">        mysql.insert()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">Operator</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">insert</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">        println(<span class="string">&quot;insert data...&quot;</span>)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MySQL</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>初始化叠加 <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaTrait</span></span>&#123;</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> mysql = <span class="keyword">new</span> <span class="type">MySQL</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">Operator</span> </span>&#123;</span><br><span class="line">    println(<span class="string">&quot;operator...&quot;</span>)</span><br><span class="line">&#125;</span><br><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">DB</span> </span>&#123;</span><br><span class="line">    println(<span class="string">&quot;db...&quot;</span>)</span><br><span class="line">&#125;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MySQL</span> <span class="keyword">extends</span> <span class="title">DB</span> <span class="keyword">with</span> <span class="title">Operator</span></span>&#123;</span><br><span class="line">    println(<span class="string">&quot;mysql...&quot;</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>功能叠加 <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaTrait</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> mysql: <span class="type">MySQL</span> = <span class="keyword">new</span> <span class="type">MySQL</span></span><br><span class="line">        mysql.operData()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">Operate</span></span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">operData</span></span>():<span class="type">Unit</span>=&#123;</span><br><span class="line">        println(<span class="string">&quot;操作数据。。&quot;</span>)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">DB</span> <span class="keyword">extends</span> <span class="title">Operate</span></span>&#123;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">operData</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">        print(<span class="string">&quot;向数据库中。。&quot;</span>)</span><br><span class="line">        <span class="keyword">super</span>.operData()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">Log</span> <span class="keyword">extends</span> <span class="title">Operate</span></span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">operData</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">super</span>.operData()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MySQL</span> <span class="keyword">extends</span> <span class="title">DB</span> <span class="keyword">with</span> <span class="title">Log</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>思考一个问题: scala中的super是什么？<br>上一个层级</p>
</blockquote>
</li>
</ol>
<h3 id="6-2-6-扩展"><a href="#6-2-6-扩展" class="headerlink" title="6.2.6 扩展"></a>6.2.6 扩展</h3><ul>
<li><p>类型检查和转换</p>
  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Person</span></span>&#123;</span><br><span class="line">&#125;</span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Person</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> person = <span class="keyword">new</span> <span class="type">Person</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">//（1）判断对象是否为某个类型的实例</span></span><br><span class="line">        <span class="keyword">val</span> bool: <span class="type">Boolean</span> = person.isInstanceOf[<span class="type">Person</span>]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> ( bool ) &#123;</span><br><span class="line">            <span class="comment">//（2）将对象转换为某个类型的实例</span></span><br><span class="line">            <span class="keyword">val</span> p1: <span class="type">Person</span> = person.asInstanceOf[<span class="type">Person</span>]</span><br><span class="line">            println(p1)</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//（3）获取类的信息</span></span><br><span class="line">        <span class="keyword">val</span> pClass: <span class="type">Class</span>[<span class="type">Person</span>] = classOf[<span class="type">Person</span>]</span><br><span class="line">        println(pClass)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>思考一个问题: 字符串真的不可变吗？</p>
<ol>
<li>字节数组引用不可变</li>
<li>字节数组内容可变</li>
</ol>
</blockquote>
</li>
<li><p>枚举类和应用类</p>
  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Test</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        println(<span class="type">Color</span>.<span class="type">RED</span>)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 枚举类</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Color</span> <span class="keyword">extends</span> <span class="title">Enumeration</span> </span>&#123;</span><br><span class="line">    <span class="keyword">val</span> <span class="type">RED</span> = <span class="type">Value</span>(<span class="number">1</span>, <span class="string">&quot;red&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> <span class="type">YELLOW</span> = <span class="type">Value</span>(<span class="number">2</span>, <span class="string">&quot;yellow&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> <span class="type">BLUE</span> = <span class="type">Value</span>(<span class="number">3</span>, <span class="string">&quot;blue&quot;</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 应用类</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">AppTest</span> <span class="keyword">extends</span> <span class="title">App</span> </span>&#123;</span><br><span class="line">    println(<span class="string">&quot;application&quot;</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li><p>Type定义新类型<br>使用type关键字可以定义新的数据数据类型名称，本质上就是类型的一个别名</p>
  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Test</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="class"><span class="keyword">type</span> <span class="title">S</span> </span>= <span class="type">String</span></span><br><span class="line">        <span class="keyword">var</span> v : <span class="type">S</span> = <span class="string">&quot;abc&quot;</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul>
<h1 id="第7章-集合"><a href="#第7章-集合" class="headerlink" title="第7章 集合"></a>第7章 集合</h1><h3 id="7-1-简介"><a href="#7-1-简介" class="headerlink" title="7.1 简介"></a>7.1 简介</h3><ul>
<li>Scala的集合有三大类：序列Seq、集Set、映射Map，所有的集合都扩展自Iterable特质。对于几乎所有的集合类，Scala都同时提供了可变和不可变的版本。</li>
<li>可变集合可以在适当的地方被更新或扩展。这意味着你可以修改，添加，移除一个集合的元素。而不可变集合类，相比之下，永远不会改变。不过，你仍然可以模拟添加，移除或更新操作。但是这些操作将在每一种情况下都返回一个新的集合，同时使原来的集合不发生改变，所以这里的不可变并不是变量本身的值不可变，而是变量指向的那个内存地址不可变</li>
<li>可变集合和不可变集合，在scala中该如何进行区分呢？我们一般可以根据集合所在包名进行区分:</li>
<li>scala.collection.immutable</li>
<li>scala.collection.mutable</li>
</ul>
<h2 id="7-2-数组"><a href="#7-2-数组" class="headerlink" title="7.2 数组"></a>7.2 数组</h2><h3 id="7-2-1-不可变数组"><a href="#7-2-1-不可变数组" class="headerlink" title="7.2.1 不可变数组"></a>7.2.1 不可变数组</h3><ol>
<li>基本语法 <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaCollection</span></span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="comment">//（1）数组定义</span></span><br><span class="line">        <span class="keyword">val</span> arr01 = <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">Int</span>](<span class="number">4</span>)</span><br><span class="line">        println(arr01.length) <span class="comment">// 4</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">//（2）数组赋值</span></span><br><span class="line">        <span class="comment">//（2.1）修改某个元素的值</span></span><br><span class="line">        arr01(<span class="number">3</span>) = <span class="number">10</span></span><br><span class="line">        <span class="keyword">val</span> i = <span class="number">10</span></span><br><span class="line">        arr01(i/<span class="number">3</span>) = <span class="number">20</span></span><br><span class="line">        <span class="comment">//（2.2）采用方法的形式修改数组的值</span></span><br><span class="line">        arr01.update(<span class="number">0</span>,<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">//（3）遍历数组</span></span><br><span class="line">        <span class="comment">//（3.1）查看数组</span></span><br><span class="line">        println(arr01.mkString(<span class="string">&quot;,&quot;</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment">//（3.2）普通遍历</span></span><br><span class="line">        <span class="keyword">for</span> (i &lt;- arr01) &#123;</span><br><span class="line">            println(i)</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//（3.3）简化遍历</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">printx</span></span>(elem:<span class="type">Int</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">            println(elem)</span><br><span class="line">        &#125;</span><br><span class="line">        arr01.foreach(printx)</span><br><span class="line">        arr01.foreach((x)=&gt;&#123;println(x)&#125;)</span><br><span class="line">        arr01.foreach(println(_))</span><br><span class="line">        arr01.foreach(println)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>基本操作 <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaCollection</span></span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="comment">// 创建数组的另外一种方式</span></span><br><span class="line">        <span class="keyword">val</span> arr1 = <span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line">        <span class="keyword">val</span> arr2 = <span class="type">Array</span>(<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>)</span><br><span class="line">        <span class="comment">// 添加数组元素，创建新数组</span></span><br><span class="line">        <span class="keyword">val</span> arr3: <span class="type">Array</span>[<span class="type">Int</span>] = arr1 :+ <span class="number">5</span></span><br><span class="line">        println( arr1 eq arr3 ) <span class="comment">// false</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> arr4: <span class="type">Array</span>[<span class="type">Int</span>] = arr1 ++: arr2</span><br><span class="line">        <span class="comment">// 添加集合</span></span><br><span class="line">        <span class="keyword">val</span> arr5: <span class="type">Array</span>[<span class="type">Int</span>] = arr1 ++ arr2</span><br><span class="line"></span><br><span class="line">        arr4.foreach(println)</span><br><span class="line">        println(<span class="string">&quot;****************&quot;</span>)</span><br><span class="line">        arr5.foreach(println)</span><br><span class="line">        println(<span class="string">&quot;****************&quot;</span>)</span><br><span class="line">        <span class="comment">// 多维数组</span></span><br><span class="line">        <span class="keyword">var</span> myMatrix = <span class="type">Array</span>.ofDim[<span class="type">Int</span>](<span class="number">3</span>,<span class="number">3</span>)</span><br><span class="line">        myMatrix.foreach(list=&gt;list.foreach(println))</span><br><span class="line">        <span class="comment">// 合并数组</span></span><br><span class="line">        <span class="keyword">val</span> arr6: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>.concat(arr1, arr2)</span><br><span class="line">        arr6.foreach(println)</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 创建指定范围的数组</span></span><br><span class="line">        <span class="keyword">val</span> arr7: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>.range(<span class="number">0</span>,<span class="number">2</span>)</span><br><span class="line">        arr7.foreach(println)</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 创建并填充指定数量的数组</span></span><br><span class="line">        <span class="keyword">val</span> arr8:<span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>.fill[<span class="type">Int</span>](<span class="number">5</span>)(<span class="number">-1</span>)</span><br><span class="line">        arr8.foreach(println)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="7-2-2-可变数组"><a href="#7-2-2-可变数组" class="headerlink" title="7.2.2 可变数组"></a>7.2.2 可变数组</h3><ol>
<li>基本语法 <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> scala.collection.mutable.<span class="type">ArrayBuffer</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaCollection</span></span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> buffer = <span class="keyword">new</span> <span class="type">ArrayBuffer</span>[<span class="type">Int</span>]</span><br><span class="line">        <span class="comment">// 增加数据</span></span><br><span class="line">        buffer.append(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line">        <span class="comment">// 修改数据</span></span><br><span class="line">        buffer.update(<span class="number">0</span>,<span class="number">5</span>)</span><br><span class="line">        buffer(<span class="number">1</span>) = <span class="number">6</span></span><br><span class="line">        <span class="comment">// 删除数据</span></span><br><span class="line">        <span class="keyword">val</span> i: <span class="type">Int</span> = buffer.remove(<span class="number">2</span>)</span><br><span class="line">        buffer.remove(<span class="number">2</span>,<span class="number">2</span>)</span><br><span class="line">        <span class="comment">// 查询数据</span></span><br><span class="line">        println(buffer(<span class="number">3</span>))</span><br><span class="line">        <span class="comment">// 循环集合</span></span><br><span class="line">        <span class="keyword">for</span> ( i &lt;- buffer ) &#123;</span><br><span class="line">            println(i)</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>基本操作 <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> scala.collection.mutable.<span class="type">ArrayBuffer</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaCollection</span></span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> buffer1 = <span class="type">ArrayBuffer</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line">        <span class="keyword">val</span> buffer2 = <span class="type">ArrayBuffer</span>(<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> buffer3: <span class="type">ArrayBuffer</span>[<span class="type">Int</span>] = buffer1 += <span class="number">5</span></span><br><span class="line">        println( buffer1 eq buffer3 ) <span class="comment">// true</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 使用 ++ 运算符会产生新的集合数组</span></span><br><span class="line">        <span class="keyword">val</span> buffer4: <span class="type">ArrayBuffer</span>[<span class="type">Int</span>] = buffer1 ++ buffer2</span><br><span class="line">        <span class="comment">// 使用 ++= 运算符会更新之前的集合，不会产生新的数组</span></span><br><span class="line">        <span class="keyword">val</span> buffer5: <span class="type">ArrayBuffer</span>[<span class="type">Int</span>] = buffer1 ++= buffer2</span><br><span class="line">        println( buffer1 eq buffer4 ) <span class="comment">// false</span></span><br><span class="line">        println( buffer1 eq buffer5 ) <span class="comment">// true</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="7-2-3-可变数组和不可变数组转换"><a href="#7-2-3-可变数组和不可变数组转换" class="headerlink" title="7.2.3 可变数组和不可变数组转换"></a>7.2.3 可变数组和不可变数组转换</h3><pre><code><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> scala.collection.mutable</span><br><span class="line"><span class="keyword">import</span> scala.collection.mutable.<span class="type">ArrayBuffer</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaCollection</span></span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> buffer = <span class="type">ArrayBuffer</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line">        <span class="keyword">val</span> array = <span class="type">Array</span>(<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 将不可变数组转换为可变数组</span></span><br><span class="line">        <span class="keyword">val</span> buffer1: mutable.<span class="type">Buffer</span>[<span class="type">Int</span>] = array.toBuffer</span><br><span class="line">        <span class="comment">// 将可变数组转换为不可变数组</span></span><br><span class="line">        <span class="keyword">val</span> array1: <span class="type">Array</span>[<span class="type">Int</span>] = buffer.toArray</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</code></pre>
<h2 id="7-3-Seq集合"><a href="#7-3-Seq集合" class="headerlink" title="7.3 Seq集合"></a>7.3 Seq集合</h2><h3 id="7-3-1-不可变List"><a href="#7-3-1-不可变List" class="headerlink" title="7.3.1 不可变List"></a>7.3.1 不可变List</h3><ol>
<li>基本语法 <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaCollection</span></span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Seq集合</span></span><br><span class="line">        <span class="keyword">val</span> list = <span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 增加数据</span></span><br><span class="line">        <span class="keyword">val</span> list1: <span class="type">List</span>[<span class="type">Int</span>] = list :+ <span class="number">1</span></span><br><span class="line">        println(list1 eq list)</span><br><span class="line">        list1.foreach(println)</span><br><span class="line">        <span class="keyword">val</span> list2: <span class="type">List</span>[<span class="type">Int</span>] = <span class="number">1</span> +: list</span><br><span class="line">        list2.foreach(println)</span><br><span class="line">        println(<span class="string">&quot;*****************&quot;</span>)</span><br><span class="line">        <span class="keyword">val</span> list3: <span class="type">List</span>[<span class="type">Int</span>] = list.updated(<span class="number">1</span>,<span class="number">5</span>)</span><br><span class="line">        println(list eq list3)</span><br><span class="line">        <span class="type">List3</span>.foreach(println)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>基本操作 <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaCollection</span></span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Seq集合</span></span><br><span class="line">        <span class="keyword">val</span> list1 = <span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line">        <span class="comment">// 空集合</span></span><br><span class="line">        <span class="keyword">val</span> list2: <span class="type">List</span>[<span class="type">Nothing</span>] = <span class="type">List</span>()</span><br><span class="line">        <span class="keyword">val</span> nil  = <span class="type">Nil</span></span><br><span class="line">        println(list2 eq nil)</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 创建集合</span></span><br><span class="line">        <span class="keyword">val</span> list3: <span class="type">List</span>[<span class="type">Int</span>]  = <span class="number">1</span>::<span class="number">2</span>::<span class="number">3</span>::<span class="type">Nil</span></span><br><span class="line">        <span class="keyword">val</span> list4: <span class="type">List</span>[<span class="type">Int</span>] = list1 ::: <span class="type">Nil</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 连接集合</span></span><br><span class="line">        <span class="keyword">val</span> list5: <span class="type">List</span>[<span class="type">Int</span>] = <span class="type">List</span>.concat(list3, list4)</span><br><span class="line">        list5.foreach(println)</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 创建一个指定重复数量的元素列表</span></span><br><span class="line">        <span class="keyword">val</span> list6: <span class="type">List</span>[<span class="type">String</span>] = <span class="type">List</span>.fill[<span class="type">String</span>](<span class="number">3</span>)(<span class="string">&quot;a&quot;</span>)</span><br><span class="line">        list6.foreach(println)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="7-3-2-可变List"><a href="#7-3-2-可变List" class="headerlink" title="7.3.2 可变List"></a>7.3.2 可变List</h3><ol>
<li>基本语法 <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> scala.collection.mutable.<span class="type">ListBuffer</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaCollection</span></span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="comment">// 可变集合</span></span><br><span class="line">        <span class="keyword">val</span> buffer = <span class="keyword">new</span> <span class="type">ListBuffer</span>[<span class="type">Int</span>]()</span><br><span class="line">        <span class="comment">// 增加数据</span></span><br><span class="line">        buffer.append(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line">        <span class="comment">// 修改数据</span></span><br><span class="line">        buffer.update(<span class="number">1</span>,<span class="number">3</span>)</span><br><span class="line">        <span class="comment">// 删除数据</span></span><br><span class="line">        buffer.remove(<span class="number">2</span>)</span><br><span class="line">        buffer.remove(<span class="number">2</span>,<span class="number">2</span>)</span><br><span class="line">        <span class="comment">// 获取数据</span></span><br><span class="line">        println(buffer(<span class="number">1</span>))</span><br><span class="line">        <span class="comment">// 遍历集合</span></span><br><span class="line">        buffer.foreach(println)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>基本操作 <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> scala.collection.mutable.<span class="type">ListBuffer</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaCollection</span></span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 可变集合</span></span><br><span class="line">        <span class="keyword">val</span> buffer1 = <span class="type">ListBuffer</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line">        <span class="keyword">val</span> buffer2 = <span class="type">ListBuffer</span>(<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 增加数据</span></span><br><span class="line">        <span class="keyword">val</span> buffer3: <span class="type">ListBuffer</span>[<span class="type">Int</span>] = buffer1 :+ <span class="number">5</span></span><br><span class="line">        <span class="keyword">val</span> buffer4: <span class="type">ListBuffer</span>[<span class="type">Int</span>] = buffer1 += <span class="number">5</span></span><br><span class="line">        <span class="keyword">val</span> buffer5: <span class="type">ListBuffer</span>[<span class="type">Int</span>] = buffer1 ++ buffer2</span><br><span class="line">        <span class="keyword">val</span> buffer6: <span class="type">ListBuffer</span>[<span class="type">Int</span>] = buffer1 ++= buffer2</span><br><span class="line"></span><br><span class="line">        println( buffer5 eq buffer1 )</span><br><span class="line">        println( buffer6 eq buffer1 )</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> buffer7: <span class="type">ListBuffer</span>[<span class="type">Int</span>] = buffer1 - <span class="number">2</span></span><br><span class="line">        <span class="keyword">val</span> buffer8: <span class="type">ListBuffer</span>[<span class="type">Int</span>] = buffer1 -= <span class="number">2</span></span><br><span class="line">        println( buffer7 eq buffer1 )</span><br><span class="line">        println( buffer8 eq buffer1 )</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="7-3-3-可变集合和不可变集合转换"><a href="#7-3-3-可变集合和不可变集合转换" class="headerlink" title="7.3.3 可变集合和不可变集合转换"></a>7.3.3 可变集合和不可变集合转换</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> scala.collection.mutable</span><br><span class="line"><span class="keyword">import</span> scala.collection.mutable.<span class="type">ListBuffer</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaCollection</span></span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> buffer = <span class="type">ListBuffer</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line">        <span class="keyword">val</span> list = <span class="type">List</span>(<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>)</span><br><span class="line"> </span><br><span class="line">        <span class="comment">// 可变集合转变为不可变集合</span></span><br><span class="line">        <span class="keyword">val</span> list1: <span class="type">List</span>[<span class="type">Int</span>] = buffer.toList</span><br><span class="line">        <span class="comment">// 不可变集合转变为可变集合</span></span><br><span class="line">        <span class="keyword">val</span> buffer1: mutable.<span class="type">Buffer</span>[<span class="type">Int</span>] = list.toBuffer</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="7-4-Set集合"><a href="#7-4-Set集合" class="headerlink" title="7.4 Set集合"></a>7.4 Set集合</h2><h3 id="7-4-1-不可变Set"><a href="#7-4-1-不可变Set" class="headerlink" title="7.4.1 不可变Set"></a>7.4.1 不可变Set</h3><ol>
<li>基本语法 <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaCollection</span></span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> set1 = <span class="type">Set</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line">        <span class="keyword">val</span> set2 = <span class="type">Set</span>(<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 增加数据</span></span><br><span class="line">        <span class="keyword">val</span> set3: <span class="type">Set</span>[<span class="type">Int</span>] = set1 + <span class="number">5</span> + <span class="number">6</span></span><br><span class="line">        <span class="keyword">val</span> set4: <span class="type">Set</span>[<span class="type">Int</span>] = set1.+(<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>)</span><br><span class="line">        println( set1 eq set3 ) <span class="comment">// false</span></span><br><span class="line">        println( set1 eq set4 ) <span class="comment">// false</span></span><br><span class="line">        set4.foreach(println)</span><br><span class="line">        <span class="comment">// 删除数据</span></span><br><span class="line">        <span class="keyword">val</span> set5: <span class="type">Set</span>[<span class="type">Int</span>] = set1 - <span class="number">2</span> - <span class="number">3</span></span><br><span class="line">        set5.foreach(println)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> set6: <span class="type">Set</span>[<span class="type">Int</span>] = set1 ++ set2</span><br><span class="line">        set6.foreach(println)</span><br><span class="line">        println(<span class="string">&quot;********&quot;</span>)</span><br><span class="line">        <span class="keyword">val</span> set7: <span class="type">Set</span>[<span class="type">Int</span>] = set2 ++: set1</span><br><span class="line">        set7.foreach(println)</span><br><span class="line">        println(set6 eq set7)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>基本操作 <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaCollection</span></span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> set1 = <span class="type">Set</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line">        <span class="keyword">val</span> set2 = <span class="type">Set</span>(<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 增加数据</span></span><br><span class="line">        <span class="keyword">val</span> set3: <span class="type">Set</span>[<span class="type">Int</span>] = set1 + <span class="number">5</span> + <span class="number">6</span></span><br><span class="line">        <span class="keyword">val</span> set4: <span class="type">Set</span>[<span class="type">Int</span>] = set1.+(<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>)</span><br><span class="line">        println( set1 eq set3 ) <span class="comment">// false</span></span><br><span class="line">        println( set1 eq set4 ) <span class="comment">// false</span></span><br><span class="line">        set4.foreach(println)</span><br><span class="line">        <span class="comment">// 删除数据</span></span><br><span class="line">        <span class="keyword">val</span> set5: <span class="type">Set</span>[<span class="type">Int</span>] = set1 - <span class="number">2</span> - <span class="number">3</span></span><br><span class="line">        set5.foreach(println)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> set6: <span class="type">Set</span>[<span class="type">Int</span>] = set1 ++ set2</span><br><span class="line">        set6.foreach(println)</span><br><span class="line">        println(<span class="string">&quot;********&quot;</span>)</span><br><span class="line">        <span class="keyword">val</span> set7: <span class="type">Set</span>[<span class="type">Int</span>] = set2 ++: set1</span><br><span class="line">        set7.foreach(println)</span><br><span class="line">        println(set6 eq set7)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="7-4-2-可变Set"><a href="#7-4-2-可变Set" class="headerlink" title="7.4.2 可变Set"></a>7.4.2 可变Set</h3><ol>
<li>基本语法 <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> scala.collection.mutable</span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaCollection</span></span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> set1 = mutable.<span class="type">Set</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line">        <span class="keyword">val</span> set2 = mutable.<span class="type">Set</span>(<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 增加数据</span></span><br><span class="line">        set1.add(<span class="number">5</span>)</span><br><span class="line">        <span class="comment">// 添加数据</span></span><br><span class="line">        set1.update(<span class="number">6</span>,<span class="literal">true</span>)</span><br><span class="line">        println(set1.mkString(<span class="string">&quot;,&quot;</span>))</span><br><span class="line">        <span class="comment">// 删除数据</span></span><br><span class="line">        set1.update(<span class="number">3</span>,<span class="literal">false</span>)</span><br><span class="line">        println(set1.mkString(<span class="string">&quot;,&quot;</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 删除数据</span></span><br><span class="line">        set1.remove(<span class="number">2</span>)</span><br><span class="line">        println(set1.mkString(<span class="string">&quot;,&quot;</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 遍历数据</span></span><br><span class="line">        set1.foreach(println)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>基本操作 <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> scala.collection.mutable</span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaCollection</span></span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> set1 = mutable.<span class="type">Set</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line">        <span class="keyword">val</span> set2 = mutable.<span class="type">Set</span>(<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 交集</span></span><br><span class="line">        <span class="keyword">val</span> set3: mutable.<span class="type">Set</span>[<span class="type">Int</span>] = set1 &amp; set2</span><br><span class="line">        println(set3.mkString(<span class="string">&quot;,&quot;</span>))</span><br><span class="line">        <span class="comment">// 差集</span></span><br><span class="line">        <span class="keyword">val</span> set4: mutable.<span class="type">Set</span>[<span class="type">Int</span>] = set1 &amp;~ set2</span><br><span class="line">        println(set4.mkString(<span class="string">&quot;,&quot;</span>))</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="7-5-Map集合"><a href="#7-5-Map集合" class="headerlink" title="7.5 Map集合"></a>7.5 Map集合</h2><p>Map(映射)是一种可迭代的键值对（key/value）结构。所有的值都可以通过键来获取。Map 中的键都是唯一的。</p>
<h3 id="7-5-1-不可变Map"><a href="#7-5-1-不可变Map" class="headerlink" title="7.5.1 不可变Map"></a>7.5.1 不可变Map</h3><ol>
<li>基本语法 <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaCollection</span></span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> map1 = <span class="type">Map</span>( <span class="string">&quot;a&quot;</span> -&gt; <span class="number">1</span>, <span class="string">&quot;b&quot;</span> -&gt; <span class="number">2</span>, <span class="string">&quot;c&quot;</span> -&gt; <span class="number">3</span> )</span><br><span class="line">        <span class="keyword">val</span> map2 = <span class="type">Map</span>( <span class="string">&quot;d&quot;</span> -&gt; <span class="number">4</span>, <span class="string">&quot;e&quot;</span> -&gt; <span class="number">5</span>, <span class="string">&quot;f&quot;</span> -&gt; <span class="number">6</span> )</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 添加数据</span></span><br><span class="line">        <span class="keyword">val</span> map3 = map1 + (<span class="string">&quot;d&quot;</span> -&gt; <span class="number">4</span>)</span><br><span class="line">        println(map1 eq map3) <span class="comment">// false</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 删除数据</span></span><br><span class="line">        <span class="keyword">val</span> map4 = map3 - <span class="string">&quot;d&quot;</span></span><br><span class="line">        println(map4.mkString(<span class="string">&quot;,&quot;</span>))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> map5: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">Int</span>] = map1 ++ map2</span><br><span class="line">        println(map5 eq map1)</span><br><span class="line">        println(map5.mkString(<span class="string">&quot;,&quot;</span>))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> map6: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">Int</span>] = map1 ++: map2</span><br><span class="line">        println(map6 eq map1)</span><br><span class="line">        println(map6.mkString(<span class="string">&quot;,&quot;</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 修改数据</span></span><br><span class="line">        <span class="keyword">val</span> map7: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">Int</span>] = map1.updated(<span class="string">&quot;b&quot;</span>, <span class="number">5</span>)</span><br><span class="line">        println(map7.mkString(<span class="string">&quot;,&quot;</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 遍历数据</span></span><br><span class="line">        map1.foreach(println)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>基本操作 <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaCollection</span></span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> map1 = <span class="type">Map</span>( <span class="string">&quot;a&quot;</span> -&gt; <span class="number">1</span>, <span class="string">&quot;b&quot;</span> -&gt; <span class="number">2</span>, <span class="string">&quot;c&quot;</span> -&gt; <span class="number">3</span> )</span><br><span class="line">        <span class="keyword">val</span> map2 = <span class="type">Map</span>( <span class="string">&quot;d&quot;</span> -&gt; <span class="number">4</span>, <span class="string">&quot;e&quot;</span> -&gt; <span class="number">5</span>, <span class="string">&quot;f&quot;</span> -&gt; <span class="number">6</span> )</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 创建空集合</span></span><br><span class="line">        <span class="keyword">val</span> empty: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">Int</span>] = <span class="type">Map</span>.empty</span><br><span class="line">        println(empty)</span><br><span class="line">        <span class="comment">// 获取指定key的值</span></span><br><span class="line">        <span class="keyword">val</span> i: <span class="type">Int</span> = map1.apply(<span class="string">&quot;c&quot;</span>)</span><br><span class="line">        println(i)</span><br><span class="line">        println(map1(<span class="string">&quot;c&quot;</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 获取可能存在的key值</span></span><br><span class="line">        <span class="keyword">val</span> maybeInt: <span class="type">Option</span>[<span class="type">Int</span>] = map1.get(<span class="string">&quot;c&quot;</span>)</span><br><span class="line">        <span class="comment">// 判断key值是否存在</span></span><br><span class="line">        <span class="keyword">if</span> ( !maybeInt.isEmpty ) &#123;</span><br><span class="line">            <span class="comment">// 获取值</span></span><br><span class="line">            println(maybeInt.get)</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="comment">// 如果不存在，获取默认值</span></span><br><span class="line">            println(maybeInt.getOrElse(<span class="number">0</span>))</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 获取可能存在的key值, 如果不存在就使用默认值</span></span><br><span class="line">        println(map1.getOrElse(<span class="string">&quot;c&quot;</span>, <span class="number">0</span>))</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="7-5-2-可变Map"><a href="#7-5-2-可变Map" class="headerlink" title="7.5.2 可变Map"></a>7.5.2 可变Map</h3><ol>
<li>基本语法 <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> scala.collection.mutable</span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaCollection</span></span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> map1 = mutable.<span class="type">Map</span>( <span class="string">&quot;a&quot;</span> -&gt; <span class="number">1</span>, <span class="string">&quot;b&quot;</span> -&gt; <span class="number">2</span>, <span class="string">&quot;c&quot;</span> -&gt; <span class="number">3</span> )</span><br><span class="line">        <span class="keyword">val</span> map2 = mutable.<span class="type">Map</span>( <span class="string">&quot;d&quot;</span> -&gt; <span class="number">4</span>, <span class="string">&quot;e&quot;</span> -&gt; <span class="number">5</span>, <span class="string">&quot;f&quot;</span> -&gt; <span class="number">6</span> )</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 添加数据</span></span><br><span class="line">        map1.put(<span class="string">&quot;d&quot;</span>, <span class="number">4</span>)</span><br><span class="line">        <span class="keyword">val</span> map3: mutable.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">Int</span>] = map1 + (<span class="string">&quot;e&quot;</span> -&gt; <span class="number">4</span>)</span><br><span class="line">        println(map1 eq map3)</span><br><span class="line">        <span class="keyword">val</span> map4: mutable.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">Int</span>] = map1 += (<span class="string">&quot;e&quot;</span> -&gt; <span class="number">5</span>)</span><br><span class="line">        println(map1 eq map4)</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 修改数据</span></span><br><span class="line">        map1.update(<span class="string">&quot;e&quot;</span>,<span class="number">8</span>)</span><br><span class="line">        map1(<span class="string">&quot;e&quot;</span>) = <span class="number">8</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 删除数据</span></span><br><span class="line">        map1.remove(<span class="string">&quot;e&quot;</span>)</span><br><span class="line">        <span class="keyword">val</span> map5: mutable.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">Int</span>] = map1 - <span class="string">&quot;e&quot;</span></span><br><span class="line">        println(map1 eq map5)</span><br><span class="line">        <span class="keyword">val</span> map6: mutable.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">Int</span>] = map1 -= <span class="string">&quot;e&quot;</span></span><br><span class="line">        println(map1 eq map6)</span><br><span class="line">        <span class="comment">// 清除集合</span></span><br><span class="line">        map1.clear()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>基本操作 <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> scala.collection.mutable</span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaCollection</span></span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> map1 = mutable.<span class="type">Map</span>( <span class="string">&quot;a&quot;</span> -&gt; <span class="number">1</span>, <span class="string">&quot;b&quot;</span> -&gt; <span class="number">2</span>, <span class="string">&quot;c&quot;</span> -&gt; <span class="number">3</span> )</span><br><span class="line">        <span class="keyword">val</span> map2 = mutable.<span class="type">Map</span>( <span class="string">&quot;d&quot;</span> -&gt; <span class="number">4</span>, <span class="string">&quot;e&quot;</span> -&gt; <span class="number">5</span>, <span class="string">&quot;f&quot;</span> -&gt; <span class="number">6</span> )</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> set: <span class="type">Set</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = map1.toSet</span><br><span class="line">        <span class="keyword">val</span> list: <span class="type">List</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = map1.toList</span><br><span class="line">        <span class="keyword">val</span> seq: <span class="type">Seq</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = map1.toSeq</span><br><span class="line">        <span class="keyword">val</span> array: <span class="type">Array</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = map1.toArray</span><br><span class="line"></span><br><span class="line">        println(set.mkString(<span class="string">&quot;,&quot;</span>))</span><br><span class="line">        println(list.mkString(<span class="string">&quot;,&quot;</span>))</span><br><span class="line">        println(seq.mkString(<span class="string">&quot;,&quot;</span>))</span><br><span class="line">        println(array.mkString(<span class="string">&quot;,&quot;</span>))</span><br><span class="line"></span><br><span class="line">        println(map1.get(<span class="string">&quot;a&quot;</span>))</span><br><span class="line">        println(map1.getOrElse(<span class="string">&quot;a&quot;</span>, <span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">        println(map1.keys)</span><br><span class="line">        println(map1.keySet)</span><br><span class="line">        println(map1.keysIterator)</span><br><span class="line">        println(map1.values)</span><br><span class="line">        println(map1.valuesIterator)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="7-6-元组"><a href="#7-6-元组" class="headerlink" title="7.6 元组"></a>7.6 元组</h2><p>在Scala语言中，我们可以将多个无关的数据元素封装为一个整体，这个整体我们称之为：元素组合，简称元组。有时也可将元组看成容纳元素的容器，其中最多只能容纳22个</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaCollection</span></span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 创建元组，使用小括号</span></span><br><span class="line">        <span class="keyword">val</span> tuple = (<span class="number">1</span>, <span class="string">&quot;zhangsan&quot;</span>, <span class="number">30</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 根据顺序号访问元组的数据</span></span><br><span class="line">        println(tuple._1)</span><br><span class="line">        println(tuple._2)</span><br><span class="line">        println(tuple._3)</span><br><span class="line">        <span class="comment">// 迭代器</span></span><br><span class="line">        <span class="keyword">val</span> iterator: <span class="type">Iterator</span>[<span class="type">Any</span>] = tuple.productIterator</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 根据索引访问元素</span></span><br><span class="line">        tuple.productElement(<span class="number">0</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 如果元组的元素只有两个，那么我们称之为对偶元组，也称之为键值对</span></span><br><span class="line">        <span class="keyword">val</span> kv: (<span class="type">String</span>, <span class="type">Int</span>) = (<span class="string">&quot;a&quot;</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">val</span> kv1: (<span class="type">String</span>, <span class="type">Int</span>) = <span class="string">&quot;a&quot;</span> -&gt; <span class="number">1</span></span><br><span class="line">        println( kv eq kv1 )</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="7-7-队列"><a href="#7-7-队列" class="headerlink" title="7.7 队列"></a>7.7 队列</h2><p>Scala也提供了队列（Queue）的数据结构，队列的特点就是先进先出。进队和出队的方法分别为enqueue和dequeue。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> scala.collection.mutable</span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaCollection</span></span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> que = <span class="keyword">new</span> mutable.<span class="type">Queue</span>[<span class="type">String</span>]()</span><br><span class="line">        <span class="comment">// 添加元素</span></span><br><span class="line">        que.enqueue(<span class="string">&quot;a&quot;</span>, <span class="string">&quot;b&quot;</span>, <span class="string">&quot;c&quot;</span>)</span><br><span class="line">        <span class="keyword">val</span> que1: mutable.<span class="type">Queue</span>[<span class="type">String</span>] = que += <span class="string">&quot;d&quot;</span></span><br><span class="line">        println(que eq que1)</span><br><span class="line">        <span class="comment">// 获取元素</span></span><br><span class="line">        println(que.dequeue())</span><br><span class="line">        println(que.dequeue())</span><br><span class="line">        println(que.dequeue())</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>思考:<br>kafka中如何保证消费数据的有序？ </p>
<ol>
<li>生产有序: Deque双端队列</li>
<li>存储有序: 存储单个分区</li>
<li>消费有序: 单个消费者，消费指定分区</li>
</ol>
</blockquote>
<h2 id="7-8-并行"><a href="#7-8-并行" class="headerlink" title="7.8 并行"></a>7.8 并行</h2><p>Scala为了充分使用多核CPU，提供了并行集合（有别于前面的串行集合），用于多核环境的并行计算。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaCollection</span></span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> result1 = (<span class="number">0</span> to <span class="number">100</span>).map&#123;x =&gt; <span class="type">Thread</span>.currentThread.getName&#125;</span><br><span class="line">        <span class="keyword">val</span> result2 = (<span class="number">0</span> to <span class="number">100</span>).par.map&#123;x =&gt; <span class="type">Thread</span>.currentThread.getName&#125;</span><br><span class="line"></span><br><span class="line">        println(result1)</span><br><span class="line">        println(result2)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>7.9 常用方法</p>
<ol>
<li>常用方法 <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaCollection</span></span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> list = <span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 集合长度</span></span><br><span class="line">        println(<span class="string">&quot;size =&gt;&quot;</span> + list.size)</span><br><span class="line">        println(<span class="string">&quot;length =&gt;&quot;</span> + list.length)</span><br><span class="line">        <span class="comment">// 判断集合是否为空</span></span><br><span class="line">        println(<span class="string">&quot;isEmpty =&gt;&quot;</span> + list.isEmpty)</span><br><span class="line">        <span class="comment">// 集合迭代器</span></span><br><span class="line">        println(<span class="string">&quot;iterator =&gt;&quot;</span> + list.iterator)</span><br><span class="line">        <span class="comment">// 循环遍历集合</span></span><br><span class="line">        list.foreach(println)</span><br><span class="line">        <span class="comment">// 将集合转换为字符串</span></span><br><span class="line">        println(<span class="string">&quot;mkString =&gt;&quot;</span> + list.mkString(<span class="string">&quot;,&quot;</span>))</span><br><span class="line">        <span class="comment">// 判断集合中是否包含某个元素</span></span><br><span class="line">        println(<span class="string">&quot;contains =&gt;&quot;</span> + list.contains(<span class="number">2</span>))</span><br><span class="line">        <span class="comment">// 取集合的前几个元素</span></span><br><span class="line">        println(<span class="string">&quot;take =&gt;&quot;</span> + list.take(<span class="number">2</span>))</span><br><span class="line">        <span class="comment">// 取集合的后几个元素</span></span><br><span class="line">        println(<span class="string">&quot;takeRight =&gt;&quot;</span> + list.takeRight(<span class="number">2</span>))</span><br><span class="line">        <span class="comment">// 查找元素</span></span><br><span class="line">        println(<span class="string">&quot;find =&gt;&quot;</span> + list.find(x =&gt; x % <span class="number">2</span>== <span class="number">0</span>))</span><br><span class="line">        <span class="comment">// 丢弃前几个元素</span></span><br><span class="line">        println(<span class="string">&quot;drop =&gt;&quot;</span> + list.drop(<span class="number">2</span>))</span><br><span class="line">        <span class="comment">// 丢弃后几个元素</span></span><br><span class="line">        println(<span class="string">&quot;dropRight =&gt;&quot;</span> + list.dropRight(<span class="number">2</span>))</span><br><span class="line">        <span class="comment">// 反转集合</span></span><br><span class="line">        println(<span class="string">&quot;reverse =&gt;&quot;</span> + list.reverse)</span><br><span class="line">        <span class="comment">// 去重</span></span><br><span class="line">        println(<span class="string">&quot;distinct =&gt;&quot;</span> + list.distinct)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>衍生集合 <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaCollection</span></span>&#123;</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> list = <span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line">        <span class="keyword">val</span> list1 = <span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line">        <span class="keyword">val</span> list2 = <span class="type">List</span>(<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 集合头</span></span><br><span class="line">        println(<span class="string">&quot;head =&gt; &quot;</span> + list.head)</span><br><span class="line">        <span class="comment">// 集合尾</span></span><br><span class="line">        println(<span class="string">&quot;tail =&gt; &quot;</span> + list.tail)</span><br><span class="line">        <span class="comment">// 集合尾迭代</span></span><br><span class="line">        println(<span class="string">&quot;tails =&gt; &quot;</span> + list.tails)</span><br><span class="line">        <span class="comment">// 集合初始值</span></span><br><span class="line">        println(<span class="string">&quot;init =&gt; &quot;</span> + list.init)</span><br><span class="line">        <span class="comment">// 集合初始值迭代</span></span><br><span class="line">        println(<span class="string">&quot;inits =&gt; &quot;</span> + list.inits)</span><br><span class="line">        <span class="comment">// 集合最后元素</span></span><br><span class="line">        println(<span class="string">&quot;last =&gt; &quot;</span> + list.last)</span><br><span class="line">        <span class="comment">// 集合并集</span></span><br><span class="line">        println(<span class="string">&quot;union =&gt; &quot;</span> + list.union(list1))</span><br><span class="line">        <span class="comment">// 集合交集</span></span><br><span class="line">        println(<span class="string">&quot;intersect =&gt; &quot;</span> + list.intersect(list1))</span><br><span class="line">        <span class="comment">// 集合差集</span></span><br><span class="line">        println(<span class="string">&quot;diff =&gt; &quot;</span> + list.diff(list1))</span><br><span class="line">        <span class="comment">// 切分集合</span></span><br><span class="line">        println(<span class="string">&quot;splitAt =&gt; &quot;</span> + list.splitAt(<span class="number">2</span>))</span><br><span class="line">        <span class="comment">// 滑动（窗口）</span></span><br><span class="line">        println(<span class="string">&quot;sliding =&gt; &quot;</span> + list.sliding(<span class="number">2</span>))</span><br><span class="line">        <span class="comment">// 滚动（没有重复）</span></span><br><span class="line">        println(<span class="string">&quot;sliding =&gt; &quot;</span> + list.sliding(<span class="number">2</span>,<span class="number">2</span>))</span><br><span class="line">        <span class="comment">// 拉链</span></span><br><span class="line">        println(<span class="string">&quot;zip =&gt; &quot;</span> + list.zip(list1))</span><br><span class="line">        <span class="comment">// 数据索引拉链</span></span><br><span class="line">        println(<span class="string">&quot;zipWithIndex =&gt; &quot;</span> + list.zipWithIndex)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>计算函数 <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaCollection</span></span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> list = <span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line">        <span class="keyword">val</span> list1 = <span class="type">List</span>(<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 集合最小值</span></span><br><span class="line">        println(<span class="string">&quot;min =&gt; &quot;</span> + list.min)</span><br><span class="line">        <span class="comment">// 集合最大值</span></span><br><span class="line">        println(<span class="string">&quot;max =&gt; &quot;</span> + list.max)</span><br><span class="line">        <span class="comment">// 集合求和</span></span><br><span class="line">        println(<span class="string">&quot;sum =&gt; &quot;</span> + list.sum)</span><br><span class="line">        <span class="comment">// 集合乘积</span></span><br><span class="line">        println(<span class="string">&quot;product =&gt; &quot;</span> + list.product)</span><br><span class="line">        <span class="comment">// 集合简化规约</span></span><br><span class="line">        println(<span class="string">&quot;reduce =&gt; &quot;</span> + list.reduce((x:<span class="type">Int</span>,y:<span class="type">Int</span>)=&gt;&#123;x+y&#125;))</span><br><span class="line">        println(<span class="string">&quot;reduce =&gt; &quot;</span> + list.reduce((x,y)=&gt;&#123;x+y&#125;))</span><br><span class="line">        println(<span class="string">&quot;reduce =&gt; &quot;</span> + list.reduce((x,y)=&gt;x+y))</span><br><span class="line">        println(<span class="string">&quot;reduce =&gt; &quot;</span> + list.reduce(_+_))</span><br><span class="line">        <span class="comment">// 集合简化规约(左)</span></span><br><span class="line">        println(<span class="string">&quot;reduceLeft =&gt; &quot;</span> + list.reduceLeft(_+_))</span><br><span class="line">        <span class="comment">// 集合简化规约(右)</span></span><br><span class="line">        println(<span class="string">&quot;reduceRight =&gt; &quot;</span> + list.reduceRight(_+_))</span><br><span class="line">        <span class="comment">// 集合折叠</span></span><br><span class="line">        println(<span class="string">&quot;fold =&gt; &quot;</span> + list.fold(<span class="number">0</span>)(_+_))</span><br><span class="line">        <span class="comment">// 集合折叠(左)</span></span><br><span class="line">        println(<span class="string">&quot;foldLeft =&gt; &quot;</span> + list.foldLeft(<span class="number">0</span>)(_+_))</span><br><span class="line">        <span class="comment">// 集合折叠(右)</span></span><br><span class="line">        println(<span class="string">&quot;foldRight =&gt; &quot;</span> + list.foldRight(<span class="number">0</span>)(_+_))</span><br><span class="line">        <span class="comment">// 集合扫描</span></span><br><span class="line">        println(<span class="string">&quot;scan =&gt; &quot;</span> + list.scan(<span class="number">0</span>)(_+_))</span><br><span class="line">        <span class="comment">// 集合扫描(左)</span></span><br><span class="line">        println(<span class="string">&quot;scanLeft =&gt; &quot;</span> + list.scanLeft(<span class="number">0</span>)(_+_))</span><br><span class="line">        <span class="comment">// 集合扫描(右)</span></span><br><span class="line">        println(<span class="string">&quot;scanRight =&gt; &quot;</span> + list.scanRight(<span class="number">0</span>)(_+_))</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>功能函数 <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaCollection</span></span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> list = <span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 集合映射</span></span><br><span class="line">        println(<span class="string">&quot;map =&gt; &quot;</span> + list.map(x=&gt;&#123;x*<span class="number">2</span>&#125;))</span><br><span class="line">        println(<span class="string">&quot;map =&gt; &quot;</span> + list.map(x=&gt;x*<span class="number">2</span>))</span><br><span class="line">        println(<span class="string">&quot;map =&gt; &quot;</span> + list.map(_*<span class="number">2</span>))</span><br><span class="line">        <span class="comment">// 集合扁平化</span></span><br><span class="line">        <span class="keyword">val</span> list1 = <span class="type">List</span>(</span><br><span class="line">            <span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>),</span><br><span class="line">            <span class="type">List</span>(<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line">        )</span><br><span class="line">        println(<span class="string">&quot;flatten =&gt;&quot;</span> + list1.flatten)</span><br><span class="line">        <span class="comment">// 集合扁平映射</span></span><br><span class="line">        println(<span class="string">&quot;flatMap =&gt;&quot;</span> + list1.flatMap(list=&gt;list))</span><br><span class="line">        <span class="comment">// 集合过滤数据</span></span><br><span class="line">        println(<span class="string">&quot;filter =&gt;&quot;</span> + list.filter(_%<span class="number">2</span> == <span class="number">0</span>))</span><br><span class="line">        <span class="comment">// 集合分组数据</span></span><br><span class="line">        println(<span class="string">&quot;groupBy =&gt;&quot;</span> + list.groupBy(_%<span class="number">2</span>))</span><br><span class="line">        <span class="comment">// 集合排序</span></span><br><span class="line">        println(<span class="string">&quot;sortBy =&gt;&quot;</span> + list.sortBy(num=&gt;num)(<span class="type">Ordering</span>.<span class="type">Int</span>.reverse))</span><br><span class="line">        println(<span class="string">&quot;sortWith =&gt;&quot;</span> + list.sortWith((left, right) =&gt; &#123;left &lt; right&#125;))</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="7-10-案例实操-WordCount-TopN"><a href="#7-10-案例实操-WordCount-TopN" class="headerlink" title="7.10 案例实操 - WordCount TopN"></a>7.10 案例实操 - WordCount TopN</h2><h3 id="7-10-1-数据准备"><a href="#7-10-1-数据准备" class="headerlink" title="7.10.1 数据准备"></a>7.10.1 数据准备</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Hello Scala</span><br><span class="line">Hello Spark</span><br><span class="line">Hello Hadoop</span><br></pre></td></tr></table></figure>

<h3 id="7-10-2-功能实现"><a href="#7-10-2-功能实现" class="headerlink" title="7.10.2 功能实现"></a>7.10.2 功能实现</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaWordCount</span></span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> list: <span class="type">List</span>[<span class="type">String</span>] = <span class="type">Source</span>.fromFile(<span class="string">&quot;input/word.txt&quot;</span>).getLines().toList</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> wordList: <span class="type">List</span>[<span class="type">String</span>] = list.flatMap(_.split(<span class="string">&quot; &quot;</span>))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> word2OneList: <span class="type">List</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = wordList.map((_,<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> word2ListMap: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">List</span>[(<span class="type">String</span>, <span class="type">Int</span>)]] = word2OneList.groupBy(_._1)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> word2CountMap: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">Int</span>] = word2ListMap.map(</span><br><span class="line">            kv =&gt; &#123;</span><br><span class="line">                (kv._1, kv._2.size)</span><br><span class="line">            &#125;</span><br><span class="line">        )</span><br><span class="line">        println(word2CountMap)</span><br><span class="line">    &#125;</span><br><span class="line">&#125; </span><br></pre></td></tr></table></figure>

<h1 id="第8章-模式匹配"><a href="#第8章-模式匹配" class="headerlink" title="第8章 模式匹配"></a>第8章 模式匹配</h1><h2 id="8-1-简介"><a href="#8-1-简介" class="headerlink" title="8.1 简介"></a>8.1 简介</h2><p>Scala中的模式匹配类似于Java中的switch语法,但是scala从语法中补充了更多的功能，可以按照指定的规则对数据或对象进行匹配, 所以更加强大。<br>java:</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> i = <span class="number">20</span></span><br><span class="line"><span class="keyword">switch</span> (i) &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="number">10</span> :</span><br><span class="line">        System.out.println(<span class="string">&quot;10&quot;</span>);</span><br><span class="line">        <span class="comment">//break;</span></span><br><span class="line">    <span class="keyword">case</span> <span class="number">20</span> : </span><br><span class="line">        System.out.println(<span class="string">&quot;20&quot;</span>);</span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">    <span class="keyword">default</span> : </span><br><span class="line">        System.out.println(<span class="string">&quot;other number&quot;</span>);</span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


<h2 id="8-2-基本语法"><a href="#8-2-基本语法" class="headerlink" title="8.2 基本语法"></a>8.2 基本语法</h2><p>模式匹配语法中，采用match关键字声明，每个分支采用case关键字进行声明，当需要匹配时，会从第一个case分支开始，如果匹配成功，那么执行对应的逻辑代码，如果匹配不成功，继续执行下一个分支进行判断。<font color ='red' >如果所有case都不匹配，那么会执行case _分支，类似于Java中default语句</font>。<font color ='blue' >如果不存在case _分支，那么会发生错误</font>。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaMatch</span></span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">var</span> a: <span class="type">Int</span> = <span class="number">10</span></span><br><span class="line">        <span class="keyword">var</span> b: <span class="type">Int</span> = <span class="number">20</span></span><br><span class="line">        <span class="keyword">var</span> operator: <span class="type">Char</span> = &#x27;d&#x27;</span><br><span class="line">        <span class="keyword">var</span> result = operator <span class="keyword">match</span> &#123;</span><br><span class="line">            <span class="keyword">case</span> &#x27;+&#x27; =&gt; a + b</span><br><span class="line">            <span class="keyword">case</span> &#x27;-&#x27; =&gt; a - b</span><br><span class="line">            <span class="keyword">case</span> &#x27;*&#x27; =&gt; a * b</span><br><span class="line">            <span class="keyword">case</span> &#x27;/&#x27; =&gt; a / b</span><br><span class="line">            <span class="keyword">case</span> _ =&gt; <span class="string">&quot;illegal&quot;</span></span><br><span class="line">        &#125;</span><br><span class="line">        println(result)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="8-3-匹配规则"><a href="#8-3-匹配规则" class="headerlink" title="8.3 匹配规则"></a>8.3 匹配规则</h2><h3 id="8-3-1-匹配常量"><a href="#8-3-1-匹配常量" class="headerlink" title="8.3.1 匹配常量"></a>8.3.1 匹配常量</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">describe</span></span>(x: <span class="type">Any</span>) = x <span class="keyword">match</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="number">5</span> =&gt; <span class="string">&quot;Int five&quot;</span></span><br><span class="line">    <span class="keyword">case</span> <span class="string">&quot;hello&quot;</span> =&gt; <span class="string">&quot;String hello&quot;</span></span><br><span class="line">    <span class="keyword">case</span> <span class="literal">true</span> =&gt; <span class="string">&quot;Boolean true&quot;</span></span><br><span class="line">    <span class="keyword">case</span> &#x27;+&#x27; =&gt; <span class="string">&quot;Char +&quot;</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="8-3-2-匹配类型"><a href="#8-3-2-匹配类型" class="headerlink" title="8.3.2 匹配类型"></a>8.3.2 匹配类型</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">describe</span></span>(x: <span class="type">Any</span>) = x <span class="keyword">match</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> i: <span class="type">Int</span> =&gt; <span class="string">&quot;Int&quot;</span></span><br><span class="line">    <span class="keyword">case</span> s: <span class="type">String</span> =&gt; <span class="string">&quot;String hello&quot;</span></span><br><span class="line">    <span class="keyword">case</span> m: <span class="type">List</span>[_] =&gt; <span class="string">&quot;List&quot;</span></span><br><span class="line">    <span class="keyword">case</span> c: <span class="type">Array</span>[<span class="type">Int</span>] =&gt; <span class="string">&quot;Array[Int]&quot;</span></span><br><span class="line">    <span class="keyword">case</span> someThing =&gt; <span class="string">&quot;something else &quot;</span> + someThing</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="8-3-3-匹配数组"><a href="#8-3-3-匹配数组" class="headerlink" title="8.3.3 匹配数组"></a>8.3.3 匹配数组</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> (arr &lt;- <span class="type">Array</span>(</span><br><span class="line">    <span class="type">Array</span>(<span class="number">0</span>),</span><br><span class="line">    <span class="type">Array</span>(<span class="number">1</span>, <span class="number">0</span>),</span><br><span class="line">    <span class="type">Array</span>(<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>),</span><br><span class="line">    <span class="type">Array</span>(<span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>),</span><br><span class="line">    <span class="type">Array</span>(<span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>),</span><br><span class="line">    <span class="type">Array</span>(<span class="string">&quot;hello&quot;</span>, <span class="number">90</span>))) &#123;</span><br><span class="line">    <span class="keyword">val</span> result = arr <span class="keyword">match</span> &#123;</span><br><span class="line">        <span class="comment">//匹配Array(0) 这个数组</span></span><br><span class="line">        <span class="keyword">case</span> <span class="type">Array</span>(<span class="number">0</span>) =&gt; <span class="string">&quot;0&quot;</span></span><br><span class="line">        <span class="comment">//匹配有两个元素的数组，然后将将元素值赋给对应的x,y</span></span><br><span class="line">        <span class="keyword">case</span> <span class="type">Array</span>(x, y) =&gt; x + <span class="string">&quot;,&quot;</span> + y</span><br><span class="line">        <span class="comment">//匹配以0开头和数组</span></span><br><span class="line">        <span class="keyword">case</span> <span class="type">Array</span>(<span class="number">0</span>, _*) =&gt; <span class="string">&quot;以0开头的数组&quot;</span></span><br><span class="line">        <span class="keyword">case</span> _ =&gt; <span class="string">&quot;something else&quot;</span></span><br><span class="line">    &#125;</span><br><span class="line">    println(<span class="string">&quot;匹配数组 result = &quot;</span> + result)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="8-3-4-匹配列表"><a href="#8-3-4-匹配列表" class="headerlink" title="8.3.4 匹配列表"></a>8.3.4 匹配列表</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> (list &lt;- <span class="type">Array</span>(</span><br><span class="line">    <span class="type">List</span>(<span class="number">0</span>),</span><br><span class="line">    <span class="type">List</span>(<span class="number">1</span>, <span class="number">0</span>),</span><br><span class="line">    <span class="type">List</span>(<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>),</span><br><span class="line">    <span class="type">List</span>(<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>),</span><br><span class="line">    <span class="type">List</span>(<span class="number">88</span>))) &#123;</span><br><span class="line">    <span class="keyword">val</span> result = list <span class="keyword">match</span> &#123;</span><br><span class="line">        <span class="keyword">case</span> <span class="type">List</span>(<span class="number">0</span>) =&gt; <span class="string">&quot;0&quot;</span> <span class="comment">//匹配List(0)</span></span><br><span class="line">        <span class="keyword">case</span> <span class="type">List</span>(x, y) =&gt; x + <span class="string">&quot;,&quot;</span> + y <span class="comment">//匹配有两个元素的List</span></span><br><span class="line">        <span class="keyword">case</span> <span class="type">List</span>(<span class="number">0</span>, _*) =&gt; <span class="string">&quot;0 ...&quot;</span></span><br><span class="line">        <span class="keyword">case</span> _ =&gt; <span class="string">&quot;something else&quot;</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    println(<span class="string">&quot;匹配列表：&quot;</span> + result)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> list1: <span class="type">List</span>[<span class="type">Int</span>] = <span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>)</span><br><span class="line"><span class="keyword">val</span> list2: <span class="type">List</span>[<span class="type">Int</span>] = <span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line"><span class="keyword">val</span> list3: <span class="type">List</span>[<span class="type">Int</span>] = <span class="type">List</span>(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">list1 <span class="keyword">match</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> first :: second :: rest =&gt; println(<span class="string">&quot;first: &quot;</span> + first + <span class="string">&quot;; &quot;</span> + <span class="string">&quot;second: &quot;</span> + second + <span class="string">&quot;;&quot;</span> + <span class="string">&quot;rest: &quot;</span> + rest)</span><br><span class="line">    <span class="keyword">case</span> _ =&gt; println(<span class="string">&quot;something else&quot;</span>)</span><br><span class="line">&#125;</span><br><span class="line">list2 <span class="keyword">match</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> first :: second :: rest =&gt; println(<span class="string">&quot;first: &quot;</span> + first + <span class="string">&quot;; &quot;</span> + <span class="string">&quot;second: &quot;</span> + second + <span class="string">&quot;;&quot;</span> + <span class="string">&quot;rest: &quot;</span> + rest)</span><br><span class="line">    <span class="keyword">case</span> _ =&gt; println(<span class="string">&quot;something else&quot;</span>)</span><br><span class="line">&#125;</span><br><span class="line">list3 <span class="keyword">match</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> first :: second :: rest =&gt; println(<span class="string">&quot;first: &quot;</span> + first + <span class="string">&quot;; &quot;</span> + <span class="string">&quot;second: &quot;</span> + second + <span class="string">&quot;;&quot;</span> + <span class="string">&quot;rest: &quot;</span> + rest)</span><br><span class="line">    <span class="keyword">case</span> _ =&gt; println(<span class="string">&quot;something else&quot;</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="8-3-5-匹配元组"><a href="#8-3-5-匹配元组" class="headerlink" title="8.3.5 匹配元组"></a>8.3.5 匹配元组</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">//匹配元组</span></span><br><span class="line"><span class="keyword">for</span> (tuple &lt;- <span class="type">Array</span>(</span><br><span class="line">    (<span class="number">0</span>, <span class="number">1</span>),</span><br><span class="line">    (<span class="number">1</span>, <span class="number">0</span>),</span><br><span class="line">    (<span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">    (<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>))) &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> result = tuple <span class="keyword">match</span> &#123;</span><br><span class="line">        <span class="keyword">case</span> (<span class="number">0</span>, _) =&gt; <span class="string">&quot;0 ...&quot;</span> <span class="comment">//是第一个元素是0的元组</span></span><br><span class="line">        <span class="keyword">case</span> (y, <span class="number">0</span>) =&gt; <span class="string">&quot;&quot;</span> + y + <span class="string">&quot;0&quot;</span> <span class="comment">// 匹配后一个元素是0的对偶元组</span></span><br><span class="line">        <span class="keyword">case</span> (a, b) =&gt; <span class="string">&quot;&quot;</span> + a + <span class="string">&quot; &quot;</span> + b</span><br><span class="line">        <span class="keyword">case</span> _ =&gt; <span class="string">&quot;something else&quot;</span> <span class="comment">//默认</span></span><br><span class="line">    &#125;</span><br><span class="line">    println(<span class="string">&quot;匹配元组:&quot;</span> + result)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="8-3-6-匹配对象"><a href="#8-3-6-匹配对象" class="headerlink" title="8.3.6 匹配对象"></a>8.3.6 匹配对象</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">User</span>(<span class="params">val name: <span class="type">String</span>, val age: <span class="type">Int</span></span>)</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">User</span></span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">apply</span></span>(name: <span class="type">String</span>, age: <span class="type">Int</span>): <span class="type">User</span> = <span class="keyword">new</span> <span class="type">User</span>(name, age)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">unapply</span></span>(user: <span class="type">User</span>): <span class="type">Option</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = &#123;</span><br><span class="line">        <span class="keyword">if</span> (user == <span class="literal">null</span>)</span><br><span class="line">            <span class="type">None</span></span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            <span class="type">Some</span>(user.name, user.age)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> user: <span class="type">User</span> = <span class="type">User</span>(<span class="string">&quot;zhangsan&quot;</span>, <span class="number">11</span>)</span><br><span class="line"><span class="keyword">val</span> result = user <span class="keyword">match</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="type">User</span>(<span class="string">&quot;zhangsan&quot;</span>, <span class="number">11</span>) =&gt; <span class="string">&quot;yes&quot;</span></span><br><span class="line">    <span class="keyword">case</span> _ =&gt; <span class="string">&quot;no&quot;</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="8-3-7-样例类"><a href="#8-3-7-样例类" class="headerlink" title="8.3.7 样例类"></a>8.3.7 样例类</h3><ul>
<li>样例类就是使用<code>case</code>关键字声明的类</li>
<li>样例类仍然是类，和普通类相比，只是其自动生成了伴生对象，并且伴生对象中自动提供了一些常用的方法，如apply、<code>unapply</code>、toString、equals、hashCode和copy。</li>
<li>样例类是为<code>模式匹配</code>而优化的类，因为其默认提供了unapply方法，因此，样例类可以直接使用模式匹配，而无需自己实现unapply方法。</li>
<li>构造器中的每一个参数都成为val，除非它被显式地声明为var（不建议这样做）<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">User</span>(<span class="params">name: <span class="type">String</span>, var age: <span class="type">Int</span></span>)</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaCaseClass</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> user: <span class="type">User</span> = <span class="type">User</span>(<span class="string">&quot;zhangsan&quot;</span>, <span class="number">11</span>)</span><br><span class="line">        <span class="keyword">val</span> result = user <span class="keyword">match</span> &#123;</span><br><span class="line">            <span class="keyword">case</span> <span class="type">User</span>(<span class="string">&quot;zhangsan&quot;</span>, <span class="number">11</span>) =&gt; <span class="string">&quot;yes&quot;</span></span><br><span class="line">            <span class="keyword">case</span> _ =&gt; <span class="string">&quot;no&quot;</span></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        println(result)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul>
<h2 id="8-4-应用场景"><a href="#8-4-应用场景" class="headerlink" title="8.4 应用场景"></a>8.4 应用场景</h2><h3 id="8-4-1-变量声明"><a href="#8-4-1-变量声明" class="headerlink" title="8.4.1 变量声明"></a>8.4.1 变量声明</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaMatch</span> </span>&#123; </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> (x, y) = (<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        println(<span class="string">s&quot;x=<span class="subst">$x</span>,y=<span class="subst">$y</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> <span class="type">Array</span>(first, second, _*) = <span class="type">Array</span>(<span class="number">1</span>, <span class="number">7</span>, <span class="number">2</span>, <span class="number">9</span>)</span><br><span class="line">        println(<span class="string">s&quot;first=<span class="subst">$first</span>,second=<span class="subst">$second</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> <span class="type">Person</span>(name, age) = <span class="type">Person</span>(<span class="string">&quot;zhangsan&quot;</span>, <span class="number">16</span>)</span><br><span class="line">        println(<span class="string">s&quot;name=<span class="subst">$name</span>,age=<span class="subst">$age</span>&quot;</span>)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Person</span>(<span class="params">name: <span class="type">String</span>, age: <span class="type">Int</span></span>)</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="8-4-2-循环匹配"><a href="#8-4-2-循环匹配" class="headerlink" title="8.4.2 循环匹配"></a>8.4.2 循环匹配</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaMatch</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> map = <span class="type">Map</span>(<span class="string">&quot;A&quot;</span> -&gt; <span class="number">1</span>, <span class="string">&quot;B&quot;</span> -&gt; <span class="number">0</span>, <span class="string">&quot;C&quot;</span> -&gt; <span class="number">3</span>)</span><br><span class="line">        <span class="keyword">for</span> ((k, v) &lt;- map) &#123; <span class="comment">//直接将map中的k-v遍历出来</span></span><br><span class="line">            println(k + <span class="string">&quot; -&gt; &quot;</span> + v) <span class="comment">//3个</span></span><br><span class="line">        &#125;</span><br><span class="line">        println(<span class="string">&quot;----------------------&quot;</span>)</span><br><span class="line">        <span class="comment">//遍历value=0的 k-v ,如果v不是0,过滤</span></span><br><span class="line">        <span class="keyword">for</span> ((k, <span class="number">0</span>) &lt;- map) &#123;</span><br><span class="line">            println(k + <span class="string">&quot; --&gt; &quot;</span> + <span class="number">0</span>) <span class="comment">// B-&gt;0</span></span><br><span class="line">        &#125;</span><br><span class="line">        println(<span class="string">&quot;----------------------&quot;</span>)</span><br><span class="line">        <span class="comment">//if v == 0 是一个过滤的条件</span></span><br><span class="line">        <span class="keyword">for</span> ((k, v) &lt;- map <span class="keyword">if</span> v &gt;= <span class="number">1</span>) &#123;</span><br><span class="line">            println(k + <span class="string">&quot; ---&gt; &quot;</span> + v) <span class="comment">// A-&gt;1 和 c-&gt;33</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="8-4-3-函数参数"><a href="#8-4-3-函数参数" class="headerlink" title="8.4.3 函数参数"></a>8.4.3 函数参数</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaMatch</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> list = <span class="type">List</span>(</span><br><span class="line">            (<span class="string">&quot;a&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">2</span>), (<span class="string">&quot;c&quot;</span>, <span class="number">3</span>)</span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">val</span> list1 = list.map &#123;</span><br><span class="line">            <span class="keyword">case</span> ( k, v ) =&gt; &#123;</span><br><span class="line">                (k, v*<span class="number">2</span>)</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        println(list1)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="8-5-偏函数"><a href="#8-5-偏函数" class="headerlink" title="8.5 偏函数"></a>8.5 偏函数</h2><p>所谓的偏函数，其实就是对集合中符合条件的数据进行处理的函数<br>偏函数也是函数的一种，通过偏函数我们可以方便的对输入参数做更精确的检查。例如该偏函数的输入类型为Int，但是我们只考虑数值为1的时候，数据该如何处理，其他不考虑。</p>
<h3 id="8-5-1-基本语法"><a href="#8-5-1-基本语法" class="headerlink" title="8.5.1 基本语法"></a>8.5.1 基本语法</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 声明偏函数</span></span><br><span class="line"><span class="keyword">val</span> pf: <span class="type">PartialFunction</span>[<span class="type">Int</span>, <span class="type">String</span>] = &#123; <span class="keyword">case</span> <span class="number">1</span> =&gt; <span class="string">&quot;one&quot;</span> &#125;</span><br><span class="line"></span><br><span class="line">。。。</span><br><span class="line"><span class="comment">// 应用偏函数</span></span><br><span class="line">println(<span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>).collect(pf))</span><br></pre></td></tr></table></figure>


<h3 id="8-5-2-案例实操"><a href="#8-5-2-案例实操" class="headerlink" title="8.5.2 案例实操"></a>8.5.2 案例实操</h3><p>将该List(1,2,3,4,5,6,”test”)中的Int类型的元素加一，并去掉字符串。</p>
<ul>
<li>不使用偏函数  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="string">&quot;test&quot;</span>)</span><br><span class="line">    .filter(_.isInstanceOf[<span class="type">Int</span>])</span><br><span class="line">    .map(_.asInstanceOf[<span class="type">Int</span>] + <span class="number">1</span>)</span><br><span class="line">    .foreach(println)</span><br></pre></td></tr></table></figure></li>
<li>使用偏函数<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="string">&quot;test&quot;</span>)</span><br><span class="line">    .collect &#123; <span class="keyword">case</span> x: <span class="type">Int</span> =&gt; x + <span class="number">1</span> &#125;</span><br><span class="line">    .foreach(println)</span><br></pre></td></tr></table></figure></li>
</ul>
<h1 id="第9章-异常"><a href="#第9章-异常" class="headerlink" title="第9章 异常"></a>第9章 异常</h1><h2 id="9-1-简介"><a href="#9-1-简介" class="headerlink" title="9.1 简介"></a>9.1 简介</h2><p>Scala异常语法处理上和Java类似，但是又不尽相同。<br>Java异常：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="keyword">int</span> a = <span class="number">10</span>;</span><br><span class="line">    <span class="keyword">int</span> b = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">int</span> c = a / b;</span><br><span class="line">&#125; <span class="keyword">catch</span> (ArithmeticException e)&#123;</span><br><span class="line">    <span class="comment">// catch时，需要将范围小的写到前面</span></span><br><span class="line">    e.printStackTrace();</span><br><span class="line">&#125; <span class="keyword">catch</span> (Exception e)&#123;</span><br><span class="line">    e.printStackTrace();</span><br><span class="line">&#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">    System.out.println(<span class="string">&quot;finally&quot;</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="9-2-基本语法"><a href="#9-2-基本语法" class="headerlink" title="9.2 基本语法"></a>9.2 基本语法</h2><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaException</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="keyword">var</span> n= <span class="number">10</span> / <span class="number">0</span></span><br><span class="line">        &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">            <span class="keyword">case</span> ex: <span class="type">ArithmeticException</span>=&gt;&#123;</span><br><span class="line">                <span class="comment">// 发生算术异常</span></span><br><span class="line">                println(<span class="string">&quot;发生算术异常&quot;</span>)</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">case</span> ex: <span class="type">Exception</span>=&gt;&#123;</span><br><span class="line">                <span class="comment">// 对异常处理</span></span><br><span class="line">                println(<span class="string">&quot;发生了异常1&quot;</span>)</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">            println(<span class="string">&quot;finally&quot;</span>)</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>Scala中的异常不区分所谓的编译时异常和运行时异常，也无需显示抛出方法异常，所以<font color ='red' >Scala中没有throws关键字</font>。</p>
<ul>
<li>如果Java程序调用scala代码，如何明确异常？<ul>
<li>增加注解 @throws[classOf[Exception]]</li>
</ul>
</li>
</ul>
<h1 id="第10章-隐式转换"><a href="#第10章-隐式转换" class="headerlink" title="第10章 隐式转换"></a>第10章 隐式转换</h1><h2 id="10-1-简介"><a href="#10-1-简介" class="headerlink" title="10.1 简介"></a>10.1 简介</h2><p>在之前的类型学习中，我们已经学习了自动类型转换，精度小的类型可以自动转换为精度大的类型，这个转换过程无需开发人员参与，由编译器自动完成，这个转换操作我们称之为隐式转换。<br>在其他的场合，隐式转换也起到了非常重要的作用。如Scala在程序编译错误时，可以通过隐式转换中类型转换机制尝试进行二次编译，将本身错误无法编译通过的代码通过类型转换后编译通过。慢慢地，这也形成了一种扩展功能的转换机制。这个听着很抽象，不好理解，不急，咱慢慢体会。</p>
<h2 id="10-2-隐式函数"><a href="#10-2-隐式函数" class="headerlink" title="10.2 隐式函数"></a>10.2 隐式函数</h2><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaImplicit</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">implicit</span> <span class="function"><span class="keyword">def</span> <span class="title">transform</span></span>( d : <span class="type">Double</span> ): <span class="type">Int</span> = &#123;</span><br><span class="line">            d.toInt</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">var</span> d : <span class="type">Double</span> = <span class="number">2.0</span></span><br><span class="line">        <span class="keyword">val</span> i : <span class="type">Int</span> = d</span><br><span class="line">        println(i)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>思考一个问题:如果有多个相同转换规则怎么办？<ul>
<li>报错</li>
</ul>
</li>
</ul>
<h2 id="10-3-隐式参数-amp-隐式变量"><a href="#10-3-隐式参数-amp-隐式变量" class="headerlink" title="10.3 隐式参数 &amp; 隐式变量"></a>10.3 隐式参数 &amp; 隐式变量</h2><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaImplicit</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">transform</span></span>( <span class="keyword">implicit</span>  d : <span class="type">Double</span> ) = &#123;</span><br><span class="line">            d.toInt</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">implicit</span> <span class="keyword">val</span> dd : <span class="type">Double</span> = <span class="number">2.0</span></span><br><span class="line">        println(transform)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="10-4-隐式类"><a href="#10-4-隐式类" class="headerlink" title="10.4 隐式类"></a>10.4 隐式类</h2><p>在Scala2.10后提供了隐式类，可以使用implicit声明类，隐式类非常强大，同样可以扩展类的功能，在集合的数据处理中，隐式类发挥了重要的作用。</p>
<ul>
<li>其所带的构造参数有且只能有一个</li>
<li>隐式类必须被定义在“类”或“伴生对象”或“包对象”里，即隐式类不能是顶级的。<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaImplicit</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> emp = <span class="keyword">new</span> <span class="type">Emp</span>()</span><br><span class="line">        emp.insertUser()</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="class"><span class="keyword">class</span> <span class="title">Emp</span> </span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">implicit</span> <span class="class"><span class="keyword">class</span> <span class="title">User</span>(<span class="params"> emp : <span class="type">Emp</span></span>) </span>&#123;</span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">insertUser</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">            println(<span class="string">&quot;insert user...&quot;</span>)</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul>
<h2 id="10-5-隐式机制"><a href="#10-5-隐式机制" class="headerlink" title="10.5 隐式机制"></a>10.5 隐式机制</h2><p>所谓的隐式机制，就是一旦出现编译错误时，编译器会从哪些地方查找对应的隐式转换规则</p>
<ul>
<li>当前代码作用域</li>
<li>当前代码上级作用域</li>
<li>当前类所在的包对象</li>
<li>当前类（对象）的父类（父类）或特质（父特质）<br>其实最直接的方式就是直接导入。</li>
</ul>
<h1 id="第11章-泛型"><a href="#第11章-泛型" class="headerlink" title="第11章 泛型"></a>第11章 泛型</h1><h2 id="11-1-简介"><a href="#11-1-简介" class="headerlink" title="11.1 简介"></a>11.1 简介</h2><p>Scala的泛型和Java中的泛型表达的含义都是一样的，对处理的数据类型进行约束，但是Scala提供了更加强大的功能</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Test</span>[<span class="type">A</span>] </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">var</span> elements: <span class="type">List</span>[<span class="type">A</span>] = <span class="type">Nil</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="11-2-泛型转换"><a href="#11-2-泛型转换" class="headerlink" title="11.2 泛型转换"></a>11.2 泛型转换</h2><p>Scala的泛型可以根据功能进行改变</p>
<h3 id="11-2-1-泛型不可变"><a href="#11-2-1-泛型不可变" class="headerlink" title="11.2.1 泛型不可变"></a>11.2.1 泛型不可变</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaGeneric</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> test1 : <span class="type">Test</span>[<span class="type">User</span>] = <span class="keyword">new</span> <span class="type">Test</span>[<span class="type">User</span>] <span class="comment">// OK</span></span><br><span class="line">        <span class="keyword">val</span> test2 : <span class="type">Test</span>[<span class="type">User</span>] = <span class="keyword">new</span> <span class="type">Test</span>[<span class="type">Parent</span>] <span class="comment">// Error</span></span><br><span class="line">        <span class="keyword">val</span> test3 : <span class="type">Test</span>[<span class="type">User</span>] = <span class="keyword">new</span> <span class="type">Test</span>[<span class="type">SubUser</span>]  <span class="comment">// Error</span></span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="class"><span class="keyword">class</span> <span class="title">Test</span>[<span class="type">T</span>] </span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="class"><span class="keyword">class</span> <span class="title">Parent</span> </span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="class"><span class="keyword">class</span> <span class="title">User</span> <span class="keyword">extends</span> <span class="title">Parent</span></span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="class"><span class="keyword">class</span> <span class="title">SubUser</span> <span class="keyword">extends</span> <span class="title">User</span> </span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


<h3 id="11-2-2-泛型协变"><a href="#11-2-2-泛型协变" class="headerlink" title="11.2.2 泛型协变"></a>11.2.2 泛型协变</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaGeneric</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> test1 : <span class="type">Test</span>[<span class="type">User</span>] = <span class="keyword">new</span> <span class="type">Test</span>[<span class="type">User</span>] <span class="comment">// OK</span></span><br><span class="line">        <span class="keyword">val</span> test2 : <span class="type">Test</span>[<span class="type">User</span>] = <span class="keyword">new</span> <span class="type">Test</span>[<span class="type">Parent</span>] <span class="comment">// Error</span></span><br><span class="line">        <span class="keyword">val</span> test3 : <span class="type">Test</span>[<span class="type">User</span>] = <span class="keyword">new</span> <span class="type">Test</span>[<span class="type">SubUser</span>]  <span class="comment">// OK</span></span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="class"><span class="keyword">class</span> <span class="title">Test</span>[+<span class="type">T</span>] </span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="class"><span class="keyword">class</span> <span class="title">Parent</span> </span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="class"><span class="keyword">class</span> <span class="title">User</span> <span class="keyword">extends</span> <span class="title">Parent</span></span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="class"><span class="keyword">class</span> <span class="title">SubUser</span> <span class="keyword">extends</span> <span class="title">User</span> </span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="11-2-3-泛型逆变"><a href="#11-2-3-泛型逆变" class="headerlink" title="11.2.3 泛型逆变"></a>11.2.3 泛型逆变</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaGeneric</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> test1 : <span class="type">Test</span>[<span class="type">User</span>] = <span class="keyword">new</span> <span class="type">Test</span>[<span class="type">User</span>] <span class="comment">// OK</span></span><br><span class="line">        <span class="keyword">val</span> test2 : <span class="type">Test</span>[<span class="type">User</span>] = <span class="keyword">new</span> <span class="type">Test</span>[<span class="type">Parent</span>] <span class="comment">// OK</span></span><br><span class="line">        <span class="keyword">val</span> test3 : <span class="type">Test</span>[<span class="type">User</span>] = <span class="keyword">new</span> <span class="type">Test</span>[<span class="type">SubUser</span>]  <span class="comment">// Error</span></span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="class"><span class="keyword">class</span> <span class="title">Test</span>[-<span class="type">T</span>] </span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="class"><span class="keyword">class</span> <span class="title">Parent</span> </span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="class"><span class="keyword">class</span> <span class="title">User</span> <span class="keyword">extends</span> <span class="title">Parent</span></span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="class"><span class="keyword">class</span> <span class="title">SubUser</span> <span class="keyword">extends</span> <span class="title">User</span> </span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


<h2 id="11-3-泛型边界"><a href="#11-3-泛型边界" class="headerlink" title="11.3 泛型边界"></a>11.3 泛型边界</h2><p>Scala的泛型可以根据功能设定类树的边界</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaGeneric</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> parent : <span class="type">Parent</span> = <span class="keyword">new</span> <span class="type">Parent</span>()</span><br><span class="line">        <span class="keyword">val</span> user : <span class="type">User</span> = <span class="keyword">new</span> <span class="type">User</span>()</span><br><span class="line">        <span class="keyword">val</span> subuser : <span class="type">SubUser</span> = <span class="keyword">new</span> <span class="type">SubUser</span>()</span><br><span class="line">        test[<span class="type">User</span>](parent) <span class="comment">// Error</span></span><br><span class="line">        test[<span class="type">User</span>](user)   <span class="comment">// OK</span></span><br><span class="line">        test[<span class="type">User</span>](subuser) <span class="comment">// OK</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">def</span>  <span class="title">test</span></span>[<span class="type">A</span>]( a : <span class="type">A</span> ): <span class="type">Unit</span> = &#123;</span><br><span class="line">        println(a)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="class"><span class="keyword">class</span> <span class="title">Parent</span> </span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="class"><span class="keyword">class</span> <span class="title">User</span> <span class="keyword">extends</span> <span class="title">Parent</span></span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="class"><span class="keyword">class</span> <span class="title">SubUser</span> <span class="keyword">extends</span> <span class="title">User</span> </span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="11-3-1-泛型上限"><a href="#11-3-1-泛型上限" class="headerlink" title="11.3.1 泛型上限"></a>11.3.1 泛型上限</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaGeneric</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> parent : <span class="type">Parent</span> = <span class="keyword">new</span> <span class="type">Parent</span>()</span><br><span class="line">        <span class="keyword">val</span> user : <span class="type">User</span> = <span class="keyword">new</span> <span class="type">User</span>()</span><br><span class="line">        <span class="keyword">val</span> subuser : <span class="type">SubUser</span> = <span class="keyword">new</span> <span class="type">SubUser</span>()</span><br><span class="line">        test[<span class="type">Parent</span>](parent) <span class="comment">// Error</span></span><br><span class="line">        test[<span class="type">User</span>](user)   <span class="comment">// OK</span></span><br><span class="line">        test[<span class="type">SubUser</span>](subuser) <span class="comment">// OK</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">def</span>  <span class="title">test</span></span>[<span class="type">A</span>&lt;:<span class="type">User</span>]( a : <span class="type">A</span> ): <span class="type">Unit</span> = &#123;</span><br><span class="line">        println(a)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="class"><span class="keyword">class</span> <span class="title">Parent</span> </span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="class"><span class="keyword">class</span> <span class="title">User</span> <span class="keyword">extends</span> <span class="title">Parent</span></span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="class"><span class="keyword">class</span> <span class="title">SubUser</span> <span class="keyword">extends</span> <span class="title">User</span> </span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="11-3-2-泛型下限"><a href="#11-3-2-泛型下限" class="headerlink" title="11.3.2 泛型下限"></a>11.3.2 泛型下限</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaGeneric</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> parent : <span class="type">Parent</span> = <span class="keyword">new</span> <span class="type">Parent</span>()</span><br><span class="line">        <span class="keyword">val</span> user : <span class="type">User</span> = <span class="keyword">new</span> <span class="type">User</span>()</span><br><span class="line">        <span class="keyword">val</span> subuser : <span class="type">SubUser</span> = <span class="keyword">new</span> <span class="type">SubUser</span>()</span><br><span class="line">        test[<span class="type">Parent</span>](parent) <span class="comment">// OK</span></span><br><span class="line">        test[<span class="type">User</span>](user)   <span class="comment">// OK</span></span><br><span class="line">        test[<span class="type">SubUser</span>](subuser) <span class="comment">// Error</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">def</span>  <span class="title">test</span></span>[<span class="type">A</span>&gt;:<span class="type">User</span>]( a : <span class="type">A</span> ): <span class="type">Unit</span> = &#123;</span><br><span class="line">        println(a)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="class"><span class="keyword">class</span> <span class="title">Parent</span> </span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="class"><span class="keyword">class</span> <span class="title">User</span> <span class="keyword">extends</span> <span class="title">Parent</span></span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="class"><span class="keyword">class</span> <span class="title">SubUser</span> <span class="keyword">extends</span> <span class="title">User</span> </span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="11-4-上下文限定"><a href="#11-4-上下文限定" class="headerlink" title="11.4 上下文限定"></a>11.4 上下文限定</h2><p>上下文限定是将泛型和隐式转换的结合产物，以下两者功能相同，使用上下文限定[A : Ordering]之后，方法内无法使用隐式参数名调用隐式参数，需要通过implicitly[Ordering[A]]获取隐式变量，如果此时无法查找到对应类型的隐式变量，会发生出错误。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaGeneric</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">f</span></span>[<span class="type">A</span> : <span class="type">Test</span>](a: <span class="type">A</span>) = println(a)</span><br><span class="line">        <span class="keyword">implicit</span> <span class="keyword">val</span> test : <span class="type">Test</span>[<span class="type">User</span>] = <span class="keyword">new</span> <span class="type">Test</span>[<span class="type">User</span>]</span><br><span class="line">        f( <span class="keyword">new</span> <span class="type">User</span>() )</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="class"><span class="keyword">class</span> <span class="title">Test</span>[<span class="type">T</span>] </span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="class"><span class="keyword">class</span> <span class="title">Parent</span> </span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="class"><span class="keyword">class</span> <span class="title">User</span> <span class="keyword">extends</span> <span class="title">Parent</span></span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="class"><span class="keyword">class</span> <span class="title">SubUser</span> <span class="keyword">extends</span> <span class="title">User</span> </span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


<h1 id="第12章-正则表达式"><a href="#第12章-正则表达式" class="headerlink" title="第12章 正则表达式"></a>第12章 正则表达式</h1><h2 id="12-1-简介"><a href="#12-1-简介" class="headerlink" title="12.1 简介"></a>12.1 简介</h2><p>正则表达式(regular expression)描述了一种字符串匹配的模式（pattern），可以用来检查一个串是否含有某种子串、将匹配的子串替换或者从某个串中取出符合某个条件的子串等。</p>
<h2 id="12-2-基本语法"><a href="#12-2-基本语法" class="headerlink" title="12.2 基本语法"></a>12.2 基本语法</h2><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaRegex</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="comment">// 构建正则表达式</span></span><br><span class="line">        <span class="keyword">val</span> pattern = <span class="string">&quot;Scala&quot;</span>.r</span><br><span class="line">        <span class="keyword">val</span> str = <span class="string">&quot;Scala is Scalable Language&quot;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 匹配字符串 - 第一个</span></span><br><span class="line">        println(pattern findFirstIn str)</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 匹配字符串 - 所有</span></span><br><span class="line">        <span class="keyword">val</span> iterator: <span class="type">Regex</span>.<span class="type">MatchIterator</span> = pattern findAllIn str</span><br><span class="line">        <span class="keyword">while</span> ( iterator.hasNext ) &#123;</span><br><span class="line">            println(iterator.next())</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        println(<span class="string">&quot;***************************&quot;</span>)</span><br><span class="line">        <span class="comment">// 匹配规则：大写，小写都可</span></span><br><span class="line">        <span class="keyword">val</span> pattern1 = <span class="keyword">new</span> <span class="type">Regex</span>(<span class="string">&quot;(S|s)cala&quot;</span>)</span><br><span class="line">        <span class="keyword">val</span> str1 = <span class="string">&quot;Scala is scalable Language&quot;</span></span><br><span class="line">        println((pattern1 findAllIn str1).mkString(<span class="string">&quot;,&quot;</span>))</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="12-2-案例实操"><a href="#12-2-案例实操" class="headerlink" title="12.2 案例实操"></a>12.2 案例实操</h2><ul>
<li>手机号正则表达式验证方法  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaRegex</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="comment">// 构建正则表达式</span></span><br><span class="line">        println(isMobileNumber(<span class="string">&quot;18801234567&quot;</span>))</span><br><span class="line">        println(isMobileNumber(<span class="string">&quot;11111111111&quot;</span>))</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">isMobileNumber</span></span>(number: <span class="type">String</span>): <span class="type">Boolean</span> =&#123;</span><br><span class="line">        <span class="keyword">val</span> regex = <span class="string">&quot;^((13[0-9])|(14[5,7,9])|(15[^4])|(18[0-9])|(17[0,1,3,5,6,7,8]))[0-9]&#123;8&#125;$&quot;</span>.r</span><br><span class="line">        <span class="keyword">val</span> length = number.length</span><br><span class="line">        regex.findFirstMatchIn(number.slice(length<span class="number">-11</span>,length)) != <span class="type">None</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
<li>提取邮件地址的域名部分  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ScalaRegex</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="comment">// 构建正则表达式</span></span><br><span class="line">        <span class="keyword">val</span> r = <span class="string">&quot;&quot;&quot;([_A-Za-z0-9-]+(?:\.[_A-Za-z0-9-\+]+)*)(@[A-Za-z0-9-]+(?:\.[A-Za-z0-9-]+)*(?:\.[A-Za-z]&#123;2,&#125;)) ?&quot;&quot;&quot;</span>.r</span><br><span class="line">        println(r.replaceAllIn(<span class="string">&quot;abc.edf+jianli@gmail.com   hello@gmail.com.cn&quot;</span>, (m =&gt; <span class="string">&quot;*****&quot;</span> + m.group(<span class="number">2</span>))))</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul>
]]></content>
      <categories>
        <category>Scala</category>
      </categories>
      <tags>
        <tag>Scala</tag>
      </tags>
  </entry>
  <entry>
    <title>Kafka</title>
    <url>/2021/11/19/Kafka/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="Kafka"><a href="#Kafka" class="headerlink" title="Kafka"></a>Kafka</h1><h1 id="第1章-Kafka概述"><a href="#第1章-Kafka概述" class="headerlink" title="第1章 Kafka概述"></a>第1章 Kafka概述</h1><h2 id="1-1-定义"><a href="#1-1-定义" class="headerlink" title="1.1 定义"></a>1.1 定义</h2><ul>
<li>传统定义：Kafka是一个<font color ='red' >分布式</font>的基于<font color ='red' >发布/订阅模式</font>的<font color ='red' >消息队列（Message Queue）</font>，主要应用于大数据实时处理领域。</li>
<li>最新定义：Kafka是一个开源的分布式事件流平台（Event Streaming Platform），被数千家公司用于高性能数据管道、流分析、数据集成和关键任务应用。<br><img src="https://i.loli.net/2021/11/19/vEKlrIhO9zG5eJX.jpg"></li>
</ul>
<h2 id="1-2-消息队列"><a href="#1-2-消息队列" class="headerlink" title="1.2 消息队列"></a>1.2 消息队列</h2><h3 id="1-2-1-传统消息队列的应用场景"><a href="#1-2-1-传统消息队列的应用场景" class="headerlink" title="1.2.1 传统消息队列的应用场景"></a>1.2.1 传统消息队列的应用场景</h3><ol>
<li><p>缓冲/消峰：有助于控制和优化数据流经过系统的速度，解决生产消息和消费消息的处理速度不一致的情况。<br> <img src="https://i.loli.net/2021/11/19/U5cl9Sw48B3pqGX.jpg"></p>
</li>
<li><p>解耦：允许你独立的扩展或修改两边的处理过程，只要确保它们遵守同样的接口约束。<br> <img src="https://i.loli.net/2021/11/19/18xKnaykRSQ2gEX.jpg"></p>
</li>
<li><p>异步通信：很多时候，用户不想也不需要立即处理消息。消息队列提供了异步处理机制，允许用户把一个消息放入队列，但并不立即处理它。想向队列中放入多少消息就放多少，然后在需要的时候再去处理它们<br> <img src="https://i.loli.net/2021/11/19/i5bymXpZ1EINc2J.jpg"></p>
</li>
</ol>
<h3 id="1-2-2-消息队列的两种模式"><a href="#1-2-2-消息队列的两种模式" class="headerlink" title="1.2.2 消息队列的两种模式"></a>1.2.2 消息队列的两种模式</h3><ol>
<li>点对点模式<ul>
<li>消费者主动拉取数据，消息收到后消息清除<br><img src="https://i.loli.net/2021/11/19/etVkIU9p6TOqjoz.jpg"></li>
</ul>
</li>
<li>发布/订阅模式<ul>
<li>可以有多个topic主题 </li>
<li>消费者消费数据之后，不删除数据</li>
<li>每个消费者相互独立，都可以消费到数据<br><img src="https://i.loli.net/2021/11/19/WwGPAJs58LiDMVe.jpg"></li>
</ul>
</li>
</ol>
<h2 id="1-3-Kafka基础架构"><a href="#1-3-Kafka基础架构" class="headerlink" title="1.3 Kafka基础架构"></a>1.3 Kafka基础架构</h2><p><img src="https://i.loli.net/2021/11/19/UQ93WOvw6DN4onj.jpg"></p>
<ol>
<li>设计思路<ul>
<li>为方便扩展，并提高吞吐量，一个topic分为多个partition</li>
<li> 配合分区的设计，提出消费者组的概念，组内每个消费者并行消费</li>
<li> 为提高可用性，为每个partition增加若干副本，类似NameNode HA</li>
<li> ZK中记录谁是leader，Kafka2.8.0以后也可以配置不采用ZK</li>
</ul>
</li>
<li>组件<ul>
<li> Producer：消息生产者，就是向Kafka broker发消息的客户端；</li>
<li> Consumer：消息消费者，向Kafka broker取消息的客户端；</li>
<li> Consumer Group（CG）：消费者组，由多个consumer组成。消费者组内每个消费者负责消费不同分区的数据，一个分区只能由一个组内消费者消费；消费者组之间互不影响。所有的消费者都属于某个消费者组，即消费者组是逻辑上的一个订阅者。</li>
<li> Broker：一台Kafka服务器就是一个broker。一个集群由多个broker组成。一个broker可以容纳多个topic。</li>
<li> Topic：可以理解为一个队列，生产者和消费者面向的都是一个topic；</li>
<li> Partition：为了实现扩展性，一个非常大的topic可以分布到多个broker（即服务器）上，一个topic可以分为多个partition，每个partition是一个有序的队列；</li>
<li> Replica：副本，为保证集群中的某个节点发生故障时，该节点上的partition数据不丢失，且Kafka仍然能够继续工作，Kafka提供了副本机制，一个topic的每个分区都有若干个副本，一个Leader和若干个Follower。</li>
<li> Leader：每个分区多个副本的“主”，生产者发送数据的对象，以及消费者消费数据的对象都是Leader。</li>
<li> Follower：每个分区多个副本中的“从”，实时从Leader中同步数据，保持和Leader数据的同步。Leader发生故障时，某个Follower会成为新的Leader。</li>
</ul>
</li>
</ol>
<h1 id="第2章-Kafka快速入门"><a href="#第2章-Kafka快速入门" class="headerlink" title="第2章 Kafka快速入门"></a>第2章 Kafka快速入门</h1><h2 id="2-1-安装部署"><a href="#2-1-安装部署" class="headerlink" title="2.1 安装部署"></a>2.1 安装部署</h2><h3 id="2-1-1-集群规划"><a href="#2-1-1-集群规划" class="headerlink" title="2.1.1 集群规划"></a>2.1.1 集群规划</h3><table>
<thead>
<tr>
<th>hadoop001</th>
<th>hadoop103</th>
<th>hadoop104</th>
</tr>
</thead>
<tbody><tr>
<td>zk</td>
<td>zk</td>
<td>zk</td>
</tr>
<tr>
<td>kafka</td>
<td>kafka</td>
<td>kafka</td>
</tr>
</tbody></table>
<h3 id="2-1-2-集群部署"><a href="#2-1-2-集群部署" class="headerlink" title="2.1.2 集群部署"></a>2.1.2 集群部署</h3><ol>
<li>官方下载地址：<a href="http://kafka.apache.org/downloads.html">http://kafka.apache.org/downloads.html</a></li>
<li>解压安装包 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">tar -zxvf kafka_2.12-3.0.0.tgz -C /opt/module/</span><br></pre></td></tr></table></figure></li>
<li>配置环境变量  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 config]$ sudo vim /etc/profile.d/set_env.sh</span><br><span class="line"></span><br><span class="line"><span class="comment">#KAFKA_HOME</span></span><br><span class="line"><span class="built_in">export</span> KAFKA_HOME=/opt/module/kafka_2.12-3.0.0</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$KAFKA_HOME</span>/bin</span><br><span class="line"></span><br><span class="line"><span class="comment"># 分发环境变量文件</span></span><br><span class="line"> xsync /etc/profile.d/set_env.sh</span><br></pre></td></tr></table></figure></li>
<li>进入到$KAFKA_HOME/config目录，修改配置文件server.properties输入以下内容 <figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="comment">#broker的全局唯一编号，不能重复</span></span><br><span class="line"><span class="meta">broker.id</span>=<span class="string">0</span></span><br><span class="line"><span class="comment">#处理网络请求的线程数量</span></span><br><span class="line"><span class="meta">num.network.threads</span>=<span class="string">3</span></span><br><span class="line"><span class="comment">#用来处理磁盘IO的线程数量</span></span><br><span class="line"><span class="meta">num.io.threads</span>=<span class="string">8</span></span><br><span class="line"><span class="comment">#发送套接字的缓冲区大小</span></span><br><span class="line"><span class="meta">socket.send.buffer.bytes</span>=<span class="string">102400</span></span><br><span class="line"><span class="comment">#接收套接字的缓冲区大小</span></span><br><span class="line"><span class="meta">socket.receive.buffer.bytes</span>=<span class="string">102400</span></span><br><span class="line"><span class="comment">#请求套接字的缓冲区大小</span></span><br><span class="line"><span class="meta">socket.request.max.bytes</span>=<span class="string">104857600</span></span><br><span class="line"><span class="comment">#kafka运行日志(数据)存放的路径，路径不需要提前创建，kafka自动帮你创建</span></span><br><span class="line"><span class="meta">log.dirs</span>=<span class="string">/opt/module/kafka/datas</span></span><br><span class="line"><span class="comment">#topic在当前broker上的分区个数</span></span><br><span class="line"><span class="meta">num.partitions</span>=<span class="string">1</span></span><br><span class="line"><span class="comment">#用来恢复和清理data下数据的线程数量</span></span><br><span class="line"><span class="meta">num.recovery.threads.per.data.dir</span>=<span class="string">1</span></span><br><span class="line"><span class="comment">#segment文件保留的最长时间，超时将被删除</span></span><br><span class="line"><span class="meta">log.retention.hours</span>=<span class="string">168</span></span><br><span class="line"><span class="comment">#配置连接Zookeeper集群地址（在zk根目录下创建/kafka，方便管理）</span></span><br><span class="line"><span class="meta">zookeeper.connect</span>=<span class="string">hadoop001:2181,hadoop002:2181,hadoop003:2181/kafka</span></span><br></pre></td></tr></table></figure></li>
<li>分发安装包 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">xsync /opt/module/kafka_2.12-3.0.0/</span><br></pre></td></tr></table></figure></li>
<li>在各个节点上修改配置文件$KAFKA_HOME/config/server.properties中的 <figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="comment"># hadoop002</span></span><br><span class="line"><span class="meta">broker.id</span>=<span class="string">1</span></span><br><span class="line"><span class="comment"># hadoop003</span></span><br><span class="line"><span class="meta">broker.id</span>=<span class="string">2</span></span><br><span class="line"><span class="comment"># hadoop004</span></span><br><span class="line"><span class="meta">broker.id</span>=<span class="string">3</span></span><br><span class="line"><span class="comment"># hadoop005</span></span><br><span class="line"><span class="meta">broker.id</span>=<span class="string">4</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>注：broker.id不得重复</p>
</blockquote>
</li>
<li>启动集群<ul>
<li>先启动Zookeeper集群，然后启动Kafka  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">zookeeper_cluster.sh start</span><br></pre></td></tr></table></figure>
<blockquote>
<p>zookeeper_cluster.sh 参考<a href="mweblib://16343691008130">Zookeeper</a>篇《编写启动/停止Zookeeper集群脚本》</p>
</blockquote>
</li>
<li>依次在hadoop002、hadoop003、hadoop004,hadoop005节点上启动Kafka  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 ~]$ kafka-server-start.sh -daemon <span class="variable">$KAFKA_HOME</span>/config/server.properties</span><br><span class="line">[atguigu@hadoop001 ~]$ ssh hadoop002 kafka-server-start.sh -daemon <span class="variable">$KAFKA_HOME</span>/config/server.properties</span><br><span class="line">[atguigu@hadoop001 ~]$ ssh hadoop003 kafka-server-start.sh -daemon <span class="variable">$KAFKA_HOME</span>/config/server.properties</span><br><span class="line">[atguigu@hadoop001 ~]$ ssh hadoop004 kafka-server-start.sh -daemon <span class="variable">$KAFKA_HOME</span>/config/server.properties</span><br><span class="line">[atguigu@hadoop001 ~]$ ssh hadoop005 kafka-server-start.sh -daemon <span class="variable">$KAFKA_HOME</span>/config/server.properties</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>关闭集群 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 ~]$ kafka-server-stop.sh -daemon <span class="variable">$KAFKA_HOME</span>/config/server.properties</span><br><span class="line">[atguigu@hadoop001 ~]$ ssh hadoop002 kafka-server-stop.sh -daemon <span class="variable">$KAFKA_HOME</span>/config/server.properties</span><br><span class="line">[atguigu@hadoop001 ~]$ ssh hadoop003 kafka-server-stop.sh -daemon <span class="variable">$KAFKA_HOME</span>/config/server.properties</span><br><span class="line">[atguigu@hadoop001 ~]$ ssh hadoop004 kafka-server-stop.sh -daemon <span class="variable">$KAFKA_HOME</span>/config/server.properties</span><br><span class="line">[atguigu@hadoop001 ~]$ ssh hadoop005 kafka-server-stop.sh -daemon <span class="variable">$KAFKA_HOME</span>/config/server.properties</span><br></pre></td></tr></table></figure></li>
<li>Kafka启动失败的话可以暴力重置一下<ol>
<li>rm -rf /opt/module/kafka_2.12-3.0.0/logs/</li>
<li>删掉zookeeper /kafka</li>
<li>尝试重新启动</li>
</ol>
</li>
</ol>
<h3 id="2-1-3-集群启停脚本"><a href="#2-1-3-集群启停脚本" class="headerlink" title="2.1.3 集群启停脚本"></a>2.1.3 集群启停脚本</h3><ol>
<li>在/home/atguigu/bin目录下创建文件kf.sh脚本文件，脚本如下： <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="meta">#! /bin/bash</span></span><br><span class="line"><span class="keyword">if</span> [ <span class="variable">$#</span> -lt 1 ]</span><br><span class="line"><span class="keyword">then</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="string">&quot;No Args Input...&quot;</span></span><br><span class="line">    <span class="built_in">exit</span> ;</span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"><span class="keyword">case</span> <span class="variable">$1</span> <span class="keyword">in</span></span><br><span class="line"><span class="string">&quot;start&quot;</span>)&#123;</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> hadoop001 hadoop002 hadoop003 hadoop004 hadoop005</span><br><span class="line">    <span class="keyword">do</span></span><br><span class="line">        <span class="built_in">echo</span> <span class="string">&quot; --------启动 <span class="variable">$i</span> Kafka-------&quot;</span></span><br><span class="line">        ssh <span class="variable">$i</span> <span class="string">&quot;kafka-server-start.sh -daemon <span class="variable">$KAFKA_HOME</span>/config/server.properties&quot;</span></span><br><span class="line">    <span class="keyword">done</span></span><br><span class="line">&#125;;;</span><br><span class="line"><span class="string">&quot;stop&quot;</span>)&#123;</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> hadoop001 hadoop002 hadoop003 hadoop004 hadoop005</span><br><span class="line">    <span class="keyword">do</span></span><br><span class="line">        <span class="built_in">echo</span> <span class="string">&quot; --------停止 <span class="variable">$i</span> Kafka-------&quot;</span></span><br><span class="line">        ssh <span class="variable">$i</span> <span class="string">&quot;kafka-server-stop.sh &quot;</span></span><br><span class="line">    <span class="keyword">done</span></span><br><span class="line">&#125;;;</span><br><span class="line">*)</span><br><span class="line">    <span class="built_in">echo</span> <span class="string">&quot;Input Args Error...&quot;</span></span><br><span class="line">;;</span><br><span class="line"><span class="keyword">esac</span></span><br></pre></td></tr></table></figure></li>
<li>添加执行权限 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 bin]$ chmod +x kf.sh</span><br></pre></td></tr></table></figure></li>
<li>启动集群命令 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 ~]$ kf.sh start</span><br></pre></td></tr></table></figure></li>
<li>停止集群命令 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 ~]$ kf.sh stop</span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="2-2-Kafka命令行操作"><a href="#2-2-Kafka命令行操作" class="headerlink" title="2.2 Kafka命令行操作"></a>2.2 Kafka命令行操作</h2><p><img src="https://i.loli.net/2021/11/19/CJ8xFmjYL6tkrMz.jpg"></p>
<h3 id="2-2-1-topic命令行操作"><a href="#2-2-1-topic命令行操作" class="headerlink" title="2.2.1 topic命令行操作"></a>2.2.1 topic命令行操作</h3><ol>
<li><p>查看操作topic命令参数kafka-topics.sh</p>
<table>
<thead>
<tr>
<th>参数</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>–bootstrap-server<a href="String:servertoconnectto">String:servertoconnectto</a></td>
<td>连接的KafkaBroker主机名称和端口号</td>
</tr>
<tr>
<td>–topic<a href="String:topic">String:topic</a></td>
<td>操作的topic名称</td>
</tr>
<tr>
<td>–create</td>
<td>创建主题</td>
</tr>
<tr>
<td>–delete</td>
<td>删除主题</td>
</tr>
<tr>
<td>–alter</td>
<td>修改主题</td>
</tr>
<tr>
<td>–list</td>
<td>查看所有主题</td>
</tr>
<tr>
<td>–describe</td>
<td>查看主题详细描述</td>
</tr>
<tr>
<td>–partitions<a href="Integer:#ofpartitions">Integer:#ofpartitions</a></td>
<td>设置分区数</td>
</tr>
<tr>
<td>–replication-factor<a href="Integer:replicationfactor">Integer:replicationfactor</a></td>
<td>设置分区副本</td>
</tr>
<tr>
<td>–config<a href="String:name=value">String:name=value</a></td>
<td>更新系统默认的配置</td>
</tr>
</tbody></table>
</li>
<li><p>查看当前服务器中的所有topic</p>
 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kafka-topics.sh --bootstrap-server hadoop001:9092 --list</span><br></pre></td></tr></table></figure></li>
<li><p>创建topic</p>
 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 bin]$ kafka-topics.sh --bootstrap-server hadoop001:9092 --create --replication-factor 2 --partitions 1 --topic first</span><br><span class="line">Created topic first.</span><br><span class="line">[atguigu@hadoop001 bin]$ kafka-topics.sh --bootstrap-server hadoop001:9092 --list</span><br><span class="line">first</span><br><span class="line">[atguigu@hadoop001 bin]$</span><br></pre></td></tr></table></figure>
<ul>
<li>选项说明：<ul>
<li>–topic 定义topic名</li>
<li>–replication-factor  定义副本数</li>
<li>–partitions  定义分区数</li>
</ul>
</li>
</ul>
</li>
<li><p>查看某个Topic的详情</p>
 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">   [atguigu@hadoop001 bin]$ kafka-topics.sh --bootstrap-server hadoop001:9092 --describe --topic first</span><br><span class="line">   Topic: first	TopicId: wF2_VwRXR1q7l6MMamjZyg	PartitionCount: 1	ReplicationFactor: 2	Configs: segment.bytes=1073741824</span><br><span class="line">Topic: first	Partition: 0	Leader: 4	Replicas: 4,2	Isr: 4,2</span><br></pre></td></tr></table></figure></li>
<li><p>修改分区数</p>
 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 bin]$ kafka-topics.sh --bootstrap-server hadoop001:9092 --describe --topic first</span><br><span class="line">Topic: first	TopicId: wF2_VwRXR1q7l6MMamjZyg	PartitionCount: 3	ReplicationFactor: 2	Configs: segment.bytes=1073741824</span><br><span class="line">	Topic: first	Partition: 0	Leader: 4	Replicas: 4,2	Isr: 4,2</span><br><span class="line">	Topic: first	Partition: 1	Leader: 0	Replicas: 0,2	Isr: 0,2</span><br><span class="line">	Topic: first	Partition: 2	Leader: 2	Replicas: 2,4	Isr: 2,4</span><br><span class="line">[atguigu@hadoop001 bin]$</span><br></pre></td></tr></table></figure></li>
<li><p>删除topic</p>
 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 bin]$ kafka-topics.sh --bootstrap-server hadoop001:9092 --delete --topic first</span><br><span class="line">[atguigu@hadoop001 bin]$ kafka-topics.sh --bootstrap-server hadoop001:9092 --list</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop001 bin]$</span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="2-2-2-生产者命令行操作"><a href="#2-2-2-生产者命令行操作" class="headerlink" title="2.2.2 生产者命令行操作"></a>2.2.2 生产者命令行操作</h3><ol>
<li>查看操作生产者命令参数<code>kafka-console-producer.sh</code><table>
<thead>
<tr>
<th>参数</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>–bootstrap-server &lt;String: server toconnect to&gt;</td>
<td>连接的Kafka Broker主机名称和端口号</td>
</tr>
<tr>
<td>–topic &lt;String: topic&gt;</td>
<td>操作的topic名称</td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
</tbody></table>
</li>
<li>发送消息 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 bin]$ kafka-console-producer.sh --bootstrap-server hadoop001:9092 --topic first</span><br><span class="line">&gt;hello world</span><br><span class="line">&gt;atguigu atguigu</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="2-2-3-针对消费者相关命令大全"><a href="#2-2-3-针对消费者相关命令大全" class="headerlink" title="2.2.3 针对消费者相关命令大全"></a>2.2.3 针对消费者相关命令大全</h3><ol>
<li>查看操作消费者命令参数 kafka-console-consumer.sh<table>
<thead>
<tr>
<th>参数</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>–bootstrap-server &lt;String: server toconnect to&gt;</td>
<td>连接的Kafka Broker主机名称和端口号</td>
</tr>
<tr>
<td>–topic &lt;String: topic&gt;</td>
<td>操作的topic名称</td>
</tr>
<tr>
<td>–group &lt;String: consumer group id&gt;</td>
<td>指定消费者组名称</td>
</tr>
<tr>
<td>–partition &lt;Integer: partition&gt;</td>
<td>指定消费哪个分区数据</td>
</tr>
<tr>
<td>–offset &lt;String: consume offset&gt;</td>
<td>指定从什么位置消费</td>
</tr>
<tr>
<td>–from-beginning</td>
<td>从头开始消费</td>
</tr>
</tbody></table>
</li>
<li>消费消息 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">bin/kafka-console-consumer.sh --bootstrap-server hadoop001:9092 --topic first</span><br><span class="line"></span><br><span class="line">kafka-console-consumer.sh --bootstrap-server hadoop001:9092 --from-beginning --topic first</span><br></pre></td></tr></table></figure>
<ul>
<li>–from-beginning：会把主题中现有的所有的数据都读取出来。</li>
</ul>
</li>
</ol>
<h1 id="第3章-Kafka生产者"><a href="#第3章-Kafka生产者" class="headerlink" title="第3章 Kafka生产者"></a>第3章 Kafka生产者</h1><h2 id="3-1-消息发送流程"><a href="#3-1-消息发送流程" class="headerlink" title="3.1 消息发送流程"></a>3.1 消息发送流程</h2><p><img src="https://i.loli.net/2021/11/19/NphlWrxSzD6UsJC.jpg"></p>
<ul>
<li><p>在消息发送过程中，涉及到两个线程，以及一个中间队列-RecordAccumulator.</p>
<ul>
<li>一是主线程，负责将消息进行封装和加工发送给消息中间件（RecordAccumulator）</li>
<li>二是send线程，负责从消息中间件中拉取数据发送到主题（Topic）的对应分区（Partition）</li>
</ul>
</li>
<li><p>流程描述</p>
<ul>
<li>main线程<ol>
<li>生产将要发送的数据封装成ProducerRecord对象，目的是发送到消息中间件</li>
<li>中间要经过拦截器列表、序列化器和分区器将消息发送到消息中间件</li>
<li>RecordAccumulator中有多个队列，与topic的分区相对应。消息发送时直接发送到分区对应的RecordAccumulator队列中</li>
</ol>
</li>
<li>sender线程<ol>
<li>当RecordAccumulator中攒够一批数据后，即达到指定量的数据之后，Sender线程将这一批数据拉取并发送给Topic。<ul>
<li>batch.size：只有数据积累到batch.size之后，sender才会发送数据。默认16k</li>
</ul>
</li>
<li>同时，如果RecordAccumulator中队列迟迟到不到指定量的数据时，会等到一定时长时发送。<ul>
<li>linger.ms：如果数据迟迟未达到batch.size，sender等待linger.time之后就会发送数据。单位ms，默认值是0ms，表示没有延迟。</li>
</ul>
</li>
</ol>
</li>
</ul>
</li>
<li><p>生产者相关调优参数</p>
<table>
<thead>
<tr>
<th>参数名称</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>batch.size</td>
<td>缓冲区一批数据最大值，默认16k</td>
</tr>
<tr>
<td>linger.ms</td>
<td>如果数据迟迟未达到batch.size，sender等待linger.time之后就会发送数据。单位ms，默认值是0ms，表示没有延迟。</td>
</tr>
<tr>
<td>buffer.memory    RecordAccumulator</td>
<td>缓冲区总大小，默认32m</td>
</tr>
<tr>
<td>retries</td>
<td>当消息发送出现错误的时候，系统会重发消息。retries表示重试次数。</td>
</tr>
<tr>
<td>compression.type</td>
<td>生产者发送的所有数据的压缩方式。默认是none，也就是不压缩。 支持的值：none、gzip、snappy、lz4和zstd。</td>
</tr>
</tbody></table>
</li>
</ul>
<h2 id="3-2-异步发送API"><a href="#3-2-异步发送API" class="headerlink" title="3.2 异步发送API"></a>3.2 异步发送API</h2><h3 id="3-2-1-普通异步发送"><a href="#3-2-1-普通异步发送" class="headerlink" title="3.2.1 普通异步发送"></a>3.2.1 普通异步发送</h3><p><img src="https://i.loli.net/2021/11/19/z6wBqsD5gpbU3rh.jpg"></p>
<ol>
<li>需求：创建Kafka生产者，采用异步的方式发送到Kafka Broker</li>
<li>编码实现<ol>
<li>maven依赖 <figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.kafka<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>kafka-clients<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.0.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br></pre></td></tr></table></figure></li>
<li>编码 <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CustomProducer</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">final</span> Properties properties = <span class="keyword">new</span> Properties();</span><br><span class="line">        <span class="comment">// 给kafka配置对象添加配置信息：bootstrap.servers</span></span><br><span class="line">        properties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">&quot;hadoop001:9092,hadoop002:9092,hadoop003:9092,hadoop004:9092,hadoop005:9092&quot;</span>);</span><br><span class="line">        <span class="comment">// key,value序列化（必须）：key.serializer，value.serializer</span></span><br><span class="line">        properties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());</span><br><span class="line">        properties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 批次大小 默认16K：batch.size</span></span><br><span class="line">        properties.put(ProducerConfig.BATCH_SIZE_CONFIG, <span class="number">16384</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 等待时间：linger.ms</span></span><br><span class="line">        properties.put(ProducerConfig.LINGER_MS_CONFIG, <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// RecordAccumulator缓冲区大小 默认32M：buffer.memory</span></span><br><span class="line">        properties.put(ProducerConfig.BUFFER_MEMORY_CONFIG, <span class="number">33554432</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3. 创建kafka生产者对象</span></span><br><span class="line">        KafkaProducer&lt;String, String&gt; kafkaProducer = <span class="keyword">new</span> KafkaProducer&lt;&gt;(properties);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 4. 调用send方法,发送消息</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">10</span>; i++) &#123;</span><br><span class="line">            kafkaProducer.send(<span class="keyword">new</span> ProducerRecord&lt;&gt;(<span class="string">&quot;first&quot;</span>,<span class="string">&quot;i am producer &quot;</span> + i));</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 5. 关闭资源</span></span><br><span class="line">        kafkaProducer.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
<li>测试：<ol>
<li>在hadoop001上开启Kafka消费者。 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 kafka]$ bin/kafka-console-consumer.sh --bootstrap-server hadoop001:9092 --topic first</span><br></pre></td></tr></table></figure></li>
<li>在IDEA中执行代码，观察hadoop001控制台中是否接收到消息。 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 kafka]$  bin/kafka-console-consumer.sh --bootstrap-server hadoop001:9092 --topic first</span><br><span class="line"></span><br><span class="line">atguigu 0</span><br><span class="line">atguigu 1</span><br><span class="line">atguigu 2</span><br><span class="line">atguigu 3</span><br><span class="line">atguigu 4</span><br></pre></td></tr></table></figure></li>
</ol>
</li>
</ol>
</li>
</ol>
<h3 id="3-2-2-带回调函数的异步发送"><a href="#3-2-2-带回调函数的异步发送" class="headerlink" title="3.2.2 带回调函数的异步发送"></a>3.2.2 带回调函数的异步发送</h3><p>回调函数会在producer收到ack时调用，为异步调用，该方法有两个参数，分别是元数据信息（RecordMetadata）和异常信息（Exception），如果Exception为null，说明消息发送成功，如果Exception不为null，说明消息发送失败。<br><img src="https://i.loli.net/2021/11/19/f7Y9QgpN8LCdiGs.jpg"></p>
<blockquote>
<p>注意：消息发送失败会自动重试，不需要我们在回调函数中手动重试。</p>
</blockquote>
<ol>
<li>编码 <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CustomProducerCallback</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1. 创建kafka生产者的配置对象</span></span><br><span class="line">        Properties properties = <span class="keyword">new</span> Properties();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 给kafka配置对象添加配置信息：bootstrap.servers</span></span><br><span class="line">        properties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">&quot;hadoop001:9092,hadoop002:9092,hadoop003:9092,hadoop004:9092,hadoop005:9092&quot;</span>);</span><br><span class="line">        <span class="comment">// key,value序列化（必须）：key.serializer，value.serializer</span></span><br><span class="line">        properties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());</span><br><span class="line">        properties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 批次大小 默认16K：batch.size</span></span><br><span class="line">        properties.put(ProducerConfig.BATCH_SIZE_CONFIG, <span class="number">16384</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 等待时间：linger.ms</span></span><br><span class="line">        properties.put(ProducerConfig.LINGER_MS_CONFIG, <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// RecordAccumulator缓冲区大小 默认32M：buffer.memory</span></span><br><span class="line">        properties.put(ProducerConfig.BUFFER_MEMORY_CONFIG, <span class="number">33554432</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3. 创建kafka生产者对象</span></span><br><span class="line">        KafkaProducer&lt;String, String&gt; kafkaProducer = <span class="keyword">new</span> KafkaProducer&lt;String, String&gt;(properties);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 4. 调用send方法,发送消息</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">10</span>; i++) &#123;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 添加回调</span></span><br><span class="line">            kafkaProducer.send(<span class="keyword">new</span> ProducerRecord&lt;&gt;(<span class="string">&quot;first&quot;</span>, <span class="string">&quot;atguigu &quot;</span> + i), <span class="keyword">new</span> Callback() &#123;</span><br><span class="line"></span><br><span class="line">                <span class="comment">// 该方法在Producer收到ack时调用，为异步调用</span></span><br><span class="line">                <span class="meta">@Override</span></span><br><span class="line">                <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onCompletion</span><span class="params">(RecordMetadata metadata, Exception exception)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">                    <span class="keyword">if</span> (exception == <span class="keyword">null</span>) &#123;</span><br><span class="line">                        <span class="comment">// 没有异常,输出信息到控制台</span></span><br><span class="line">                        System.out.println(<span class="string">&quot;onCompletion 主题：&quot;</span> + metadata.topic() + <span class="string">&quot;-&gt;&quot;</span> + <span class="string">&quot;分区：&quot;</span> + metadata.partition());</span><br><span class="line">                    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                        <span class="comment">// 出现异常打印</span></span><br><span class="line">                        exception.printStackTrace();</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;);</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 延迟一会会看到数据发往不同分区</span></span><br><span class="line">            Thread.sleep(<span class="number">2</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 5. 关闭资源</span></span><br><span class="line">        kafkaProducer.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>测试   <ol>
<li>在hadoop001上开启Kafka消费者。 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 kafka]$ bin/kafka-console-consumer.sh --bootstrap-server hadoop001:9092 --topic first</span><br></pre></td></tr></table></figure></li>
<li>在IDEA中执行代码，观察hadoop001控制台中是否接收到消息。 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 kafka]$  bin/kafka-console-consumer.sh --bootstrap-server hadoop001:9092 --topic first</span><br><span class="line">atguigu 0</span><br><span class="line">atguigu 1</span><br><span class="line">atguigu 2</span><br><span class="line">atguigu 3</span><br><span class="line">atguigu 4</span><br></pre></td></tr></table></figure></li>
<li>在IDEA控制台观察回调信息 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">onCompletion 主题：first-&gt;分区：2</span><br><span class="line">onCompletion 主题：first-&gt;分区：1</span><br><span class="line">onCompletion 主题：first-&gt;分区：1</span><br><span class="line">onCompletion 主题：first-&gt;分区：0</span><br><span class="line">onCompletion 主题：first-&gt;分区：2</span><br></pre></td></tr></table></figure></li>
</ol>
</li>
</ol>
<h2 id="3-3-同步发送API"><a href="#3-3-同步发送API" class="headerlink" title="3.3 同步发送API"></a>3.3 同步发送API</h2><p><img src="https://i.loli.net/2021/11/19/f1SAOroePk75mbG.jpg"><br>由于send方法返回的是一个Future对象，根据Futrue对象的特点，我们也可以实现同步发送的效果，只需在调用Future对象的get方法即可。</p>
<ol>
<li>编码 <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CustomProducerSync</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> InterruptedException, ExecutionException </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1. 创建kafka生产者的配置对象</span></span><br><span class="line">        Properties properties = <span class="keyword">new</span> Properties();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2. 给kafka配置对象添加配置信息</span></span><br><span class="line">        <span class="comment">// properties.put(&quot;bootstrap.servers&quot;,&quot;hadoop001:9092&quot;);</span></span><br><span class="line">        properties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">&quot;hadoop001:9092,hadoop002:9092,hadoop003:9092,hadoop004:9092,hadoop005:9092&quot;</span>);</span><br><span class="line">        <span class="comment">// key,value序列化（必须）：key.serializer，value.serializer</span></span><br><span class="line">        properties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());</span><br><span class="line">        properties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 批次大小 默认16K：batch.size</span></span><br><span class="line">        properties.put(ProducerConfig.BATCH_SIZE_CONFIG, <span class="number">16384</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 等待时间：linger.ms</span></span><br><span class="line">        properties.put(ProducerConfig.LINGER_MS_CONFIG, <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// RecordAccumulator缓冲区大小 默认32M：buffer.memory</span></span><br><span class="line">        properties.put(ProducerConfig.BUFFER_MEMORY_CONFIG, <span class="number">33554432</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置ack</span></span><br><span class="line">        <span class="comment">// properties.put(&quot;acks&quot;, &quot;all&quot;);</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 重试次数</span></span><br><span class="line">        <span class="comment">// properties.put(&quot;retries&quot;, 3);</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3. 创建kafka生产者对象</span></span><br><span class="line">        KafkaProducer&lt;String, String&gt; kafkaProducer = <span class="keyword">new</span> KafkaProducer&lt;&gt;(properties);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 4. 调用send方法,发送消息</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">10</span>; i++) &#123;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 异步发送 默认</span></span><br><span class="line"><span class="comment">//            kafkaProducer.send(new ProducerRecord&lt;&gt;(&quot;first&quot;,&quot;kafka&quot; + i));</span></span><br><span class="line">            <span class="comment">// 同步发送</span></span><br><span class="line">            <span class="keyword">final</span> Future&lt;RecordMetadata&gt; future = kafkaProducer.send(<span class="keyword">new</span> ProducerRecord&lt;&gt;(<span class="string">&quot;first&quot;</span>, <span class="string">&quot;kafka&quot;</span> + i));</span><br><span class="line">            <span class="keyword">final</span> RecordMetadata recordMetadata = future.get();</span><br><span class="line">            System.out.println(<span class="string">&quot;topic:&quot;</span> + recordMetadata.topic() + <span class="string">&quot;--&gt;&quot;</span> + <span class="string">&quot;partition&quot;</span> + recordMetadata.partition());</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 5. 关闭资源</span></span><br><span class="line">        kafkaProducer.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>测试<ol>
<li>在hadoop001上开启Kafka消费者。 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 kafka]$ bin/kafka-console-consumer.sh --bootstrap-server hadoop001:9092 --topic first</span><br></pre></td></tr></table></figure></li>
<li>在IDEA中执行代码，观察hadoop001控制台中是否接收到消息。 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 kafka]$  bin/kafka-console-consumer.sh --bootstrap-server hadoop001:9092 --topic first</span><br><span class="line">kafka0</span><br><span class="line">kafka1</span><br><span class="line">kafka2</span><br><span class="line">kafka3</span><br><span class="line">kafka4</span><br><span class="line">kafka5</span><br><span class="line">kafka6</span><br><span class="line">kafka7</span><br><span class="line">kafka8</span><br><span class="line">kafka9</span><br></pre></td></tr></table></figure></li>
<li>在IDEA控制台观察同步返回结果 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">topic:first--&gt;partition0</span><br><span class="line">topic:first--&gt;partition2</span><br><span class="line">topic:first--&gt;partition1</span><br><span class="line">topic:first--&gt;partition0</span><br><span class="line">topic:first--&gt;partition1</span><br><span class="line">topic:first--&gt;partition2</span><br><span class="line">topic:first--&gt;partition0</span><br><span class="line">topic:first--&gt;partition1</span><br><span class="line">topic:first--&gt;partition2</span><br><span class="line">topic:first--&gt;partition0</span><br></pre></td></tr></table></figure></li>
</ol>
</li>
</ol>
<h2 id="3-4-生产者分区"><a href="#3-4-生产者分区" class="headerlink" title="3.4 生产者分区"></a>3.4 生产者分区</h2><h3 id="3-4-1-分区的好处"><a href="#3-4-1-分区的好处" class="headerlink" title="3.4.1 分区的好处"></a>3.4.1 分区的好处</h3><p><img src="https://i.loli.net/2021/11/19/e8UvXuCBkH92pN6.jpg"></p>
<ol>
<li><font color ='red' >便于合理使用存储资源</font>，每个Partition在一个Broker上存储一部分数据，如果Broker足够多，那么就可以设置足够多的分区，因此整个集群就可以存储任意大小的数据了；</li>
<li><font color ='red' >提高并发度</font>，消费者可以以分区为单位进行消费。</li>
</ol>
<h3 id="3-4-2-生产者发送消息的分区策略"><a href="#3-4-2-生产者发送消息的分区策略" class="headerlink" title="3.4.2 生产者发送消息的分区策略"></a>3.4.2 生产者发送消息的分区策略</h3><ol>
<li><p>默认的分区器 DefaultPartitioner</p>
 <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">DefaultPartitioner</span> <span class="keyword">implements</span> <span class="title">Partitioner</span> </span>&#123;</span><br><span class="line">    ···</span><br><span class="line">    ···</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">partition</span><span class="params">(String topic, Object key, <span class="keyword">byte</span>[] keyBytes, Object value, <span class="keyword">byte</span>[] valueBytes, Cluster cluster,</span></span></span><br><span class="line"><span class="params"><span class="function">                         <span class="keyword">int</span> numPartitions)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (keyBytes == <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> stickyPartitionCache.partition(topic, cluster);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// hash the keyBytes to choose a partition</span></span><br><span class="line">        <span class="keyword">return</span> Utils.toPositive(Utils.murmur2(keyBytes)) % numPartitions;</span><br><span class="line">    &#125;</span><br><span class="line">    ···</span><br><span class="line">    ···</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li><p>ProducerRecord</p>
 <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ProducerRecord</span>&lt;<span class="title">K</span>, <span class="title">V</span>&gt; </span>&#123;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * 指明partition的情况下，直接将指明的值作为partiton值。</span></span><br><span class="line"><span class="comment">    * 例如partition=0，所有数据写入分区0</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">ProducerRecord</span><span class="params">(String topic, Integer partition, Long timestamp, K key, V value, Iterable&lt;Header&gt; headers)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (topic == <span class="keyword">null</span>)</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException(<span class="string">&quot;Topic cannot be null.&quot;</span>);</span><br><span class="line">        <span class="keyword">if</span> (timestamp != <span class="keyword">null</span> &amp;&amp; timestamp &lt; <span class="number">0</span>)</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException(</span><br><span class="line">                    String.format(<span class="string">&quot;Invalid timestamp: %d. Timestamp should always be non-negative or null.&quot;</span>, timestamp));</span><br><span class="line">        <span class="keyword">if</span> (partition != <span class="keyword">null</span> &amp;&amp; partition &lt; <span class="number">0</span>)</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException(</span><br><span class="line">                    String.format(<span class="string">&quot;Invalid partition: %d. Partition number should always be non-negative or null.&quot;</span>, partition));</span><br><span class="line">        <span class="keyword">this</span>.topic = topic;</span><br><span class="line">        <span class="keyword">this</span>.partition = partition;</span><br><span class="line">        <span class="keyword">this</span>.key = key;</span><br><span class="line">        <span class="keyword">this</span>.value = value;</span><br><span class="line">        <span class="keyword">this</span>.timestamp = timestamp;</span><br><span class="line">        <span class="keyword">this</span>.headers = <span class="keyword">new</span> RecordHeaders(headers);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">ProducerRecord</span><span class="params">(String topic, Integer partition, Long timestamp, K key, V value)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>(topic, partition, timestamp, key, value, <span class="keyword">null</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">   </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">ProducerRecord</span><span class="params">(String topic, Integer partition, K key, V value, Iterable&lt;Header&gt; headers)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>(topic, partition, <span class="keyword">null</span>, key, value, headers);</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">ProducerRecord</span><span class="params">(String topic, Integer partition, K key, V value)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>(topic, partition, <span class="keyword">null</span>, key, value, <span class="keyword">null</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">    * 没有指明partition值但有key的情况下，将key的hash值与topic的partition数进行取余得到partition值；</span></span><br><span class="line"><span class="comment">    * 例如：key1的hash值=5， key2的hash值=6 ，topic的partition数=2，那么key1 对应的value1写入1号分区，key2对应的value2写入0号分区。</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">ProducerRecord</span><span class="params">(String topic, K key, V value)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>(topic, <span class="keyword">null</span>, <span class="keyword">null</span>, key, value, <span class="keyword">null</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">    * 既没有partition值又没有key值的情况下，Kafka采用Sticky Partition（黏性分区器），会随机选择一个分区，并尽可能一直使用该分区，待该分区的batch已满或者已完成，Kafka再随机一个分区进行使用（和上一次的分区不同）。</span></span><br><span class="line"><span class="comment">    * 例如：第一次随机选择0号分区，等0号分区当前批次满了（16k）或者linger.ms设置的时间到， Kafka再随机一个分区进行使用（如果还是0会继续随机）。</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">ProducerRecord</span><span class="params">(String topic, V value)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>(topic, <span class="keyword">null</span>, <span class="keyword">null</span>, <span class="keyword">null</span>, value, <span class="keyword">null</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li><p>源码跟踪</p>
<ul>
<li>KafkaProducer#send(ProducerRecord&lt;K,V&gt;)</li>
<li>KafkaProducer#doSend</li>
<li>KafkaProducer#partition  <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">int</span> <span class="title">partition</span><span class="params">(ProducerRecord&lt;K, V&gt; record, <span class="keyword">byte</span>[] serializedKey, <span class="keyword">byte</span>[] serializedValue, Cluster cluster)</span> </span>&#123;</span><br><span class="line">    Integer partition = record.partition();</span><br><span class="line">    <span class="keyword">return</span> partition != <span class="keyword">null</span> ?</span><br><span class="line">            partition :</span><br><span class="line">            partitioner.partition(</span><br><span class="line">                    record.topic(), record.key(), serializedKey, record.value(), serializedValue, cluster);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>将数据发往指定partition的情况下，例如，将所有数据发往分区1中。</p>
<ol>
<li>编码 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">public class CustomProducerCallbackPartitions &#123;</span><br><span class="line">    public static void main(String[] args) &#123;</span><br><span class="line"></span><br><span class="line">        // 1. 创建kafka生产者的配置对象</span><br><span class="line">        Properties properties = new Properties();</span><br><span class="line"></span><br><span class="line">        // 2. 给kafka配置对象添加配置信息</span><br><span class="line">        // properties.put(<span class="string">&quot;bootstrap.servers&quot;</span>,<span class="string">&quot;hadoop001:9092&quot;</span>);</span><br><span class="line">        properties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">&quot;hadoop001:9092,hadoop002:9092,hadoop003:9092,hadoop004:9092,hadoop005:9092&quot;</span>);</span><br><span class="line">        // key,value序列化（必须）：key.serializer，value.serializer</span><br><span class="line">        properties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());</span><br><span class="line">        properties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        KafkaProducer&lt;String, String&gt; kafkaProducer = new KafkaProducer&lt;&gt;(properties);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (int i = 0; i &lt; 5; i++) &#123;</span><br><span class="line">            // 指定数据发送到1号分区，key为空（IDEA中ctrl + p查看参数）</span><br><span class="line">            kafkaProducer.send(new ProducerRecord&lt;&gt;(<span class="string">&quot;first&quot;</span>, 1,<span class="string">&quot;&quot;</span>,<span class="string">&quot;kafka  &quot;</span> + i), new <span class="function"><span class="title">Callback</span></span>() &#123;</span><br><span class="line">                @Override</span><br><span class="line">                public void onCompletion(RecordMetadata recordMetadata, Exception e) &#123;</span><br><span class="line">                    <span class="keyword">if</span> (e == null)&#123;</span><br><span class="line">                        System.out.println(<span class="string">&quot;onCompletion 主题：&quot;</span> + recordMetadata.topic() + <span class="string">&quot;-&gt;&quot;</span> + <span class="string">&quot;分区：&quot;</span> + recordMetadata.partition()</span><br><span class="line">                        );</span><br><span class="line">                    &#125;<span class="keyword">else</span> &#123;</span><br><span class="line">                        e.printStackTrace();</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        kafkaProducer.close();</span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>测试<ol>
<li>在hadoop001上开启Kafka消费者。 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 kafka]$ bin/kafka-console-consumer.sh --bootstrap-server hadoop001:9092 --topic first</span><br></pre></td></tr></table></figure></li>
<li>在IDEA中执行代码，观察hadoop001控制台中是否接收到消息。 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 kafka]$  bin/kafka-console-consumer.sh --bootstrap-server hadoop001:9092 --topic first</span><br><span class="line">atguigu 0</span><br><span class="line">atguigu 1</span><br><span class="line">atguigu 2</span><br><span class="line">atguigu 3</span><br><span class="line">atguigu 4</span><br></pre></td></tr></table></figure></li>
<li>在IDEA控制台观察回调信息 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">onCompletion 主题：first-&gt;分区：1</span><br><span class="line">onCompletion 主题：first-&gt;分区：1</span><br><span class="line">onCompletion 主题：first-&gt;分区：1</span><br><span class="line">onCompletion 主题：first-&gt;分区：1</span><br><span class="line">onCompletion 主题：first-&gt;分区：1</span><br></pre></td></tr></table></figure></li>
</ol>
</li>
</ol>
</li>
<li><p>没有指明partition值但有key的情况下，将key的字节数组hash值与topic的partition数进行取余得到partition值；</p>
<ol>
<li>编码 <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CustomProducerCallbackWithoutPartition</span> </span>&#123;</span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">    </span><br><span class="line">            <span class="comment">// 1. 创建kafka生产者的配置对象</span></span><br><span class="line">            Properties properties = <span class="keyword">new</span> Properties();</span><br><span class="line">    </span><br><span class="line">            <span class="comment">// 2. 给kafka配置对象添加配置信息</span></span><br><span class="line">            <span class="comment">// properties.put(&quot;bootstrap.servers&quot;,&quot;hadoop001:9092&quot;);</span></span><br><span class="line">            properties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">&quot;hadoop001:9092,hadoop002:9092,hadoop003:9092,hadoop004:9092,hadoop005:9092&quot;</span>);</span><br><span class="line">            <span class="comment">// key,value序列化（必须）：key.serializer，value.serializer</span></span><br><span class="line">            properties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());</span><br><span class="line">            properties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">            KafkaProducer&lt;String, String&gt; kafkaProducer = <span class="keyword">new</span> KafkaProducer&lt;&gt;(properties);</span><br><span class="line">    </span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">10</span>; i++) &#123;</span><br><span class="line">                <span class="comment">// 依次指定key值为i ，数据key的hash值与3个分区求余，分别发往1、2、0</span></span><br><span class="line">                <span class="keyword">int</span> finalI = i;</span><br><span class="line">                kafkaProducer.send(<span class="keyword">new</span> ProducerRecord&lt;&gt;(<span class="string">&quot;first&quot;</span>, i + <span class="string">&quot;&quot;</span>, <span class="string">&quot;atguigu &quot;</span> + i), <span class="keyword">new</span> Callback() &#123;</span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onCompletion</span><span class="params">(RecordMetadata recordMetadata, Exception e)</span> </span>&#123;</span><br><span class="line">                        <span class="keyword">if</span> (e == <span class="keyword">null</span>) &#123;</span><br><span class="line">                            System.out.println(<span class="string">&quot;key:&quot;</span> + finalI + <span class="string">&quot; 主题：&quot;</span> + recordMetadata.topic() + <span class="string">&quot;-&gt;&quot;</span></span><br><span class="line">                                    + <span class="string">&quot;分区：&quot;</span> + recordMetadata.partition()</span><br><span class="line">                            );</span><br><span class="line">                        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                            e.printStackTrace();</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;);</span><br><span class="line">            &#125;</span><br><span class="line">    </span><br><span class="line">            kafkaProducer.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>测试<ol>
<li>在hadoop001上开启Kafka消费者。 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 kafka]$ bin/kafka-console-consumer.sh --bootstrap-server hadoop001:9092 --topic first</span><br></pre></td></tr></table></figure></li>
<li>在IDEA中执行代码，观察hadoop001控制台中是否接收到消息。 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 kafka]$  bin/kafka-console-consumer.sh --bootstrap-server hadoop001:9092 --topic first</span><br><span class="line">atguigu 0</span><br><span class="line">atguigu 2</span><br><span class="line">atguigu 3</span><br><span class="line">atguigu 9</span><br><span class="line">atguigu 1</span><br><span class="line">atguigu 5</span><br><span class="line">atguigu 7</span><br><span class="line">atguigu 8</span><br><span class="line">atguigu 4</span><br><span class="line">atguigu 6</span><br></pre></td></tr></table></figure></li>
<li>在IDEA控制台观察回调信息 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">key:0 主题：first-&gt;分区：2</span><br><span class="line">key:2 主题：first-&gt;分区：2</span><br><span class="line">key:3 主题：first-&gt;分区：2</span><br><span class="line">key:9 主题：first-&gt;分区：2</span><br><span class="line">key:1 主题：first-&gt;分区：0</span><br><span class="line">key:5 主题：first-&gt;分区：0</span><br><span class="line">key:7 主题：first-&gt;分区：0</span><br><span class="line">key:8 主题：first-&gt;分区：0</span><br><span class="line">key:4 主题：first-&gt;分区：1</span><br><span class="line">key:6 主题：first-&gt;分区：1</span><br></pre></td></tr></table></figure>
<h3 id="3-4-3-自定义分区器"><a href="#3-4-3-自定义分区器" class="headerlink" title="3.4.3 自定义分区器"></a>3.4.3 自定义分区器</h3>研发人员可以根据企业需求，自己重新实现分区器。</li>
</ol>
</li>
</ol>
</li>
<li><p>需求<br> 实现一个分区器，发送过来的数据中如果包含atguigu，就发往0号分区，不包含atguigu，就发往1号分区</p>
</li>
<li><p>实现步骤</p>
<ol>
<li>定义类实现Partitioner接口</li>
<li>重写partition()方法</li>
</ol>
</li>
<li><p>自定义分区器编码</p>
 <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CustomProducerCallbackUsingMyPartition</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        Properties properties = <span class="keyword">new</span> Properties();</span><br><span class="line">        properties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">&quot;hadoop001:9092,hadoop002:9092,hadoop003:9092,hadoop004:9092,hadoop005:9092&quot;</span>);</span><br><span class="line">        properties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());</span><br><span class="line">        properties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());</span><br><span class="line"></span><br><span class="line">        properties.put(<span class="string">&quot;batch.size&quot;</span>, <span class="number">16384</span>);</span><br><span class="line">        properties.put(<span class="string">&quot;linger.ms&quot;</span>, <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        properties.put(<span class="string">&quot;buffer.memory&quot;</span>,<span class="number">33554432</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 添加自定义分区器</span></span><br><span class="line">        properties.put(ProducerConfig.PARTITIONER_CLASS_CONFIG, MyPartitioner.class.getName());</span><br><span class="line"></span><br><span class="line">        KafkaProducer&lt;String, String&gt; kafkaProducer = <span class="keyword">new</span> KafkaProducer&lt;&gt;(properties);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">5</span>; i++) &#123;</span><br><span class="line"></span><br><span class="line">            kafkaProducer.send(<span class="keyword">new</span> ProducerRecord&lt;&gt;(<span class="string">&quot;first&quot;</span>, i % <span class="number">2</span> == <span class="number">0</span> ? <span class="string">&quot;atguigu &quot;</span> + i : i + <span class="string">&quot;&quot;</span>), <span class="keyword">new</span> Callback() &#123;</span><br><span class="line">                <span class="meta">@Override</span></span><br><span class="line">                <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onCompletion</span><span class="params">(RecordMetadata recordMetadata, Exception e)</span> </span>&#123;</span><br><span class="line">                    <span class="keyword">if</span> (e == <span class="keyword">null</span>) &#123;</span><br><span class="line">                        System.out.println(<span class="string">&quot;主题：&quot;</span> + recordMetadata.topic() + <span class="string">&quot;-&gt;&quot;</span></span><br><span class="line">                                + <span class="string">&quot;分区：&quot;</span> + recordMetadata.partition()</span><br><span class="line">                        );</span><br><span class="line">                    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                        e.printStackTrace();</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        kafkaProducer.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li><p>使用分区器的方法，在生产者的配置中添加分区器参数</p>
 <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CustomProducerCallbackUsingMyPartition</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        Properties properties = <span class="keyword">new</span> Properties();</span><br><span class="line">        properties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">&quot;hadoop001:9092,hadoop002:9092,hadoop003:9092,hadoop004:9092,hadoop005:9092&quot;</span>);</span><br><span class="line">        properties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG);</span><br><span class="line">        properties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());</span><br><span class="line"></span><br><span class="line">        properties.put(<span class="string">&quot;batch.size&quot;</span>, <span class="number">16384</span>);</span><br><span class="line">        properties.put(<span class="string">&quot;linger.ms&quot;</span>, <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        properties.put(<span class="string">&quot;buffer.memory&quot;</span>,<span class="number">33554432</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 添加自定义分区器</span></span><br><span class="line">        properties.put(ProducerConfig.PARTITIONER_CLASS_CONFIG, MyPartitioner.class.getName());</span><br><span class="line"></span><br><span class="line">        KafkaProducer&lt;String, String&gt; kafkaProducer = <span class="keyword">new</span> KafkaProducer&lt;&gt;(properties);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">5</span>; i++) &#123;</span><br><span class="line"></span><br><span class="line">            kafkaProducer.send(<span class="keyword">new</span> ProducerRecord&lt;&gt;(<span class="string">&quot;first&quot;</span>, i % <span class="number">2</span> == <span class="number">0</span> ? <span class="string">&quot;atguigu &quot;</span> + i : i + <span class="string">&quot;&quot;</span>), <span class="keyword">new</span> Callback() &#123;</span><br><span class="line">                <span class="meta">@Override</span></span><br><span class="line">                <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onCompletion</span><span class="params">(RecordMetadata recordMetadata, Exception e)</span> </span>&#123;</span><br><span class="line">                    <span class="keyword">if</span> (e == <span class="keyword">null</span>) &#123;</span><br><span class="line">                        System.out.println(<span class="string">&quot;主题：&quot;</span> + recordMetadata.topic() + <span class="string">&quot;-&gt;&quot;</span></span><br><span class="line">                                + <span class="string">&quot;分区：&quot;</span> + recordMetadata.partition()</span><br><span class="line">                        );</span><br><span class="line">                    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                        e.printStackTrace();</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        kafkaProducer.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li><p>测试</p>
<ol>
<li>在hadoop001上开启Kafka消费者。 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kafka-console-consumer.sh --bootstrap-server hadoop001:9092 --topic first</span><br><span class="line">atguigu 0</span><br><span class="line">atguigu 2</span><br><span class="line">atguigu 4</span><br><span class="line">1</span><br><span class="line">3</span><br></pre></td></tr></table></figure></li>
<li>在IDEA控制台观察回调信息 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">主题：first-&gt;分区：0</span><br><span class="line">主题：first-&gt;分区：0</span><br><span class="line">主题：first-&gt;分区：0</span><br><span class="line">主题：first-&gt;分区：1</span><br><span class="line">主题：first-&gt;分区：1</span><br></pre></td></tr></table></figure>
<h1 id="第4章-Kafka-Broker"><a href="#第4章-Kafka-Broker" class="headerlink" title="第4章 Kafka Broker"></a>第4章 Kafka Broker</h1><h2 id="4-1-Kafka-Broker工作流程"><a href="#4-1-Kafka-Broker工作流程" class="headerlink" title="4.1 Kafka Broker工作流程"></a>4.1 Kafka Broker工作流程</h2><h3 id="4-1-1-Zookeeper中存储的Kafka信息"><a href="#4-1-1-Zookeeper中存储的Kafka信息" class="headerlink" title="4.1.1 Zookeeper中存储的Kafka信息"></a>4.1.1 Zookeeper中存储的Kafka信息</h3><img src="https://i.loli.net/2021/11/19/h7ixP2akuVMmpQT.jpg"></li>
</ol>
</li>
<li><p>在zookeeper的服务端存储的Kafka相关信息：</p>
<ul>
<li><code>/kafka/admin</code>: 存储管理信息。主要为删除主题事件，分区迁移事件，优先副本选举信息 (一般为临时节点)</li>
<li><code>/kafka/brokers</code>: 存储 Broker 相关信息。broker 节点以及节点上的主题相关信息<ul>
<li><code>/kafka/brokers/topics/[topic]</code>: 存储某个topic的partitions所有分配信息  <figure class="highlight json"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;removing_replicas&quot;</span>: &#123;&#125;,</span><br><span class="line">  <span class="attr">&quot;partitions&quot;</span>: &#123; </span><br><span class="line">      <span class="attr">&quot;2&quot;</span>: [<span class="number">3</span>, <span class="number">4</span>], # 同步副本组brokerId列表</span><br><span class="line">      <span class="attr">&quot;1&quot;</span>: [<span class="number">2</span>, <span class="number">3</span>], </span><br><span class="line">      <span class="attr">&quot;0&quot;</span>: [<span class="number">1</span>, <span class="number">2</span>] </span><br><span class="line">    &#125;,</span><br><span class="line">  <span class="attr">&quot;topic_id&quot;</span>: <span class="string">&quot;wuuibxosTj2iZnI-8pknXw&quot;</span>,</span><br><span class="line">  <span class="attr">&quot;adding_replicas&quot;</span>: &#123;&#125;,</span><br><span class="line">  <span class="attr">&quot;version&quot;</span>: <span class="number">3</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li><code>/kafka/brokers/topics/[topic]/partitions/[broker.id]/state</code>: partition状态信息,记录谁是Leader，有哪些服务器可用  <figure class="highlight json"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;controller_epoch&quot;</span>: <span class="number">1</span>, #表示kafka集群中的中央控制器选举次数</span><br><span class="line">  <span class="attr">&quot;leader&quot;</span>: <span class="number">3</span>, #表示该partition选举leader的brokerId,</span><br><span class="line">  <span class="attr">&quot;version&quot;</span>: <span class="number">1</span>,</span><br><span class="line">  <span class="attr">&quot;leader_epoch&quot;</span>: <span class="number">0</span>, #该partition leader选举次数</span><br><span class="line">  <span class="attr">&quot;isr&quot;</span>: [<span class="number">3</span>, <span class="number">4</span>] #[同步副本组brokerId列表]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li><code>/kafka/brokers/ids[0...N]</code>: 记录有哪些服务器, 每个broker的配置文件中都需要指定一个数字类型的id(全局不可重复),此节点为临时znode(EPHEMERAL)  <figure class="highlight json"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;listener_security_protocol_map&quot;</span>: &#123; <span class="attr">&quot;PLAINTEXT&quot;</span>: <span class="string">&quot;PLAINTEXT&quot;</span> &#125;,</span><br><span class="line">  <span class="attr">&quot;endpoints&quot;</span>: [<span class="string">&quot;PLAINTEXT://hadoop001:9092&quot;</span>],</span><br><span class="line">  <span class="attr">&quot;jmx_port&quot;</span>: <span class="number">-1</span>, # jmx端口号</span><br><span class="line">  <span class="attr">&quot;features&quot;</span>: &#123;&#125;,</span><br><span class="line">  <span class="attr">&quot;host&quot;</span>: <span class="string">&quot;hadoop001&quot;</span>, #主机名或ip地址</span><br><span class="line">  <span class="attr">&quot;timestamp&quot;</span>: <span class="string">&quot;1636338340201&quot;</span>, # kafka broker初始启动时的时间戳</span><br><span class="line">  <span class="attr">&quot;port&quot;</span>: <span class="number">9092</span>, # kafka broker的服务端端口号,由server.properties中参数port确定</span><br><span class="line">  <span class="attr">&quot;version&quot;</span>: <span class="number">5</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><code>/kafka/cluster</code>: 存储 kafka 集群信息</li>
<li><code>/kafka/controller</code>: 存储控制器节点信息,辅助选举Leader  <figure class="highlight json"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">     <span class="attr">&quot;version&quot;</span>: <span class="number">1</span>, </span><br><span class="line">     <span class="attr">&quot;brokerid&quot;</span>: <span class="number">1</span>, </span><br><span class="line">     <span class="attr">&quot;timestamp&quot;</span>: <span class="string">&quot;1636338340300&quot;</span> </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li><code>/kafka/controller_epoch</code><ul>
<li>此值为一个数字,kafka集群中第一个broker第一次启动时为1，以后只要集群中center controller中央控制器所在broker变更或挂掉，就会重新选举新的center controller，每次center controller变更controller_epoch值就会 + 1; <h3 id="4-1-2-Leader选举流程"><a href="#4-1-2-Leader选举流程" class="headerlink" title="4.1.2 Leader选举流程"></a>4.1.2 Leader选举流程</h3>kafka集群中有2个种leader，一种是broker的leader即controller leader，还有一种就是partition的leader<br><img src="https://i.loli.net/2021/11/19/xu9tYf7TRJ1hN2m.jpg"></li>
</ul>
</li>
</ul>
</li>
</ol>
<h4 id="4-1-2-1-Controller-Leader"><a href="#4-1-2-1-Controller-Leader" class="headerlink" title="4.1.2.1 Controller Leader"></a>4.1.2.1 Controller Leader</h4><p>控制器（Controller），是 Apache Kafka 的核心组件。它的主要作用是在 ZooKeeper 的帮助下管理和协调整个 Kafka 集群。控制器其实就是一个 Broker，只不过它除了具有一般 Broker 的功能以外，还负责 Leader 的选举。</p>
<h5 id="4-1-2-1-1-如何选举控制器"><a href="#4-1-2-1-1-如何选举控制器" class="headerlink" title="4.1.2.1.1 如何选举控制器"></a>4.1.2.1.1 如何选举控制器</h5><p>集群中任意一台 Broker 都能充当控制器的角色，但是，在运行过程中，只能有一个 Broker 成为控制器，行使其管理和协调的职责。实际上，Broker 在启动时，会尝试去 ZooKeeper 中创建 /controller 节点。Kafka 当前选举控制器的规则是：第一个在 ZooKeeper 成功创建 /controller 临时节点的 Broker 会被指定为控制器。<br><img src="https://i.loli.net/2021/11/19/VD6FaWlZjnR2Ey8.jpg"></p>
<ol>
<li>第一个在 ZooKeeper 中成功创建 /controller 临时节点的 Broker 会被指定为控制器。</li>
<li>其他 Broker 在控制器节点上创建 Zookeeper watch 对象。</li>
<li>如果控制器被关闭或者与 Zookeeper 断开连接，Zookeeper 临时节点就会消失。集群中的其他 Broker 通过 watch 对象得到状态变化的通知，它们会尝试让自己成为新的控制器。</li>
<li>第一个在 Zookeeper 里创建一个临时节点 /controller 的 Broker 成为新控制器。其他 Broker 在新控制器节点上创建 Zookeeper watch 对象。</li>
<li>每个新选出的控制器通过 Zookeeper 的条件递增操作获得一个全新的、数值更大的 controller epoch。其他节点会忽略旧的 epoch 的消息。</li>
<li>当控制器发现一个 Broker 已离开集群，并且这个 Broker 是某些 Partition 的 Leader。此时，控制器会遍历这些 Partition，并用轮询方式确定谁应该成为新 Leader，随后，新 Leader 开始处理生产者和消费者的请求，而 Follower 开始从 Leader 那里复制消息。<blockquote>
<p>总结：<br>Kafka 使用 Zookeeper 的临时节点来选举控制器，并在节点加入集群或退出集群时通知控制器。控制器负责在节点加入或离开集群时进行 Partition Leader 选举。控制器使用 epoch 来避免“脑裂”，“脑裂”是指两个节点同时被认为自己是当前的控制器</p>
</blockquote>
</li>
</ol>
<h4 id="4-1-2-2-Partition-leader"><a href="#4-1-2-2-Partition-leader" class="headerlink" title="4.1.2.2 Partition leader"></a>4.1.2.2 Partition leader</h4><ol>
<li>Controller Leader监听brokers节点变化,并决定分区、副本分配和分区Leader选举</li>
<li>Controller Leader 将节点信息上传到ZK <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/brokers/topics/[topic]/partitions/[broker.id]/state</span><br><span class="line">&#123;&quot;controller_epoch&quot;:1,&quot;leader&quot;:1,&quot;version&quot;:1,&quot;leader_epoch&quot;:0,&quot;isr&quot;:[1,2]&#125;</span><br></pre></td></tr></table></figure></li>
<li>如果当前Partition Leader的Broker挂了</li>
<li>Controller Leader监听到节点变化</li>
<li>Controller Leader获取ISR</li>
<li>选举新的Leader（在ISR中排在前面的优先）</li>
<li>更新Leader及ISR完成Partition Leader 选举</li>
<li>如果 ISR 为空了，就说明 Leader 副本也“挂掉”了，Kafka 需要重新选举一个新的 Leader。<ul>
<li>Kafka 把所有不在 ISR 中的存活副本都称为非同步副本。通常来说，非同步副本落后 Leader 太多，因此，如果选择这些副本作为新 Leader，就可能出现数据的丢失。毕竟，这些副本中保存的消息远远落后于老 Leader 中的消息。在 Kafka 中，选举这种副本的过程称为 Unclean 领导者选举。Broker 端参数 unclean.leader.election.enable 控制是否允许 Unclean 领导者选举。</li>
<li>开启 Unclean 领导者选举可能会造成数据丢失，但好处是：它使得 Partition Leader 副本一直存在，不至于停止对外提供服务，因此提升了高可用性。反之，禁止 Unclean 领导者选举的好处在于维护了数据的一致性，避免了消息丢失，但牺牲了高可用性。</li>
</ul>
</li>
</ol>
<h3 id="4-1-3-Kafka-Broker启动流程"><a href="#4-1-3-Kafka-Broker启动流程" class="headerlink" title="4.1.3 Kafka Broker启动流程"></a>4.1.3 Kafka Broker启动流程</h3><p><img src="https://i.loli.net/2021/11/19/FZSaxJR1PQIpXts.jpg"></p>
<table>
<thead>
<tr>
<th>参数名称</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>acks</td>
<td>0：生产者发送过来的数据，不需要等数据落盘应答。<br>1：生产者发送过来的数据，Leader收到数据后应答。<br>-1（all）：生产者发送过来的数据，Leader+和isr队列里面的所有节点收齐数据后应答。默认值是-1。</td>
</tr>
<tr>
<td>enable.idempotence</td>
<td>是否开启事务，默认true，开启事务</td>
</tr>
<tr>
<td>max.in.flight.requests.per.connection</td>
<td>允许最多没有返回ack的次数，默认为5，开启幂等性要保证该值是 1-5的数字。</td>
</tr>
<tr>
<td>log.segment.bytes</td>
<td>Kafka中log日志是分成一块块存储的，此配置是指log日志划分 成块的大小，默认值1G。</td>
</tr>
<tr>
<td>log.retention.hours</td>
<td>Kafka中数据保存的时间，默认7天</td>
</tr>
</tbody></table>
<h2 id="4-2-数据可靠性保证"><a href="#4-2-数据可靠性保证" class="headerlink" title="4.2 数据可靠性保证"></a>4.2 数据可靠性保证</h2><h3 id="4-2-1-ack应答级别"><a href="#4-2-1-ack应答级别" class="headerlink" title="4.2.1 ack应答级别"></a>4.2.1 ack应答级别</h3><p>当producer向leader发送数据时，可以通过request.required.acks参数来设置数据可靠性的级别</p>
<ul>
<li>0：生产者发送过来的数据，不需要等数据落盘应答。这种情况下数据传输效率最高，但是数据可靠性确是最低的<br>  <img src="https://i.loli.net/2021/11/19/TVCWoOZ5A3NuR8D.jpg"></li>
<li>1：生产者发送过来的数据，Leader收到数据后应答。<ul>
<li>数据丢失分析：producer发送数据到leader，leader写本地日志成功，返回客户端成功；此时ISR中的副本还没有来得及拉取该消息，leader就宕机了，那么此次发送的消息就会丢失<br><img src="https://i.loli.net/2021/11/19/SEUKrXFvO6ex5Ms.jpg"></li>
</ul>
</li>
<li>-1（all）：生产者发送过来的数据，Leader和ISR队列里面的所有节点收齐数据后应答。<ul>
<li>acks=-1的情况下，数据发送到leader, ISR的follower全部完成数据同步后，leader此时挂掉，那么会选举出新的leader，数据不会丢失<br>  <img src="https://i.loli.net/2021/11/19/yQGx8LTE4cYfXPV.jpg"></li>
<li>思考：Leader收到数据，所有Follower都开始同步数据，但有一个Follower，因为某种故障，迟迟不能与Leader进行同步，那这个问题怎么解决呢？</li>
<li>Leader维护了一个动态的in-sync replica set（ISR），意为和Leader保持同步的Follower集合。当ISR中的Follower完成数据的同步之后，Leader就会给producer发送ack。如果Follower长时间未向Leader同步数据，则该Follower将被踢出ISR，该时间阈值由replica.lag.time.max.ms参数设定，默认30s。Leader发生故障之后，就会从ISR中选举新的Leader。</li>
<li>数据重复分析：acks=-1的情况下，数据发送到leader后 ，部分ISR的副本同步，leader此时挂掉。比如follower1和follower2都有可能变成新的leader, producer端会得到返回异常，producer端会重新发送数据，数据可能会重复<br>  <img src="https://i.loli.net/2021/11/19/5XCGBIZmNsOjQ1F.jpg"><br>  <img src="https://i.loli.net/2021/11/19/mHgn63Gj2yaLs7Z.jpg"></li>
</ul>
</li>
</ul>
<h3 id="4-2-2-Leader和Follower故障处理细节"><a href="#4-2-2-Leader和Follower故障处理细节" class="headerlink" title="4.2.2 Leader和Follower故障处理细节"></a>4.2.2 Leader和Follower故障处理细节</h3><ul>
<li><p>LEO（Log End Offset）：每个副本的最后一个offset</p>
</li>
<li><p>HW（High Watermark）：所有副本中最小的LEO，也是消费者能见到的最大的offset</p>
</li>
<li><p>Leader故障</p>
<ul>
<li>leader 发生故障之后，会从 ISR 中选出一个新的 leader</li>
<li>为保证多个副本之间的数据一致性，其余的 follower 会先将各自的 log 文件高于 HW 的部分截掉，然后从新的 leader同步数据<blockquote>
<p>注意：这只能保证副本之间的数据一致性，并不能保证数据不丢失或者不重复。</p>
</blockquote>
</li>
</ul>
</li>
<li><p>Follower故障</p>
<ul>
<li>follower 发生故障后会被临时踢出 ISR</li>
<li>待该 follower 恢复后，follower 会读取本地磁盘记录的上次的 HW，并将 log 文件高于 HW 的部分截取掉，从 HW 开始向 leader 进行同步。</li>
<li>等该 follower 的 LEO 大于等于该 Partition 的 HW，即 follower 追上 leader 之后，就可以重新加入 ISR 了。</li>
</ul>
</li>
</ul>
<h3 id="4-2-3-精确一次（Exactly-Once）"><a href="#4-2-3-精确一次（Exactly-Once）" class="headerlink" title="4.2.3 精确一次（Exactly Once）"></a>4.2.3 精确一次（Exactly Once）</h3><ul>
<li>至少一次（At Least Once）：例如，将服务器的ACK级别设置为-1（ISR完全应答），且min.insync.replicas设置大于等于2.可以保证Producer到Broker之间不会丢失数据。<ul>
<li><code>min.insync.replicas</code>: ack为-1时生效，表示ISR里应答的最小follower数量。默认为1（leader本身也算一个！），所以当ISR里除了leader本身，没有其他的follower，即使ack设为-1，运行过程中也只会保存leader一份数据，相当于1（leader应答）的效果，不能保证不丢数据。</li>
<li>所以需要将min.insync.replicas设置大于等于2，才能保证有其他副本同步到数据</li>
</ul>
</li>
<li>最多一次（At Most Once）：例如，将服务器ACK级别设置为0（无需应答），可以保证生产者每条消息只会被发送一次。<ul>
<li>At Least Once可以保证数据不丢失，但是不能保证数据不重复</li>
<li>At Most Once可以保证数据不重复，但是不能保证数据不丢失。</li>
</ul>
</li>
<li>精确一次（Exactly Once）：对于一些非常重要的信息，比如和钱相关的数据，要求数据既不能重复也不丢失。每条消息肯定会被传输一次且仅传输一次。<ul>
<li>Kafka通过 幂等性（Idempotence）和事务（Transaction）这两种机制实现了 精确一次（exactly once）语义</li>
</ul>
</li>
</ul>
<h3 id="4-2-4-幂等性"><a href="#4-2-4-幂等性" class="headerlink" title="4.2.4 幂等性"></a>4.2.4 幂等性</h3><ol>
<li>幂等性原理<ul>
<li>幂等性就是指Producer不论向Broker发送多少次重复数据，Broker端都只会持久化一条。幂等性结合至少一次（At Least Once），就构成了Kafka的精确一次（Exactly Once）。</li>
<li>即：精确一次（Exactly Once） = 至少一次（At Least Once） + 幂等性 。</li>
<li>开启幂等性的Producer在初始化的时候会被分配一个ProducerID，发往同一Partition的消息会附带Sequence Number。而Broker端会对&lt;ProducerID, Partition, SeqNumber&gt;主键做缓存，当具有相同主键的消息提交时，Broker只会持久化一条。</li>
<li>注意：Producer重启PID就会变化，即&lt;PID, Partition, SeqNumber&gt; 主键变化，所以<font color ='red' >幂等性无法保证跨分区跨会话的Exactly Once</font>。<br>  <img src="https://i.loli.net/2021/11/19/pba75dhHBmuX6TP.jpg"></li>
</ul>
</li>
<li>如何使用幂等性<ul>
<li>开启参数enable.idempotence 默认为true，false关闭</li>
<li>相关参数max.in.flight.requests.per.connection 该参数指定了生产者在收到服务器响应之前可以发送多少个消息。它的值越高，就会占用越多的内存，不过也会提升吞吐量。把它设为 1 可以保证消息是按照发送的顺序写入服务器的，即使发生了重试</li>
<li><font color ='red' >幂等性保证的是单分区单会话exactly once</font></li>
</ul>
</li>
</ol>
<h3 id="4-2-5-生产者事务"><a href="#4-2-5-生产者事务" class="headerlink" title="4.2.5 生产者事务"></a>4.2.5 生产者事务</h3><ul>
<li>保证多次提交到不同主题和不同分区的消息的原子性，即要么全部发送成功，要么全部发送失败</li>
<li>保证conumser-transform-produce 应用模式中，消息能被原子性转换。</li>
</ul>
<ol>
<li><p>Kafka事务原理<br> <img src="https://i.loli.net/2021/11/19/i8TlgIzKCsAGyqH.jpg"></p>
</li>
<li><p>流程描述</p>
<ul>
<li><p>Producer准备TransactionId</p>
<ul>
<li>Kafka事务为了实现Producer的主从功能，提出了TransactionId的概念，同一个TransactionId只能有一个在运行，后面启动的Producer会使得前面的Producer立即抛出异常。</li>
</ul>
</li>
<li><p>初始化事务</p>
<ul>
<li>Producer会首先向任意一个broker发送查找自己对应事务协调器的请求，获取请求后，Producer会向事务协调器请求PID,同时在这个过程中，如果发现对应TransactionId有之前未完成的任务，它还会做以下两件事：<ul>
<li>恢复Producer对应TransactionId之前未完成的事务（Commit/Abort）</li>
<li>对PID对应epoch进行递增，防止脑裂问题。</li>
</ul>
</li>
</ul>
</li>
<li><p>开始事务：</p>
<ul>
<li>本地记录事务状态为开始，但是协调器只有在接受到事务第一条消息后，才会标记为事务真正的开始。</li>
</ul>
</li>
<li><p>进行事务：</p>
<ul>
<li>在kafka事务中，会原子的支持Consumer-Process-Producer过程，因此在这个过程中还提供了一个API</li>
<li>producer.sendOffsetsToTransaction();这个过程会将消费的offset暂存在协调器中，等事务提交时统一提交</li>
</ul>
</li>
<li><p>提交/回滚：</p>
<ul>
<li>当提交或回滚的时候，协调器会进行一个两段提交<ul>
<li>第一阶段，将事务日志，将此事务设置为PREPARE_COMMIT或PREPARE_ABORT</li>
<li>第二阶段，发送Transaction Marker给事务涉及到的Leader发送标记信息，标记此条信息为已提交或已放弃</li>
</ul>
</li>
<li>当完成第二阶段后，协调器最终会将此事务标记为COMPLETE_COMMIT或COMPLETE_ABORT</li>
</ul>
</li>
<li><p>事务故障恢复</p>
<ul>
<li>__transaction_state-分区-Leader存储事务信息的特殊主题，负责传递和持久化事务状态，通过持久化状态，可以使得协调器即使崩溃，也能选举新的Leader继续补全事务</li>
<li>在提交阶段，为了防止其他Leader崩溃而没有收到commit消息，协调器会先保存事务状态，再发送Transaction Marker消息</li>
<li>Producer 在发送 beginTransaction() 时，如果出现 timeout 或者错误：Producer 只需要重试即可；</li>
<li>Producer 在发送数据时出现错误：Producer 应该 abort 这个事务，如果 Produce 没有 abort（比如设置了重试无限次，并且 batch 超时设置得非常大），TransactionCoordinator 将会在这个事务超时之后 abort 这个事务操作；</li>
<li>Producer 发送 commitTransaction() 时出现 timeout 或者错误：Producer 应该重试这个请求（幂等保证）；</li>
<li>Coordinator Failure：如果 Transaction Coordinator 发生切换（事务 topic leader 切换），Coordinator 可以从日志中恢复。如果发送事务有处于 PREPARE_COMMIT 或 PREPARE_ABORT 状态，那么直接执行 commit 或者 abort 操作，如果是一个正在进行的事务，Coordinator 的失败并不需要 abort 事务，producer 只需要向新的 Coordinator 发送请求即可。</li>
</ul>
</li>
</ul>
</li>
<li><p>事务模板</p>
 <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 1初始化事务</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">initTransactions</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 2开启事务</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">beginTransaction</span><span class="params">()</span> <span class="keyword">throws</span> ProducerFencedException</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 3在事务内提交已经消费的偏移量</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">sendOffsetsToTransaction</span><span class="params">(Map&lt;TopicPartition, OffsetAndMetadata&gt; offsets,</span></span></span><br><span class="line"><span class="params"><span class="function">                              String consumerGroupId)</span> <span class="keyword">throws</span> ProducerFencedException</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 4提交事务</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">commitTransaction</span><span class="params">()</span> <span class="keyword">throws</span> ProducerFencedException</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 5放弃事务（类似于回滚事务的操作）</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">abortTransaction</span><span class="params">()</span> <span class="keyword">throws</span> ProducerFencedException</span>;</span><br></pre></td></tr></table></figure></li>
<li><p>编码测试</p>
 <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CustomProducerTransaction</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">final</span> Properties properties = <span class="keyword">new</span> Properties();</span><br><span class="line">        <span class="comment">// 给kafka配置对象添加配置信息：bootstrap.servers</span></span><br><span class="line">        properties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">&quot;hadoop001:9092,hadoop002:9092,hadoop003:9092,hadoop004:9092,hadoop005:9092&quot;</span>);</span><br><span class="line">        <span class="comment">// key,value序列化（必须）：key.serializer，value.serializer</span></span><br><span class="line">        properties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());</span><br><span class="line">        properties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 批次大小 默认16K：batch.size</span></span><br><span class="line">        properties.put(ProducerConfig.BATCH_SIZE_CONFIG, <span class="number">16384</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 等待时间：linger.ms</span></span><br><span class="line">        properties.put(ProducerConfig.LINGER_MS_CONFIG, <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// RecordAccumulator缓冲区大小 默认32M：buffer.memory</span></span><br><span class="line">        properties.put(ProducerConfig.BUFFER_MEMORY_CONFIG, <span class="number">33554432</span>);</span><br><span class="line"></span><br><span class="line">        properties.put(ProducerConfig.TRANSACTIONAL_ID_CONFIG, <span class="string">&quot;transaction_id_0&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3. 创建kafka生产者对象</span></span><br><span class="line">        KafkaProducer&lt;String, String&gt; kafkaProducer = <span class="keyword">new</span> KafkaProducer&lt;&gt;(properties);</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 1初始化事务</span></span><br><span class="line">        kafkaProducer.initTransactions();</span><br><span class="line">        <span class="comment">// 2开启事务</span></span><br><span class="line">        kafkaProducer.beginTransaction();</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="comment">// 4. 调用send方法,发送消息</span></span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">1090000000</span>; i++) &#123;</span><br><span class="line">                kafkaProducer.send(<span class="keyword">new</span> ProducerRecord&lt;&gt;(<span class="string">&quot;first&quot;</span>,<span class="string">&quot;i am producer &quot;</span> + i));</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">// 4提交事务</span></span><br><span class="line">            kafkaProducer.commitTransaction();</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">             <span class="comment">// 5放弃事务（类似于回滚事务的操作）</span></span><br><span class="line">            kafkaProducer.abortTransaction();</span><br><span class="line">        &#125;<span class="keyword">finally</span> &#123;</span><br><span class="line">            <span class="comment">// 5. 关闭资源</span></span><br><span class="line">            kafkaProducer.close();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="4-3-文件存储"><a href="#4-3-文件存储" class="headerlink" title="4.3 文件存储"></a>4.3 文件存储</h2><h3 id="4-3-1-文件存储机制"><a href="#4-3-1-文件存储机制" class="headerlink" title="4.3.1 文件存储机制"></a>4.3.1 文件存储机制</h3><blockquote>
<p>思考：Topic数据到底是怎么存储的呢？</p>
</blockquote>
<ol>
<li>启动生产者，并发送消息 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 ~]$ kafka-console-producer.sh --bootstrap-server hadoop001:9092 --topic first</span><br><span class="line">&gt;hello world</span><br></pre></td></tr></table></figure></li>
<li>查看hadoop001（或者其他任意broker）的/opt/module/kafka_2.12-3.0.0/logs/first-0/路径上的文件 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 bin]$ kafka-topics.sh --bootstrap-server hadoop001:9092 --describe -topic first</span><br><span class="line">Topic: first	TopicId: LtoXnmphRHmMEVuj_o5yyw	PartitionCount: 3	ReplicationFactor: 2	Configs: segment.bytes=1073741824</span><br><span class="line">	Topic: first	Partition: 0	Leader: 3	Replicas: 3,5	Isr: 3,5</span><br><span class="line">	Topic: first	Partition: 1	Leader: 5	Replicas: 5,1	Isr: 5,1</span><br><span class="line">	Topic: first	Partition: 2	Leader: 1	Replicas: 1,4	Isr: 1,4</span><br><span class="line">[atguigu@dw-server-001 bin]$</span><br><span class="line">[atguigu@hadoop001 ~]$ ll /opt/module/kafka_2.12-3.0.0/logs/first-0/</span><br><span class="line">总用量 60</span><br><span class="line">-rw-rw-r--. 1 atguigu atguigu 10485760 11月  9 21:16 00000000000000000000.index</span><br><span class="line">-rw-rw-r--. 1 atguigu atguigu    41536 11月  9 21:16 00000000000000000000.log</span><br><span class="line">-rw-rw-r--. 1 atguigu atguigu 10485756 11月  9 21:16 00000000000000000000.timeindex</span><br><span class="line">-rw-rw-r--. 1 atguigu atguigu        8 11月  9 11:08 leader-epoch-checkpoint</span><br><span class="line">-rw-rw-r--. 1 atguigu atguigu       43 11月  9 10:56 partition.metadata</span><br><span class="line">[atguigu@hadoop001 ~]$</span><br></pre></td></tr></table></figure></li>
<li>查看log日志 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 kafka]$ bin/kafka-console-producer.sh --bootstrap-server hadoop001:9092 --topic first</span><br><span class="line">&gt;hello world</span><br></pre></td></tr></table></figure></li>
<li>通过工具查看index和log信息 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 kafka]$ kafka-run-class.sh kafka.tools.DumpLogSegments --files datas/first-0/00000000000000000000.index </span><br><span class="line"></span><br><span class="line">Dumping datas/first-0/00000000000000000000.index</span><br><span class="line">offset: 3 position: 152</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop001 kafka]$ bin/kafka-run-class.sh kafka.tools.DumpLogSegments --files datas/first-0/00000000000000000000.log</span><br><span class="line"></span><br><span class="line">Dumping datas/first-0/00000000000000000000.log</span><br><span class="line">Starting offset: 0</span><br><span class="line">baseOffset: 0 lastOffset: 1 count: 2 baseSequence: -1 lastSequence: -1 producerId: -1 producerEpoch: -1 partitionLeaderEpoch: 0 isTransactional: <span class="literal">false</span> isControl: <span class="literal">false</span> position: 0 CreateTime: 1636338440962 size: 75 magic: 2 compresscodec: none crc: 2745337109 isvalid: <span class="literal">true</span></span><br><span class="line">baseOffset: 2 lastOffset: 2 count: 1 baseSequence: -1 lastSequence: -1 producerId: -1 producerEpoch: -1 partitionLeaderEpoch: 0 isTransactional: <span class="literal">false</span> isControl: <span class="literal">false</span> position: 75 CreateTime: 1636351749089 size: 77 magic: 2 compresscodec: none crc: 273943004 isvalid: <span class="literal">true</span></span><br><span class="line">baseOffset: 3 lastOffset: 3 count: 1 baseSequence: -1 lastSequence: -1 producerId: -1 producerEpoch: -1 partitionLeaderEpoch: 0 isTransactional: <span class="literal">false</span> isControl: <span class="literal">false</span> position: 152 CreateTime: 1636351749119 size: 77 magic: 2 compresscodec: none crc: 106207379 isvalid: <span class="literal">true</span></span><br><span class="line">baseOffset: 4 lastOffset: 8 count: 5 baseSequence: -1 lastSequence: -1 producerId: -1 producerEpoch: -1 partitionLeaderEpoch: 0 isTransactional: <span class="literal">false</span> isControl: <span class="literal">false</span> position: 229 CreateTime: 1636353061435 size: 141 magic: 2 compresscodec: none crc: 157376877 isvalid: <span class="literal">true</span></span><br><span class="line">baseOffset: 9 lastOffset: 13 count: 5 baseSequence: -1 lastSequence: -1 producerId: -1 producerEpoch: -1 partitionLeaderEpoch: 0 isTransactional: <span class="literal">false</span> isControl: <span class="literal">false</span> position: 370 CreateTime: 1636353204051 size: 146 magic: 2 compresscodec: none crc: 4058582827 isvalid: <span class="literal">true</span></span><br></pre></td></tr></table></figure></li>
<li>Topic数据的存储机制<br> topic是逻辑上的概念，而partition是物理上的概念，<font color ='red' >每个partition对应于一个log文件</font>，该log文件中存储的就是Producer生产的数据。<font color ='red' >Producer生产的数据会被不断追加到该log文件末端</font>，为防止log文件过大导致数据定位效率低下，<font color ='red' >Kafka采取了分片和索引机制</font>，将每个partition分为多个segment。<font color ='red' >每个segment对应两个文件：“.index”文件和“.log”文件</font>。这些文件位于一个文件夹下，该文件夹的命名规则为：topic名称+分区序号，例如：first-0。<br> <img src="https://i.loli.net/2021/11/19/Xtf8CTPNYw1bv63.jpg"></li>
<li>index文件和log文件详解<ul>
<li><code>.index</code>文件存储大量的索引信息，索引信息按照数组的逻辑排列。<ul>
<li>Index存储数据采用的是稀疏索引存储</li>
<li>log.index.interval.bytes: 引项字节间隔密度，会影响索引文件中的区间密度和查询效率默认4kb</li>
</ul>
</li>
<li><code>.log</code>文件存储大量的数据，数据直接紧密排列，索引文件中的元数据指向对应数据文件中message的物理偏移地址。<ul>
<li><code>log.segment.bytes</code>：Kafka中log日志是分成一块块存储的，此配置是指log日志划分 成块的大小，默认值1G。</li>
</ul>
</li>
</ul>
</li>
<li>日志切分文件逻辑<ul>
<li>日志文件和索引文件都会存在多个文件，组成多个 SegmentLog</li>
<li>当前日志分段文件的大小超过了 broker 端参数 log.segment.bytes 配置的值。log.segment.bytes 参数的默认值为 1073741824，即 1GB。</li>
<li>当前日志分段中消息的最大时间戳与当前系统的时间戳的差值大于 log.roll.ms 或 log.roll.hours 参数配置的值。如果同时配置了 log.roll.ms 和 log.roll.hours 参数，那么 log.roll.ms 的优先级高。默认情况下，只配置了 log.roll.hours 参数，其值为168，即 7 天</li>
<li>偏移量索引文件或时间戳索引文件的大小达到 broker 端参数 log.index.size.max.bytes 配置的值。log.index.size.max.bytes 的默认值为 10485760，即 10MB</li>
<li>追加的消息的偏移量与当前日志分段的偏移量之间的差值大于 Integer.MAX_VALUE，即要追加的消息的偏移量不能转变为相对偏移量<ul>
<li>为什么是 Integer.MAX_VALUE ？<ul>
<li>在偏移量索引文件中，每个索引项共占用 8 个字节，并分为两部分。相对偏移量和物理地址。</li>
<li>相对偏移量：表示消息相对与基准偏移量的偏移量，占 4 个字节</li>
<li>物理地址：消息在日志分段文件中对应的物理位置，也占 4 个字节</li>
<li>4 个字节刚好对应 Integer.MAX_VALUE ，如果大于 Integer.MAX_VALUE ，则不能用 4 个字节进行表示了。</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>索引文件切分过程<ul>
<li>索引文件会根据 log.index.size.max.bytes 值进行预先分配空间，即文件创建的时候就是最大值，当真正的进行索引文件切分的时候，才会将其裁剪到实际数据大小的文件。这一点是跟日志文件有所区别的地方。其意义降低了代码逻辑的复杂性。</li>
</ul>
</li>
<li>查找消息<ol>
<li>offset 查询<br> <img src="https://i.loli.net/2021/11/19/MIv3NqtbdhCcRDw.png"><ol>
<li>偏移量索引由相对偏移量和物理地址组成。</li>
<li>可以通过kafka-dump-log.sh命令解析.index 文件 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kafka-dump-log.sh --files ./00000000000000000000.index</span><br><span class="line">offset:0 position:0</span><br><span class="line">offset:20 position:320</span><br><span class="line">offset:43 position:1220</span><br></pre></td></tr></table></figure>
<blockquote>
<p>注意：offset 与 position 没有直接关系，由于存在数据删除和日志清理。</p>
</blockquote>
</li>
<li>如何查看 偏移量为 23 的消息？<ul>
<li>Kafka 中存在一个 ConcurrentSkipListMap 来保存在每个日志分段，通过跳跃表方式，定位到在 00000000000000000000.index ，通过二分法在偏移量索引文件中找到不大于 23 的最大索引项，即 offset 20 那栏，然后从日志分段文件中的物理位置为320 开始顺序查找偏移量为 23 的消息。</li>
</ul>
</li>
</ol>
</li>
<li>时间戳方式查询<ol>
<li>通过时间戳方式进行查找消息，需要通过查找时间戳索引和偏移量索引两个文件。</li>
<li>时间戳索引索引格式<br> <img src="https://i.loli.net/2021/11/19/XBykJIseCf87pMm.jpg"><br> <img src="https://i.loli.net/2021/11/19/Bh6sgKwkiZFS82d.jpg"></li>
<li>查找时间戳为 1557554753430 开始的消息？<ul>
<li>将 1557554753430 和每个日志分段中最大时间戳 largestTimeStamp 逐一对比，直到找到不小于 1557554753430 所对应的日志分段。日志分段中的 largestTimeStamp 的计算是先查询该日志分段所对应时间戳索引文件，找到最后一条索引项，若最后一条索引项的时间戳字段值大于 0 ，则取该值，否则去该日志分段的最近修改时间。</li>
<li>找到相应日志分段之后，使用二分法进行定位，与偏移量索引方式类似，找到不大于 1557554753430 最大索引项，也就是 [1557554753420 430]。</li>
<li>拿着偏移量为 430 到偏移量索引文件中使用二分法找到不大于 430 最大索引项，即 [20，320] 。</li>
<li>日志文件中从 320 的物理位置开始查找不小于 1557554753430 数据。<blockquote>
<p>注意：timestamp文件中的 offset 与 index 文件中的 relativeOffset 不是一一对应的哦。因为数据的写入是各自追加。</p>
</blockquote>
</li>
</ul>
</li>
</ol>
</li>
</ol>
</li>
</ol>
<h3 id="4-3-2-文件清理策略"><a href="#4-3-2-文件清理策略" class="headerlink" title="4.3.2 文件清理策略"></a>4.3.2 文件清理策略</h3><ul>
<li>Kafka中默认的日志保存时间为7天，可以通过调整log.retention.hours参数，修改保存时间。</li>
<li>那么日志一旦超过了设置的时间，怎么处理呢？<ul>
<li>Kafka中提供的日志清理策略有delete和compact两种。<ul>
<li>delete日志删除：将过期数据删除。<ul>
<li>log.cleanup.policy = delete    所有数据启用删除策略</li>
<li>cleanup.policy= delete     单个主题数据启用删除策略</li>
<li>思考：如果一个segment中有一部分数据过期，一部分没有过期，怎么处理？<br>  <img src="https://i.loli.net/2021/11/19/4c5bpKFfz3H8R1Q.jpg"></li>
</ul>
</li>
<li>compact日志压缩：对于相同key的不同value值，只保留最后一个版本。<ul>
<li>log.cleanup.policy = compact  所有数据启用压缩策略</li>
<li>cleanup.policy = compact单个主题数据启用删除策略<h2 id="4-4-Kafka-高效读写数据"><a href="#4-4-Kafka-高效读写数据" class="headerlink" title="4.4 Kafka 高效读写数据"></a>4.4 Kafka 高效读写数据</h2></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<ol>
<li>Kafka本身是分布式集群，可以采用分区技术，并发度高。</li>
<li>顺序写磁盘 <ul>
<li>Kafka的producer生产数据，要写入到log文件中，写的过程是一直追加到文件末端，为顺序写。官网有数据表明，同样的磁盘，顺序写能到600M/s，而随机写只有100K/s。这与磁盘的机械机构有关，顺序写之所以快，是因为其省去了大量磁头寻址的时间。<br>  <img src="https://i.loli.net/2021/11/19/8lStAREKQoIFXwT.jpg"></li>
</ul>
</li>
<li>零复制技术<ul>
<li>Kafka的数据加工处理操作交由Kafka生产者和Kafka消费者处理。</li>
<li>Kafka Broker应用层不关心存储的数据，所以就不用走应用层，传输效率高。<br>  <img src="https://i.loli.net/2021/11/19/epTbkQq1EvDM3ul.jpg"></li>
</ul>
</li>
</ol>
<h1 id="第5章-Kafka消费者"><a href="#第5章-Kafka消费者" class="headerlink" title="第5章 Kafka消费者"></a>第5章 Kafka消费者</h1><h2 id="5-1-Kafka消费者工作流程"><a href="#5-1-Kafka消费者工作流程" class="headerlink" title="5.1 Kafka消费者工作流程"></a>5.1 Kafka消费者工作流程</h2><p><img src="https://i.loli.net/2021/11/19/OuTZbBEeAcFd7gU.jpg"></p>
<ul>
<li>Kafka中消息是以topic进行分类的，生产者生产消息，消费者消费消息，都是面向topic的。</li>
<li>一个topic下的每一个分区都单独维护一个offset，所以分发到不同分区中的数据是不同的数据。消费者的分区维护是一个消费者组一个主题的一个分区维护一个offset。<br>消费者相关调优参数</li>
<li>消费逻辑<ol>
<li>单个消费者，可以消费Kafka中所有主题、所有分区</li>
<li>单个消费者与消费者之间相互独立。都能同时消费所有主题、所有分区。</li>
<li>消费者组，可以消费Kafka中所有主题、所有分区。只不过在消费的过程中，考虑到并发，每个消费者组中的消费者，只能消费指定分区。</li>
<li>消费者组和消费者组之间一点关系都没有。都能同时消费所有主题、所有分区。</li>
</ol>
</li>
<li>消费者相关调优参数</li>
</ul>
<table>
<thead>
<tr>
<th>参数名称</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>bootstrap.servers</td>
<td>向Kafka集群建立初始连接用到的host/port列表。</td>
</tr>
<tr>
<td>group.id</td>
<td>标记消费者所属的消费者组</td>
</tr>
<tr>
<td>enable.auto.commit</td>
<td>默认值为true，消费者会自动周期性地向服务器提交偏移量。</td>
</tr>
<tr>
<td>auto.commit.interval.ms</td>
<td>如果设置了 enable.auto.commit 的值为true， 则该值定义了消费者偏移量向Kafka提交的频率，默认5s</td>
</tr>
<tr>
<td>auto.offset.reset</td>
<td>当Kafka中没有初始偏移量或当前偏移量在服务器中不存在（如，数据被 删除了），该如何处理？ <br>earliest：自动重置偏移量到最早的偏移量 <br>latest：自动重置偏移量为最新的偏移量 <br>none：如果消费组原来的（previous）偏移量不存在，则向消费者抛异常 <br>anything：向消费者抛异常</td>
</tr>
</tbody></table>
<h2 id="5-2-消费方式"><a href="#5-2-消费方式" class="headerlink" title="5.2 消费方式"></a>5.2 消费方式</h2><p><img src="https://i.loli.net/2021/11/19/q54B7gmylYAhF8i.jpg"></p>
<ul>
<li>pull（拉）模式：consumer采用从broker中主动拉取数据。Kafka采用这种方式。</li>
<li>push（推）模式：Kafka没有采用这种方式，因为由broker决定消息发送速率，很难适应所有消费者的消费速率。例如推送的速度是50m/s，Consumer1、Consumer2就来不及处理消息。</li>
<li>pull模式不足之处是，如果Kafka没有数据，消费者可能会陷入循环中，一直返回空数据。针对这一点，Kafka的消费者在消费数据时会传入一个时长参数timeout，如果当前没有数据可供消费，consumer会等待一段时间之后再返回，这段时长即为timeout。</li>
</ul>
<h3 id="5-2-1-独立消费者"><a href="#5-2-1-独立消费者" class="headerlink" title="5.2.1 独立消费者"></a>5.2.1 独立消费者</h3><blockquote>
<p>注意：在消费者API代码中必须配置消费者组，命令行启动消费者不填写消费者组会被自动填写随机的消费者组。</p>
</blockquote>
<ol>
<li>需求：创建一个独立消费者，消费first主题中数据。</li>
<li>编码 <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.atguigu.kafka.consumer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerConfig;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecord;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecords;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.KafkaConsumer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.time.Duration;</span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CustomConsumer</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1.创建消费者的配置对象</span></span><br><span class="line">        Properties properties = <span class="keyword">new</span> Properties();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2.给消费者配置对象添加参数</span></span><br><span class="line">        properties.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">&quot;hadoop102:9092&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 配置序列化 必须</span></span><br><span class="line">properties.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, <span class="string">&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span>);</span><br><span class="line">        properties.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, <span class="string">&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 配置消费者组（组名任意起名） 必须</span></span><br><span class="line">        properties.put(ConsumerConfig.GROUP_ID_CONFIG, <span class="string">&quot;test&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 创建消费者对象</span></span><br><span class="line">        KafkaConsumer&lt;String, String&gt; kafkaConsumer = <span class="keyword">new</span> KafkaConsumer&lt;String, String&gt;(properties);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 注册要消费的主题（可以消费多个主题）</span></span><br><span class="line">        ArrayList&lt;String&gt; topics = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">        topics.add(<span class="string">&quot;first&quot;</span>);</span><br><span class="line">        kafkaConsumer.subscribe(topics);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 拉取数据打印</span></span><br><span class="line">        <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">            <span class="comment">// 设置1s中消费一批数据</span></span><br><span class="line">            ConsumerRecords&lt;String, String&gt; consumerRecords = kafkaConsumer.poll(Duration.ofSeconds(<span class="number">1</span>));</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 打印消费到的数据</span></span><br><span class="line">            <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; consumerRecord : consumerRecords) &#123;</span><br><span class="line">                System.out.println(consumerRecord);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>测试<ol>
<li>在IDEA中执行消费者程序</li>
<li>在Kafka集群控制台，创建Kafka生产者，并输入数据 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 kafka]$ kafka-console-producer.sh --bootstrap-server hadoop102:9092 --topic first</span><br><span class="line">&gt;hello</span><br></pre></td></tr></table></figure></li>
<li>在IDEA控制台观察接收到的数据 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ConsumerRecord(topic = first, partition = 1, leaderEpoch = 3, offset = 0, CreateTime = 1629160841112, serialized key size = -1, serialized value size = 5, headers = RecordHeaders(headers = [], isReadOnly = <span class="literal">false</span>), key = null, value = hello)</span><br></pre></td></tr></table></figure></li>
</ol>
</li>
</ol>
<h3 id="5-2-2-消费者组案例"><a href="#5-2-2-消费者组案例" class="headerlink" title="5.2.2 消费者组案例"></a>5.2.2 消费者组案例</h3><ul>
<li>Consumer Group（CG）：消费者组，由多个consumer组成。<ul>
<li>消费者组内每个消费者负责消费不同分区的数据，一个分区只能由一个组内消费者消费；</li>
<li>消费者组之间互不影响。所有的消费者都属于某个消费者组，即消费者组是逻辑上的一个订阅者。</li>
</ul>
</li>
</ul>
<h2 id="5-3-分区分配策略"><a href="#5-3-分区分配策略" class="headerlink" title="5.3 分区分配策略"></a>5.3 分区分配策略</h2><ul>
<li>一个consumer group中有多个consumer，一个 topic有多个partition，所以必然会涉及到partition的分配问题，即确定哪个partition由哪个consumer来消费。</li>
<li>Kafka有三种主流的分区分配策略： Range、RoundRobin、Sticky。</li>
<li>可以通过配置参数partition.assignment.strategy，修改分区的分配策略。默认策略是Range +  Sticky。Kafka可以同时使用多个分区分配策略。</li>
</ul>
<h3 id="5-3-1-Range"><a href="#5-3-1-Range" class="headerlink" title="5.3.1 Range"></a>5.3.1 Range</h3><p><img src="https://i.loli.net/2021/11/19/s4QuYAL2UBqRfm8.jpg"></p>
<ol>
<li>Range分区策略原理<ul>
<li>按照消费者总数和分区总数进行整除运算来获得一个跨度，然后将分区按照跨度进行平均分配，以保证分区尽可能均匀地分配给所有的消费者。对于每一个topic，RangeAssignor策略会将消费组内所有订阅这个topic的消费者按照名称的字典序排序，然后为每个消费者划分固定的分区范围，如果不够平均分配，那么字典序靠前的消费者会被多分配一个分区。</li>
<li>假如现在有 7 个分区，3 个消费者，排序后的分区将会是0,1,2,3,4,5,6；消费者排序完之后将会是C0-0,C1-0,C2-0。</li>
<li>通过 partitions数/consumer数 来决定每个消费者应该消费几个分区。如果除不尽，那么前面几个消费者将会多消费 1 个分区。</li>
<li>例如，7/3 = 3 余 1 ，除不尽，那么 消费者 C0-0 便会多消费 1 个分区</li>
</ul>
<blockquote>
<p>注意：如果只是针对 1 个 topic 而言，C0-0消费者多消费1个分区影响不是很大。但是如果有 N 多个 topic，那么针对每个 topic，消费者 C0-0 都将多消费 1 个分区，topic越多，C0-0 消费的分区会比其他消费者明显多消费 N 个分区。<br> 容易产生数据倾斜！</p>
</blockquote>
</li>
<li>Range分区分配策略案例<ul>
<li>修改主题first为7个分区  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 kafka]$ bin/kafka-topics.sh --bootstrap-server hadoop102:9092 --alter --topic first --partitions 7</span><br></pre></td></tr></table></figure>
<ul>
<li>注意：分区数可以增加，但是不能减少。</li>
</ul>
</li>
<li>创建三个线程启动三个消费者CustomConsumer组成消费者组，组名都为“test”  <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> tech.anzhen.kafka.consumer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerConfig;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecord;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecords;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.KafkaConsumer;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.serialization.StringDeserializer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.time.Duration;</span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CustomConsumerGroup</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">final</span> Properties properties = <span class="keyword">new</span> Properties();</span><br><span class="line"></span><br><span class="line">        properties.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">&quot;hadoop001:9092,hadoop002:9092,hadoop003:9092,hadoop004:9092,hadoop005:9092&quot;</span>);</span><br><span class="line"></span><br><span class="line">        properties.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());</span><br><span class="line">        properties.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());</span><br><span class="line"></span><br><span class="line">        properties.put(ConsumerConfig.GROUP_ID_CONFIG, <span class="string">&quot;123&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">new</span> Thread(() -&gt; consumerThread(properties)).start();</span><br><span class="line">        <span class="keyword">new</span> Thread(() -&gt; consumerThread(properties)).start();</span><br><span class="line">        <span class="keyword">new</span> Thread(() -&gt; consumerThread(properties)).start();</span><br><span class="line">        <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">consumerThread</span><span class="params">(Properties properties)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">final</span> KafkaConsumer&lt;Object, Object&gt; kafkaConsumer = <span class="keyword">new</span> KafkaConsumer&lt;&gt;(properties);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">final</span> ArrayList&lt;String&gt; topicList = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">        topicList.add(<span class="string">&quot;first&quot;</span>);</span><br><span class="line">        kafkaConsumer.subscribe(topicList);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">            <span class="keyword">final</span> ConsumerRecords&lt;Object, Object&gt; poll = kafkaConsumer.poll(Duration.ofSeconds(<span class="number">1</span>));</span><br><span class="line">            <span class="keyword">if</span> (poll != <span class="keyword">null</span>) &#123;</span><br><span class="line">                <span class="keyword">for</span> (ConsumerRecord&lt;Object, Object&gt; consumerRecord : poll) &#123;</span><br><span class="line">                    System.out.println(Thread.currentThread().getName() + <span class="string">&quot; &quot;</span> + consumerRecord);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>启动CustomProducer生产者，发送500条消息，随机发送到不同的分区：<ul>
<li>说明：Kafka默认的分区分配策略就是Range + Sticky，所有不需要修改策略。<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> tech.anzhen.kafka.producer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.KafkaProducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.ProducerConfig;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.ProducerRecord;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CustomProducer</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">        Properties properties = <span class="keyword">new</span> Properties();</span><br><span class="line"></span><br><span class="line">        properties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">&quot;hadoop001:9092,hadoop002:9092,hadoop003:9092,hadoop004:9092,hadoop005:9092&quot;</span>);</span><br><span class="line"></span><br><span class="line">        properties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, <span class="string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>);</span><br><span class="line">        properties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, <span class="string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>);</span><br><span class="line"></span><br><span class="line">        KafkaProducer&lt;String, String&gt; kafkaProducer = <span class="keyword">new</span> KafkaProducer&lt;&gt;(properties);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">500</span>; i++) &#123;</span><br><span class="line"></span><br><span class="line">            kafkaProducer.send(<span class="keyword">new</span> ProducerRecord&lt;&gt;(<span class="string">&quot;first&quot;</span>, <span class="string">&quot;atguigu &quot;</span> + i));</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 避免发送到同一个分区</span></span><br><span class="line">            Thread.sleep(<span class="number">2</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        kafkaProducer.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>观察数据分区<br>  <img src="https://i.loli.net/2021/11/19/o3FS1q8waTPmdCe.jpg"></li>
</ul>
</li>
</ol>
<h3 id="5-3-2-RoundRobin"><a href="#5-3-2-RoundRobin" class="headerlink" title="5.3.2 RoundRobin"></a>5.3.2 RoundRobin</h3><p><img src="https://i.loli.net/2021/11/19/IjrJBkqwRslfc1m.jpg"></p>
<ol>
<li>RoundRobin分区策略原理<ul>
<li>RoundRobin 轮询分区策略，是把所有的 partition 和所有的 consumer 都列出来，然后按照 hascode 进行排序，最后通过轮询算法来分配 partition 给到各个消费者。</li>
</ul>
</li>
<li>RoundRobin分区分配策略案例<ol>
<li>修改CustomConsumer消费者代码中分区分配策略为RoundRobin。 <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> tech.anzhen.kafka.consumer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerConfig;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecord;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecords;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.KafkaConsumer;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.RoundRobinAssignor;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.serialization.StringDeserializer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.time.Duration;</span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.ConcurrentHashMap;</span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.CountDownLatch;</span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.atomic.AtomicInteger;</span><br><span class="line"><span class="keyword">import</span> java.util.function.BiConsumer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CustomConsumerRoundRobin</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">static</span> ConcurrentHashMap&lt;String, AtomicInteger&gt; map = <span class="keyword">new</span> ConcurrentHashMap&lt;&gt;();</span><br><span class="line">    <span class="keyword">static</span> CountDownLatch countDownLatch = <span class="keyword">new</span> CountDownLatch(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">        <span class="keyword">final</span> Properties properties = <span class="keyword">new</span> Properties();</span><br><span class="line"></span><br><span class="line">        properties.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">&quot;bd-server-001:9092,bd-server-002:9092,bd-server-003:9092,bd-server-004:9092,bd-server-005:9092&quot;</span>);</span><br><span class="line"></span><br><span class="line">        properties.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());</span><br><span class="line">        properties.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 修改分区分配策略</span></span><br><span class="line">        properties.put(ConsumerConfig.PARTITION_ASSIGNMENT_STRATEGY_CONFIG, RoundRobinAssignor.class.getName());</span><br><span class="line"></span><br><span class="line">        properties.put(ConsumerConfig.GROUP_ID_CONFIG, <span class="string">&quot;123&quot;</span>);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="keyword">new</span> Thread(() -&gt; consumerThread(properties)).start();</span><br><span class="line">        <span class="keyword">new</span> Thread(() -&gt; consumerThread(properties)).start();</span><br><span class="line">        <span class="keyword">new</span> Thread(() -&gt; consumerThread(properties)).start();</span><br><span class="line"></span><br><span class="line">        countDownLatch.await();</span><br><span class="line">        map.forEach((s, atomicInteger) -&gt; &#123;</span><br><span class="line">            System.out.println(<span class="string">&quot;线程: &quot;</span> + s.split(<span class="string">&quot;,&quot;</span>)[<span class="number">0</span>] + <span class="string">&quot; 分区: &quot;</span> + s.split(<span class="string">&quot;,&quot;</span>)[<span class="number">1</span>] + <span class="string">&quot; 消费数量：&quot;</span> + atomicInteger.get());</span><br><span class="line">        &#125;);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">consumerThread</span><span class="params">(Properties properties)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">final</span> KafkaConsumer&lt;Object, Object&gt; kafkaConsumer = <span class="keyword">new</span> KafkaConsumer&lt;&gt;(properties);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">final</span> ArrayList&lt;String&gt; topicList = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">        topicList.add(<span class="string">&quot;first&quot;</span>);</span><br><span class="line">        kafkaConsumer.subscribe(topicList);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">            <span class="keyword">final</span> ConsumerRecords&lt;Object, Object&gt; poll = kafkaConsumer.poll(Duration.ofSeconds(<span class="number">1</span>));</span><br><span class="line">            <span class="keyword">if</span> (poll != <span class="keyword">null</span>) &#123;</span><br><span class="line">                <span class="keyword">for</span> (ConsumerRecord&lt;Object, Object&gt; consumerRecord : poll) &#123;</span><br><span class="line">                    <span class="keyword">final</span> AtomicInteger atomicInteger = map.putIfAbsent(Thread.currentThread().getName() + <span class="string">&quot;,&quot;</span> + consumerRecord.partition(), <span class="keyword">new</span> AtomicInteger(<span class="number">1</span>));</span><br><span class="line">                    <span class="keyword">if</span> ( atomicInteger != <span class="keyword">null</span>) &#123;</span><br><span class="line">                        atomicInteger.incrementAndGet();</span><br><span class="line">                    &#125;</span><br><span class="line">                    <span class="keyword">if</span> (consumerRecord.value().toString().equals(<span class="string">&quot;over&quot;</span>)) &#123;</span><br><span class="line">                        countDownLatch.countDown();</span><br><span class="line">                        <span class="keyword">return</span>;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ol>
<ul>
<li>观察分区分布<br>  <img src="https://i.loli.net/2021/11/19/bB4OuDdJseglSIf.jpg"></li>
</ul>
</li>
</ol>
<h3 id="5-3-3-Sticky"><a href="#5-3-3-Sticky" class="headerlink" title="5.3.3 Sticky"></a>5.3.3 Sticky</h3><p>特殊的分配策略StickyAssignor，Kafka从0.11.x版本开始引入这种分配策略，首先会尽量均衡的放置分区到消费者上面，在出现同一消费者组内消费者出现问题的时候，会尽量保持原有分配的分区不变化。</p>
<ul>
<li>修改分区分配策略为粘性  <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> tech.anzhen.kafka.consumer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerConfig;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecord;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecords;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.KafkaConsumer;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.RoundRobinAssignor;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.StickyAssignor;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.serialization.StringDeserializer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.time.Duration;</span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.ConcurrentHashMap;</span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.CountDownLatch;</span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.atomic.AtomicInteger;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CustomConsumerStickyAssignor</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">static</span> ConcurrentHashMap&lt;String, AtomicInteger&gt; map = <span class="keyword">new</span> ConcurrentHashMap&lt;&gt;();</span><br><span class="line">    <span class="keyword">static</span> CountDownLatch countDownLatch = <span class="keyword">new</span> CountDownLatch(<span class="number">1</span>);</span><br><span class="line">    <span class="keyword">static</span> AtomicInteger total = <span class="keyword">new</span> AtomicInteger(<span class="number">5001</span>);</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">        <span class="keyword">final</span> Properties properties = <span class="keyword">new</span> Properties();</span><br><span class="line"></span><br><span class="line">        properties.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">&quot;bd-server-001:9092,bd-server-002:9092,bd-server-003:9092,bd-server-004:9092,bd-server-005:9092&quot;</span>);</span><br><span class="line"></span><br><span class="line">        properties.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());</span><br><span class="line">        properties.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 修改分区分配策略</span></span><br><span class="line">        properties.put(ConsumerConfig.PARTITION_ASSIGNMENT_STRATEGY_CONFIG, StickyAssignor.class.getName());</span><br><span class="line"></span><br><span class="line">        properties.put(ConsumerConfig.GROUP_ID_CONFIG, <span class="string">&quot;1234&quot;</span>);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="keyword">new</span> Thread(() -&gt; consumerThread(properties,<span class="number">0</span>)).start();</span><br><span class="line">        <span class="keyword">new</span> Thread(() -&gt; consumerThread(properties,<span class="number">1</span>)).start();</span><br><span class="line">        <span class="keyword">new</span> Thread(() -&gt; consumerThread(properties,<span class="number">2</span>)).start();</span><br><span class="line"></span><br><span class="line">        countDownLatch.await();</span><br><span class="line">        map.forEach((s, atomicInteger) -&gt; &#123;</span><br><span class="line">            System.out.println(<span class="string">&quot;线程: &quot;</span> + s.split(<span class="string">&quot;,&quot;</span>)[<span class="number">0</span>] + <span class="string">&quot; 分区: &quot;</span> + s.split(<span class="string">&quot;,&quot;</span>)[<span class="number">1</span>] + <span class="string">&quot; 消费数量：&quot;</span> + atomicInteger.get());</span><br><span class="line">        &#125;);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">consumerThread</span><span class="params">(Properties properties, <span class="keyword">int</span> index)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">final</span> KafkaConsumer&lt;Object, Object&gt; kafkaConsumer = <span class="keyword">new</span> KafkaConsumer&lt;&gt;(properties);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">final</span> ArrayList&lt;String&gt; topicList = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">        topicList.add(<span class="string">&quot;first&quot;</span>);</span><br><span class="line">        kafkaConsumer.subscribe(topicList);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">            <span class="keyword">final</span> ConsumerRecords&lt;Object, Object&gt; poll = kafkaConsumer.poll(Duration.ofSeconds(<span class="number">1</span>));</span><br><span class="line">            <span class="keyword">if</span> (poll != <span class="keyword">null</span>) &#123;</span><br><span class="line">                <span class="keyword">for</span> (ConsumerRecord&lt;Object, Object&gt; consumerRecord : poll) &#123;</span><br><span class="line">                    <span class="keyword">final</span> AtomicInteger atomicInteger = map.putIfAbsent(Thread.currentThread().getName() + <span class="string">&quot;,&quot;</span> + consumerRecord.partition(), <span class="keyword">new</span> AtomicInteger(<span class="number">1</span>));</span><br><span class="line">                    <span class="keyword">if</span> ( atomicInteger != <span class="keyword">null</span>) &#123;</span><br><span class="line">                        atomicInteger.incrementAndGet();</span><br><span class="line">                    &#125;</span><br><span class="line">                    <span class="keyword">if</span> (total.decrementAndGet() == <span class="number">0</span>) &#123;</span><br><span class="line">                        countDownLatch.countDown();</span><br><span class="line">                        <span class="keyword">return</span>;</span><br><span class="line">                    &#125;</span><br><span class="line">                    <span class="keyword">if</span> (index == <span class="number">2</span> &amp;&amp; total.get() &lt; <span class="number">4900</span>) &#123;</span><br><span class="line">                        System.out.println(<span class="string">&quot;线程2挂掉了&quot;</span>);</span><br><span class="line">                        <span class="keyword">return</span>;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
<li>观察消息分布<br>  <img src="https://i.loli.net/2021/11/19/FY1Hnhak7oQjPuq.jpg"></li>
</ul>
<h2 id="5-4-offset相关"><a href="#5-4-offset相关" class="headerlink" title="5.4 offset相关"></a>5.4 offset相关</h2><h3 id="5-4-1-offset的维护"><a href="#5-4-1-offset的维护" class="headerlink" title="5.4.1 offset的维护"></a>5.4.1 offset的维护</h3><ul>
<li>由于consumer在消费过程中可能会出现断电宕机等故障，consumer恢复后，需要从故障前的位置的继续消费，所以consumer需要实时记录自己消费到了哪个offset，以便故障恢复后继续消费。</li>
<li>Kafka0.9版本之前，consumer默认将offset保存在Zookeeper中，</li>
<li>从0.9版本开始，consumer默认将offset保存在Kafka一个内置的topic中，该topic为<code>__consumer_offsets</code>。</li>
</ul>
<ol>
<li>消费offset案例<ul>
<li>思想：__consumer_offsets为Kafka中的topic，那就可以通过消费者进行消费。</li>
<li>采用命令行方式，创建一个新的topic  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@bd-server-001 ~]<span class="comment"># kafka-topics.sh --bootstrap-server bd-server-001:9092 --create --topic test-consumer-offset --partitions 2 --replication-factor 2</span></span><br><span class="line">Created topic test-consumer-offset.</span><br><span class="line">[root@bd-server-001 ~]<span class="comment"># kafka-topics.sh --bootstrap-server bd-server-001:9092 --list</span></span><br><span class="line">__consumer_offsets</span><br><span class="line">first</span><br><span class="line">test-consumer-offset</span><br></pre></td></tr></table></figure></li>
<li>在配置文件$KAFKA_HOME/config/consumer.properties中添加配置exclude.internal.topics=false，默认是true，表示不能消费系统主题。为了查看该系统主题数据，所以该参数修改为false。</li>
<li>启动生产者往atguigu生产数据  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@bd-server-001 ~]<span class="comment"># kafka-console-producer.sh --topic test-consumer-offset --bootstrap-server  bd-server-001:9092</span></span><br><span class="line">&gt;123</span><br><span class="line">&gt;321</span><br><span class="line">&gt;1234567</span><br></pre></td></tr></table></figure></li>
<li>启动消费者消费atguigu数据  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@bd-server-001 ~]<span class="comment"># kafka-console-consumer.sh --bootstrap-server bd-server-001:9092 --topic test-consumer-offset --group test</span></span><br><span class="line">123</span><br><span class="line">321</span><br><span class="line">1234567</span><br></pre></td></tr></table></figure>
<ul>
<li>注意：必须指定消费者组。</li>
</ul>
</li>
<li>查看消费者消费主题__consumer_offsets  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@bd-server-001 ~]<span class="comment"># kafka-console-consumer.sh --topic __consumer_offsets --bootstrap-server  bd-server-001:9092 --consumer.config $KAFKA_HOME/config/consumer.properties  --formatter &quot;kafka.coordinator.group.GroupMetadataManager\$OffsetsMessageFormatter&quot; --from-beginning</span></span><br><span class="line">···</span><br><span class="line">[test-consumer-group,__consumer_offsets,26]::OffsetAndMetadata(offset=0, leaderEpoch=Optional.empty, metadata=, commitTimestamp=1637325778217, expireTimestamp=None)</span><br><span class="line">[test-consumer-group,__consumer_offsets,29]::OffsetAndMetadata(offset=0, leaderEpoch=Optional.empty, metadata=, commitTimestamp=1637325778217, expireTimestamp=None)</span><br><span class="line">[test-consumer-group,__consumer_offsets,34]::OffsetAndMetadata(offset=0, leaderEpoch=Optional.empty, metadata=, commitTimestamp=1637325778217, expireTimestamp=None)</span><br><span class="line">[test-consumer-group,__consumer_offsets,10]::OffsetAndMetadata(offset=0, leaderEpoch=Optional.empty, metadata=, commitTimestamp=1637325778217, expireTimestamp=None)</span><br><span class="line">[test-consumer-group,__consumer_offsets,32]::OffsetAndMetadata(offset=0, leaderEpoch=Optional.empty, metadata=, commitTimestamp=1637325778217, expireTimestamp=None)</span><br><span class="line">[test-consumer-group,__consumer_offsets,40]::OffsetAndMetadata(offset=8792, leaderEpoch=Optional[3], metadata=, commitTimestamp=1637325778217, expireTimestamp=None)</span><br><span class="line">···</span><br></pre></td></tr></table></figure>
<h3 id="5-4-2-自动提交offset"><a href="#5-4-2-自动提交offset" class="headerlink" title="5.4.2 自动提交offset"></a>5.4.2 自动提交offset</h3>为了使我们能够专注于自己的业务逻辑，Kafka提供了自动提交offset的功能。<br>自动提交offset的相关参数：</li>
</ul>
</li>
</ol>
<ul>
<li>enable.auto.commit：是否开启自动提交offset功能，默认是true</li>
<li>auto.commit.interval.ms：自动提交offset的时间间隔，默认是5s  <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> tech.anzhen.kafka.producer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.KafkaProducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.ProducerConfig;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.ProducerRecord;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.serialization.StringSerializer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.TimeUnit;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CustomProducer</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">        <span class="keyword">final</span> Properties properties = <span class="keyword">new</span> Properties();</span><br><span class="line">        <span class="comment">// 给kafka配置对象添加配置信息：bootstrap.servers</span></span><br><span class="line">        properties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">&quot;bd-server-001:9092,bd-server-002:9092,bd-server-003:9092,bd-server-004:9092,bd-server-005:9092&quot;</span>);</span><br><span class="line">        <span class="comment">// key,value序列化（必须）：key.serializer，value.serializer</span></span><br><span class="line">        properties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());</span><br><span class="line">        properties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 批次大小 默认16K：batch.size</span></span><br><span class="line">        properties.put(ProducerConfig.BATCH_SIZE_CONFIG, <span class="number">16384</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 等待时间：linger.ms</span></span><br><span class="line">        properties.put(ProducerConfig.LINGER_MS_CONFIG, <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// RecordAccumulator缓冲区大小 默认32M：buffer.memory</span></span><br><span class="line">        properties.put(ProducerConfig.BUFFER_MEMORY_CONFIG, <span class="number">33554432</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3. 创建kafka生产者对象</span></span><br><span class="line">        KafkaProducer&lt;String, String&gt; kafkaProducer = <span class="keyword">new</span> KafkaProducer&lt;&gt;(properties);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 4. 调用send方法,发送消息</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">5000</span>; i++) &#123;</span><br><span class="line">            kafkaProducer.send(<span class="keyword">new</span> ProducerRecord&lt;&gt;(<span class="string">&quot;first&quot;</span>, i + <span class="string">&quot;&quot;</span>, <span class="string">&quot;i am producer &quot;</span> + i));</span><br><span class="line">        &#125;</span><br><span class="line">        TimeUnit.SECONDS.sleep(<span class="number">10</span>);</span><br><span class="line">        kafkaProducer.send(<span class="keyword">new</span> ProducerRecord&lt;&gt;(<span class="string">&quot;first&quot;</span>, <span class="string">&quot;over&quot;</span>));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 5. 关闭资源</span></span><br><span class="line">        kafkaProducer.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="5-4-3-自动重置Offset"><a href="#5-4-3-自动重置Offset" class="headerlink" title="5.4.3 自动重置Offset"></a>5.4.3 自动重置Offset</h3><ul>
<li><code>auto.offset.reset = earliest | latest | none</code>, 默认是latest</li>
<li>当Kafka中没有初始偏移量（消费者组第一次消费）或服务器上不再存在当前偏移量时（例如该数据已被删除），该怎么办：<ul>
<li>earliest：自动将偏移量重置为最早的偏移量</li>
<li>latest（默认值）：自动将偏移量重置为最新偏移量</li>
<li>none：如果未找到消费者组的先前偏移量，则向消费者抛出异常<br><img src="https://i.loli.net/2021/11/19/koWfR193XKUJgMe.jpg"></li>
</ul>
</li>
</ul>
<h3 id="5-4-4-手动提交offset"><a href="#5-4-4-手动提交offset" class="headerlink" title="5.4.4 手动提交offset"></a>5.4.4 手动提交offset</h3><ul>
<li>虽然自动提交offset十分简单便利，但由于其是基于时间提交的，开发人员难以把握offset提交的时机。因此Kafka还提供了手动提交offset的API。</li>
<li>手动提交offset的方法有两种：分别是commitSync（同步提交）和commitAsync（异步提交）。<ul>
<li>两者的相同点是，都会将本次poll的一批数据最高的偏移量提交；</li>
<li>不同点是，同步提交阻塞当前线程，一直到提交成功，并且会自动失败重试（由不可控因素导致，也会出现提交失败）；而异步提交则没有失败重试机制，故有可能提交失败。</li>
</ul>
</li>
<li>同步提交offset<ul>
<li>由于同步提交offset有失败重试机制，故更加可靠，以下为同步提交offset的示例。  <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> tech.anzhen.kafka.consumer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerConfig;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecord;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecords;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.KafkaConsumer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.time.Duration;</span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span>: Anzhen</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@description</span>:</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@PROJECT</span>_NAME: sgg-big-data</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@create</span>: 2021-11-19 20:49</span></span><br><span class="line"><span class="comment"> **/</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CustomConsumerByHand</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 1. 创建kafka消费者配置类</span></span><br><span class="line">        Properties properties = <span class="keyword">new</span> Properties();</span><br><span class="line">        <span class="comment">// 2. 添加配置参数</span></span><br><span class="line">        <span class="comment">// 添加连接</span></span><br><span class="line">        properties.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">&quot;bd-server-001:9092,bd-server-002:9092,bd-server-003:9092,bd-server-004:9092,bd-server-005:9092&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 配置序列化 必须</span></span><br><span class="line">        properties.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, <span class="string">&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span>);</span><br><span class="line">        properties.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, <span class="string">&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 配置消费者组</span></span><br><span class="line">        properties.put(ConsumerConfig.GROUP_ID_CONFIG, <span class="string">&quot;test&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 是否自动提交offset</span></span><br><span class="line">        properties.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, <span class="string">&quot;false&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//3. 创建kafka消费者</span></span><br><span class="line">        KafkaConsumer&lt;String, String&gt; consumer = <span class="keyword">new</span> KafkaConsumer&lt;&gt;(properties);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//4. 设置消费主题  形参是列表</span></span><br><span class="line">        consumer.subscribe(Arrays.asList(<span class="string">&quot;first&quot;</span>));</span><br><span class="line"></span><br><span class="line">        <span class="comment">//5. 消费数据</span></span><br><span class="line">        <span class="keyword">while</span> (<span class="keyword">true</span>)&#123;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 读取消息</span></span><br><span class="line">            ConsumerRecords&lt;String, String&gt; consumerRecords = consumer.poll(Duration.ofSeconds(<span class="number">1</span>));</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 输出消息</span></span><br><span class="line">            <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; consumerRecord : consumerRecords) &#123;</span><br><span class="line">                System.out.println(consumerRecord.value());</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 同步提交offset</span></span><br><span class="line">            consumer.commitSync();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>异步提交offset<ul>
<li>虽然同步提交offset更可靠一些，但是由于其会阻塞当前线程，直到提交成功。因此吞吐量会受到很大的影响。因此更多的情况下，会选用异步提交offset的方式。</li>
<li>以下为异步提交offset的示例：  <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1. 创建kafka消费者配置类</span></span><br><span class="line">        Properties properties = <span class="keyword">new</span> Properties();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2. 添加配置参数</span></span><br><span class="line">        <span class="comment">// 添加连接</span></span><br><span class="line">        properties.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">&quot;hadoop102:9092&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 配置序列化 必须</span></span><br><span class="line">properties.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, <span class="string">&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span>);</span><br><span class="line">        properties.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, <span class="string">&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 配置消费者组</span></span><br><span class="line">        properties.put(ConsumerConfig.GROUP_ID_CONFIG, <span class="string">&quot;test&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 是否自动提交offset</span></span><br><span class="line">        properties.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, <span class="string">&quot;false&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//3. 创建Kafka消费者</span></span><br><span class="line">        KafkaConsumer&lt;String, String&gt; consumer = <span class="keyword">new</span> KafkaConsumer&lt;&gt;(properties);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//4. 设置消费主题  形参是列表</span></span><br><span class="line">        consumer.subscribe(Arrays.asList(<span class="string">&quot;first&quot;</span>));</span><br><span class="line"></span><br><span class="line">        <span class="comment">//5. 消费数据</span></span><br><span class="line">        <span class="keyword">while</span> (<span class="keyword">true</span>)&#123;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 读取消息</span></span><br><span class="line">            ConsumerRecords&lt;String, String&gt; consumerRecords = consumer.poll(Duration.ofSeconds(<span class="number">1</span>));</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 输出消息</span></span><br><span class="line">            <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; consumerRecord : consumerRecords) &#123;</span><br><span class="line">                System.out.println(consumerRecord.value());</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 异步提交offset</span></span><br><span class="line">            consumer.commitAsync(<span class="keyword">new</span> OffsetCommitCallback() &#123;</span><br><span class="line">                <span class="comment">/**</span></span><br><span class="line"><span class="comment">                 * 回调函数输出</span></span><br><span class="line"><span class="comment">                 * <span class="doctag">@param</span> offsets   offset信息</span></span><br><span class="line"><span class="comment">                 * <span class="doctag">@param</span> exception 异常</span></span><br><span class="line"><span class="comment">                 */</span></span><br><span class="line">                <span class="meta">@Override</span></span><br><span class="line">                <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onComplete</span><span class="params">(Map&lt;TopicPartition, OffsetAndMetadata&gt; offsets, Exception exception)</span> </span>&#123;</span><br><span class="line">                    <span class="comment">// 如果出现异常打印</span></span><br><span class="line">                    <span class="keyword">if</span> (exception != <span class="keyword">null</span> )&#123;</span><br><span class="line">                        System.err.println(<span class="string">&quot;Commit failed for &quot;</span> + offsets);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>
<h3 id="5-4-5-数据漏消费和重复消费分析"><a href="#5-4-5-数据漏消费和重复消费分析" class="headerlink" title="5.4.5 数据漏消费和重复消费分析"></a>5.4.5 数据漏消费和重复消费分析</h3><p>无论是同步提交还是异步提交offset，都有可能会造成数据的漏消费或者重复消费。先提交offset后消费，有可能造成数据的漏消费；而先消费后提交offset，有可能会造成数据的重复消费。所以需要Consumer事务保证<font color ='red' >精准一次性消费</font></p>
<h2 id="5-5-Consumer事务（精准一次性消费）"><a href="#5-5-Consumer事务（精准一次性消费）" class="headerlink" title="5.5 Consumer事务（精准一次性消费）"></a>5.5 Consumer事务（精准一次性消费）</h2><p><img src="https://i.loli.net/2021/11/19/mlHzxE5U2XwLYyd.jpg"><br>如果想完成Consumer端的精准一次性消费，那么需要Kafka消费端将消费过程和提交offset过程做原子绑定。此时我们需要将Kafka的offset保存到支持事务的自定义介质（比如MySQL）</p>
<h1 id="第6章-Kafka-Eagle监控"><a href="#第6章-Kafka-Eagle监控" class="headerlink" title="第6章 Kafka-Eagle监控"></a>第6章 Kafka-Eagle监控</h1><ul>
<li>官网：<a href="https://www.kafka-eagle.org/">https://www.kafka-eagle.org/</a></li>
</ul>
<ol>
<li>关闭Kafka集群 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 kafka]$ kf.sh stop</span><br></pre></td></tr></table></figure></li>
<li>修改$KAFAK_HOME/bin/kafka-server-start.sh命令修改如下参数值： <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> [ <span class="string">&quot;x<span class="variable">$KAFKA_HEAP_OPTS</span>&quot;</span> = <span class="string">&quot;x&quot;</span> ]; <span class="keyword">then</span></span><br><span class="line">    <span class="built_in">export</span> KAFKA_HEAP_OPTS=<span class="string">&quot;-server -Xms2G -Xmx2G -XX:PermSize=128m -XX:+UseG1GC -XX:MaxGCPauseMillis=200 -XX:ParallelGCThreads=8 -XX:ConcGCThreads=5 -XX:InitiatingHeapOccupancyPercent=70&quot;</span></span><br><span class="line">    <span class="built_in">export</span> JMX_PORT=<span class="string">&quot;9999&quot;</span></span><br><span class="line">    <span class="comment">#export KAFKA_HEAP_OPTS=&quot;-Xmx1G -Xms1G&quot;</span></span><br><span class="line"><span class="keyword">fi</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>注意：修改之后在启动Kafka之前要分发之其他节点<br>xsync kafka-server-start.sh</p>
</blockquote>
</li>
<li>上传压缩包kafka-eagle-bin-2.0.8.tar.gz到集群/opt/software目录</li>
<li>解压到本地 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 software]$ tar -zxvf kafka-eagle-bin-2.0.8.tar.gz </span><br></pre></td></tr></table></figure></li>
<li>将efak-web-2.0.8-bin.tar.gz解压至/opt/module <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 kafka-eagle-bin-2.0.8]$ tar -zxvf efak-web-2.0.8-bin.tar.gz -C /opt/module/</span><br></pre></td></tr></table></figure></li>
<li>修改配置文件 /opt/module/efak-web-2.0.8/conf/system-config.properties <figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="comment">######################################</span></span><br><span class="line"><span class="comment"># multi zookeeper &amp; kafka cluster list</span></span><br><span class="line"><span class="comment"># Settings prefixed with &#x27;kafka.eagle.&#x27; will be deprecated, use &#x27;efak.&#x27; instead</span></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line"><span class="meta">efak.zk.cluster.alias</span>=<span class="string">cluster1</span></span><br><span class="line"><span class="meta">cluster1.zk.list</span>=<span class="string">hadoop102:2181,hadoop103:2181,hadoop104:2181/kafka</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line"><span class="comment"># zookeeper enable acl</span></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line"><span class="meta">cluster1.zk.acl.enable</span>=<span class="string">false</span></span><br><span class="line"><span class="meta">cluster1.zk.acl.schema</span>=<span class="string">digest</span></span><br><span class="line"><span class="meta">cluster1.zk.acl.username</span>=<span class="string">test</span></span><br><span class="line"><span class="meta">cluster1.zk.acl.password</span>=<span class="string">test123</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line"><span class="comment"># broker size online list</span></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line"><span class="meta">cluster1.efak.broker.size</span>=<span class="string">20</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line"><span class="comment"># zk client thread limit</span></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line"><span class="meta">kafka.zk.limit.size</span>=<span class="string">32</span></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line"><span class="comment"># EFAK webui port</span></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line"><span class="meta">efak.webui.port</span>=<span class="string">8048</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line"><span class="comment"># kafka jmx acl and ssl authenticate</span></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line"><span class="meta">cluster1.efak.jmx.acl</span>=<span class="string">false</span></span><br><span class="line"><span class="meta">cluster1.efak.jmx.user</span>=<span class="string">keadmin</span></span><br><span class="line"><span class="meta">cluster1.efak.jmx.password</span>=<span class="string">keadmin123</span></span><br><span class="line"><span class="meta">cluster1.efak.jmx.ssl</span>=<span class="string">false</span></span><br><span class="line"><span class="meta">cluster1.efak.jmx.truststore.location</span>=<span class="string">/data/ssl/certificates/kafka.truststore</span></span><br><span class="line"><span class="meta">cluster1.efak.jmx.truststore.password</span>=<span class="string">ke123456</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line"><span class="comment"># kafka offset storage</span></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line"><span class="comment"># offset保存在kafka</span></span><br><span class="line"><span class="meta">cluster1.efak.offset.storage</span>=<span class="string">kafka</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line"><span class="comment"># kafka jmx uri</span></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line"><span class="meta">cluster1.efak.jmx.uri</span>=<span class="string">service:jmx:rmi:///jndi/rmi://%s/jmxrmi</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line"><span class="comment"># kafka metrics, 15 days by default</span></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line"><span class="meta">efak.metrics.charts</span>=<span class="string">true</span></span><br><span class="line"><span class="meta">efak.metrics.retain</span>=<span class="string">15</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line"><span class="comment"># kafka sql topic records max</span></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line"><span class="meta">efak.sql.topic.records.max</span>=<span class="string">5000</span></span><br><span class="line"><span class="meta">efak.sql.topic.preview.records.max</span>=<span class="string">10</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line"><span class="comment"># delete kafka topic token</span></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line"><span class="meta">efak.topic.token</span>=<span class="string">keadmin</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line"><span class="comment"># kafka sasl authenticate</span></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line"><span class="meta">cluster1.efak.sasl.enable</span>=<span class="string">false</span></span><br><span class="line"><span class="meta">cluster1.efak.sasl.protocol</span>=<span class="string">SASL_PLAINTEXT</span></span><br><span class="line"><span class="meta">cluster1.efak.sasl.mechanism</span>=<span class="string">SCRAM-SHA-256</span></span><br><span class="line"><span class="meta">cluster1.efak.sasl.jaas.config</span>=<span class="string">org.apache.kafka.common.security.scram.ScramLoginModule required username=&quot;kafka&quot; password=&quot;kafka-eagle&quot;;</span></span><br><span class="line"><span class="meta">cluster1.efak.sasl.client.id</span>=<span class="string"></span></span><br><span class="line"><span class="meta">cluster1.efak.blacklist.topics</span>=<span class="string"></span></span><br><span class="line"><span class="meta">cluster1.efak.sasl.cgroup.enable</span>=<span class="string">false</span></span><br><span class="line"><span class="meta">cluster1.efak.sasl.cgroup.topics</span>=<span class="string"></span></span><br><span class="line"><span class="meta">cluster2.efak.sasl.enable</span>=<span class="string">false</span></span><br><span class="line"><span class="meta">cluster2.efak.sasl.protocol</span>=<span class="string">SASL_PLAINTEXT</span></span><br><span class="line"><span class="meta">cluster2.efak.sasl.mechanism</span>=<span class="string">PLAIN</span></span><br><span class="line"><span class="meta">cluster2.efak.sasl.jaas.config</span>=<span class="string">org.apache.kafka.common.security.plain.PlainLoginModule required username=&quot;kafka&quot; password=&quot;kafka-eagle&quot;;</span></span><br><span class="line"><span class="meta">cluster2.efak.sasl.client.id</span>=<span class="string"></span></span><br><span class="line"><span class="meta">cluster2.efak.blacklist.topics</span>=<span class="string"></span></span><br><span class="line"><span class="meta">cluster2.efak.sasl.cgroup.enable</span>=<span class="string">false</span></span><br><span class="line"><span class="meta">cluster2.efak.sasl.cgroup.topics</span>=<span class="string"></span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line"><span class="comment"># kafka ssl authenticate</span></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line"><span class="meta">cluster3.efak.ssl.enable</span>=<span class="string">false</span></span><br><span class="line"><span class="meta">cluster3.efak.ssl.protocol</span>=<span class="string">SSL</span></span><br><span class="line"><span class="meta">cluster3.efak.ssl.truststore.location</span>=<span class="string"></span></span><br><span class="line"><span class="meta">cluster3.efak.ssl.truststore.password</span>=<span class="string"></span></span><br><span class="line"><span class="meta">cluster3.efak.ssl.keystore.location</span>=<span class="string"></span></span><br><span class="line"><span class="meta">cluster3.efak.ssl.keystore.password</span>=<span class="string"></span></span><br><span class="line"><span class="meta">cluster3.efak.ssl.key.password</span>=<span class="string"></span></span><br><span class="line"><span class="meta">cluster3.efak.ssl.endpoint.identification.algorithm</span>=<span class="string">https</span></span><br><span class="line"><span class="meta">cluster3.efak.blacklist.topics</span>=<span class="string"></span></span><br><span class="line"><span class="meta">cluster3.efak.ssl.cgroup.enable</span>=<span class="string">false</span></span><br><span class="line"><span class="meta">cluster3.efak.ssl.cgroup.topics</span>=<span class="string"></span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line"><span class="comment"># kafka sqlite jdbc driver address</span></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line"><span class="comment"># 配置mysql连接</span></span><br><span class="line"><span class="meta">efak.driver</span>=<span class="string">com.mysql.jdbc.Driver</span></span><br><span class="line"><span class="meta">efak.url</span>=<span class="string">jdbc:mysql://hadoop102:3306/ke?useUnicode=true&amp;characterEncoding=UTF-8&amp;zeroDateTimeBehavior=convertToNull</span></span><br><span class="line"><span class="meta">efak.username</span>=<span class="string">root</span></span><br><span class="line"><span class="meta">efak.password</span>=<span class="string">000000</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line"><span class="comment"># kafka mysql jdbc driver address</span></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line"><span class="comment">#efak.driver=com.mysql.cj.jdbc.Driver</span></span><br><span class="line"><span class="comment">#efak.url=jdbc:mysql://127.0.0.1:3306/ke?useUnicode=true&amp;characterEncoding=UTF-8&amp;zeroDateTimeBehavior=convertToNull</span></span><br><span class="line"><span class="comment">#efak.username=root</span></span><br><span class="line"><span class="comment">#efak.password=123456</span></span><br></pre></td></tr></table></figure></li>
<li>添加环境变量 /etc/profile.d/my_env.sh <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kafkaEFAK</span></span><br><span class="line"><span class="built_in">export</span> KE_HOME=/opt/module/efak</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$KE_HOME</span>/bin</span><br></pre></td></tr></table></figure></li>
<li>启动<ol>
<li>启动ZK以及KAFKA</li>
<li>启动efak <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 efak]$ bin/ke.sh start</span><br><span class="line">Version 2.0.8 -- Copyright 2016-2021</span><br><span class="line">*****************************************************************</span><br><span class="line">* EFAK Service has started success.</span><br><span class="line">* Welcome, Now you can visit <span class="string">&#x27;http://192.168.10.102:8048&#x27;</span></span><br><span class="line">* Account:admin ,Password:123456</span><br><span class="line">*****************************************************************</span><br><span class="line">* &lt;Usage&gt; ke.sh [start|status|stop|restart|stats] &lt;/Usage&gt;</span><br><span class="line">* &lt;Usage&gt; https://www.kafka-eagle.org/ &lt;/Usage&gt;</span><br><span class="line">*****************************************************************</span><br></pre></td></tr></table></figure>
<blockquote>
<p>说明：如果停止efak，执行命令 ke.sh stop</p>
</blockquote>
</li>
</ol>
</li>
<li>登录页面查看监控数据<br> <a href="http://192.168.10.102:8048/">http://192.168.10.102:8048/</a></li>
</ol>
]]></content>
      <categories>
        <category>Kafka</category>
      </categories>
      <tags>
        <tag>interview</tag>
        <tag>大数据</tag>
        <tag>Kafka</tag>
      </tags>
  </entry>
  <entry>
    <title>Java并发与多线程</title>
    <url>/2021/11/07/Java%E5%B9%B6%E5%8F%91%E4%B8%8E%E5%A4%9A%E7%BA%BF%E7%A8%8B/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="并发与多线程"><a href="#并发与多线程" class="headerlink" title="并发与多线程"></a>并发与多线程</h1><h2 id="volatile"><a href="#volatile" class="headerlink" title="volatile"></a>volatile</h2><ol>
<li>volatile 有什么用？<ul>
<li>volatile 保证内存可见性和禁止指令重排。volatile 可以提供部分原子性。</li>
<li>volatile 用于多线程环境下的单次操作(单次读或者单次写)。</li>
</ul>
</li>
<li>volatile 变量和 atomic 变量有什么不同？<ul>
<li>volatile 变量，可以确保先行关系，即写操作会发生在后续的读操作之前，但它并不能保证原子性。例如用 volatile 修饰 count 变量，那么 count++ 操作就不是原子性的。</li>
<li>AtomicInteger 类提供的 atomic 方法，可以让这种操作具有原子性。例如 #getAndIncrement() 方法，会原子性的进行增量操作把当前值加一，其它数据类型和引用变量也可以进行相似操作。</li>
</ul>
</li>
<li>Java 中能创建 volatile 数组吗?<ul>
<li>能创建但指向引用,不保证内部元素;</li>
</ul>
</li>
<li>volatile 能使得一个非原子操作变成原子操作吗？<ul>
<li>对任意单个volatile变量的读/写具有原子性，但类似于volatile++这种读写复合操作不具有原子性。</li>
</ul>
</li>
<li>volatile 修饰符的有过什么实践？<ul>
<li>用 volatile 修饰 long 和 double 变量,使其读写支持原子性;</li>
<li>状态标志 Boolean 值;</li>
<li>独立观察,单独监测某个多个线程共享的变量</li>
<li>轻量级锁</li>
</ul>
</li>
<li>volatile 类型变量提供什么保证？<ul>
<li>volatile 变量提供顺序性和可见性保证</li>
<li>避免指令重排</li>
<li>可见性保证</li>
</ul>
</li>
<li>volatile 和 synchronized 的区别？<ul>
<li>volatile 本质是在告诉 JVM 当前变量在寄存器（工作内存）中的值是不确定的，需要从主存中读取。synchronized 则是锁定当前变量，只有当前线程可以访问该变量，其他线程被阻塞住。</li>
<li>volatile 仅能使用在变量级别。synchronized 则可以使用在变量、方法、和类级别的。</li>
<li>volatile 仅能实现变量的修改可见性，不能保证原子性。而synchronized 则可以保证变量的修改可见性和原子性。</li>
<li>volatile 不会造成线程的阻塞。synchronized 可能会造成线程的阻塞。</li>
<li>volatile 标记的变量不会被编译器优化。synchronized标记的变量可以被编译器优化。</li>
</ul>
</li>
<li>什么场景下可以使用 volatile 替换 synchronized ？<ul>
<li>只需要保证共享资源的可见性的时候可以使用 volatile 替代，synchronized 保证可操作的原子性一致性和可见性。</li>
<li>volatile 适用于新值不依赖于旧值的情形。</li>
<li>1 写 N 读。</li>
<li>不与其他变量构成不变性条件时候使用 volatile 。<h2 id="synchronized"><a href="#synchronized" class="headerlink" title="synchronized"></a>synchronized</h2></li>
</ul>
</li>
<li>synchronized 的原理是什么?<ul>
<li>synchronized是 Java 内置的关键字，它提供了一种独占的加锁方式。</li>
<li>synchronized的获取和释放锁由JVM实现，用户不需要显示的释放锁，非常方便。</li>
<li>synchronized 也有一定的局限性。<ul>
<li>当线程尝试获取锁的时候，如果获取不到锁会一直阻塞。</li>
<li>如果获取锁的线程进入休眠或者阻塞，除非当前线程异常，否则其他线程尝试获取锁必须一直等待。</li>
</ul>
</li>
</ul>
</li>
<li>同步方法和同步块，哪个是更好的选择？<ul>
<li>同步块是更好的选择，因为它不会锁住整个对象（当然你也可以让它锁住整个对象）。同步方法会锁住整个对象，哪怕这个类中有多个不相关联的同步块，这通常会导致他们停止执行并需要等待获得这个对象上的锁。</li>
<li>同步块更要符合开放调用的原则，只在需要锁住的代码块锁住相应的对象，这样从侧面来说也可以避免死锁。</li>
</ul>
</li>
<li>当一个线程进入某个对象的一个 synchronized 的实例方法后，其它线程是否可进入此对象的其它方法？<ul>
<li>如果其他方法没有 synchronized 的话，其他线程是可以进入的。</li>
<li>所以要开放一个线程安全的对象时，得保证每个方法都是线程安全的。、</li>
</ul>
</li>
<li>在监视器(Monitor)内部，是如何做线程同步的？<ul>
<li>监视器和锁在 Java 虚拟机中是一块使用的。监视器监视一块同步代码块，确保一次只有一个线程执行同步代码块。每一个监视器都和一个对象引用相关联。线程在获取锁之前不允许执行同步代码。</li>
</ul>
</li>
<li>Java 如何实现“自旋”（spin） <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SpinLock</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> AtomicReference&lt;Thread&gt; sign =<span class="keyword">new</span> AtomicReference&lt;&gt;();</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">lock</span><span class="params">()</span> </span>&#123; <span class="comment">// &lt;1&gt;</span></span><br><span class="line">        Thread current = Thread.currentThread();</span><br><span class="line">        <span class="keyword">while</span>(!sign .compareAndSet(<span class="keyword">null</span>, current)) &#123;</span><br><span class="line">            <span class="comment">// &lt;1.1&gt;</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">unlock</span> <span class="params">()</span> </span>&#123; <span class="comment">// &lt;2&gt;</span></span><br><span class="line">        Thread current = Thread.currentThread();</span><br><span class="line">        sign .compareAndSet(current, <span class="keyword">null</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>&lt;1&gt; 处，#lock() 方法，如果获得不到锁，就会“死循环”，直到或得到锁为止。考虑到“死循环”会持续占用 CPU ，可能导致其它线程无法获得到 CPU 执行，可以在 &lt;1.1&gt; 处增加 Thread.yiead() 代码段，出让下 CPU 。</li>
<li>&lt;2&gt; 处，#unlock() 方法，释放锁。</li>
</ul>
</li>
</ol>
<p>#Java Lock 接口<br>    - <img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/15852735326621.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
<ol>
<li>什么是 Java Lock 接口？<ul>
<li>java.util.concurrent.locks.Lock 接口，比 synchronized 提供更具拓展性的锁操作。它允许更灵活的结构，可以具有完全不同的性质，并且可以支持多个相关类的条件对象。它的优势有：<ul>
<li>可以使锁更公平。</li>
<li>可以使线程在等待锁的时候响应中断。</li>
<li>可以让线程尝试获取锁，并在无法获取锁的时候立即返回或者等待一段时间。</li>
<li>可以在不同的范围，以不同的顺序获取和释放锁。</li>
</ul>
</li>
</ul>
</li>
<li>什么是可重入锁（ReentrantLock）？<ul>
<li>举例来说明锁的可重入性。代码如下：  <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">UnReentrant</span></span>&#123;</span><br><span class="line">    Lock lock = <span class="keyword">new</span> Lock();</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">outer</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        lock.lock();</span><br><span class="line">        inner();</span><br><span class="line">        lock.unlock();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">inner</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        lock.lock();</span><br><span class="line">        <span class="comment">//do something</span></span><br><span class="line">        lock.unlock();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>outer() 方法中调用了 #inner() 方法，#outer() 方法先锁住了 lock ，这样 #inner() 就不能再获取 lock 。</li>
<li>其实调用 #outer() 方法的线程已经获取了 lock 锁，但是不能在 #inner() 方法中重复利用已经获取的锁资源，这种锁即称之为不可重入。</li>
<li>可重入就意味着：线程可以进入任何一个它已经拥有的锁所同步着的代码块。</li>
<li>synchronized、ReentrantLock 都是可重入的锁，可重入锁相对来说简化了并发编程的开发。</li>
<li>ReenTrantLock 的实现是一种自旋锁，通过循环调用 CAS 操作来实现加锁。它的性能比较好也是因为避免了使线程进入内核态的阻塞状态。想尽办法避免线程进入内核的阻塞状态是我们去分析和理解锁设计的关键钥匙。</li>
</ul>
</li>
<li>synchronized 和 ReentrantLock 异同？<ul>
<li>相同点<ul>
<li>都实现了多线程同步和内存可见性语义。</li>
<li>都是可重入锁。</li>
</ul>
</li>
<li>不同点<ul>
<li>同步实现机制不同<ul>
<li>synchronized 通过 Java 对象头锁标记和 Monitor 对象实现同步。</li>
<li>ReentrantLock 通过CAS、AQS（AbstractQueuedSynchronizer）和 LockSupport（用于阻塞和解除阻塞）实现同步。</li>
</ul>
</li>
<li>可见性实现机制不同<ul>
<li>synchronized 依赖 JVM 内存模型保证包含共享变量的多线程内存可见性。</li>
<li>ReentrantLock 通过 ASQ 的 volatile state 保证包含共享变量的多线程内存可见性。</li>
</ul>
</li>
<li>使用方式不同<ul>
<li>synchronized 可以修饰实例方法（锁住实例对象）、静态方法（锁住类对象）、代码块（显示指定锁对象）。</li>
<li>ReentrantLock 显示调用 tryLock 和 lock 方法，需要在 finally 块中释放锁。</li>
</ul>
</li>
<li>功能丰富程度不同<ul>
<li>synchronized 不可设置等待时间、不可被中断（interrupted）。</li>
<li>ReentrantLock 提供有限时间等候锁（设置过期时间）、可中断锁（lockInterruptibly）、condition（提供 await、condition（提供 await、signal 等方法）等丰富功能</li>
</ul>
</li>
<li>锁类型不同<ul>
<li>synchronized 只支持非公平锁。</li>
<li>ReentrantLock 提供公平锁和非公平锁实现。当然，在大部分情况下，非公平锁是高效的选择。</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>ReadWriteLock 是什么？<ul>
<li>ReadWriteLock ，读写锁是，用来提升并发程序性能的锁分离技术的 Lock 实现类。可以用于 “多读少写” 的场景，读写锁支持多个读操作并发执行，写操作只能由一个线程来操作。</li>
<li>ReadWriteLock 对向数据结构相对不频繁地写入，但是有多个任务要经常读取这个数据结构的这类情况进行了优化。ReadWriteLock 使得你可以同时有多个读取者，只要它们都不试图写入即可。如果写锁已经被其他任务持有，那么任何读取者都不能访问，直至这个写锁被释放为止。</li>
<li>ReadWriteLock 对程序性能的提高主要受制于如下几个因素：<ul>
<li>数据被读取的频率与被修改的频率相比较的结果。</li>
<li>读取和写入的时间</li>
<li>有多少线程竞争</li>
<li>是否在多处理机器上运行</li>
</ul>
</li>
</ul>
</li>
<li>Condition 是什么？<ul>
<li><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/15852741845639.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10" alt="-w762"><h2 id="Java-内存模型"><a href="#Java-内存模型" class="headerlink" title="Java 内存模型"></a>Java 内存模型</h2></li>
</ul>
</li>
<li>两个线程之间是如何通信的呢？<ul>
<li>线程之间的通信方式，目前有共享内存和消息传递两种。<ul>
<li>在共享内存的并发模型里，线程之间共享程序的公共状态，线程之间通过写-读内存中的公共状态来隐式进行通信。典型的共享内存通信方式，就是通过共享对象进行通信。<ul>
<li><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/15852745337896.png?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></li>
</ul>
</li>
<li>在消息传递的并发模型里，线程之间没有公共状态，线程之间必须通过明确的发送消息来显式进行通信。在 Java 中典型的消息传递方式，就是 #wait() 和 #notify() ，或者 BlockingQueue 。<ul>
<li><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/15852745812427.png?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>为什么代码会重排序？<ul>
<li>在执行程序时，为了提供性能，处理器和编译器常常会对指令进行重排序，但是不能随意重排序，不是你想怎么排序就怎么排序，它需要满足以下两个条件：<ul>
<li>在单线程环境下不能改变程序运行的结果。</li>
<li>存在数据依赖关系的不允许重排序</li>
</ul>
<blockquote>
<p>重排序不会影响单线程环境的执行结果，但是会破坏多线程的执行语义。</p>
</blockquote>
</li>
</ul>
</li>
</ol>
<h2 id="Java-并发容器"><a href="#Java-并发容器" class="headerlink" title="Java 并发容器"></a>Java 并发容器</h2><ol>
<li>Java 中 ConcurrentHashMap 的并发度是什么？<ul>
<li>锁单个Node节点</li>
</ul>
</li>
<li>ConcurrentHashMap 为何读不用加锁？<ul>
<li>Node 的 val 和 next 均为 volatile 型。</li>
<li>tabAt(..,) 和 casTabAt(…) 对应的 Unsafe 操作实现了 volatile 语义。</li>
</ul>
</li>
<li>CopyOnWriteArrayList 可以用于什么应用场景？<ul>
<li>CopyOnWriteArrayList(免锁容器)的好处之一是当多个迭代器同时遍历和修改这个列表时，不会抛出ConcurrentModificationException 异常。在 CopyOnWriteArrayList 中，写入将导致创建整个底层数组的副本，而源数组将保留在原地，使得复制的数组在被修改时，读取操作可以安全地执行。<ul>
<li>由于写操作的时候，需要拷贝数组，会消耗内存，如果原数组的内容比较多的情况下，可能导致 ygc 或者 fgc 。</li>
<li>不能用于实时读的场景，像拷贝数组、新增元素都需要时间，所以调用一个 set 操作后，读取到数据可能还是旧的,虽然 CopyOnWriteArrayList 能做到最终一致性,但是还是没法满足实时性要求。</li>
</ul>
</li>
<li>CopyOnWriteArrayList 透露的思想：</li>
<li>读写分离，读和写分开</li>
<li>最终一致性</li>
<li>使用另外开辟空间的思路，来解决并发冲突<h2 id="Java-阻塞队列"><a href="#Java-阻塞队列" class="headerlink" title="Java 阻塞队列"></a>Java 阻塞队列</h2></li>
</ul>
</li>
<li>什么是阻塞队列？有什么适用场景？<ul>
<li>阻塞队列（BlockingQueue）是一个支持两个附加操作的队列。这两个附加的操作是：<ul>
<li>在队列为空时，获取元素的线程会等待队列变为非空。</li>
<li>当队列满时，存储元素的线程会等待队列可用。</li>
</ul>
</li>
<li>阻塞队列常用于生产者和消费者的场景：<ul>
<li>生产者是往队列里添加元素的线程，消费者是从队列里拿元素的线程</li>
<li>阻塞队列就是生产者存放元素的容器，而消费者也只从容器里拿元素。</li>
</ul>
</li>
<li>BlockingQueue 接口，是 Queue 的子接口，它的主要用途并不是作为容器，而是作为线程同步的的工具，因此他具有一个很明显的特性：<ul>
<li>当生产者线程试图向 BlockingQueue 放入元素时，如果队列已满，则线程被阻塞。</li>
<li>当消费者线程试图从中取出一个元素时，如果队列为空，则该线程会被阻塞。</li>
<li>正是因为它所具有这个特性，所以在程序中多个线程交替向BlockingQueue中 放入元素，取出元素，它可以很好的控制线程之间的通信。</li>
<li>阻塞队列使用最经典的场景，就是 Socket 客户端数据的读取和解析：<ul>
<li>读取数据的线程不断将数据放入队列。</li>
<li>然后，解析线程不断从队列取数据解析。</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>Java 提供了哪些阻塞队列的实现？<ul>
<li>ArrayBlockingQueue ：一个由数组结构组成的有界阻塞队列。</li>
<li>LinkedBlockingQueue ：一个由链表结构组成的有界阻塞队列。</li>
<li>PriorityBlockingQueue ：一个支持优先级排序的无界阻塞队列。</li>
<li>DelayQueue：支持延时获取元素的无界阻塞队列，即可以指定多久才能从队列中获取当前元素。</li>
<li>SynchronousQueue：一个不存储元素的阻塞队列。</li>
</ul>
</li>
<li>简述 ConcurrentLinkedQueue 和 LinkedBlockingQueue 的用处和不同之处？<ul>
<li>阻塞队列，典型例子是 LinkedBlockingQueue 。使用阻塞队列的好处：多线程操作共同的队列时不需要额外的同步，另外就是队列会自动平衡负载，即那边（生产与消费两边）处理快了就会被阻塞掉，从而减少两边的处理速度差距。</li>
<li>非阻塞队列，典型例子是 ConcurrentLinkedQueue 。当许多线程共享访问一个公共集合时，ConcurrentLinkedQueue 是一个恰当的选择。</li>
<li>具体的选择，如下：<ul>
<li>LinkedBlockingQueue 多用于任务队列。<ul>
<li>单生产者，单消费者</li>
<li>多生产者，单消费者</li>
</ul>
</li>
<li>ConcurrentLinkedQueue 多用于消息队列。<ul>
<li>单生产者，多消费者</li>
<li>多生产者，多消费者</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ol>
<h2 id="Java-原子操作类"><a href="#Java-原子操作类" class="headerlink" title="Java 原子操作类"></a>Java 原子操作类</h2><ol>
<li>什么是原子操作？<ul>
<li>原子操作（Atomic Operation），意为”不可被中断的一个或一系列操作”。</li>
<li>处理器使用基于对缓存加锁或总线加锁的方式，来实现多处理器之间的原子操作。</li>
<li>在 Java 中，可以通过锁和循环 CAS 的方式来实现原子操作。CAS操作 —— Compare &amp; Set ，或是 Compare &amp; Swap ，现在几乎所有的 CPU 指令都支持 CAS 的原子操作。</li>
</ul>
</li>
<li>CAS 操作有什么缺点？<ul>
<li>ABA 问题<ul>
<li>比如说一个线程 one 从内存位置 V 中取出 A ，这时候另一个线程 two 也从内存中取出 A ，并且 two 进行了一些操作变成了 B ，然后 two 又将 V 位置的数据变成 A ，这时候线程 one 进行 CAS 操作发现内存中仍然是 A ，然后 one 操作成功。尽管线程 one 的 CAS 操作成功，但可能存在潜藏的问题。</li>
</ul>
</li>
<li>循环时间长开销大<ul>
<li>对于资源竞争严重（线程冲突严重）的情况，CAS 自旋的概率会比较大，从而浪费更多的 CPU 资源，效率低于</li>
</ul>
</li>
<li>只能保证一个共享变量的原子操作<ul>
<li>当对一个共享变量执行操作时，我们可以使用循环 CAS 的方式来保证原子操作，但是对多个共享变量操作时，循环 CAS 就无法保证操作的原子性，这个时候就可以用锁。<h2 id="Java-并发工具类"><a href="#Java-并发工具类" class="headerlink" title="Java 并发工具类"></a>Java 并发工具类</h2></li>
</ul>
</li>
</ul>
</li>
<li>Semaphore 是什么？<ul>
<li>Semaphore ，是一种新的同步类，它是一个计数信号。从概念上讲，从概念上讲，信号量维护了一个许可集合。<ul>
<li>如有必要，在许可可用前会阻塞每一个 #acquire() 方法，然后再获取该许可。</li>
<li>每个 #release() 方法，添加一个许可，从而可能释放一个正在阻塞的获取者。</li>
<li>但是，不使用实际的许可对象，Semaphore 只对可用许可的数量进行计数，并采取相应的行动。</li>
</ul>
</li>
</ul>
</li>
<li>说说 CountDownLatch 原理<ul>
<li>CountDownLatch ，字面意思是减小计数（CountDown）的门闩（Latch）。它要做的事情是，等待指定数量的计数被减少，意味着门闩被打开，然后进行执行。</li>
<li>CountDownLatch 默认的构造方法是 CountDownLatch(int count) ，其参数表需要减少的计数，主线程调用 #await() 方法告诉 CountDownLatch 阻塞等待指定数量的计数被减少，然后其它线程调用 CountDownLatch 的 #countDown() 方法，减小计数(不会阻塞)。等待计数被减少到零，主线程结束阻塞等待，继续往下执行。</li>
</ul>
</li>
<li>说说 CyclicBarrier 原理<ul>
<li>CyclicBarrier ，字面意思是可循环使用（Cyclic）的屏障（Barrier）。它要做的事情是，让一组线程到达一个屏障（也可以叫同步点）时被阻塞，直到最后一个线程到达屏障时，屏障才会开门，所有被屏障拦截的线程才会继续干活。</li>
<li>CyclicBarrier 默认的构造方法是 CyclicBarrier(int parties) ，其参数表示屏障拦截的线程数量，每个线程调用 #await() 方法告诉 CyclicBarrier 我已经到达了屏障，然后当前线程被阻塞，直到 parties 个线程到达，结束阻塞。</li>
</ul>
</li>
<li>CyclicBarrier 和 CountdownLatch 有什么区别？<ul>
<li>CyclicBarrier 可以重复使用，而 CountdownLatch 不能重复使用。</li>
<li>CountDownLatch 其实可以把它看作一个计数器，只不过这个计数器的操作是原子操作。</li>
<li>CyclicBarrier 一个同步辅助类，它允许一组线程互相等待，直到到达某个公共屏障点 (common barrier point)</li>
</ul>
<table>
<thead>
<tr>
<th>CountDownLatch</th>
<th>CyclicBarrier</th>
</tr>
</thead>
<tbody><tr>
<td>减计数方式</td>
<td>加计数方式</td>
</tr>
<tr>
<td>计算为 0 时释放所有等待的线程</td>
<td>计数达到指定值时释放所有等待线程</td>
</tr>
<tr>
<td>计数为 0 时，无法重置</td>
<td>计数达到指定值时，计数置为 0 重新开始</td>
</tr>
<tr>
<td>调用 #countDown() 方法计数减一，调用 #await() 方法只进行阻塞，对计数没任何影响</td>
<td>调用 #await() 方法计数加 1 ，若加 1 后的值不等于构造方法的值，则线程阻塞</td>
</tr>
<tr>
<td>不可重复利用</td>
<td>可重复利用</td>
</tr>
</tbody></table>
</li>
</ol>
<h2 id="Java-线程池"><a href="#Java-线程池" class="headerlink" title="Java 线程池"></a>Java 线程池</h2><ul>
<li><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/15852766289037.png?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></li>
</ul>
<ol>
<li>什么是 Executor 框架？<ul>
<li>Executor 框架，是一个根据一组执行策略调用，调度，执行和控制的异步任务的框架。</li>
<li>无限制的创建线程，会引起应用程序内存溢出。所以创建一个线程池是个更好的的解决方案，因为可以限制线程的数量并且可以回收再利用这些线程。利用 Executor 框架，可以非常方便的创建一个线程池。</li>
</ul>
</li>
<li>为什么使用 Executor 框架？<ul>
<li>每次执行任务创建线程 new Thread() 比较消耗性能，创建一个线程是比较耗时、耗资源的。</li>
<li>调用 new Thread() 创建的线程缺乏管理，被称为野线程，而且可以无限制的创建，线程之间的相互竞争会导致过多占用系统资源而导致系统瘫痪，还有线程之间的频繁交替也会消耗很多系统资源。</li>
<li>接使用 new Thread() 启动的线程不利于扩展，比如定时执行、定期执行、定时定期执行、线程中断等都不便实现。</li>
</ul>
</li>
<li>在 Java 中 Executor 和 Executors 的区别？<ul>
<li>Executors 是 Executor 的工具类，不同方法按照我们的需求创建了不同的线程池，来满足业务的需求。</li>
<li>Executor 是接口对象，能执行我们的线程任务。<ul>
<li>ExecutorService 接口，继承了 Executor 接口，并进行了扩展，提供了更多的方法我们能获得任务执行的状态并且可以获取任务的返回值。</li>
<li>使用 ThreadPoolExecutor ，可以创建自定义线程池。</li>
<li>Future 表示异步计算的结果，他提供了检查计算是否完成的方法，以等待计算的完成，并可以使用 #get() 方法，获取计算的结果。</li>
</ul>
</li>
</ul>
</li>
<li>创建线程池的几种方式？<ul>
<li>Executors 创建的线程池，分成普通任务线程池，和定时任务线程池。<ul>
<li>普通任务线程池<ul>
<li>1、#newFixedThreadPool(int nThreads) 方法，创建一个固定长度的线程池。每当提交一个任务就创建一个线程，直到达到线程池的最大数量，这时线程规模将不再变化。当线程发生未预期的错误而结束时，线程池会补充一个新的线程。</li>
<li>2、#newCachedThreadPool() 方法，创建一个可缓存的线程池。如果线程池的规模超过了处理需求，将自动回收空闲线程。当需求增加时，则可以自动添加新线程。线程池的规模不存在任何限制。</li>
<li>3、#newSingleThreadExecutor() 方法，创建一个单线程的线程池。它创建单个工作线程来执行任务，如果这个线程异常结束，会创建一个新的来替代它。它的特点是，能确保依照任务在队列中的顺序来串行执行。</li>
</ul>
</li>
<li>定时任务线程池<ul>
<li>4、#newScheduledThreadPool(int corePoolSize) 方法，创建了一个固定长度的线程池，而且以延迟或定时的方式来执行任务，类似 Timer 。</li>
<li>5、#newSingleThreadExecutor() 方法，创建了一个固定长度为 1 的线程池，而且以延迟或定时的方式来执行任务，类似 Timer 。</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>如何使用 ThreadPoolExecutor 创建线程池？ <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">ThreadPoolExecutor</span><span class="params">(<span class="keyword">int</span> corePoolSize,</span></span></span><br><span class="line"><span class="params"><span class="function">                      <span class="keyword">int</span> maximumPoolSize,</span></span></span><br><span class="line"><span class="params"><span class="function">                      <span class="keyword">long</span> keepAliveTime,</span></span></span><br><span class="line"><span class="params"><span class="function">                      TimeUnit unit,</span></span></span><br><span class="line"><span class="params"><span class="function">                      BlockingQueue&lt;Runnable&gt; workQueue,</span></span></span><br><span class="line"><span class="params"><span class="function">                      ThreadFactory threadFactory,</span></span></span><br><span class="line"><span class="params"><span class="function">                      RejectedExecutionHandler handler)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (corePoolSize &lt; <span class="number">0</span> ||</span><br><span class="line">        maximumPoolSize &lt;= <span class="number">0</span> ||</span><br><span class="line">        maximumPoolSize &lt; corePoolSize ||</span><br><span class="line">        keepAliveTime &lt; <span class="number">0</span>)</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException();</span><br><span class="line">    <span class="keyword">if</span> (workQueue == <span class="keyword">null</span> || threadFactory == <span class="keyword">null</span> || handler == <span class="keyword">null</span>)</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> NullPointerException();</span><br><span class="line">    <span class="keyword">this</span>.corePoolSize = corePoolSize;</span><br><span class="line">    <span class="keyword">this</span>.maximumPoolSize = maximumPoolSize;</span><br><span class="line">    <span class="keyword">this</span>.workQueue = workQueue;</span><br><span class="line">    <span class="keyword">this</span>.keepAliveTime = unit.toNanos(keepAliveTime);</span><br><span class="line">    <span class="keyword">this</span>.threadFactory = threadFactory;</span><br><span class="line">    <span class="keyword">this</span>.handler = handler;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>corePoolSize 参数，核心线程数大小，当线程数 &lt; corePoolSize ，会创建线程执行任务。</li>
<li>maximumPoolSize 参数，最大线程数， 当线程数 &gt;= corePoolSize 的时候，会把任务放入 workQueue 队列中。</li>
<li>keepAliveTime 参数，保持存活时间，当线程数大于 corePoolSize 的空闲线程能保持的最大时间。</li>
<li>unit 参数，时间单位。</li>
<li>workQueue 参数，保存任务的阻塞队列。</li>
<li>handler 参数，超过阻塞队列的大小时，使用的拒绝策略。</li>
<li>threadFactory 参数，创建线程的工厂。</li>
</ul>
</li>
<li>ThreadPoolExecutor 有哪些拒绝策略？<ul>
<li>ThreadPoolExecutor 默认有四个拒绝策略：<ul>
<li>ThreadPoolExecutor.AbortPolicy() ，直接抛出异常 RejectedExecutionException 。</li>
<li>ThreadPoolExecutor.CallerRunsPolicy() ，直接调用 run 方法并且阻塞执行。</li>
<li>ThreadPoolExecutor.DiscardPolicy() ，直接丢弃后来的任务。</li>
<li>ThreadPoolExecutor.DiscardOldestPolicy() ，丢弃在队列中队首的任务。</li>
<li>如果有需要，可以自己实现 RejectedExecutionHandler 接口，实现自定义的拒绝逻辑</li>
</ul>
</li>
</ul>
</li>
<li>线程池的关闭方式有几种？<ul>
<li>ThreadPoolExecutor 提供了两个方法，用于线程池的关闭，分别是：<ul>
<li>shutdown() 方法，不会立即终止线程池，而是要等所有任务缓存队列中的任务都执行完后才终止，但再也不会接受新的任务。</li>
<li>shutdownNow() 方法，立即终止线程池，并尝试打断正在执行的任务，并且清空任务缓存队列，返回尚未执行的任务。</li>
</ul>
</li>
</ul>
</li>
<li>Java 线程池大小为何会大多被设置成 CPU 核心数 +1 ？<ul>
<li>如果是 CPU 密集型应用，则线程池大小设置为 N+1<ul>
<li>因为 CPU 密集型任务使得 CPU 使用率很高，若开过多的线程数，只能增加上下文切换的次数，因此会带来额外的开销。</li>
</ul>
</li>
<li>如果是 IO 密集型应用，则线程池大小设置为 2N+1<ul>
<li>IO密 集型任务 CPU 使用率并不高，因此可以让 CPU 在等待 IO 的时候去处理别的任务，充分利用 CPU 时间。</li>
</ul>
</li>
<li>如果是混合型应用，那么分别创建线程池<ul>
<li>可以将任务分成 IO 密集型和 CPU 密集型任务，然后分别用不同的线程池去处理。 只要分完之后两个任务的执行时间相差不大，那么就会比串行执行来的高效。</li>
<li>因为如果划分之后两个任务执行时间相差甚远，那么先执行完的任务就要等后执行完的任务，最终的时间仍然取决于后执行完的任务，而且还要加上任务拆分与合并的开销，得不偿失。</li>
</ul>
</li>
</ul>
</li>
<li>线程池容量的动态调整？<ul>
<li>ThreadPoolExecutor 提供了动态调整线程池容量大小的方法：<ul>
<li>setCorePoolSize：设置核心池大小。</li>
<li>setMaximumPoolSize：设置线程池最大能创建的线程数目大小。</li>
</ul>
</li>
<li>当上述参数从小变大时，ThreadPoolExecutor 进行线程赋值，还可能立即创建新的线程来执行任务。</li>
</ul>
</li>
<li>什么是 Callable、Future、FutureTask ？ <ul>
<li>Callable: 可以认为是带有回调的 Runnable 。</li>
<li>Future: 表示异步任务，是还没有完成的任务给出的未来结果。所以说 Callable 用于产生结果，Future 用于获取结果。</li>
<li>FutureTask; 表示一个可以取消的异步运算。<ul>
<li>它有启动和取消运算、查询运算是否完成和取回运算结果等方法。只有当运算完成的时候结果才能取回，如果运算尚未完成 get 方法将会阻塞。</li>
<li>一个 FutureTask 对象，可以对调用了 Callable 和 Runnable 的对象进行包装，由于 FutureTask 也是继承了 Runnable 接口，所以它可以提交给 Executor 来执行。</li>
</ul>
</li>
</ul>
</li>
<li>线程池执行任务的过程？<ul>
<li>刚创建时，里面没有线程调用 execute() 方法，添加任务时：<ul>
<li>如果正在运行的线程数量小于核心参数 corePoolSize ，继续创建线程运行这个任务<ul>
<li>否则，如果正在运行的线程数量大于或等于 corePoolSize ，将任务加入到阻塞队列中。<ul>
<li>否则，如果队列已满，同时正在运行的线程数量小于核心参数 maximumPoolSize ，继续创建线程运行这个任务。<ul>
<li>否则，如果队列已满，同时正在运行的线程数量大于或等于 maximumPoolSize ，根据设置的拒绝策略处理。</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>完成一个任务，继续取下一个任务处理。<ul>
<li>没有任务继续处理，线程被中断或者线程池被关闭时，线程退出执行，如果线程池被关闭，线程结束。</li>
<li>否则，判断线程池正在运行的线程数量是否大于核心线程数，如果是，线程结束，否则线程阻塞。因此线程池任务全部执行完成后，继续留存的线程池大小为 corePoolSize 。</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>线程池中 submit 和 execute 方法有什么区别？<ul>
<li>两个方法都可以向线程池提交任务。<ul>
<li>execute(…) 方法，返回类型是 void ，它定义在 Executor 接口中。</li>
<li>submit(…) 方法，可以返回持有计算结果的 Future 对象，它定义在 ExecutorService 接口中，它扩展了 Executor 接口，其它线程池类像 ThreadPoolExecutor 和 ScheduledThreadPoolExecutor 都有这些方法。</li>
</ul>
</li>
</ul>
</li>
<li>如果你提交任务时，线程池队列已满，这时会发生什么？<ul>
<li>重点在于线程池的队列是有界还是无界的。</li>
</ul>
</li>
<li>Fork/Join 框架是什么？<ul>
<li>Fork/Join 框架是一个实现了 ExecutorService接口 的多线程处理器。它可以把一个大的任务划分为若干个小的任务并发执行，充分利用可用的资源，进而提高应用的执行效率。</li>
<li>Fork 就是把一个大任务切分为若干子任务并行的执行</li>
<li>Join 就是合并这些子任务的执行结果，最后得到这个大任务的结果。</li>
<li>比如计算 1+2+…＋10000 ，可以分割成 10 个子任务，每个子任务分别对 1000 个数进行求和，最终汇总这 10 个子任务的结果。</li>
</ul>
</li>
<li>如何让一段程序并发的执行，并最终汇总结果？<ul>
<li>1、CountDownLatch：允许一个或者多个线程等待前面的一个或多个线程完成，构造一个 CountDownLatch 时指定需要 countDown 的点的数量，每完成一点就 countDown 一下。当所有点都完成，CountDownLatch 的 #await() 就解除阻塞。</li>
<li>2、CyclicBarrier：可循环使用的 Barrier ，它的作用是让一组线程到达一个 Barrier 后阻塞，直到所有线程都到达 Barrier 后才能继续执行。</li>
<li>3、Fork/Join 框架，fork 把大任务分解成多个小任务，然后汇总多个小任务的结果得到最终结果。使用一个双端队列，当线程空闲时从双端队列的另一端领取任务。</li>
</ul>
</li>
</ol>
]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>interview</tag>
      </tags>
  </entry>
  <entry>
    <title>RocketMQ</title>
    <url>/2021/11/07/RocketMQ/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="RocketMQ"><a href="#RocketMQ" class="headerlink" title="RocketMQ"></a>RocketMQ</h1><ol>
<li>RocketMQ 由哪些角色组成？<ul>
<li><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/15850381816926.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></li>
<li>生产者（Producer）：负责产生消息，生产者向消息服务器发送由业务应用程序系统生成的消息。</li>
<li>消费者（Consumer）：负责消费消息，消费者从消息服务器拉取信息并将其输入用户应用程序。</li>
<li>消息服务器（Broker）：是消息存储中心，主要作用是接收来自 Producer 的消息并存储， Consumer 从这里取得消息。</li>
<li>名称服务器（NameServer）：用来保存 Broker 相关 Topic 等元信息并给 Producer ，提供 Consumer 查找 Broker 信息。</li>
</ul>
</li>
<li>请描述下 RocketMQ 的整体流程？<pre><code> - ![](https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/15850382480500.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10)
</code></pre>
<ul>
<li>启动 Namesrv，Namesrv起 来后监听端口，等待 Broker、Producer、Consumer 连上来，相当于一个路由控制中心。</li>
<li>Broker 启动，<strong>跟所有的 Namesrv 保持长连接</strong>，定时发送心跳包。心跳包中，包含当前 Broker 信息(IP+端口等)以及存储所有 Topic 信息。注册成功后，Namesrv 集群中就有 Topic 跟 Broker 的映射关系。</li>
<li>收发消息前，先创建 Topic 。创建 Topic 时，需要指定该 Topic 要存储在 哪些 Broker上。也可以在发送消息时自动创建Topic。</li>
<li>Producer 发送消息。启动时，先跟 Namesrv 集群中的其中一台建立长连接，并从Namesrv 中获取当前发送的 Topic 存在哪些 Broker 上，然后跟对应的 Broker 建立长连接，直接向 Broker 发消息</li>
<li>Consumer 消费消息。Consumer 跟 Producer 类似。跟其中一台 Namesrv 建立长连接，获取当前订阅 Topic 存在哪些 Broker 上，然后直接跟 Broker 建立连接通道，开始消费消息。</li>
</ul>
</li>
<li>请说说你对 Namesrv 的了解？<ul>
<li>Namesrv 用于存储 Topic、Broker 关系信息，功能简单，稳定性高。<ul>
<li>多个 Namesrv 之间相互没有通信，单台 Namesrv 宕机不影响其它 Namesrv 与集群。多个 Namesrv 之间的信息共享，通过 Broker 主动向多个 Namesrv 都发起心跳。正如上文所说，Broker 需要跟所有 Namesrv 连接。</li>
<li>即使整个 Namesrv 集群宕机，已经正常工作的 Producer、Consumer、Broker 仍然能正常工作，但新起的 Producer、Consumer、Broker 就无法工作。(这点和 Dubbo 有些不同，不会缓存 Topic 等元信息到本地文件。)</li>
</ul>
</li>
<li> Namesrv 压力不会太大，平时主要开销是在维持心跳和提供 Topic-Broker 的关系数据。但有一点需要注意，Broker 向 Namesr 发心跳时，会带上当前自己所负责的所有 Topic 信息，如果 Topic 个数太多（万级别），会导致一次心跳中，就 Topic 的数据就几十 M，网络情况差的话，网络传输失败，心跳失败，导致 Namesrv 误认为 Broker 心跳失败。</li>
</ul>
</li>
<li>如何配置 Namesrv 地址到生产者和消费者？<ul>
<li><strong>编程方式</strong>，就像 producer.setNamesrvAddr(“ip:port”) 。</li>
<li>Java 启动参数设置，使用 rocketmq.namesrv.addr 。</li>
<li>环境变量，使用 NAMESRV_ADDR 。</li>
<li>HTTP 端点，例如说：<a href="http://namesrv.rocketmq.xxx.com/">http://namesrv.rocketmq.xxx.com</a> 地址，通过 DNS 解析获得 Namesrv 真正的地址。</li>
</ul>
</li>
<li>请说说你对 Broker 的了解？<ul>
<li>高并发读写服务。Broker的高并发读写主要是依靠以下两点:<ul>
<li>消息顺序写，所有 Topic 数据同时只会写一个文件，一个文件满1G ，再写新文件，真正的顺序写盘，使得发消息 TPS 大幅提高。</li>
<li>消息随机读，RocketMQ 尽可能让读命中系统 Pagecache ，因为操作系统访问 Pagecache 时，即使只访问 1K 的消息，系统也会提前预读出更多的数据，在下次读时就可能命中 Pagecache ，减少 IO 操作。</li>
</ul>
</li>
<li>负载均衡与动态伸缩。<ul>
<li>负载均衡：Broker 上存 Topic 信息，Topic 由多个队列组成，队列会平均分散在多个 Broker 上，而 Producer 的发送机制保证消息尽量平均分布到所有队列中，最终效果就是所有消息都平均落在每个 Broker 上。</li>
<li>动态伸缩能力（非顺序消息）：Broker 的伸缩性体现在两个维度：Topic、Broker。<ul>
<li>Topic 维度：假如一个 Topic 的消息量特别大，但集群水位压力还是很低，就可以扩大该 Topic 的队列数， Topic 的队列数跟发送、消费速度成正比。<ul>
<li>Topic 的队列数一旦扩大，就无法很方便的缩小。因为，生产者和消费者都是基于相同的队列数来处理。如果真的想要缩小，只能新建一个 Topic ，然后使用它。</li>
</ul>
</li>
<li>Broker 维度：如果集群水位很高了，需要扩容，直接加机器部署 Broker 就可以。Broker 启动后向 Namesrv 注册，Producer、Consumer 通过 Namesrv 发现新Broker，立即跟该 Broker 直连，收发消息。<ul>
<li>新增的 Broker 想要下线，想要下线也比较麻烦，暂时没特别好的方案。大体的前提是，消费者消费完该 Broker 的消息，生产者不往这个 Broker 发送消息。</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>高可用 &amp; 高可靠。<ul>
<li>高可用：集群部署时一般都为主备，备机实时从主机同步消息，如果其中一个主机宕机，备机提供消费服务，但不提供写服务。</li>
<li>高可靠：所有发往 Broker 的消息，有同步刷盘和异步刷盘机制。<ul>
<li>同步刷盘时，消息写入物理文件才会返回成功。</li>
<li>异步刷盘时，只有机器宕机，才会产生消息丢失，Broker 挂掉可能会发生，但是机器宕机崩溃是很少发生的，除非突然断电。如果 Broker 挂掉，未同步到硬盘的消息，还在 Pagecache 中呆着。</li>
</ul>
</li>
</ul>
</li>
<li>Broker 与 Namesrv 的心跳机制。<ul>
<li>单个 Broker 跟所有 Namesrv 保持心跳请求，心跳间隔为30秒，心跳请求中包括当前 Broker 所有的 Topic 信息。</li>
<li>Namesrv 会反查 Broker 的心跳信息，如果某个 Broker 在 2 分钟之内都没有心跳，则认为该 Broker 下线，调整 Topic 跟 Broker 的对应关系。但此时 Namesrv 不会主动通知Producer、Consumer 有 Broker 宕机。也就说，只能等 Producer、Consumer 下次定时拉取 Topic 信息的时候，才会发现有 Broker 宕机。</li>
</ul>
</li>
</ul>
</li>
<li>Broker 如何实现消息的存储？<ul>
<li>?????????????????????????</li>
</ul>
</li>
<li>请说说你对 Producer 的了解？<ul>
<li>获得 Topic-Broker 的映射关系。<ul>
<li>Producer 启动时，也需要指定 Namesrv 的地址，从 Namesrv 集群中选一台建立长连接。如果该 Namesrv 宕机，会自动连其他 Namesrv ，直到有可用的 Namesrv 为止。</li>
<li>生产者每 30 秒从 Namesrv 获取 Topic 跟 Broker 的映射关系，更新到本地内存中。然后再跟 Topic 涉及的所有 Broker 建立长连接，每隔 30 秒发一次心跳。</li>
<li>在 Broker 端也会每 10 秒扫描一次当前注册的 Producer ，如果发现某个 Producer 超过 2 分钟都没有发心跳，则断开连接。</li>
</ul>
</li>
<li>生产者端的负载均衡。<ul>
<li>生产者发送时，会自动轮询当前所有可发送的broker，一条消息发送成功，下次换另外一个broker发送，以达到消息平均落到所有的broker上。</li>
<li>假如某个 Broker 宕机，意味生产者最长需要 30 秒才能感知到。在这期间会向宕机的 Broker 发送消息。当一条消息发送到某个 Broker 失败后，会自动再重发 2 次，假如还是发送失败，则抛出发送失败异常。客户端里会自动轮询另外一个 Broker 重新发送，这个对于用户是透明的。</li>
</ul>
</li>
</ul>
</li>
<li>Producer 发送消息有几种方式？<ul>
<li>Producer 发送消息，有三种方式：<ul>
<li>同步方式</li>
<li>异步方式</li>
<li>Oneway 方式</li>
</ul>
</li>
</ul>
</li>
<li>请说说你对 Consumer 的了解？<ul>
<li>获得 Topic-Broker 的映射关系。<ul>
<li>Consumer 启动时需要指定 Namesrv 地址，与其中一个 Namesrv 建立长连接。消费者每隔 30 秒从 Namesrv 获取所有Topic 的最新队列情况，这意味着某个 Broker 如果宕机，客户端最多要 30 秒才能感知。连接建立后，从 Namesrv 中获取当前消费 Topic 所涉及的 Broker，直连 Broker 。</li>
<li>Consumer 跟 Broker 是长连接，会每隔 30 秒发心跳信息到Broker 。Broker 端每 10 秒检查一次当前存活的 Consumer ，若发现某个 Consumer 2 分钟内没有心跳，就断开与该 Consumer 的连接，并且向该消费组的其他实例发送通知，触发该消费者集群的负载均衡。</li>
</ul>
</li>
<li>消费者端的负载均衡。根据消费者的消费模式不同，负载均衡方式也不同。<ul>
<li>集群消费：一个 Topic 可以由同一个消费这分组( Consumer Group )下所有消费者分担消费。<ul>
<li>具体例子：假如 TopicA 有 6 个队列，，每个消费者分组起了 2 个消费者实例，那么每个消费者负责消费 3 个队列。如果再增加一个消费者分组相同消费者实例，即当前共有 3 个消费者同时消费 6 个队列，那每个消费者负责 2 个队列的消费。</li>
</ul>
</li>
<li>广播消费：每个消费者消费 Topic 下的所有队列。</li>
</ul>
</li>
</ul>
</li>
<li>消费者消费模式有几种？<ul>
<li>集群消费：一个 Consumer Group 中的各个 Consumer 实例分摊去消费消息，即一条消息只会投递到一个 Consumer Group 下面的一个实例。<ul>
<li>实际上，每个 Consumer 是平均分摊 Message Queue 的去做拉取消费。例如某个 Topic 有 3 个队列，其中一个 Consumer Group 有 3 个实例（可能是 3 个进程，或者 3 台机器），那么每个实例只消费其中的 1 个队列。</li>
<li>而由 Producer 发送消息的时候是轮询所有的队列，所以消息会平均散落在不同的队列上，可以认为队列上的消息是平均的。那么实例也就平均地消费消息了。</li>
<li>这种模式下，消费进度的存储会持久化到 Broker 。</li>
<li>当新建一个 Consumer Group 时，默认情况下，该分组的消费者会从 min offset 开始重新消费消息。</li>
</ul>
</li>
<li>广播消费：消息将对一 个Consumer Group 下的各个 Consumer 实例都投递一遍。即即使这些 Consumer 属于同一个Consumer Group ，消息也会被 Consumer Group 中的每个 Consumer 都消费一次。<ul>
<li>实际上，是一个消费组下的每个消费者实例都获取到了 Topic 下面的每个 Message Queue 去拉取消费。所以消息会投递到每个消费者实例。</li>
<li>这种模式下，消费进度会存储持久化到实例本地。</li>
</ul>
</li>
</ul>
</li>
<li>消费者获取消息有几种模式？<ul>
<li>PushConsumer推送模式（虽然 RocketMQ 使用的是长轮询）的消费者。消息的能及时被消费。使用非常简单，内部已处理如线程池消费、流控、负载均衡、异常处理等等的各种场景。</li>
<li>PullConsumer拉取模式的消费者。应用主动控制拉取的时机，怎么拉取，怎么消费等。主动权更高。但要自己处理各种场景。</li>
<li>决绝绝大多数场景下，我们只会使用 PushConsumer 推送模式。</li>
</ul>
</li>
<li>如何对消息进行重放？<ul>
<li>消费位点就是一个数字，把 Consumer Offset 改一下，就可以达到重放的目的了。</li>
</ul>
</li>
<li>什么是顺序消息？如何实现？<ul>
<li>消费消息的顺序要同发送消息的顺序一致。由于 Consumer 消费消息的时候是针对 Message Queue 顺序拉取并开始消费，且一条 Message Queue 只会给一个消费者（集群模式下），所以能够保证同一个消费者实例对于 Queue 上消息的消费是顺序地开始消费（不一定顺序消费完成，因为消费可能并行）。</li>
<li>RocketMQ 提供了两种顺序级别：<ul>
<li>顺序消息包括两块：Producer 的顺序发送，和 Consumer 的顺序消费。</li>
<li>普通顺序消息 ：Producer 将相关联的消息发送到相同的消息队列。</li>
<li>严格顺序消息 ：在【普通顺序消息】的基础上，Consumer 严格顺序消费。</li>
</ul>
</li>
</ul>
</li>
<li>顺序消息扩容的过程中，如何在不停写的情况下保证消息顺序？<ul>
<li>成倍扩容，实现扩容前后，同样的 key，hash 到原队列，或者 hash 到新扩容的队列。</li>
<li>扩容前，记录旧队列中的最大位点。</li>
<li>对于每个 Consumer Group ，保证旧队列中的数据消费完，再消费新队列，也即：先对新队列进行禁读即可。</li>
</ul>
</li>
<li>什么是定时消息？如何实现？<ul>
<li>定时消息，是指消息发到 Broker 后，不能立刻被 Consumer 消费，要到特定的时间点或者等待特定的时间后才能被消费。</li>
<li>可通过配置文件，自定义每个延迟级别对应的延迟时间。当然，这是全局的。</li>
<li>如果想要实现任一时刻的延迟消息，比较简单的方式是插入延迟消息到数据库中，然后通过定时任务轮询，到达指定时间，发送到 RocketMQ 中。</li>
<li>实现原理：<ul>
<li>定时消息发送到 Broker 后，会被存储 Topic 为 SCHEDULE_TOPIC_XXXX 中，并且所在 Queue 编号为延迟级别 - 1 。（需要 -1 的原因是，延迟级别是从 1 开始的。如果延迟级别为 0 ，意味着无需延迟。）</li>
<li>Broker 针对每个 SCHEDULE_TOPIC_XXXX 的队列，都创建一个定时任务，顺序扫描到达时间的延迟消息，重新存储到延迟消息原始的 Topic 的原始 Queue 中，这样它就可以被 Consumer 消费到。<ul>
<li>为什么是“顺序扫描到达时间的延迟消息”？因为先进 SCHEDULE_TOPIC_XXXX 的延迟消息，在其所在的队列，意味着先到达延迟时间。</li>
<li>会不会存在重复扫描的情况？每个 SCHEDULE_TOPIC_XXXX 的扫描进度，会每 10s 存储到 config/delayOffset.json 文件中，所以正常情况下，不会存在重复扫描。如果异常关闭，则可能导致重复扫描。</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>什么是消息重试？如何实现？<ul>
<li>消息重试，Consumer 消费消息失败后，要提供一种重试机制，令消息再消费一次。</li>
<li>Consumer 会将消费失败的消息发回 Broker，进入延迟消息队列。即，消费失败的消息，不会立即消费。也就是说，消息重试是构建在定时消息之上的功能。</li>
<li>消息重试的主要流程：<ul>
<li>Consumer 消费失败，将消息发送回 Broker 。</li>
<li>Broker 收到重试消息之后置换 Topic ，存储消息。</li>
<li>Consumer 会拉取该 Topic 对应的 retryTopic 的消息。</li>
<li>Consumer 拉取到 retryTopic 消息之后，置换到原始的 Topic ，把消息交给 Listener 消费。<ul>
<li>Consumer 消息失败后，会将消息的 Topic 修改为 %RETRY% + Topic 进行，添加 “RETRY_TOPIC” 属性为原始 Topic ，然后再返回给 Broker 中。</li>
<li>Broker 收到重试消息之后，会有两次修改消息的 Topic 。<ul>
<li>首先，会将消息的 Topic 修改为 %RETRY% + ConsumerGroup ，因为这个消息是当前消费这分组消费失败，只能被这个消费组所重新消费。注意，消费者会默认订阅 Topic 为 %RETRY% + ConsumerGroup 的消息。</li>
<li>然后，会将消息的 Topic 修改为 SCHEDULE_TOPIC_XXXX ，添加 “REAL_TOPIC” 属性为 %RETRY% + ConsumerGroup ，因为重试消息需要延迟消费。</li>
<li>Consumer 会拉取该 Topic 对应的 retryTopic 的消息，此处的 retryTopic 为 %RETRY% + ConsumerGroup 。<br>Consumer 拉取到 retryTopic 消息之后，置换到原始的 Topic ，因为有消息的 “RETRY_TOPIC” 属性是原始 Topic ，然后把消息交给 Listener 消费。</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>基于RocketMQ的分布式事务解决方案<ul>
<li><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/15850377370094.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></li>
<li>1、在扣款之前，先发送预备消息</li>
<li>2、发送预备消息成功后，执行本地扣款事务</li>
<li>3、扣款成功后，再发送确认消息</li>
<li>4、消息端（加钱业务）可以看到确认消息，消费此消息，进行加钱<blockquote>
<p>注意：上面的确认消息可以为commit消息，可以被订阅者消费；也可以是Rollback消息，即执行本地扣款事务失败后，提交rollback消息，即删除那个预备消息，订阅者无法消费</p>
</blockquote>
</li>
<li>异常1：如果发送预备消息失败，下面的流程不会走下去；这个是正常的</li>
<li>异常2：如果发送预备消息成功，但执行本地事务失败；这个也没有问题，因为此预备消息不会被消费端订阅到，消费端不会执行业务。</li>
<li>异常3：如果发送预备消息成功，执行本地事务成功，但发送确认消息失败；这个就有问题了，因为用户A扣款成功了，但加钱业务没有订阅到确认消息，无法加钱。这里出现了数据不一致。</li>
<li><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/15850378513462.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></li>
<li>RocketMq解决上面的问题，核心思路就是【状态回查】，也就是RocketMq会定时遍历commitlog中的预备消息。<blockquote>
<p>因为预备消息最终肯定会变为commit消息或Rollback消息，所以遍历预备消息去回查本地业务的执行状态，如果发现本地业务没有执行成功就rollBack，如果执行成功就发送commit消息。</p>
</blockquote>
</li>
</ul>
</li>
</ol>
]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>interview</tag>
        <tag>消息队列</tag>
      </tags>
  </entry>
  <entry>
    <title>SpringMVC</title>
    <url>/2021/11/07/SpringMVC/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="Spring-MVC"><a href="#Spring-MVC" class="headerlink" title="Spring-MVC"></a>Spring-MVC</h1><ol>
<li>Spring MVC 框架有什么用？<ul>
<li>Spring Web MVC 框架提供”模型-视图-控制器”( Model-View-Controller )架构和随时可用的组件，用于开发灵活且松散耦合的 Web 应用程序。</li>
<li>MVC 模式有助于分离应用程序的不同方面，如输入逻辑，业务逻辑和 UI 逻辑，同时在所有这些元素之间提供松散耦合。</li>
</ul>
</li>
<li>介绍下 Spring MVC 的核心组件？<ul>
<li>Spring MVC 一共有九大核心组件，分别是：<ul>
<li>MultipartResolver</li>
<li>LocaleResolver</li>
<li>ThemeResolver</li>
<li>HandlerMapping</li>
<li>HandlerAdapter</li>
<li>HandlerExceptionResolver</li>
<li>RequestToViewNameTranslator</li>
<li>ViewResolver</li>
<li>FlashMapManager</li>
</ul>
</li>
</ul>
</li>
<li>描述一下 DispatcherServlet 的工作流程？<br> <img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/15850164969118.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10" alt="-w803"><ul>
<li><ol>
<li>发送请求:用户向服务器发送 HTTP 请求，请求被 Spring MVC 的调度控制器 DispatcherServlet 捕获。</li>
</ol>
</li>
<li><ol start="2">
<li>映射处理器:DispatcherServlet 根据请求 URL ，调用 HandlerMapping 获得该 Handler 配置的所有相关的对象（包括 Handler 对象以及 Handler 对象对应的拦截器），最后以 HandlerExecutionChain 对象的形式返回。</li>
</ol>
</li>
<li><ol start="3">
<li>处理器适配:</li>
<li>DispatcherServlet 根据获得的 Handler，选择一个合适的HandlerAdapter 。（附注：如果成功获得 HandlerAdapter 后，此时将开始执行拦截器的 #preHandler(…) 方法）。</li>
<li>提取请求 Request 中的模型数据，填充 Handler 入参，开始执行Handler（Controller)。 在填充Handler的入参过程中，根据你的配置，Spring 将帮你做一些额外的工作：<ol>
<li>HttpMessageConverter ：会将请求消息（如 JSON、XML 等数据）转换成一个对象。</li>
<li>数据转换：对请求消息进行数据转换。如 String 转换成 Integer、Double 等。</li>
<li>数据格式化：对请求消息进行数据格式化。如将字符串转换成格式化数字或格式化日期等。</li>
<li>数据验证： 验证数据的有效性（长度、格式等），验证结果存储到 BindingResult 或 Error 中。</li>
</ol>
</li>
</ol>
</li>
<li><ol start="4">
<li>Handler(Controller) 执行完成后，向 DispatcherServlet 返回一个 ModelAndView 对象。</li>
</ol>
</li>
<li><ol start="5">
<li>解析视图:根据返回的 ModelAndView ，选择一个适合的 ViewResolver</li>
</ol>
</li>
<li>6 7 渲染视图 + 响应请求</li>
<li><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/15850167871281.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></li>
</ul>
</li>
<li>@Controller 注解有什么用？<ul>
<li>@Controller 注解，它将一个类标记为 Spring Web MVC 控制器 Controller 。</li>
</ul>
</li>
<li>@RestController 和 @Controller 有什么区别？<ul>
<li>@RestController 注解，在 @Controller 基础上，增加了 @ResponseBody 注解，更加适合目前前后端分离的架构下，提供 Restful API ，返回例如 JSON 数据格式。当然，返回什么样的数据格式，根据客户端的 “ACCEPT” 请求头来决定。</li>
</ul>
</li>
<li>@RequestMapping 注解有什么用？<ul>
<li>@RequestMapping 注解，用于将特定 HTTP 请求方法映射到将处理相应请求的控制器中的特定类/方法。此注释可应用于两个级别：<ul>
<li>类级别：映射请求的 URL。</li>
<li>方法级别：映射 URL 以及 HTTP 请求方法。</li>
</ul>
</li>
</ul>
</li>
<li>@RequestMapping 和 @GetMapping 注解的不同之处在哪里？<ul>
<li>@RequestMapping 可注解在类和方法上；@GetMapping 仅可注册在方法上。</li>
<li>@RequestMapping 可进行 GET、POST、PUT、DELETE 等请求方法</li>
<li>@GetMapping 是 @RequestMapping 的 GET 请求方法的特例，目的是为了提高清晰度。</li>
</ul>
</li>
<li>返回 JSON 格式使用什么注解？<ul>
<li>可以使用 @ResponseBody 注解，或者使用包含 @ResponseBody 注解的 @RestController 注解。</li>
<li>当然，还是需要配合相应的支持 JSON 格式化的 HttpMessageConverter 实现类。例如，Spring MVC 默认使用 MappingJackson2HttpMessageConverter 。</li>
</ul>
</li>
<li>介绍一下 WebApplicationContext ？<ul>
<li>WebApplicationContext 是实现ApplicationContext接口的子类，专门为 WEB 应用准备的。</li>
<li>它允许从相对于 Web 根目录的路径中加载配置文件，完成初始化 Spring MVC 组件的工作。</li>
<li>从 WebApplicationContext 中，可以获取 ServletContext 引用，整个 Web 应用上下文对象将作为属性放置在 ServletContext 中，以便 Web 应用环境可以访问 Spring 上下文。</li>
</ul>
</li>
<li>Spring MVC 的异常处理？<ul>
<li>Spring MVC 提供了异常解析器 HandlerExceptionResolver 接口，将处理器( handler )执行时发生的异常，解析( 转换 )成对应的 ModelAndView 结果。代码如下：</li>
<li>一般情况下，我们使用 @ExceptionHandler 注解来实现过异常的处理</li>
</ul>
</li>
<li>Spring MVC 有什么优点？<ul>
<li>使用真的真的真的非常方便，无论是添加 HTTP 请求方法映射的方法，还是不同数据格式的响应。</li>
<li>提供拦截器机制，可以方便的对请求进行拦截处理。</li>
<li>提供异常机制，可以方便的对异常做统一处理。</li>
<li>可以任意使用各种视图技术，而不仅仅局限于 JSP ，例如 Freemarker、Thymeleaf 等等。</li>
<li>不依赖于 Servlet API (目标虽是如此，但是在实现的时候确实是依赖于 Servlet 的，当然仅仅依赖 Servlet ，而不依赖 Filter、Listener )。</li>
</ul>
</li>
<li>Spring MVC 怎样设定重定向和转发 ？<ul>
<li>结果转发：在返回值的前面加 “forward:/“ 。</li>
<li>重定向：在返回值的前面加上 “redirect:/“ 。</li>
</ul>
</li>
<li>Spring MVC 的 Controller 是不是单例？<ul>
<li>绝绝绝大多数情况下，Controller 是单例。</li>
<li>那么，Controller 里一般不建议存在共享的变量。</li>
</ul>
</li>
<li>Spring MVC 和 Struts2 的异同？<ul>
<li>入口不同<ul>
<li>Spring MVC 的入门是一个 Servlet 控制器。</li>
<li>Struts2 入门是一个 Filter 过滤器。</li>
</ul>
</li>
<li>配置映射不同，<ul>
<li>Spring MVC 是基于方法开发，传递参数是通过方法形参，一般设置为单例。</li>
<li>Struts2 是基于类开发，传递参数是通过类的属性，只能设计为多例。</li>
</ul>
</li>
</ul>
</li>
<li>详细介绍下 Spring MVC 拦截器？<ul>
<li><code>org.springframework.web.servlet.HandlerInterceptor</code> ，拦截器接口。代码如下：  <figure class="highlight java"><table><tr><td class="code"><pre><span class="line">        <span class="comment">// HandlerInterceptor.java</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * 拦截处理器，在 &#123;<span class="doctag">@link</span> HandlerAdapter#handle(HttpServletRequest, HttpServletResponse, Object)&#125; 执行之前</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        <span class="function"><span class="keyword">default</span> <span class="keyword">boolean</span> <span class="title">preHandle</span><span class="params">(HttpServletRequest request, HttpServletResponse response, Object handler)</span></span></span><br><span class="line"><span class="function">        		<span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        	<span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * 拦截处理器，在 &#123;<span class="doctag">@link</span> HandlerAdapter#handle(HttpServletRequest, HttpServletResponse, Object)&#125; 执行成功之后</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        <span class="function"><span class="keyword">default</span> <span class="keyword">void</span> <span class="title">postHandle</span><span class="params">(HttpServletRequest request, HttpServletResponse response, Object handler,</span></span></span><br><span class="line"><span class="params"><span class="function">        		<span class="meta">@Nullable</span> ModelAndView modelAndView)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * 拦截处理器，在 &#123;<span class="doctag">@link</span> HandlerAdapter#handle(HttpServletRequest, HttpServletResponse, Object)&#125; 执行完之后，无论成功还是失败</span></span><br><span class="line"><span class="comment">         *</span></span><br><span class="line"><span class="comment">         * 并且，只有该处理器 &#123;<span class="doctag">@link</span> #preHandle(HttpServletRequest, HttpServletResponse, Object)&#125; 执行成功之后，才会被执行</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        <span class="function"><span class="keyword">default</span> <span class="keyword">void</span> <span class="title">afterCompletion</span><span class="params">(HttpServletRequest request, HttpServletResponse response, Object handler,</span></span></span><br><span class="line"><span class="params"><span class="function">        		<span class="meta">@Nullable</span> Exception ex)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>preHandle(…) 方法，调用 Controller 方法之前执行。<ul>
<li>preHandle(…) 方法，按拦截器定义顺序调用。若任一拦截器返回 false ，则 Controller 方法不再调用。    </li>
</ul>
</li>
<li>postHandle(…) 方法，调用 Controller 方法之后执行。<ul>
<li>postHandle(…) 和 #afterCompletion(…) 方法，按拦截器定义逆序调用。</li>
<li>postHandler(…) 方法，在调用 Controller 方法之后执行。</li>
</ul>
</li>
<li>afterCompletion(…) 方法，处理完 Controller 方法返回结果之后执行,无论调用 Controller 方法是否成功，都会执行。<ul>
<li>afterCompletion(…) 方法，只有该拦截器在 #preHandle(…) 方法返回 true 时，才能够被调用，且一定会被调用。为什么“且一定会被调用”呢？即使 #afterCompletion(…) 方法，按拦截器定义逆序调用时，前面的拦截器发生异常，后面的拦截器还能够调用，即无视异常。</li>
</ul>
</li>
</ul>
</li>
<li>Spring MVC 的拦截器可以做哪些事情？<ul>
<li>记录访问日志。</li>
<li>记录异常日志。</li>
<li>需要登陆的请求操作，拦截未登陆的用户。</li>
<li>参数替换</li>
</ul>
</li>
<li>Spring MVC 的拦截器和 Filter 过滤器有什么差别？<ul>
<li>功能相同：拦截器和 Filter都能实现相应的功能，谁也不比谁强。</li>
<li>容器不同：拦截器构建在 Spring MVC 体系中；Filter 构建在 Servlet 容器之上。</li>
<li>使用便利性不同：拦截器提供了三个方法，分别在不同的时机执行；过滤器仅提供一个方法，当然也能实现拦截器的执行时机的效果，就是麻烦一些。</li>
</ul>
</li>
</ol>
<h2 id="REST"><a href="#REST" class="headerlink" title="REST"></a>REST</h2><ol>
<li>REST 代表着什么?<ul>
<li>REST 代表着抽象状态转移，它是根据 HTTP 协议从客户端发送数据到服务端，例如：服务端的一本书可以以 XML 或 JSON 格式传递到客户端。</li>
</ul>
</li>
<li>资源是什么?<ul>
<li>资源是指数据在 REST 架构中如何显示的。将实体作为资源公开 ，它允许客户端通过 HTTP 方法如：GET, POST,PUT, DELETE 等读，写，修改和创建资源。</li>
</ul>
</li>
<li>什么是安全的 REST 操作?<ul>
<li>REST 接口是通过 HTTP 方法完成操作。<ul>
<li>一些HTTP操作是安全的，如 GET 和 HEAD ，它不能在服务端修改资源换句话说，PUT,POST 和 DELETE 是不安全的，因为他们能修改服务端的资源。</li>
</ul>
</li>
<li>是否安全的界限，在于是否修改服务端的资源。</li>
</ul>
</li>
<li>什么是幂等操作? 为什么幂等操作如此重要?<ul>
<li>有一些HTTP方法，如：GET，不管你使用多少次它都能产生相同的结果，在没有任何一边影响的情况下，发送多个 GET 请求到相同的URI 将会产生相同的响应结果。因此，这就是所谓幂等操作。</li>
<li>换句话说，POST方法不是幂等操作 ，因为如果发送多个 POST 请求，它将在服务端创建不同的资源。但是，假如你用PUT更新资源，它将是幂等操作。</li>
</ul>
</li>
<li>REST 是可扩展的或说是协同的吗?<ul>
<li>是的，REST 是可扩展的和可协作的。它既不托管一种特定的技术选择，也不定在客户端或者服务端。你可以用 Java, C++, Python, 或 JavaScript 来创建 RESTful Web 服务，也可以在客户端使用它们。</li>
<li>这里的“可拓展”、“协同”对应到我们平时常说的，“跨语言”、“语言无关”。</li>
</ul>
</li>
<li>REST 用哪种 HTTP 方法呢?<ul>
<li>REST 能用任何的 HTTP 方法，但是，最受欢迎的是：<ul>
<li>用 GET 来检索服务端资源</li>
<li>用 POST 来创建服务端资源</li>
<li>用 PUT 来更新服务端资源</li>
<li>用 DELETE 来删除服务端资源。</li>
</ul>
</li>
</ul>
</li>
<li>删除的 HTTP 状态返回码是什么 ?<ul>
<li>在删除成功之后，您的 REST API 应该返回什么状态代码，并没有严格的规则。它可以返回 200 或 204 没有内容。<ul>
<li>一般来说，如果删除操作成功，响应主体为空，返回 204 。</li>
<li>如果删除请求成功且响应体不是空的，则返回 200 。</li>
</ul>
</li>
</ul>
</li>
<li>REST API 是无状态的吗?<ul>
<li>是的，REST API 应该是无状态的，因为它是基于 HTTP 的，它也是无状态的。</li>
<li>REST API 中的请求应该包含处理它所需的所有细节。它不应该依赖于以前或下一个请求或服务器端维护的一些数据，例如会话。</li>
</ul>
</li>
<li>REST安全吗? 你能做什么来保护它?<ul>
<li>REST 通常不是安全的，但是您可以通过使用 Spring Security 来保护它。</li>
<li>至少，你可以通过在 Spring Security 配置文件中使用 HTTP 来启用 HTTP Basic Auth 基本认证。</li>
<li>类似地，如果底层服务器支持 HTTPS ，你可以使用 HTTPS 公开 REST API 。</li>
</ul>
</li>
<li>RestTemplate 的优势是什么?<ul>
<li>在 Spring Framework 中，RestTemplate 类是 模板方法模式 的实现。跟其他主流的模板类相似，如 JdbcTemplate 或 JmsTempalte ，它将在客户端简化跟 RESTful Web 服务的集成。正如在 RestTemplate 例子中显示的一样，你能非常容易地用它来调用 RESTful Web 服务。</li>
<li>实际场景我还是更喜欢使用 OkHttp 作为 HTTP 库，因为更好的性能，使用也便捷，并且无需依赖 Spring 库。</li>
</ul>
</li>
<li>HttpMessageConverter 在 Spring REST 中代表什么?<ul>
<li>HttpMessageConverter 是一种策略接口 ，它指定了一个转换器，它可以转换 HTTP 请求和响应。Spring REST 用这个接口转换 HTTP 响应到多种格式，例如：JSON 或 XML 。</li>
<li>每个 HttpMessageConverter 实现都有一种或几种相关联的MIME协议。Spring 使用 “Accept” 的标头来确定客户端所期待的内容类型。</li>
<li>然后，它将尝试找到一个注册的 HTTPMessageConverter ，它能够处理特定的内容类型，并使用它将响应转换成这种格式，然后再将其发送给客户端。</li>
</ul>
</li>
<li>@PathVariable 注解，在 Spring MVC 做了什么? 为什么 REST 在 Spring 中如此有用<ul>
<li>@PathVariable 注解，是 Spring MVC 中有用的注解之一，它允许您从 URI 读取值，比如查询参数。它在使用 Spring 创建 RESTful Web 服务时特别有用，因为在 REST 中，资源标识符是 URI 的一部分。</li>
</ul>
</li>
</ol>
]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>interview</tag>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title>MyBatis</title>
    <url>/2021/11/07/MyBatis/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="MyBatis"><a href="#MyBatis" class="headerlink" title="MyBatis"></a>MyBatis</h1><ol>
<li><p>MyBatis 编程步骤</p>
<ul>
<li>创建 SqlSessionFactory 对象。</li>
<li>通过 SqlSessionFactory 获取 SqlSession 对象。</li>
<li>通过 SqlSession 获得 Mapper 代理对象。</li>
<li>通过 Mapper 代理对象，执行数据库操作。</li>
<li>执行成功，则使用 SqlSession 提交事务。</li>
<li>执行失败，则使用 SqlSession 回滚事务。</li>
<li>最终，关闭会话。</li>
</ul>
</li>
<li><p>#{} 和 ${} 的区别是什么？</p>
<ul>
<li>${} 是 Properties 文件中的变量占位符，它可以用于 XML 标签属性值和 SQL 内部，属于字符串替换,#{} 是预编译处理，可以有效防止 SQL 注入，提高系统安全性</li>
<li>${} 也可以对传递进来的参数原样拼接在 SQL 中,可能有 SQL 注入的风险。</li>
</ul>
</li>
<li><p>当实体类中的属性名和表中的字段名不一样 ，怎么办？</p>
<ul>
<li>在查询的 SQL 语句中定义字段名的别名</li>
<li>配置自动的下划线转驼峰的功能</li>
<li>通过 <resultMap> 来映射字段名和实体类属性名的一一对应的关系</li>
</ul>
</li>
<li><p>Mybatis 动态 SQL 是做什么的？都有哪些动态 SQL ？能简述一下动态 SQL 的执行原理吗？</p>
<ul>
<li>Mybatis 动态 SQL ，可以让我们在 XML 映射文件内，以 XML 标签的形式编写动态 SQL ，完成逻辑判断和动态拼接 SQL 的功能。</li>
<li>Mybatis 提供了 9 种动态 SQL 标签：<code>&lt;if /&gt;、&lt;choose /&gt;、&lt;when /&gt;、&lt;otherwise /&gt;、&lt;trim /&gt;、&lt;where /&gt;、&lt;set /&gt;、&lt;foreach /&gt;、&lt;bind /&gt; </code>。</li>
<li>其执行原理为，使用 OGNL 的表达式，从 SQL 参数对象中计算表达式的值，根据表达式的值动态拼接 SQL ，以此来完成动态 SQL 的功能。</li>
</ul>
</li>
<li><p>最佳实践中，通常一个 XML 映射文件，都会写一个 Mapper 接口与之对应。请问，这个 Mapper 接口的工作原理是什么？Mapper 接口里的方法，参数不同时，方法能重载吗？</p>
<ul>
<li>接口的全限名，就是映射文件中的 “namespace” 的值。</li>
<li>接口的方法名，就是映射文件中 MappedStatement 的 “id” 值。</li>
<li>接口方法内的参数，就是传递给 SQL 的参数。</li>
<li>另外，Mapper 接口的实现类，通过 MyBatis 使用 JDK Proxy 自动生成其代理对象 Proxy ，而代理对象 Proxy 会拦截接口方法，从而“调用”对应的 MappedStatement 方法，最终执行 SQL ，返回执行结果。整体流程如下图：<ul>
<li><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/15850110187997.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></li>
</ul>
</li>
<li>其中，SqlSession 在调用 Executor 之前，会获得对应的 MappedStatement 方法。例如：DefaultSqlSession#select(String statement, Object parameter, RowBounds rowBounds, ResultHandler handler) 方法，代码如下：  <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// DefaultSqlSession.java</span></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">select</span><span class="params">(String statement, Object parameter, RowBounds rowBounds, ResultHandler handler)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="comment">// 获得 MappedStatement 对象</span></span><br><span class="line">        MappedStatement ms = configuration.getMappedStatement(statement);</span><br><span class="line">        <span class="comment">// 执行查询</span></span><br><span class="line">        executor.query(ms, wrapCollection(parameter), rowBounds, handler);</span><br><span class="line">    &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">        <span class="keyword">throw</span> ExceptionFactory.wrapException(<span class="string">&quot;Error querying database.  Cause: &quot;</span> + e, e);</span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">        ErrorContext.instance().reset();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>Mapper 接口绑定有几种实现方式,分别是怎么实现的?</p>
<ul>
<li>通过 XML Mapper 里面写 SQL 来绑定。在这种情况下，要指定 XML 映射文件里面的 “namespace” 必须为接口的全路径名。</li>
<li>通过注解绑定，就是在接口的方法上面加上 @Select、@Update、@Insert、@Delete 注解，里面包含 SQL 语句来绑定。</li>
<li>是第二种的特例，也是通过注解绑定，在接口的方法上面加上 @SelectProvider、@UpdateProvider、@InsertProvider、@DeleteProvider 注解，通过 Java 代码，生成对应的动态 SQL </li>
</ul>
</li>
<li><p>Mybatis 的 XML Mapper文件中，不同的 XML 映射文件，id 是否可以重复？</p>
<ul>
<li>不同的 XML Mapper 文件，如果配置了 “namespace” ，那么 id 可以重复；如果没有配置 “namespace” ，那么 id 不能重复。毕竟”namespace” 不是必须的，只是最佳实践而已。</li>
<li>原因就是，namespace + id 是作为 Map&lt;String, MappedStatement&gt; 的 key 使用的。如果没有 “namespace”，就剩下 id ，那么 id 重复会导致数据互相覆盖。如果有了 “namespace”，自然 id 就可以重复，”namespace”不同，namespace + id 自然也就不同。</li>
</ul>
</li>
<li><p>如何获取自动生成的(主)键值?</p>
<ul>
<li>Mysql 自增主键<ul>
<li>使用 useGeneratedKeys + keyProperty 属性  <figure class="highlight xml"><table><tr><td class="code"><pre><span class="line">    <span class="tag">&lt;<span class="name">insert</span> <span class="attr">id</span>=<span class="string">&quot;insert&quot;</span> <span class="attr">parameterType</span>=<span class="string">&quot;Person&quot;</span> <span class="attr">useGeneratedKeys</span>=<span class="string">&quot;true&quot;</span> <span class="attr">keyProperty</span>=<span class="string">&quot;id&quot;</span>&gt;</span></span><br><span class="line">        INSERT INTO person(name, pswd)</span><br><span class="line">        VALUE (#&#123;name&#125;, #&#123;pswd&#125;)</span><br><span class="line">    <span class="tag">&lt;/<span class="name">insert</span>&gt;</span></span><br><span class="line">    ```    </span><br><span class="line">- 使用 `<span class="tag">&lt;<span class="name">selectKey</span> /&gt;</span>` 标签</span><br><span class="line"></span><br><span class="line">    ```xml</span><br><span class="line">        <span class="tag">&lt;<span class="name">insert</span> <span class="attr">id</span>=<span class="string">&quot;insert&quot;</span> <span class="attr">parameterType</span>=<span class="string">&quot;Person&quot;</span> <span class="attr">useGeneratedKeys</span>=<span class="string">&quot;true&quot;</span> <span class="attr">keyProperty</span>=<span class="string">&quot;id&quot;</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">selectKey</span> <span class="attr">keyProperty</span>=<span class="string">&quot;id&quot;</span> <span class="attr">resultType</span>=<span class="string">&quot;long&quot;</span> <span class="attr">order</span>=<span class="string">&quot;AFTER&quot;</span>&gt;</span></span><br><span class="line">            SELECT LAST_INSERT_ID()</span><br><span class="line">        <span class="tag">&lt;/<span class="name">selectKey</span>&gt;</span> </span><br><span class="line">            INSERT INTO person(name, pswd)</span><br><span class="line">            VALUE (#&#123;name&#125;, #&#123;pswd&#125;)</span><br><span class="line">        <span class="tag">&lt;/<span class="name">insert</span>&gt;</span>            </span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>Oracle序列自增  <figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">insert</span> <span class="attr">id</span>=<span class="string">&quot;add&quot;</span> <span class="attr">parameterType</span>=<span class="string">&quot;Student&quot;</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">selectKey</span> <span class="attr">keyProperty</span>=<span class="string">&quot;student_id&quot;</span> <span class="attr">resultType</span>=<span class="string">&quot;int&quot;</span> <span class="attr">order</span>=<span class="string">&quot;BEFORE&quot;</span>&gt;</span></span><br><span class="line">        select student_sequence.nextval FROM dual</span><br><span class="line">    <span class="tag">&lt;/<span class="name">selectKey</span>&gt;</span></span><br><span class="line">    INSERT INTO student(student_id, student_name, student_age)</span><br><span class="line">    VALUES (#&#123;student_id&#125;,#&#123;student_name&#125;,#&#123;student_age&#125;)</span><br><span class="line"><span class="tag">&lt;/<span class="name">insert</span>&gt;</span></span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>Mybatis 执行批量插入，能返回数据库主键列表吗？</p>
<ul>
<li>能，JDBC 都能做，Mybatis 当然也能做。</li>
</ul>
</li>
<li><p>MyBatis 如何执行批量插入?</p>
<ul>
<li>单条插傻循环</li>
<li>拼接insert into values (),(),();</li>
<li>使用BatchExecutor批处理</li>
</ul>
</li>
<li><p>在 Mapper 中如何传递多个参数?</p>
<ul>
<li>使用 Map 集合，装载多个参数进行传递</li>
<li>保持传递多个参数，使用 @Param 注解</li>
<li>保持传递多个参数，不使用 @Param 注解，#{param2}</li>
<li>使用对象定义VO，装载多个参数进行传递</li>
</ul>
</li>
<li><p>Mybatis 是否可以映射 Enum 枚举类？</p>
<ul>
<li>Mybatis 可以映射枚举类，对应的实现类为 EnumTypeHandler 或 EnumOrdinalTypeHandler 。<ul>
<li>EnumTypeHandler ，基于 Enum.name 属性( String )。默认。</li>
<li>EnumOrdinalTypeHandler ，基于 Enum.ordinal 属性( int )。可通过 <setting name="defaultEnumTypeHandler" value="EnumOrdinalTypeHandler" /> 来设置。</li>
</ul>
</li>
</ul>
</li>
<li><p>Mybatis 都有哪些 Executor 执行器？它们之间的区别是什么？</p>
<ul>
<li>Mybatis 有四种 Executor 执行器，分别是 SimpleExecutor、ReuseExecutor、BatchExecutor、CachingExecutor 。<ul>
<li>SimpleExecutor ：每执行一次 update 或 select 操作，就创建一个 Statement 对象，用完立刻关闭 Statement 对象。</li>
<li>ReuseExecutor ：执行 update 或 select 操作，以 SQL 作为key 查找缓存的 Statement 对象，存在就使用，不存在就创建；用完后，不关闭 Statement 对象，而是放置于缓存 Map&lt;String, Statement&gt; 内，供下一次使用。简言之，就是重复使用 Statement 对象。</li>
<li>BatchExecutor ：执行 update 操作（没有 select 操作，因为 JDBC 批处理不支持 select 操作），将所有 SQL 都添加到批处理中（通过 addBatch 方法），等待统一执行（使用 executeBatch 方法）。它缓存了多个 Statement 对象，每个 Statement 对象都是调用 addBatch 方法完毕后，等待一次执行 executeBatch 批处理。实际上，整个过程与 JDBC 批处理是相同。</li>
<li>CachingExecutor ：在上述的三个执行器之上，增加二级缓存的功能。</li>
</ul>
</li>
</ul>
</li>
<li><p>介绍 MyBatis 的一级缓存和二级缓存的概念和实现原理？</p>
<ul>
<li><p>一级缓存：</p>
<ul>
<li><p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/15850119544106.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
</li>
<li><p>每个SqlSession中持有了Executor，每个Executor中有一个LocalCache。当用户发起查询时，MyBatis根据当前执行的语句生成MappedStatement，在Local Cache进行查询，如果缓存命中的话，直接返回结果给用户，如果缓存没有命中的话，查询数据库，结果写入Local Cache，最后返回结果给用户</p>
</li>
</ul>
</li>
<li><p>二级缓存</p>
<ul>
<li><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/15850120537632.png?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></li>
<li>二级缓存开启后，同一个namespace下的所有操作语句，都影响着同一个Cache，即二级缓存被多个SqlSession共享，是一个全局的变量。</li>
<li>当开启缓存后，数据的查询执行的流程就是 二级缓存 -&gt; 一级缓存 -&gt; 数据库。</li>
</ul>
</li>
</ul>
</li>
<li><p>Mybatis 是否支持延迟加载？如果支持，它的实现原理是什么？</p>
<ul>
<li>Mybatis 仅支持 association 关联对象和 collection 关联集合对象的延迟加载。其中，association 指的就是一对一，collection 指的就是一对多查询。</li>
<li>在 Mybatis 配置文件中，可以配置 <setting name="lazyLoadingEnabled" value="true" /> 来启用延迟加载的功能。默认情况下，延迟加载的功能是关闭的。</li>
<li>它的原理是，使用 CGLIB 或 Javassist( 默认 ) 创建目标对象的代理对象。当调用代理对象的延迟加载属性的 getting 方法时，进入拦截器方法。比如调用 a.getB().getName() 方法，进入拦截器的 invoke(…) 方法，发现 a.getB() 需要延迟加载时，那么就会单独发送事先保存好的查询关联 B 对象的 SQL ，把 B 查询上来，然后调用a.setB(b) 方法，于是 a 对象 b 属性就有值了，接着完成a.getB().getName() 方法的调用。这就是延迟加载的基本原理。</li>
</ul>
</li>
<li><p>简述 Mybatis 的插件运行原理？以及如何编写一个插件？</p>
<ul>
<li>Mybatis 仅可以编写针对 ParameterHandler、ResultSetHandler、StatementHandler、Executor 这 4 种接口的插件。</li>
<li>Mybatis 使用 JDK 的动态代理，为需要拦截的接口生成代理对象以实现接口方法拦截功能，每当执行这 4 种接口对象的方法时，就会进入拦截方法，具体就是 InvocationHandler 的 #invoke(…)方法。当然，只会拦截那些你指定需要拦截的方法。</li>
</ul>
</li>
<li><p>Mybatis 是如何进行分页的？分页插件的原理是什么？</p>
<ul>
<li>Mybatis 使用 RowBounds 对象进行分页，它是针对 ResultSet 结果集执行的内存分页，而非数据库分页</li>
<li>分页插件的基本原理是使用 Mybatis 提供的插件接口，实现自定义分页插件。在插件的拦截方法内，拦截待执行的 SQL ，然后重写 SQL ，根据dialect 方言，添加对应的物理分页语句和物理分页参数。</li>
<li>目前使用比较广泛的 MyBatis 分页插件有：Mybatis-PageHelper、MyBatis-Plus</li>
</ul>
</li>
<li><p>MyBatis 与 Hibernate 有哪些不同？</p>
<ul>
<li>Mybatis 学习门槛低，简单易学，程序员直接编写原生态 SQL ，可严格控制 SQL 执行性能，灵活度高。但是灵活的前提是 MyBatis 无法做到数据库无关性，如果需要实现支持多种数据库的软件则需要自定义多套 SQL 映射文件，工作量大</li>
<li>Hibernate 对象/关系映射能力强，数据库无关性好。如果用 Hibernate 开发可以节省很多代码，提高效率。但是 Hibernate 的缺点是学习门槛高，要精通门槛更高，而且怎么设计 O/R 映射，在性能和对象模型之间如何权衡，以及怎样用好 Hibernate 需要具有很强的经验和能力才行</li>
</ul>
</li>
<li><p>JDBC 编程有哪些不足之处，MyBatis是如何解决这些问题的？</p>
<ul>
<li>问题一：SQL 语句写在代码中造成代码不易维护，且代码会比较混乱。<ul>
<li>解决方式：将 SQL 语句配置在 Mapper XML 文件中，与 Java 代码分离。</li>
</ul>
</li>
<li>问题二：根据参数不同，拼接不同的 SQL 语句非常麻烦。例如 SQL 语句的 WHERE 条件不一定，可能多也可能少，占位符需要和参数一一对应。<ul>
<li>解决方式：MyBatis 提供 <where />、<if /> 等等动态语句所需要的标签，并支持 OGNL 表达式，简化了动态 SQL 拼接的代码，提升了开发效率。</li>
</ul>
</li>
<li>问题三，对结果集解析麻烦，SQL 变化可能导致解析代码变化，且解析前需要遍历。<ul>
<li>解决方式：Mybatis 自动将 SQL 执行结果映射成 Java 对象。</li>
</ul>
</li>
</ul>
</li>
<li><p>Mybatis 映射文件中，如果 A 标签通过 include 引用了B标签的内容，请问，B 标签能否定义在 A 标签的后面，还是说必须定义在A标签的前面？</p>
<ul>
<li>虽然 Mybatis 解析 XML 映射文件是按照顺序解析的。但是，被引用的 B 标签依然可以定义在任何地方，Mybatis 都可以正确识别。也就是说，无需按照顺序，进行定义。</li>
<li>原理是，Mybatis 解析 A 标签，发现 A 标签引用了 B 标签，但是 B 标签尚未解析到，尚不存在，此时，Mybatis 会将 A 标签标记为未解析状态。然后，继续解析余下的标签，包含 B 标签，待所有标签解析完毕，Mybatis 会重新解析那些被标记为未解析的标签，此时再解析A标签时，B 标签已经存在，A 标签也就可以正常解析完成了。</li>
</ul>
</li>
<li><p>简述 Mybatis 的 XML 映射文件和 Mybatis 内部数据结构之间的映射关系？</p>
<ul>
<li>Mybatis 将所有 XML 配置信息都封装到 All-In-One 重量级对象Configuration内部。</li>
<li>在 XML Mapper 文件中：<ul>
<li><parameterMap> 标签，会被解析为 ParameterMap 对象，其每个子元素会被解析为 ParameterMapping 对象。</li>
<li><resultMap> 标签，会被解析为 ResultMap 对象，其每个子元素会被解析为 ResultMapping 对象。</li>
<li>每一个 <code>&lt;select&gt;、&lt;insert&gt;、&lt;update&gt;、&lt;delete&gt;</code> 标签，均会被解析为一个 MappedStatement 对象，标签内的 SQL 会被解析为一个 BoundSql 对象。</li>
</ul>
</li>
</ul>
</li>
</ol>
]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>interview</tag>
        <tag>ORM</tag>
      </tags>
  </entry>
  <entry>
    <title>Nginx</title>
    <url>/2021/11/07/Nginx/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="Nginx"><a href="#Nginx" class="headerlink" title="Nginx"></a>Nginx</h1><ol>
<li><p>请解释一下什么是 Nginx ？</p>
<ul>
<li>Nginx ，是一个 Web 服务器和反向代理服务器，用于 HTTP、HTTPS、SMTP、POP3 和 IMAP 协议。</li>
<li>作为 http server</li>
<li>反向代理服务器</li>
<li>正向代理</li>
<li>实现负载均衡</li>
</ul>
</li>
<li><p>Nginx 常用命令？</p>
<ul>
<li>启动 nginx 。</li>
<li>停止 nginx -s stop 或 nginx -s quit 。</li>
<li>重载配置 ./sbin/nginx -s reload(平滑重启) 或 service nginx reload 。</li>
<li>重载指定配置文件 .nginx -c /usr/local/nginx/conf/nginx.conf 。</li>
<li>查看 nginx 版本 nginx -v 。</li>
<li>检查配置文件是否正确 nginx -t 。</li>
<li>显示帮助信息 nginx -h 。</li>
</ul>
</li>
<li><p>Nginx 常用配置？</p>
 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">worker_processes  8; # 工作进程个数</span><br><span class="line">worker_connections  65535; # 每个工作进程能并发处理（发起）的最大连接数（包含所有连接数）</span><br><span class="line"># 错误日志打印地址</span><br><span class="line">error_log         /data/logs/nginx/error.log;</span><br><span class="line"># 访问日志打印地址 </span><br><span class="line">access_log      /data/logs/nginx/access.log; </span><br><span class="line"># 日志格式</span><br><span class="line">log_format  main  &#x27;$remote_addr&quot;$request&quot; &#x27;&#x27;$status $upstream_addr &quot;$request_time&quot;&#x27;; </span><br><span class="line">listen       80; # 监听端口</span><br><span class="line"># 允许域名</span><br><span class="line">server_name  rrc.test.jiedaibao.com; </span><br><span class="line"># 项目根目录</span><br><span class="line">root  /data/release/rrc/web; </span><br><span class="line"># 访问根文件</span><br><span class="line">index  index.php index.html index.htm; </span><br></pre></td></tr></table></figure></li>
<li><p>Nginx 日志格式中的<code>$time_local</code>表示的是什么时间？请求开始的时间？请求结束的时间？其次，当我们从前到后观察日志中的 $time_local 时间时，有时候会发现时间顺序前后错乱的现象，请说明原因？</p>
<ul>
<li><code>$time_local</code> ：在服务器里请求开始写入本地的时间。</li>
<li>因为请求发生时间有前有后，所以会时间顺序前后错乱。</li>
</ul>
</li>
<li><p>Nginx 有哪些优点？</p>
<ul>
<li>跨平台、配置简单。</li>
<li>非阻塞、高并发连接</li>
<li>内存消耗小</li>
<li>成本低廉，且开源。</li>
<li>稳定性高，宕机的概率非常小。</li>
</ul>
</li>
<li><p>使用“反向代理服务器”的优点是什么？</p>
<ul>
<li>反向代理服务器可以隐藏源服务器的存在和特征。它充当互联网云和 Web 服务器之间的中间层。这对于安全方面来说是很好的，特别是当我们使用 Web 托管服务时。</li>
</ul>
</li>
<li><p>什么是正向代理？</p>
<ul>
<li>一个位于客户端和原始服务器(origin server)之间的服务器，为了从原始服务器取得内容，客户端向代理发送一个请求并指定目标(原始服务器)，然后代理向原始服务器转交请求并将获得的内容返回给客户端。</li>
<li>客户端才能使用正向代理。</li>
<li>正向代理总结就一句话：代理端代理的是客户端</li>
</ul>
</li>
<li><p>什么是反向代理？</p>
<ul>
<li>反向代理（Reverse Proxy）方式，是指以代理服务器来接受 Internet上的连接请求，然后将请求，发给内部网络上的服务器并将从服务器上得到的结果返回给 Internet 上请求连接的客户端，此时代理服务器对外就表现为一个反向代理服务器。</li>
<li>反向代理总结就一句话：代理端代理的是服务端。</li>
</ul>
</li>
<li><p>LVS、Nginx、HAproxy 有什么区别？</p>
<ul>
<li>LVS ：是基于四层的转发。</li>
<li>HAproxy ： 是基于四层和七层的转发，是专业的代理服务器。</li>
<li>Nginx ：是 WEB 服务器，缓存服务器，又是反向代理服务器，可以做七层的转发。 <ul>
<li>Nginx 引入 TCP 插件之后，也可以支持四层的转发。</li>
</ul>
</li>
</ul>
</li>
<li><p>请解释 Nginx 如何处理 HTTP 请求？</p>
<ul>
<li>首先，Nginx 在启动时，会解析配置文件，得到需要监听的端口与 IP 地址，然后在 Nginx 的 Master 进程里面先初始化好这个监控的Socket(创建 S ocket，设置 addr、reuse 等选项，绑定到指定的 ip 地址端口，再 listen 监听)。</li>
<li>然后，再 fork(一个现有进程可以调用 fork 函数创建一个新进程。由 fork 创建的新进程被称为子进程 )出多个子进程出来。</li>
<li>之后，子进程会竞争 accept 新的连接。此时，客户端就可以向 nginx 发起连接了。当客户端与nginx进行三次握手，与 nginx 建立好一个连接后。此时，某一个子进程会 accept 成功，得到这个建立好的连接的 Socket ，然后创建 nginx 对连接的封装，即 ngx_connection_t 结构体。</li>
<li>接着，设置读写事件处理函数，并添加读写事件来与客户端进行数据的交换。</li>
<li>Nginx 或客户端来主动关掉连接，到此，一个连接就寿终正寝了。</li>
</ul>
</li>
<li><p>什么是动态资源、静态资源分离？</p>
<ul>
<li>动态资源、静态资源分离，是让动态网站里的动态网页根据一定规则把不变的资源和经常变的资源区分开来，动静资源做好了拆分以后我们就可以根据静态资源的特点将其做缓存操作，这就是网站静态化处理的核心思路。</li>
<li>动态资源、静态资源分离简单的概括是：动态文件与静态文件的分离。</li>
</ul>
</li>
<li><p>为什么要做动、静分离？</p>
<ul>
<li>在我们的软件开发中，有些请求是需要后台处理的（如：.jsp,.do 等等），有些请求是不需要经过后台处理的（如：css、html、jpg、js 等等文件），这些不需要经过后台处理的文件称为静态文件，否则动态文件</li>
<li>因此我们后台处理忽略静态文件。这会有人又说那我后台忽略静态文件不就完了吗？当然这是可以的，但是这样后台的请求次数就明显增多了。在我们对资源的响应速度有要求的时候，我们应该使用这种动静分离的策略去解决动、静分离将网站静态资源（HTML，JavaScript，CSS，img等文件）与后台应用分开部署，提高用户访问静态代码的速度，降低对后台应用访问</li>
<li>这里我们将静态资源放到 Nginx 中，动态资源转发到 Tomcat 服务器中去。 </li>
<li>因为现在七牛、阿里云等 CDN 服务已经很成熟，主流的做法，是把静态资源缓存到 CDN 服务中，从而提升访问速度。<ul>
<li>相比本地的 Nginx 来说，CDN 服务器由于在国内有更多的节点，可以实现用户的就近访问。</li>
<li>并且，CDN 服务可以提供更大的带宽，不像我们自己的应用服务，提供的带宽是有限的。</li>
</ul>
</li>
</ul>
</li>
<li><p>什么叫 CDN 服务？</p>
<ul>
<li>CDN ，即内容分发网络。</li>
<li>其目的是，通过在现有的 Internet中 增加一层新的网络架构，将网站的内容发布到最接近用户的网络边缘，使用户可就近取得所需的内容，提高用户访问网站的速度。</li>
<li>一般来说，因为现在 CDN 服务比较大众，所以基本所有公司都会使用 CDN 服务。</li>
</ul>
</li>
<li><p>Nginx 有哪些负载均衡策略？</p>
<ul>
<li><p>轮询（默认）round_robin：每个请求按时间顺序逐一分配到不同的后端服务器，如果后端服务器 down 掉，能自动剔除。</p>
</li>
<li><p>IP 哈希 ip_hash：每个请求按访问 ip 的 hash 结果分配，这样每个访客固定访问一个后端服务器，可以解决 session 共享的问题。当然，实际场景下，一般不考虑使用 ip_hash 解决 session 共享。</p>
</li>
<li><p>最少连接 least_conn：下一个请求将被分派到活动连接数量最少的服务器</p>
</li>
<li><p>权重</p>
  <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">weight=1; # (weight 默认为1.weight越大，负载的权重就越大)</span><br><span class="line">down; # (down 表示单前的server暂时不参与负载)</span><br><span class="line">backup; # (其它所有的非backup机器down或者忙的时候，请求backup机器)</span><br><span class="line">max_fails=1; # 允许请求失败的次数默认为 1 。当超过最大次数时，返回</span><br><span class="line">proxy_next_upstream 模块定义的错误</span><br><span class="line">fail_timeout=30; # max_fails 次失败后，暂停的时间</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>Nginx 如何实现后端服务的健康检查？</p>
<ul>
<li>利用 nginx 自带模块 ngx_http_proxy_module 和 ngx_http_upstream_module 对后端节点做健康检查。</li>
<li>利用 nginx_upstream_check_module 模块对后端节点做健康检查。</li>
</ul>
</li>
</ol>
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>interview</tag>
        <tag>Linux</tag>
        <tag>负载均衡</tag>
      </tags>
  </entry>
  <entry>
    <title>Java基础</title>
    <url>/2021/11/07/Java%E5%9F%BA%E7%A1%80/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="Java核心基础"><a href="#Java核心基础" class="headerlink" title="Java核心基础"></a>Java核心基础</h1><h2 id="数据类型"><a href="#数据类型" class="headerlink" title="数据类型"></a>数据类型</h2><ol>
<li><p>JAVA基本数据类型所占长度、</p>
<table>
<thead>
<tr>
<th>基本类型</th>
<th>大小</th>
<th>最小值</th>
<th>最大值</th>
</tr>
</thead>
<tbody><tr>
<td>int</td>
<td>32bit</td>
<td>-2^31</td>
<td>+2^32 -1</td>
</tr>
<tr>
<td>short</td>
<td>16bit</td>
<td>-2^15</td>
<td>+2^15 -1</td>
</tr>
<tr>
<td>byte</td>
<td>8bit</td>
<td>-128</td>
<td>+127</td>
</tr>
<tr>
<td>long</td>
<td>64bit</td>
<td>-2^63</td>
<td>+2^63 -1</td>
</tr>
<tr>
<td>float</td>
<td>32bit</td>
<td>IEEE754</td>
<td>IEEE754</td>
</tr>
<tr>
<td>double</td>
<td>64bit</td>
<td>IEEE754</td>
<td>IEEE754</td>
</tr>
<tr>
<td>char</td>
<td>16bit</td>
<td>Unicode 0</td>
<td>Unicode 2^16 -1</td>
</tr>
<tr>
<td>boolean</td>
<td>1bit</td>
<td>—-</td>
<td>—-</td>
</tr>
</tbody></table>
<ul>
<li>JAVA基本数据类型的长度是平台无关的，32位系统和64位系统一样，因为JAVA是运行在JVM上的。</li>
</ul>
</li>
<li><p>Java String 占用内存大小分析</p>
<ul>
<li>Java 对象在虚拟机的结构如下：<ul>
<li>对象头（object header）：8 个字节（保存对象的 class 信息、ID、在虚拟机中的状态）</li>
<li>Java 原始类型数据：如 int, float, char 等类型的数据</li>
<li>引用（reference）：4 个字节</li>
<li>填充符（padding）</li>
</ul>
</li>
</ul>
</li>
</ol>
<h2 id="线程基础"><a href="#线程基础" class="headerlink" title="线程基础"></a>线程基础</h2><ol>
<li>什么是线程？<ul>
<li>线程是操作系统能够进行运算调度的最小单位，它被包含在进程之中，是进程中的实际运作单位</li>
</ul>
</li>
<li>线程和进程有什么区别？<ul>
<li>线程是进程的子集，一个进程可以有很多线程，每条线程并行执行不同的任务。不同的进程使用不同的内存空间，而所有的线程共享一片相同的内存空间。别把它和栈内存搞混，每个线程都拥有单独的栈内存用来存储本地数据</li>
</ul>
</li>
<li>如何在Java中实现线程？<ul>
<li>继承java.lang.Thread 类或者直接调用Runnable接口来重写run()方法实现线程</li>
</ul>
</li>
<li>用Runnable还是Thread？<ul>
<li>Runnable可以继承其他类</li>
</ul>
</li>
<li>Thread 类中的start() 和 run() 方法有什么区别？<ul>
<li>start()方法被用来启动新创建的线程，而且start()内部调用了run()方法，这和直接调用run()方法的效果不一样。当<strong>调用run()方法的时候，只会是在原来的线程中调用，没有新的线程启动，start()方法才会启动新线程</strong></li>
</ul>
</li>
<li>Java中Runnable和Callable有什么不同？<ul>
<li>Callable的 call() 方法有返回值和抛出异常，而Runnable的run()方法没有这些功能。</li>
<li>Callable可以返回装载有计算结果的Future对象。</li>
</ul>
</li>
<li>什么是线程安全？Vector是一个线程安全类吗？<ul>
<li>多线程操作下数据一致性</li>
</ul>
</li>
</ol>
<h2 id="ThreadLocal-相关"><a href="#ThreadLocal-相关" class="headerlink" title="ThreadLocal 相关"></a>ThreadLocal 相关</h2><ol>
<li>作为线程局部变量使用</li>
<li><code>set(T value)</code>:获取<code>ThreadLocalMap</code>(静态内部类)并保存value,为空则调用<code>createMap(Thread t, T firstValue)</code>;</li>
<li><code>getMap(Thread t)</code>:获取<code>ThreadLocalMap</code>,不为空则获取对应值;为空调用<code>setInitialValue()</code> 初始化value;内部调用<code>initialValue()</code>;</li>
<li>需要初始化值,要重写<code>initialValue</code></li>
<li><a href="http://blog.csdn.net/sonny543/article/details/51336457">threadlocal原理及常用应用场景</a></li>
</ol>
<h2 id="java-集合框架-队列相关"><a href="#java-集合框架-队列相关" class="headerlink" title="java 集合框架 队列相关"></a>java 集合框架 队列相关</h2><ol>
<li>HashMap <ul>
<li>基于Hash表的非同步实现,允许K-V 为null;</li>
<li>底层基于数组实现(HashMap.Entry[]),单项为一个链表</li>
<li>HashMap.Entry 包含K,V,next Entry&lt;K,V&gt;,hash</li>
<li>put(K,V) 通过hash(key.hashCode())计算出hash值决定其在数组中的存储位置，如果此位置上有对象的话，再去使用 equals方法进行比较，如果对此链上的每个对象的 equals 方法比较都为 false，则将该对象放到数组当中，然后将数组中该位置以前存在的那个对象链接到此对象的后面</li>
<li>get(K) 首先计算key的hashCode，找到数组中对应位置的某一元素，然后通过key的equals方法在对应位置的链表中找到需要的元素。</li>
<li>当hash冲突很多时，HashMap退化成链表。</li>
<li>key为null时，都放到table[0]</li>
<li>扩容默认负载因子0.75,重新计算位置单个Entry在新数组中的位置 (resize)</li>
<li>fast-fail volatile modCount</li>
<li>java 8 HashMap 改为 数组+链表/红黑树,同一hash位下链表元素&gt;=8时,链表转换为红黑树</li>
</ul>
</li>
<li>ConcurrentHashMap<ul>
<li>ConcurrentHashMap允许多个修改操作并发进行，其关键在于使用了锁分离技术。</li>
<li><font color="red" >待补充源码及具体实现</font></li>
</ul>
</li>
<li>LinkedHashMap<ul>
<li>LinkedHashMap继承于HashMap，底层使用哈希表和双向链表来保存所有元素，并且它是非同步，允许使用null值和null键</li>
<li>重新定义了数组中保存的元素Entry，来实现自己的链接列表特性。该Entry除了保存当前对象的引用外，还保存了其上一个元素before和下一个元素after的引用，从而构成了双向链接列表</li>
</ul>
</li>
<li>HashSet<ul>
<li>HashSet由哈希表支持,基于HashMap实现，不保证set的迭代顺序，并允许使用null元素。</li>
</ul>
</li>
<li>LinkedHashSet<ul>
<li>对于LinkedHashSet而言，它继承与HashSet、又基于LinkedHashMap来实现的。LinkedHashSet底层使用LinkedHashMap来保存所有元素，它继承与HashSet，其所有的方法操作上又与HashSet相同。</li>
</ul>
</li>
<li>ArrayList<ul>
<li>ArrayList是List接口的可变数组非同步实现，并允许包括null在内的所有元素。底层使用数组实现</li>
<li>该集合是可变长度数组，数组扩容时，会将老数组中的元素重新拷贝一份到新的数组中，每次数组容量增长大约是其容量的<font color="red">1.5倍</font>，这种操作的代价很高。</li>
<li>采用了Fail-Fast机制，面对并发的修改时，迭代器很快就会完全失败，而不是冒着在将来某个不确定时间发生任意不确定行为的风险</li>
</ul>
</li>
<li>LinkedList<ul>
<li>LinkedList是List接口的双向链表非同步实现，并允许包括null在内的所有元素。</li>
<li>底层的数据结构是基于双向链表的，该数据结构我们称为节点(Node)</li>
<li>双向链表节点对应的静态内部类Node<E>的实例，Node中包含成员变量：next，prev，item。</li>
</ul>
</li>
</ol>
]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>interview</tag>
      </tags>
  </entry>
  <entry>
    <title></title>
    <url>/2021/11/07/Spring/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="Spring"><a href="#Spring" class="headerlink" title="Spring"></a>Spring</h1><h2 id="Spring-整体相关的面试"><a href="#Spring-整体相关的面试" class="headerlink" title="Spring 整体相关的面试"></a>Spring 整体相关的面试</h2><ol>
<li>什么是 Spring Framework？<ul>
<li>Spring 是一个开源应用框架，旨在降低应用程序开发的复杂度。</li>
<li>它是轻量级、松散耦合的。随着 Spring 的体系越来越庞大，大家被 Spring 的配置搞懵逼了，所以后来出了 Spring Boot 。</li>
<li>它具有分层体系结构，允许用户选择组件，同时还为 J2EE 应用程序开发提供了一个有凝聚力的框架。</li>
<li>它可以集成其他框架，如 Spring MVC、Hibernate、MyBatis 等，所以又称为框架的框架( 粘合剂、脚手架 )。</li>
</ul>
</li>
<li>Spring Framework 中有多少个模块，它们分别是什么？<ul>
<li><img src="https://s2.loli.net/2022/03/08/Gfdneam1stSNTYu.jpg"></li>
<li>Spring 核心容器:核心容器提供 Spring 框架的基本功能。核心容器的主要组件是 BeanFactory，它是工厂模式的实现。BeanFactory 使用控制反转 （IOC）模式将应用程序的配置和依赖性规范与实际的应用程序代码分开。<ul>
<li>Spring Core</li>
<li>Spring Bean</li>
<li>Spring Context:Spring 上下文是一个配置文件，向 Spring 框架提供上下文信息。Spring 上下文包括企业服务，例如 JNDI、EJB、电子邮件、国际化、事件机制、校验和调度功能。</li>
<li>SpEL (Spring Expression Language):Spring 表达式语言全称为 “Spring Expression Language”，缩写为 “SpEL” ，类似于 Struts2 中使用的 OGNL 表达式语言，能在运行时构建复杂表达式、存取对象图属性、对象方法调用等等，并且能与 Spring 功能完美整合，如能用来配置 Bean 定义。</li>
</ul>
</li>
<li>数据访问:Data Access 。<ul>
<li>JDBC: Spring 对 JDBC 的封装模块，提供了对关系数据库的访问。</li>
<li>ORM: Spring ORM 模块，提供了对 hibernate5 和 JPA 的集成。<ul>
<li>hibernate5 是一个 ORM 框架。</li>
<li>JPA 是一个 Java 持久化 API 。</li>
</ul>
</li>
<li>Transaction:Spring 简单而强大的事务管理功能，包括声明式事务和编程式事务。</li>
</ul>
</li>
<li>Web: 提供了创建 Web 应用程序的支持。它包含以下模块：<ul>
<li>WebMVC 框架是一个全功能的构建 Web 应用程序的 MVC 实现。通过策略接口，MVC 框架变成为高度可配置的，MVC 容纳了大量视图技术，其中包括 JSP、Velocity、Tiles、iText 和 POI。</li>
<li>WebFlux:基于 Reactive 库的响应式的 Web 开发框架</li>
<li>WebSocket:Websocket 提供了一个在 Web 应用中实现高效、双向通讯，需考虑客户端(浏览器)和服务端之间高频和低延时消息交换的机制。</li>
<li>一般的应用场景有：在线交易、网页聊天、游戏、协作、数据可视化等。</li>
</ul>
</li>
<li>AOP:支持面向切面编程。它包含以下模块：<ul>
<li>AOP通过配置管理特性，Spring AOP 模块直接将面向方面的编程功能集成到了 Spring 框架中。所以，可以很容易地使 Spring 框架管理的任何对象支持 AOP。</li>
<li>Spring AOP 模块为基于 Spring 的应用程序中的对象提供了事务管理服务。通过使用 Spring AOP，不用依赖 EJB 组件，就可以将声明性事务管理集成到应用程序中。</li>
</ul>
</li>
<li>JMS</li>
<li>Test</li>
<li>Messaging</li>
</ul>
</li>
<li>使用 Spring 框架能带来哪些好处？<ul>
<li>DI ：依赖注入，使得构造器和 JavaBean、properties 文件中的依赖关系一目了然。</li>
<li>轻量级：与 EJB 容器相比较，IoC 容器更加趋向于轻量级。这样一来 IoC 容器在有限的内存和 CPU 资源的情况下，进行应用程序的开发和发布就变得十分有利。</li>
<li>面向切面编程(AOP)： Spring 支持面向切面编程，同时把应用的业务逻辑与系统的服务分离开来。</li>
<li>集成主流框架：Spring 并没有闭门造车，Spring 集成了已有的技术栈，比如 ORM 框架、Logging 日期框架、J2EE、Quartz 和 JDK Timer ，以及其他视图技术。</li>
<li>模块化：Spring 框架是按照模块的形式来组织的。由包和类的命名，就可以看出其所属的模块，开发者仅仅需要选用他们需要的模块即可。</li>
<li>便捷的测试：要 测试一项用Spring开发的应用程序 十分简单，因为测试相关的环境代码都已经囊括在框架中了。更加简单的是，利用 JavaBean 形式的 POJO 类，可以很方便的利用依赖注入来写入测试数据。</li>
<li>Web 框架：Spring 的 Web 框架亦是一个精心设计的 Web MVC 框架，为开发者们在 Web 框架的选择上提供了一个除了主流框架比如 Struts 、过度设计的、不流行 Web 框架的以外的有力选项。</li>
<li>事务管理：Spring 提供了一个便捷的事务管理接口，适用于小型的本地事物处理（比如在单 DB 的环境下）和复杂的共同事物处理（比如利用 JTA 的复杂 DB 环境）。</li>
</ul>
</li>
<li>Spring 框架中都用到了哪些设计模式？<ul>
<li>代理模式 — 在 AOP 和 remoting 中被用的比较多。</li>
<li>单例模式 — 在 Spring 配置文件中定义的 Bean 默认为单例模式。</li>
<li>模板方法 — 用来解决代码重复的问题。比如 RestTemplate、JmsTemplate、JdbcTemplate 。</li>
<li>前端控制器 — Spring提供了 DispatcherServlet 来对请求进行分发。</li>
<li>依赖注入 — 贯穿于 BeanFactory / ApplicationContext 接口的核心理念。</li>
<li>工厂模式 — BeanFactory 用来创建对象的实例。</li>
</ul>
</li>
</ol>
<h2 id="Spring-IoC-相关的面试题"><a href="#Spring-IoC-相关的面试题" class="headerlink" title="Spring IoC 相关的面试题"></a>Spring IoC 相关的面试题</h2><ol>
<li>什么是 Spring IoC 容器？<ul>
<li><img src="https://s2.loli.net/2022/03/08/ChtRG5upeASvPwr.jpg"></li>
<li>Spring 框架的核心是 Spring IoC 容器。容器创建 Bean 对象，将它们装配在一起，配置它们并管理它们的完整生命周期。<ul>
<li>Spring 容器使用依赖注入来管理组成应用程序的 Bean 对象。</li>
<li>容器通过读取提供的配置元数据 Bean Definition 来接收对象进行实例化，配置和组装的指令。</li>
<li>该配置元数据 Bean Definition 可以通过 XML，Java 注解或 Java Config 代码提供。</li>
</ul>
</li>
</ul>
</li>
<li>什么是依赖注入？<ul>
<li>在依赖注入中，你不必主动、手动创建对象，但必须描述如何创建它们。你不是直接在代码中将组件和服务连接在一起，而是描述配置文件中哪些组件需要哪些服务。</li>
<li>然后，再由 IoC 容器将它们装配在一起。</li>
</ul>
</li>
<li>IoC 和 DI 有什么区别？<ul>
<li> IOC就是由 Spring IOC 容器来负责对象的生命周期和对象之间的关系；容器控制应用程序，由容器反向的向应用程序注入应用程序所需要的外部资源。</li>
<li> DI应用程序依赖容器创建并注入它所需要的外部资源；</li>
</ul>
</li>
<li>可以通过多少种方式完成依赖注入？<ul>
<li>可以通过多少种方式完成依赖注入？<ul>
<li>接口注入</li>
<li>构造函数注入</li>
<li>setter 注入</li>
</ul>
</li>
<li>实际场景下，setting 注入使用的更多。</li>
</ul>
</li>
<li>Spring 中有多少种 IoC 容器？<ul>
<li>Spring 提供了两种( 不是“个” ) IoC 容器，分别是 BeanFactory、ApplicationContext <ul>
<li>BeanFactory：spring-beans 项目提供，就像一个包含 Bean 集合的工厂类。它会在客户端要求时实例化 Bean 对象。</li>
<li>ApplicationContext：接口扩展了 BeanFactory 接口，它在 BeanFactory 基础上提供了一些额外的功能。内置如下功能：<ul>
<li>MessageSource ：管理 message ，实现国际化等功能。</li>
<li>ApplicationEventPublisher ：事件发布。</li>
<li>ResourcePatternResolver ：多资源加载。</li>
<li>EnvironmentCapable ：系统 Environment（profile + Properties）相关。</li>
<li>Lifecycle ：管理生命周期。</li>
<li>Closable ：关闭，释放资源</li>
<li>InitializingBean：自定义初始化。</li>
<li>BeanNameAware：设置 beanName 的 Aware 接口。</li>
<li>常用WebApplicationContext ClassPathXmlApplicationContext</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>请介绍下常用的 BeanFactory 容器？<ul>
<li>BeanFactory 最常用的是 XmlBeanFactory 。它可以根据 XML 文件中定义的内容，创建相应的 Bean。</li>
<li>ListableBeanFactory:实现了枚举方法可以列举出当前 BeanFactory 中所有的 bean 对象而不必根据 name 一个一个的获取。</li>
</ul>
</li>
<li>请介绍下常用的 ApplicationContext 容器？<ul>
<li>ClassPathXmlApplicationContext ：从 ClassPath 的 XML 配置文件中读取上下文，并生成上下文定义。应用程序上下文从程序环境变量中取得。示例代码如下：  <figure class="highlight java"><table><tr><td class="code"><pre><span class="line">ApplicationContext context = <span class="keyword">new</span> ClassPathXmlApplicationContext(“bean.xml”);</span><br></pre></td></tr></table></figure></li>
<li>FileSystemXmlApplicationContext ：由文件系统中的XML配置文件读取上下文。示例代码如下：  <figure class="highlight java"><table><tr><td class="code"><pre><span class="line">ApplicationContext context = <span class="keyword">new</span> FileSystemXmlApplicationContext(“bean.xml”); </span><br></pre></td></tr></table></figure></li>
<li>Spring Boot 使用的是ConfigServletWebServerApplicationContext ApplicationContext 容器，。</li>
</ul>
</li>
<li>列举一些 IoC 的一些好处？<ul>
<li>它将最小化应用程序中的代码量。</li>
<li>它以最小的影响和最少的侵入机制促进松耦合。</li>
<li>它支持即时的实例化和延迟加载 Bean 对象。</li>
<li>它将使应用程序易于测试，因为它不需要单元测试用例中的任何单例或 JNDI 查找机制。</li>
</ul>
</li>
<li>简述 Spring IoC 的实现机制？<ul>
<li>Spring 中的 IoC 的实现原理，就是工厂模式加反射机制</li>
</ul>
</li>
</ol>
<h2 id="Spring-Bean"><a href="#Spring-Bean" class="headerlink" title="Spring Bean"></a>Spring Bean</h2><ol>
<li><p>什么是 Spring Bean ？</p>
<ul>
<li>Bean 由 Spring IoC 容器实例化，配置，装配和管理。</li>
<li>Bean 是基于用户提供给 IoC 容器的配置元数据 Bean Definition 创建。</li>
</ul>
</li>
<li><p>Spring 有哪些配置Bean的方式</p>
<ul>
<li>XML 配置文件。</li>
<li>注解配置。</li>
<li>Java Config 配置，使用 @Bean 和 @Configuration 来实现。</li>
<li>具体举例<ul>
<li>Dubbo 服务的配置，使用 XML 。</li>
<li>Spring MVC 请求的配置，艿艿喜欢使用 @RequestMapping 注解。</li>
<li>Spring MVC 拦截器的配置，艿艿喜欢 Java Config 配置。</li>
<li>Spring Boot 以Java Config 配置为主。</li>
</ul>
</li>
</ul>
</li>
<li><p>Spring 支持几种 Bean Scope ？</p>
<ul>
<li>Singleton - 每个 Spring IoC 容器仅有一个单 Bean 实例。默认</li>
<li>Prototype - 每次请求都会产生一个新的实例。</li>
<li>Request - 每一次 HTTP 请求都会产生一个新的 Bean 实例，并且该 Bean 仅在当前 HTTP 请求内有效。</li>
<li>Session - 每一个的 Session 都会产生一个新的 Bean 实例，同时该 Bean 仅在当前 HTTP Session 内有效。</li>
<li>Application - 每一个 Web Application 都会产生一个新的 Bean ，同时该 Bean 仅在当前 Web Application 内有效。</li>
</ul>
</li>
<li><p>Spring Bean 在容器的生命周期是什么样的？</p>
<ul>
<li>Spring Bean 的初始化流程如下：<ul>
<li>实例化 Bean 对象<ul>
<li>Spring 容器根据配置中的 Bean Definition(定义)中实例化 Bean 对象。(Bean Definition 可以通过 XML，Java 注解或 Java Config 代码提供)。</li>
<li>Spring 使用依赖注入填充所有属性，如 Bean 中所定义的配置。</li>
</ul>
</li>
<li>Aware 相关的属性，注入到 Bean 对象<ul>
<li>如果 Bean 实现 BeanNameAware 接口，则工厂通过传递 Bean 的 beanName 来调用 #setBeanName(String name) 方法。</li>
<li>如果 Bean 实现 BeanFactoryAware 接口，工厂通过传递自身的实例来调用 #setBeanFactory(BeanFactory beanFactory) 方法。</li>
</ul>
</li>
<li>调用相应的方法，进一步初始化 Bean 对象<ul>
<li>如果存在与 Bean 关联的任何 BeanPostProcessor 们，则调用 #preProcessBeforeInitialization(Object bean, String beanName) 方法。</li>
<li>如果 Bean 实现 InitializingBean 接口，则会调用 #afterPropertiesSet() 方法。</li>
<li>如果为 Bean 指定了 init 方法（例如 <bean /> 的 init-method 属性），那么将调用该方法。</li>
<li>如果存在与 Bean 关联的任何 BeanPostProcessor 们，则将调用 #postProcessAfterInitialization(Object bean, String beanName) 方法。</li>
</ul>
</li>
</ul>
</li>
<li>Spring Bean 的销毁流程如下：<ul>
<li>如果 Bean 实现 DisposableBean 接口，当 spring 容器关闭时，会调用 #destroy() 方法。</li>
<li>如果为 bean 指定了 destroy 方法（例如 <bean /> 的 destroy-method 属性），那么将调用该方法。<ul>
<li><img src="https://s2.loli.net/2022/03/08/kG3rdih45nALEFO.jpg"></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>什么是 Spring 装配？</p>
<ul>
<li>装配，和上文提到的 DI 依赖注入，实际是一个东西。</li>
</ul>
</li>
<li><p>自动装配有哪些方式？</p>
<ul>
<li>Spring 容器能够自动装配 Bean 。也就是说，可以通过检查 BeanFactory 的内容让 Spring 自动解析 Bean 的协作者。</li>
<li>自动装配的不同模式：<ul>
<li>no - 这是默认设置，表示没有自动装配。应使用显式 Bean 引用进行装配。</li>
<li>byName - 它根据 Bean 的名称注入对象依赖项。它匹配并装配其属性与 XML 文件中由相同名称定义的 Bean 。</li>
<li>【最常用】byType - 它根据类型注入对象依赖项。如果属性的类型与 XML 文件中的一个 Bean 类型匹配，则匹配并装配属性。</li>
<li>构造函数 - 它通过调用类的构造函数来注入依赖项。它有大量的参数。</li>
<li>autodetect - 首先容器尝试通过构造函数使用 autowire 装配，如果不能，则尝试通过 byType 自动装配。</li>
</ul>
</li>
</ul>
</li>
<li><p>解释什么叫延迟加载？</p>
<ul>
<li>默认情况下，容器启动之后会将所有作用域为单例的 Bean 都创建好，但是有的业务场景我们并不需要它提前都创建好。此时，我们可以在Bean 中设置 lzay-init = “true” 。</li>
<li>这样，当容器启动之后，作用域为单例的 Bean ，就不在创建。而是在获得该 Bean 时，才真正在创建加载。</li>
</ul>
</li>
<li><p>Spring 框架中的单例 Bean 是线程安全的么？</p>
<ul>
<li>Spring 框架并没有对单例 Bean 进行任何多线程的封装处理。</li>
</ul>
</li>
<li><p>Spring Bean 怎么解决循环依赖的问题？</p>
<ul>
<li><p><img src="https://s2.loli.net/2022/03/08/KHAGPVekZOWIa6r.jpg"></p>
</li>
<li><p>Spring 在创建 bean 的时候并不是等它完全完成，而是在创建过程中将创建中的 bean 的 ObjectFactory 提前曝光（即加入到 singletonFactories 缓存中）。</p>
</li>
<li><p>这样，一旦下一个 bean 创建的时候需要依赖 bean ，则直接使用 ObjectFactory 的 #getObject() 方法来获取了。</p>
</li>
<li><p>实例如 A 依赖 B，B 依赖 C，C 依赖 A：</p>
<ul>
<li>首先 A 完成初始化第一步并将自己提前曝光出来（通过 ObjectFactory 将自己提前曝光），在初始化的时候，发现自己依赖对象 B，此时就会去尝试 get(B)，这个时候发现 B 还没有被创建出来</li>
<li>然后 B 就走创建流程，在 B 初始化的时候，同样发现自己依赖 C，C 也没有被创建出来</li>
<li>这个时候 C 又开始初始化进程，但是在初始化的过程中发现自己依赖 A，于是尝试 get(A)，这个时候由于 A 已经添加至缓存中（一般都是添加至三级缓存 singletonFactories ），通过 ObjectFactory 提前曝光，所以可以通过 ObjectFactory#getObject() 方法来拿到 A 对象，C 拿到 A 对象后顺利完成初始化，然后将自己添加到一级缓存中</li>
<li>回到 B ，B 也可以拿到 C 对象，完成初始化，A 可以顺利拿到 B 完成初始化。到这里整个链路就已经完成了初始化过程了</li>
</ul>
</li>
</ul>
</li>
<li><p>BeanFactory和FactoryBean的区别</p>
<ul>
<li>BeanFactory是个Factory，也就是IOC容器或对象工厂</li>
<li>FactoryBean是个Bean。</li>
<li>Spring中所有的Bean都是由BeanFactory(也就是IOC容器)来进行管理的。但对FactoryBean而言，这个Bean不是简单的Bean，而是一个能生产或者修饰对象生成的工厂Bean,它的实现与设计模式中的工厂模式和修饰器模式类似 <h2 id="Spring-注解"><a href="#Spring-注解" class="headerlink" title="Spring 注解"></a>Spring 注解</h2></li>
</ul>
</li>
<li><p>什么是基于注解的容器配置？</p>
<ul>
<li>不使用 XML 来描述 Bean 装配，开发人员通过在相关的类，方法或字段声明上使用注解将配置移动到组件类本身。它可以作为 XML 设置的替代方案</li>
<li>以Java Config 配置Bean的方式。</li>
</ul>
</li>
<li><p>如何在 Spring 中启动注解装配？</p>
<ul>
<li><code>&lt;context：annotation-config /&gt;</code></li>
<li>Spring Boot默认情况下已经开启。</li>
</ul>
</li>
<li><p>@Component, @Controller, @Repository, @Service 有何区别？</p>
<ul>
<li>@Component ：它将 Java 类标记为 Bean 。它是任何 Spring 管理组件的通用构造型。</li>
<li>@Controller ：它将一个类标记为 Spring Web MVC 控制器。</li>
<li>@Service ：此注解是组件注解的特化。它不会对 @Component 注解提供任何其他行为。您可以在服务层类中使用 @Service 而不是 @Component ，因为它以更好的方式指定了意图。</li>
<li>@Repository ：这个注解是具有类似用途和功能的 @Component 注解的特化。它为 DAO 提供了额外的好处。它将 DAO 导入 IoC 容器，并使未经检查的异常有资格转换为 Spring DataAccessException 。</li>
</ul>
</li>
<li><p>@Required 注解有什么用？</p>
<ul>
<li>此注解仅指示必须在配置时使用 Bean 定义中的显式属性值或使用自动装配填充受影响的 Bean 属性。</li>
<li>如果尚未填充受影响的 Bean 属性，则容器将抛出 BeanInitializationException 异常。</li>
</ul>
</li>
<li><p>@Autowired 注解有什么用？</p>
<ul>
<li>@Autowired 注解，可以更准确地控制应该在何处以及如何进行自动装配。<ul>
<li>此注解用于在 setter 方法，构造函数，具有任意名称或多个参数的属性或方法上自动装配 Bean。</li>
<li>默认情况下，它是类型驱动的注入。</li>
</ul>
</li>
</ul>
</li>
<li><p>@Qualifier 注解有什么用？</p>
<ul>
<li>当你创建多个相同类型的 Bean ，并希望仅使用属性装配其中一个 Bean 时，您可以使用 @Qualifier 注解和 @Autowired 通过指定 ID 应该装配哪个确切的 Bean 来消除歧义。</li>
</ul>
</li>
<li><p>@Autowired注解与@Resource注解的区别</p>
<ul>
<li>@Autowired为Spring提供的注解，需要导入包org.springframework.beans.factory.annotation.Autowired;只按照byType注入。</li>
<li>@Autowired注解是按照类型（byType）装配依赖对象，默认情况下它要求依赖对象必须存在，如果允许null值，可以设置它的required属性为false。如果我们想使用按照名称（byName）来装配，可以结合@Qualifier注解一起使用。(通过类型匹配找到多个candidate,在没有@Qualifier、@Primary注解的情况下，会使用对象名作为最后的fallback匹配)</li>
<li>@Resource默认按照ByName自动注入，由J2EE提供，需要导入包javax.annotation.Resource。@Resource有两个重要的属性：name和type，而Spring将@Resource注解的name属性解析为bean的名字，而type属性则解析为bean的类型。所以，如果使用name属性，则使用byName的自动注入策略，而使用type属性时则使用byType自动注入策略。如果既不制定name也不制定type属性，这时将通过反射机制使用byName自动注入策略。</li>
<li>@Resource装配顺序：<ul>
<li>①如果同时指定了name和type，则从Spring上下文中找到唯一匹配的bean进行装配，找不到则抛出异常。</li>
<li>②如果指定了name，则从上下文中查找名称（id）匹配的bean进行装配，找不到则抛出异常。</li>
<li>③如果指定了type，则从上下文中找到类似匹配的唯一bean进行装配，找不到或是找到多个，都会抛出异常。</li>
<li>④如果既没有指定name，又没有指定type，则自动按照byName方式进行装配；如果没有匹配，则回退为一个原始类型进行匹配，如果匹配则自动装配。</li>
</ul>
</li>
</ul>
</li>
</ol>
<h2 id="Spring-AOP"><a href="#Spring-AOP" class="headerlink" title="Spring AOP"></a>Spring AOP</h2><ol>
<li>什么是 AOP ？<ul>
<li>AOP(Aspect-Oriented Programming)，即面向切面编程, 它与 OOP( Object-Oriented Programming, 面向对象编程) 相辅相成， 提供了与 OOP 不同的抽象软件结构的视角。</li>
<li>在 OOP 中，以类( Class )作为基本单元</li>
<li>在 AOP 中，以切面( Aspect )作为基本单元。</li>
</ul>
</li>
<li>什么是 Aspect ？<ul>
<li>Aspect 由 PointCut 和 Advice 组成,@Aspect 注解的类就是切面。<ul>
<li>它既包含了横切逻辑的定义，也包括了连接点的定义。</li>
<li>Spring AOP 就是负责实施切面的框架，它将切面所定义的横切逻辑编织到切面所指定的连接点中。</li>
<li>AOP 的工作重心在于如何将增强编织目标对象的连接点上, 这里包含两个工作:<ul>
<li>如何通过 PointCut 和 Advice 定位到特定的 JoinPoint 上。</li>
<li>如何在 Advice 中编写切面代码。</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>什么是 JoinPoint ?<ul>
<li>JoinPoint ，切点，程序运行中的一些时间点, 例如：<ul>
<li> 一个方法的执行。</li>
<li> 或者是一个异常的处理。</li>
</ul>
</li>
<li> 在 Spring AOP 中，JoinPoint 总是方法的执行点。</li>
</ul>
</li>
<li>什么是 PointCut ？<ul>
<li>PointCut 是匹配 JoinPoint 的条件。</li>
<li>Advice 是和特定的 PointCut 关联的，并且在 PointCut 相匹配的 JoinPoint 中执行。即 Advice =&gt; PointCut =&gt; JoinPoint 。</li>
<li>在 Spring 中, 所有的方法都可以认为是 JoinPoint ，但是我们并不希望在所有的方法上都添加 Advice 。而 PointCut 的作用，就是提供一组规则(使用 AspectJ PointCut expression language 来描述) 来匹配 JoinPoint ，给满足规则的 JoinPoint 添加 Advice 。</li>
</ul>
</li>
<li>关于 JoinPoint 和 PointCut 的区别<ul>
<li>首先，Advice 通过 PointCut 查询需要被织入的 JoinPoint 。</li>
<li>然后，Advice 在查询到 JoinPoint 上执行逻辑。</li>
</ul>
</li>
<li>什么是 Advice ？<ul>
<li>特定 JoinPoint 处的 Aspect 所采取的动作称为 Advice 。</li>
<li>Spring AOP 使用一个 Advice 作为拦截器，在 JoinPoint “周围”维护一系列的拦截器。</li>
</ul>
</li>
<li>有哪些类型的 Advice？<ul>
<li>1.@Before前置通知在切入点运行前执行，不会影响切入点的逻辑</li>
<li>2.@After后置通知在切入点正常运行结束后执行，如果切入点抛出异常，则在抛出异常前执行</li>
<li>3.@AfterThrowing异常通知:在切入点抛出异常前执行，如果切入点正常运行（未抛出异常），则不执行</li>
<li>4.@AfterReturning返回通知:在切入点正常运行结束后执行，如果切入点抛出异常，则不执行</li>
<li>5.@Around环绕通知是功能最强大的通知，可以在切入点执行前后自定义一些操作。环绕通知需要负责决定是继续处理join point(调用ProceedingJoinPoint的proceed方法)还是中断执行</li>
</ul>
</li>
<li>什么是 Target ？<ul>
<li>Target ，织入 Advice 的目标对象。目标对象也被称为 Advised Object 。<ul>
<li>因为 Spring AOP 使用运行时代理的方式来实现 Aspect ，因此 Advised Object 总是一个代理对象(Proxied Object) 。</li>
<li>注意, Advised Object 指的不是原来的对象，而是织入 Advice 后所产生的代理对象。</li>
<li>Advice + Target Object = Advised Object = Proxy 。</li>
</ul>
</li>
</ul>
</li>
<li>AOP 有哪些实现方式？<ul>
<li>静态代理 - 指使用 AOP 框架提供的命令进行编译，从而在编译阶段就可生成 AOP 代理类，因此也称为编译时增强。<ul>
<li>例如，SkyWalking 基于 Java Agent 机制，配置上 ByteBuddy 库，实现类加载时编织时增强，从而实现链路追踪的透明埋点。</li>
</ul>
</li>
<li>动态代理 - 在运行时在内存中“临时”生成 AOP 动态代理类，因此也被称为运行时增强。目前 Spring 中使用了两种动态代理库：<ul>
<li>JDK 动态代理</li>
<li>CGLIB</li>
</ul>
</li>
</ul>
</li>
<li>Spring 如何使用 AOP 切面？<ul>
<li>基于 XML 方式的切面实现。</li>
<li>基于 注解 方式的切面实现。</li>
</ul>
</li>
</ol>
<h2 id="Spring-Transaction-相关的面试题"><a href="#Spring-Transaction-相关的面试题" class="headerlink" title="Spring Transaction 相关的面试题"></a>Spring Transaction 相关的面试题</h2><ol>
<li>列举 Spring 支持的事务管理类型？<ul>
<li>声明式事务：通过使用注解或基于 XML 的配置事务，从而事务管理与业务代码分离。</li>
<li>编程式事务：通过编码的方式实现事务管理，需要在代码中显式的调用事务的获得、提交、回滚。它为您提供极大的灵活性，但维护起来非常困难。</li>
</ul>
</li>
<li>Spring 事务如何和不同的数据持久层框架做集成？<ul>
<li>Spring 事务的管理，是通过org.springframework.transaction.PlatformTransactionManager 进行管理<ul>
<li>PlatformTransactionManager 是负责事务管理的接口，一共有三个接口方法，分别负责事务的获得、提交、回滚。<ul>
<li><code>getTransaction(TransactionDefinition definition)</code> 方法，根据事务定义 TransactionDefinition ，获得 TransactionStatus 。<ul>
<li>为什么不是创建事务？因为如果当前如果已经有事务，则不会进行创建，一般来说会跟当前线程进行绑定。如果不存在事务，则进行创建。</li>
<li>为什么返回的是 TransactionStatus 对象？在 TransactionStatus 中，不仅仅包含事务属性，还包含事务的其它信息，例如是否只读、是否为新创建的事务等等。</li>
</ul>
</li>
<li><code>commit(TransactionStatus status)</code> 方法，根据 TransactionStatus 情况，提交事务。<ul>
<li>为什么根据 TransactionStatus 情况，进行提交？<ul>
<li>例如，带@Transactional 注解的的 A 方法，会调用 @Transactional 注解的的 B 方法。</li>
<li>在 B 方法结束调用后，会执行 PlatformTransactionManager#commit(TransactionStatus status) 方法，此处事务是不能、也不会提交的。</li>
<li>而是在 A 方法结束调用后，执行 PlatformTransactionManager#commit(TransactionStatus status) 方法，提交事务。</li>
</ul>
</li>
</ul>
</li>
<li><code>rollback(TransactionStatus status)</code> 方法，根据 TransactionStatus 情况，回滚事务。<ul>
<li>为什么根据 TransactionStatus 情况，进行回滚？原因同 #commit(TransactionStatus status) 方法。</li>
</ul>
</li>
</ul>
</li>
<li>PlatformTransactionManager 有抽象子类 <code>org.springframework.transaction.support.AbstractPlatformTransactionManager</code> ，基于 模板方法模式 ，实现事务整体逻辑的骨架，而抽象 <code>doCommit(DefaultTransactionStatus status)</code>、<code>doRollback(DefaultTransactionStatus status)</code> 等等方法，交由子类类来实现</li>
<li>不同的数据持久层框架，会有其对应的 PlatformTransactionManager 实现类<ul>
<li><img src="https://s2.loli.net/2022/03/08/j3u5gsL6BdCkIaU.jpg"></li>
<li>所有的实现类，都基于 AbstractPlatformTransactionManager 这个骨架类。</li>
<li>HibernateTransactionManager ，和 Hibernate5 的事务管理做集成。</li>
<li>DataSourceTransactionManager ，和 JDBC 的事务管理做集成。所以，它也适用于 MyBatis、Spring JDBC 等等。</li>
<li>JpaTransactionManager ，和 JPA 的事务管理做集成。</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>为什么在 Spring 事务中不能切换数据源？<ul>
<li>在 Spring 的事务管理中，所使用的数据库连接会和当前线程所绑定，即使我们设置了另外一个数据源，使用的还是当前的数据源连接。</li>
<li>多个数据源且需要事务的场景，本身会带来多事务一致性的问题，暂时没有特别好的解决方案。</li>
<li>所以一般一个应用，推荐除非了读写分离所带来的多数据源，其它情况下，建议只有一个数据源。并且，随着微服务日益身形，一个服务对应一个 DB 是比较常见的架构选择。</li>
</ul>
</li>
<li>@Transactional 注解有哪些属性？如何使用？<ul>
<li>@Transactional 注解的属性如下：<table>
<thead>
<tr>
<th>属性</th>
<th>类型</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>value</td>
<td>String</td>
<td>可选的限定描述符，指定使用的事务管理器</td>
</tr>
<tr>
<td>propagation</td>
<td>enum: Propagation</td>
<td>可选的事务传播行为设置</td>
</tr>
<tr>
<td>isolation</td>
<td>enum: Isolation</td>
<td>可选的事务隔离级别设置</td>
</tr>
<tr>
<td>readOnly</td>
<td>boolean</td>
<td>读写或只读事务，默认读写</td>
</tr>
<tr>
<td>timeout</td>
<td>int (in seconds granularity)</td>
<td>事务超时时间设置</td>
</tr>
<tr>
<td>rollbackFor</td>
<td>Class对象数组，必须继承自Throwable</td>
<td>导致事务回滚的异常类数组</td>
</tr>
<tr>
<td>rollbackForClassName</td>
<td>类名数组，必须继承自Throwable</td>
<td>导致事务回滚的异常类名字数组</td>
</tr>
<tr>
<td>noRollbackFor</td>
<td>Class对象数组，必须继承自Throwable</td>
<td>不会导致事务回滚的异常类数组</td>
</tr>
<tr>
<td>noRollbackForClassName</td>
<td>类名数组，必须继承自Throwable</td>
<td>不会导致事务回滚的异常类名字数组</td>
</tr>
</tbody></table>
<ul>
<li>具体用法如下：<ul>
<li>@Transactional 可以作用于接口、接口方法、类以及类方法上。当作用于类上时，该类的所有 public 方法将都具有该类型的事务属性，同时，我们也可以在方法级别使用该标注来覆盖类级别的定义。</li>
<li>虽然 @Transactional 注解可以作用于接口、接口方法、类以及类方法上，但是 Spring 建议不要在接口或者接口方法上使用该注解，因为这只有在使用基于接口的代理时它才会生效。另外， @Transactional 注解应该只被应用到 public 方法上，这是由 Spring AOP 的本质决定的。如果你在 protected、private 或者默认可见性的方法上使用 @Transactional 注解，这将被忽略，也不会抛出任何异常。这一点，非常需要注意。</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>什么是事务的传播级别？分成哪些传播级别？<ul>
<li>事务的传播行为，指的是当前带有事务配置的方法，需要怎么处理事务。  <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// ========== 支持当前事务的情况 ========== </span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 如果当前存在事务，则使用该事务。</span></span><br><span class="line"><span class="comment"> * 如果当前没有事务，则创建一个新的事务。</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">int</span> PROPAGATION_REQUIRED = <span class="number">0</span>;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 如果当前存在事务，则使用该事务。</span></span><br><span class="line"><span class="comment"> * 如果当前没有事务，则以非事务的方式继续运行。</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">int</span> PROPAGATION_SUPPORTS = <span class="number">1</span>;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 如果当前存在事务，则使用该事务。</span></span><br><span class="line"><span class="comment"> * 如果当前没有事务，则抛出异常。</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">int</span> PROPAGATION_MANDATORY = <span class="number">2</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// ========== 不支持当前事务的情况 ========== </span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 创建一个新的事务。</span></span><br><span class="line"><span class="comment"> * 如果当前存在事务，则把当前事务挂起。</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">int</span> PROPAGATION_REQUIRES_NEW = <span class="number">3</span>;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 以非事务方式运行。</span></span><br><span class="line"><span class="comment"> * 如果当前存在事务，则把当前事务挂起。</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">int</span> PROPAGATION_NOT_SUPPORTED = <span class="number">4</span>;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 以非事务方式运行。</span></span><br><span class="line"><span class="comment"> * 如果当前存在事务，则抛出异常。</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">int</span> PROPAGATION_NEVER = <span class="number">5</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// ========== 其他情况 ========== </span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 如果当前存在事务，则创建一个事务作为当前事务的嵌套事务来运行。</span></span><br><span class="line"><span class="comment"> * 如果当前没有事务，则等价于 &#123;<span class="doctag">@link</span> TransactionDefinition#PROPAGATION_REQUIRED&#125;</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">int</span> PROPAGATION_NESTED = <span class="number">6</span>;</span><br><span class="line">        </span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>什么是事务的超时属性？<ul>
<li>所谓事务超时，就是指一个事务所允许执行的最长时间，如果超过该时间限制但事务还没有完成，则自动回滚事务。</li>
<li>在 TransactionDefinition 中以 int 的值来表示超时时间，其单位是秒。</li>
</ul>
</li>
<li>什么是事务的只读属性？<ul>
<li>事务的只读属性是指，对事务性资源进行只读操作或者是读写操作。</li>
<li>所谓事务性资源就是指那些被事务管理的资源，比如数据源、JMS 资源，以及自定义的事务性资源等等。</li>
<li>如果确定只对事务性资源进行只读操作，那么我们可以将事务标志为只读的，以提高事务处理的性能。</li>
</ul>
</li>
<li>什么是事务的回滚规则？<ul>
<li>回滚规则，定义了哪些异常会导致事务回滚而哪些不会。</li>
<li>默认情况下，事务只有遇到运行期异常时才会回滚，而在遇到检查型异常时不会回滚（这一行为与EJB的回滚行为是一致的）。</li>
<li>但是你可以声明事务在遇到特定的检查型异常时像遇到运行期异常那样回滚。同样，你还可以声明事务遇到特定的异常不回滚，即使这些异常是运行期异常。</li>
<li>注意，事务的回滚规则，并不是数据库事务规范中的名词，而是 Spring 自身所定义的。</li>
</ul>
</li>
<li>简单介绍 TransactionStatus ？ <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// TransactionStatus.java</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">TransactionStatus</span> <span class="keyword">extends</span> <span class="title">SavepointManager</span>, <span class="title">Flushable</span> </span>&#123;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 是否是新创建的事务</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">boolean</span> <span class="title">isNewTransaction</span><span class="params">()</span></span>;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 是否有 Savepoint</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * 在 &#123;<span class="doctag">@link</span> TransactionDefinition#PROPAGATION_NESTED&#125; 传播级别使用。</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">boolean</span> <span class="title">hasSavepoint</span><span class="params">()</span></span>;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 设置为只回滚</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">setRollbackOnly</span><span class="params">()</span></span>;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 是否为只回滚</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">boolean</span> <span class="title">isRollbackOnly</span><span class="params">()</span></span>;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 执行 flush 操作</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">flush</span><span class="params">()</span></span>;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 是否事务已经完成</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">boolean</span> <span class="title">isCompleted</span><span class="params">()</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>为什么没有事务对象呢？在 TransactionStatus 的实现类 DefaultTransactionStatus 中，有个 Object transaction 属性，表示事务对象。</li>
<li>isNewTransaction() 方法，表示是否是新创建的事务。有什么用呢？答案结合 「Spring 事务如何和不同的数据持久层框架做集成？」 问题，我们对 commit(TransactionStatus status) 方法的解释。通过该方法，我们可以判断，当前事务是否当前方法所创建的，只有创建事务的方法，才能且应该真正的提交事务。</li>
</ul>
</li>
<li>使用 Spring 事务有什么优点？<ul>
<li>通过 PlatformTransactionManager ，为不同的数据层持久框架提供统一的 API ，无需关心到底是原生 JDBC、Spring JDBC、JPA、Hibernate 还是 MyBatis 。</li>
<li>通过使用声明式事务，使业务代码和事务管理的逻辑分离，更加清晰。</li>
</ul>
</li>
</ol>
]]></content>
  </entry>
  <entry>
    <title>JVM</title>
    <url>/2021/11/07/JVM/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="JVM"><a href="#JVM" class="headerlink" title="JVM"></a>JVM</h1><ol>
<li><p>JVM 由哪些部分组成？</p>
<ul>
<li><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/15847504847911.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></li>
<li><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/15847532504574.png?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></li>
<li>类加载器：在JVM启动时或者类运行时将需要的class加载到JVM。</li>
<li>运行时数据区：将内存划分为若干个区以模拟实际机器上的存储，记录，调度功能模块。</li>
<li>执行引擎：执行引擎的任务是负责执行 class 文件中包含的字节码指令，相当于实际机器上的 CPU。</li>
<li>本地方法调用：调用 C 或 C++ 实现的本地方法的代码返回结果。</li>
</ul>
</li>
<li><p>JVM 运行内存（运行时数据区）的分类？</p>
<ul>
<li>程序计数器（线程私有）：记录正在执行的Java方法的字节码指令地址，唯一没有OutOfMemoryError情况的区域</li>
<li>栈内存（线程私有）：描述Java方法执行时的内存模型 <ul>
<li>每个方法在执行的时候，都会创建一个栈帧用于存储局部变量，操作数，动态链接，方法出口等信息</li>
<li>每个方法调用都意味着一个栈帧再虚拟机栈中入栈到出栈的过程</li>
<li>局部变量表：基本数据类型（boolean,byte,short,int,long,float,double,char），对象引用（reference类型，不等同与对象，是指针或者资源地址），returnAddress类型（指向一条字节码指令的位置）</li>
<li>线程执行栈深度超出限制，跑出StackOverFlowError</li>
</ul>
</li>
<li>本地方法栈：<ul>
<li>和 Java 虚拟机栈的作用类似，区别是该区域为 JVM 提供使用 Native 方法的服务</li>
</ul>
</li>
<li>堆内存（线程共享）：所有线程共享的一块区域，存放对象实例，垃圾收集器管理的主要区域。<ul>
<li>目前主要的垃圾回收算法都是分代收集算法，所以 Java 堆中还可以细分为：新生代和老年代；再细致一点的有 Eden 空间、From Survivor 空间、To Survivor 空间等，默认情况下新生代按照 8:1:1 的比例来分配。</li>
<li>Java 堆可以处于物理上不连续的内存空间中，只要逻辑上是连续的即可，就像我们的磁盘一样。</li>
<li>可固定，可拓展通过-Xms -Xmx控制</li>
<li>堆内存无法分配内存，且无法拓展，抛出OutOfMemory</li>
</ul>
</li>
<li>方法区（线程共享）：主要用于已被虚拟机加载的类信息、静态变量、常量、JIT编译后的代码<ul>
<li>JDK 1.8 的对 JVM 架构的改造将类元数据放到本地内存中，另外，将常量池和静态变量放到 Java 堆里</li>
<li>JDK 1.7 <code>java.lang.OutOfMemoryError: PermGen space</code> -XX:MaxPermSize -XX:PermSize</li>
<li>JDK 1.8 -XX:MetaspaceSize，初始空间大小;-XX:MaxMetaspaceSize，最大空间，默认是没有限制的</li>
<li>运行时常量池</li>
</ul>
<blockquote>
<p><a href="https://www.cnblogs.com/paddix/p/5309550.html">Java8内存模型—永久代(PermGen)和元空间(Metaspace)</a></p>
</blockquote>
</li>
</ul>
</li>
<li><p>直接内存是不是虚拟机运行时数据区的一部分？</p>
<ul>
<li>直接内存(Direct Memory)不是虚拟机运行时数据区的一部分</li>
<li>使用 native 函数库直接分配堆外内存，使用Java 堆中的 DirectByteBuffer 对象作为这块内存的引用进行操作，避免了在 Java 堆和 Native 堆中来回复制数据显著提高性能</li>
<li>本机直接内存的分配不会受到 Java 堆大小的限制，受到本机总内存大小限制。</li>
<li>配置虚拟机参数时，不要忽略直接内存，防止出现 OutOfMemoryError 异常。</li>
</ul>
</li>
<li><p>Java内存模型?</p>
<ul>
<li><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/15848403457744.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></li>
<li>定义程序中各个变量的访问规则</li>
<li>所有的变量都存储在主内存</li>
<li>线程有自己的工作内存，工作内存保存了该线程使用到的变量的主内存副本拷贝</li>
<li>线程间变量值的传递均需要通过主内存来实现</li>
<li>原子性: synchronized保证了原子性，提供了两个高级的字节码指令monitorenter和monitorexit</li>
<li>可见性：Java内存模型是通过在变量修改后将新值同步回主内存，在变量读取前从主内存刷新变量值的这种依赖主内存作为传递媒介的方式来实现的。可以使用synchronized，volatile， final实现</li>
<li>有序性：在Java中，可以使用synchronized和volatile来保证多线程之间操作的有序性；volatile关键字会禁止指令重排。synchronized关键字保证同一时刻只允许一条线程操作。</li>
</ul>
</li>
<li><p>直接内存（堆外内存）与堆内存比较？</p>
<ul>
<li>直接内存申请空间耗费更高的性能，当频繁申请到一定量时尤为明显。</li>
<li>直接内存 IO 读写的性能要优于普通的堆内存，在多次读写操作的情况下差异明显。</li>
</ul>
</li>
<li><p>为什么要废弃永久代？</p>
<ul>
<li>由于永久代内存经常不够用或发生内存泄露，爆出异常 java.lang.OutOfMemoryError: PermGen</li>
<li>字符串存在永久代中，容易出现性能问题和内存溢出。</li>
<li>类及方法的信息等比较难确定其大小，因此对于永久代的大小指定比较困难，太小容易出现永久代溢出，太大则容易导致老年代溢出。</li>
<li>永久代会为 GC 带来不必要的复杂度，并且回收效率偏低。</li>
</ul>
</li>
<li><p>Java 内存堆和栈区别？</p>
<ul>
<li><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/15847579303911.png?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></li>
<li>栈内存存储基本类型的变量和对象的引用变量；堆内存用来存储Java中的对象，无论是成员变量，局部变量，还是类变量，它们指向的对象都存储在堆内存中。</li>
<li>栈内存归属于单个线程，每个线程都会有一个栈内存，其存储的变量只能在其所属线程中可见，即栈内存可以理解成线程的私有内存；堆内存中的对象对所有线程可见。堆内存中的对象可以被所有线程访问。</li>
<li>栈溢出 java.lang.StackOverFlowError；堆溢出java.lang.OutOfMemoryError</li>
<li>栈的内存要远远小于堆内存，如果你使用递归的话，那么你的栈很快就会充满。-Xss 选项设置栈内存的大小，-Xms 选项可以设置堆的开始时的大小。</li>
</ul>
<blockquote>
<p>JVM 中堆和栈属于不同的内存区域，使用目的也不同。栈常用于保存方法帧和局部变量，而对象总是在堆上分配。栈通常都比堆小，也不会在多个线程之间共享，而堆被整个 JVM 的所有线程共享。</p>
</blockquote>
</li>
<li><p>JAVA 对象创建的过程？</p>
<pre><code> - ![](https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/15847585045516.png?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10)                                                    
 - ![](https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/15847593821321.png?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10)        
</code></pre>
<ol>
<li>检测类是否被加载:当虚拟机遇到 new 指令时，首先先去检查这个指令的参数是否能在常量池(方法区-运行时常量池)中定位到一<strong>个类的符号引用</strong>，并且检查这个符号引用代表的类是否已被加载、解析和初始化过。如果没有，就执行类加载过程</li>
<li>为对象分配内存：<ul>
<li>内存空间绝对规整：虚拟机只需要在被占用的内存和可用空间之间移动指针即可，这种方式被称为“<strong>指针碰撞</strong>”。</li>
<li>内存不规整：虚拟机需要维护一个列表，来记录哪些内存是可用的。分配内存的时候需要找到一个可用的内存空间，然后在列表上记录下已被分配，这种方式成为“<strong>空闲列表</strong>”。</li>
</ul>
</li>
<li>为分配的内存空间初始化零值：对象的内存分配完成后，还需要将对象的内存空间都初始化为零值，这样能保证对象的实例字段即使没有赋初值，也可以直接使用。</li>
<li>对对象进行其他设置：在对象头中设置对象所属的类，类的元数据信息，对象的 hashcode ，GC 分代年龄等信息。</li>
<li>执行 init 方法：Java 在编译之后会在字节码文件中生成 init 方法，称之为实例构造器，该实例构造器会将语句块，变量初始化，调用父类的构造器等操作收敛到 init 方法中，收敛顺序为：<ol>
<li>父类变量初始化</li>
<li>父类语句块</li>
<li>父类构造函数</li>
<li>子类变量初始化</li>
<li>子类语句块</li>
<li>子类构造函数</li>
</ol>
</li>
</ol>
</li>
<li><p>A a = new A() 经历过什么过程?</p>
<blockquote>
<p>同上</p>
</blockquote>
</li>
<li><p>对象的内存布局是怎样的？JAVA对象模型？</p>
<ul>
<li>对象头：对象头包括两部分信息。<ul>
<li>第一部分，是存储对象自身的运行时数据，如哈希码，GC 分代年龄，锁状态标志，线程持有的锁等等。</li>
<li>第二部分，是类型指针，即对象指向类元数据的指针。</li>
</ul>
</li>
<li>实例数据：对象真正存储的有效信息</li>
<li>对齐填充：不是必然的存在，就是为了对齐。</li>
</ul>
</li>
<li><p>对象是如何定位访问的？</p>
<ul>
<li>对象的访问定位有两种：<ol>
<li>句柄定位：Java 堆会画出一块内存来作为句柄池，reference 中存储的就是对象的句柄地址，而句柄中包含了对象实例数据与类型数据各自的具体地址信息。<ul>
<li><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/15847606523598.png?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></li>
</ul>
</li>
<li>直接指针访问：Java 堆对象的不居中就必须考虑如何放置访问类型数据的相关信息，而 reference 中存储的直接就是对象地址。<ul>
<li><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/15847606809648.png?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></li>
</ul>
</li>
</ol>
</li>
<li>对比两种方式？<ul>
<li>使用句柄来访问的最大好处，就是 reference 中存储的是稳定的句柄地址，在对象被移动（垃圾收集时移动对象是非常普遍的行为）时只会改变句柄中的实例数据指针，而 reference 本身不需要修改。</li>
<li>使用直接指针访问方式的最大好处就是速度更快，它节省了一次指针定位的时间开销。</li>
</ul>
</li>
</ul>
</li>
<li><p>有哪些 OutOfMemoryError 异常？</p>
<ul>
<li>除了程序计数器外，虚拟机内存的其它几个运行时区域都有发生的 OutOfMemoryError(简称为“OOM”) 异常的可能。</li>
<li>Java 堆溢出:不停new对象，保证GCRoot可达性</li>
<li>虚拟机栈和本地方法栈溢出：栈容量由 -Xss参数设定 递归</li>
<li>方法区和运行时常量池溢出 大量类<blockquote>
<p>从 JDK8 开始，就变成元数据区的内存溢出。</p>
</blockquote>
</li>
<li>元数据区的内存溢出:-XX:MaxMetaspaceSize=10m</li>
<li>本机直接内存溢出 申请对外内存</li>
</ul>
</li>
<li><p>当出现了内存溢出，怎么排错？</p>
<ol>
<li>首先，控制台查看错误日志。</li>
<li>使用 JDK 自带的 jvisualvm 工具查看系统的堆栈日志。<code>jstat</code> <code>jmap</code></li>
<li>定位出内存溢出的空间：堆，栈还是永久代（JDK8 以后不会出现永久代的内存溢出）。<ol>
<li>如果是堆内存溢出，看是否创建了超大的对象。</li>
<li>如果是栈内存溢出，看是否创建了超大的对象，或者产生了死循环。</li>
</ol>
</li>
</ol>
<blockquote>
<p><a href="https://www.jianshu.com/p/2fdee831ed03">Java内存溢出(OOM)异常完全指南</a></p>
</blockquote>
</li>
<li><p>Java 中会存在内存泄漏吗？</p>
<ul>
<li>Hibernate 的 Session（一级缓存）中的对象属于持久态，垃圾回收器是不会回收这些对象的，然而这些对象中可能存在无用的垃圾对象。</li>
<li>使用 Netty 的堆外的 ByteBuf 对象，在使用完后，并未归还，导致使用的一点一点在泄露。</li>
</ul>
</li>
</ol>
<h2 id="垃圾收集器与内存分配策略"><a href="#垃圾收集器与内存分配策略" class="headerlink" title="垃圾收集器与内存分配策略"></a>垃圾收集器与内存分配策略</h2><ol>
<li><p>什么是垃圾回收机制？</p>
<ul>
<li>Java 中对象是采用 new 或者反射的方法创建的，这些对象的创建都是在堆(Heap)中分配的，所有对象的回收都是由 Java 虚拟机通过垃圾回收机制完成的。GC 为了能够正确释放对象，会监控每个对象的运行状况，对他们的申请、引用、被引用、赋值等状况进行监控。</li>
<li>Java 程序员不用担心内存管理，因为垃圾收集器会自动进行管理。</li>
<li>可以调用下面的方法之一：<code>System.gc()</code> 或 <code>Runtime.getRuntime().gc()</code> ，但 JVM 也可以屏蔽掉显示的垃圾回收调用。</li>
</ul>
</li>
<li><p>为什么不建议在程序中显式的声明 System.gc() ？</p>
<ul>
<li>因为显式声明是做堆内存全扫描，也就是 Full GC ，是需要停止所有的活动的(Stop The World Collection)，对应用很大可能存在影响。</li>
<li>调用 System.gc() 方法后，不会立即执行 Full GC ，而是虚拟机自己决定的。</li>
</ul>
</li>
<li><p>如果一个对象的引用被设置为 null , GC 会立即释放该对象的内存么?</p>
<ul>
<li>不会, 这个对象将会在下一次 GC 循环中被回收。</li>
</ul>
</li>
<li><p><code>finalize()</code> 方法什么时候被调用？它的目的是什么？</p>
<ul>
<li><code>finallize()</code>方法，是在释放该对象内存前由 GC (垃圾回收器)调用。</li>
<li><del>通常建议在这个方法中释放该对象持有的资源，例如持有的堆外内存、和远程服务的长连接。</del></li>
<li>对于一个对象，该方法有且仅会被调用一次。</li>
</ul>
</li>
<li><p>如何判断一个对象是否已经死去？</p>
<ul>
<li>引用计数<ul>
<li>每个对象有一个引用计数属性，新增一个引用时计数加 1 ，引用释放时计数减 1 ，计数为 0 时可以回收。此方法简单，无法解决对象相互循环引用的问题。目前在用的有 Python、ActionScript3 等语言。</li>
</ul>
</li>
<li>可达性分析<ul>
<li>从 GC Roots 开始向下搜索，搜索所走过的路径称为引用链。当一个对象到 GC Roots 没有任何引用链相连时，则证明此对象是不可用的。不可达对象。目前在用的有 Java、C# 等语言。</li>
</ul>
</li>
</ul>
</li>
<li><p>如果 A 和 B 对象循环引用，是否可以被 GC？</p>
<ul>
<li>可以，因为 Java 采用可达性分析的判断方式。</li>
</ul>
</li>
<li><p>在 Java 语言里，可作为 GC Roots 的对象包括以下几种？</p>
<ul>
<li>虚拟机栈（栈帧中的本地变量表）中引用的对象。(参数)</li>
<li>方法区中的类静态属性引用的对象。</li>
<li>方法区中常量引用的对象。</li>
<li>本地方法栈中 JNI(即一般说的 Native 方法)中引用的对象。</li>
</ul>
</li>
<li><p>方法区是否能被回收？</p>
<ul>
<li>方法区可以被回收，但是价值很低，主要回收废弃的常量和无用的类。</li>
<li>如何判断无用的类，需要完全满足如下三个条件：<ul>
<li>该类所有实例都被回收（Java 堆中没有该类的对象）。</li>
<li>加载该类的 ClassLoader 已经被回收。</li>
<li>该类对应的 java.lang.Class 对象没有在任何地方被引用，无法在任何地方利用反射访问该类。</li>
</ul>
</li>
</ul>
</li>
<li><p>Java 对象有哪些引用类型?</p>
<ul>
<li>强引用<ul>
<li>以前我们使用的大部分引用实际上都是强引用，这是使用最普遍的引用。如果一个对象具有强引用，那就类似于必不可少的生活用品，垃圾回收器绝不会回收它。当内存空间不足，Java 虚拟机宁愿抛出 OutOfMemoryError 错误，使程序异常终止，也不会靠随意回收具有强引用的对象来解决内存不足问题。</li>
</ul>
</li>
<li>软引用（SoftReference）<ul>
<li>如果一个对象只具有软引用，那就类似于可有可物的生活用品。如果内存空间足够，垃圾回收器就不会回收它，如果内存空间不足了，就会回收这些对象的内存。只要垃圾回收器没有回收它，该对象就可以被程序使用。软引用可用来实现内存敏感的高速缓存。</li>
<li>软引用可以和一个引用队列（ReferenceQueue）联合使用，如果软引用所引用的对象被垃圾回收，JAVA 虚拟机就会把这个软引用加入到与之关联的引用队列中。</li>
<li><code>Mybatis SoftCache</code></li>
</ul>
</li>
<li>弱引用（WeakReference）<ul>
<li>如果一个对象只具有弱引用，那就类似于可有可物的生活用品。弱引用与软引用的区别在于：只具有弱引用的对象拥有更短暂的生命周期。在垃圾回收器线程扫描它 所管辖的内存区域的过程中，一旦发现了只具有弱引用的对象，不管当前内存空间足够与否，都会回收它的内存。不过，由于垃圾回收器是一个优先级很低的线程， 因此不一定会很快发现那些只具有弱引用的对象。</li>
<li>弱引用可以和一个引用队列（ReferenceQueue）联合使用，如果弱引用所引用的对象被垃圾回收，Java虚拟机就会把这个弱引用加入到与之关联的引用队列中。</li>
<li><code>Mybatis WeakCache</code></li>
</ul>
</li>
<li>虚引用（PhantomReference）<ul>
<li>“虚引用”顾名思义，就是形同虚设，与其他几种引用都不同，虚引用并不会决定对象的生命周期。如果一个对象仅持有虚引用，那么它就和没有任何引用一样，在任何时候都可能被垃圾回收。</li>
<li>虚引用主要用来跟踪对象被垃圾回收的活动。虚引用与软引用和弱引用的一个区别在于：虚引用必须和引用队列（ReferenceQueue）联合使用。当垃 圾回收器准备回收一个对象时，如果发现它还有虚引用，就会在回收对象的内存之前，把这个虚引用加入到与之关联的引用队列中。程序可以通过判断引用队列中是否已经加入了虚引用，来了解被引用的对象是否将要被垃圾回收。程序如果发现某个虚引用已经被加入到引用队列，那么就可以在所引用的对象的内存被回收之前采取必要的行动。</li>
</ul>
</li>
</ul>
</li>
<li><p>WeakReference 与 SoftReference的区别？</p>
<ul>
<li>虽然 WeakReference 与 SoftReference 都有利于提高 GC 和 内存的效率。但是 WeakReference 一旦失去最后一个强引用，就会被 GC 回收而 SoftReference 虽然不能阻止被回收，但是可以延迟到 JVM 内存不足的时候。</li>
</ul>
</li>
<li><p>为什么要有不同的引用类型？</p>
<ul>
<li>不像 C 语言，我们可以控制内存的申请和释放，在 Java 中有时候我们需要适当的控制对象被回收的时机，因此就诞生了不同的引用类型，可以说不同的引用类型实则是对 GC 回收时机不可控的妥协。有以下几个使用场景可以充分的说明：</li>
<li>利用软引用和弱引用解决 OOM 问题。用一个 HashMap 来保存图片的路径和相应图片对象关联的软引用之间的映射关系，在内存不足时，JVM 会自动回收这些缓存图片对象所占用的空间，从而有效地避免了 OOM 的问题.</li>
<li>通过软引用实现 Java 对象的高速缓存。比如我们创建了一 Person 的类，如果每次需要查询一个人的信息，哪怕是几秒中之前刚刚查询过的，都要重新构建一个实例，这将引起大量 Person 对象的消耗，并且由于这些对象的生命周期相对较短，会引起多次 GC 影响性能。此时，通过软引用和 HashMap 的结合可以构建高速缓存，提供性能。</li>
</ul>
</li>
<li><p>JVM 垃圾回收算法？</p>
<ul>
<li><p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/15847757529717.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
</li>
<li><p>标记-清除算法</p>
<ul>
<li>标记-清除算法将垃圾回收分为两个阶段：标记阶段和清除阶段。</li>
<li>一种可行的实现是，在标记阶段，首先通过根节点，标记所有从根节点开始的可达对象。因此，未被标记的对象就是未被引用的垃圾对象（好多资料说标记出要回收的对象，其实明白大概意思就可以了）。然后，在清除阶段，清除所有未被标记的对象。</li>
<li>缺点：<ol>
<li>效率问题，标记和清除两个过程的效率都不高。</li>
<li>空间问题，标记清除之后会产生大量不连续的内存碎片，空间碎片太多可能会导致以后在程序运行过程中需要分配较大的对象时，无法找到足够的连续内存而不得不提前触发另一次垃圾收集动作。</li>
</ol>
</li>
</ul>
</li>
<li><p>标记-整理算法</p>
<ul>
<li><p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/15847758515877.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
</li>
<li><p>标记整理算法，类似与标记清除算法，不过它标记完对象后，不是直接对可回收对象进行清理，而是让所有存活的对象都向一端移动，然后直接清理掉边界以外的内存。</p>
</li>
<li><p>优点：</p>
<ol>
<li>相对标记清除算法，解决了内存碎片问题。</li>
<li>没有内存碎片后，对象创建内存分配也更快速了（可以使用TLAB进行分配）。</li>
</ol>
</li>
<li><p>缺点：</p>
<ol start="3">
<li>效率问题，（同标记清除算法）标记和整理两个过程的效率都不高。</li>
</ol>
</li>
</ul>
</li>
<li><p>复制算法</p>
<ul>
<li><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/15847759518607.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></li>
<li>复制算法，可以解决效率问题，它将可用内存按容量划分为大小相等的两块，每次只使用其中的一块，当这一块内存用完了，就将还存活着的对象复制到另一块上面，然后再把已经使用过的内存空间一次清理掉，这样使得每次都是对整个半区进行内存回收，内存分配时也就不用考虑内存碎片等复杂情况，只要移动堆顶指针，按顺序分配内存即可（还可使用TLAB进行高效分配内存）。</li>
<li>图的上半部分是未回收前的内存区域，图的下半部分是回收后的内存区域。通过图，可以发现不管回收前还是回收后都有一半的空间未被利用。</li>
<li>优点：<ol start="4">
<li>效率高，没有内存碎片。</li>
</ol>
</li>
<li>缺点：<ol start="5">
<li>浪费一半的内存空间。</li>
<li>复制收集算法在对象存活率较高时就要进行较多的复制操作，效率将会变低。</li>
</ol>
</li>
</ul>
</li>
<li><p>分代收集算法</p>
<ul>
<li><p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/15847762126339.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
</li>
<li><p>当前商业虚拟机都是采用分代收集算法，它根据对象存活周期的不同将内存划分为几块，一般是把 Java 堆分为新生代和老年代，然后根据各个年代的特点采用最适当的收集算法。</p>
</li>
<li><p>在新生代中，每次垃圾收集都发现有大批对象死去，只有少量存活，就选用复制算法。</p>
</li>
<li><p>而老年代中，因为对象存活率高，没有额外空间对它进行分配担保，就必须使用“标记清理”或者“标记整理”算法来进行回收。</p>
</li>
<li><p>对象分配策略：</p>
<ul>
<li>对象优先在 Eden 区域分配，如果对象过大直接分配到 Old 区域。</li>
<li>长时间存活的对象进入到 Old 区域。</li>
</ul>
</li>
<li><p>改进自复制算法</p>
<ul>
<li>现在的商业虚拟机都采用这种收集算法来回收新生代，IBM 公司的专门研究表明，新生代中的对象 98% 是“朝生夕死”的，所以并不需要按照 1:1 的比例来划分内存空间，而是将内存分为一块较大的 Eden 空间和两块较小的 Survivor 空间，每次使用 Eden 和其中一块 Survivor 。当回收时，将 Eden 和 Survivor 中还存活着的对象一次性地复制到另外一块 Survivor 空间上，最后清理掉 Eden 和刚才用过的 Survivor 空间。</li>
<li>HotSpot 虚拟机默认 Eden 和 2 块 Survivor 的大小比例是 8:1:1，也就是每次新生代中可用内存空间为整个新生代容量的 90%（80%+10%），只有 10% 的内存会被“浪费”。当然，98% 的对象可回收只是一般场景下的数据，我们没有办法保证每次回收都只有不多于 10% 的对象存活，当 Survivor 空间不够用时，需要依赖其他内存（这里指老年代）进行分配担保（Handle Promotion）。</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>什么是安全点？</p>
<ul>
<li>SafePoint 安全点，顾名思义是指一些特定的位置，当线程运行到这些位置时，线程的一些状态可以被确定(the thread’s representation of it’s Java machine state is well described)，比如记录OopMap 的状态，从而确定 GC Root 的信息，使 JVM 可以安全的进行一些操作，比如开始 GC 。</li>
</ul>
</li>
<li><p>那些位置可以作为安全点</p>
<ul>
<li>循环的末尾 (防止大循环的时候一直不进入 Safepoint ，而其他线程在等待它进入 Safepoint )。</li>
<li>方法返回前。</li>
<li>调用方法的 Call 之后。</li>
<li>抛出异常的位置。</li>
</ul>
</li>
<li><p>GC发生时停止到安全点的方式</p>
<ul>
<li>主动式<ul>
<li> JVM 设置一个全局变量，线程去按照某种策略检查这个变量一旦发现是 SafePoint 就主动挂起。轮询点==安全点</li>
<li> HostSpot 虚拟机采用的是主动式使线程中断。</li>
</ul>
</li>
<li>抢先式<ul>
<li>JVM发出信号，所有线程全部停止，检查非安全点的线程，让其恢复跑到安全点</li>
</ul>
</li>
<li>安全区域<ul>
<li>如果程序长时间不执行，比如线程调用的 sleep 方法，这时候程序无法响应 JVM 中断请求这时候线程无法到达安全点，显然 JVM 也不可能等待程序唤醒，这时候就需要安全区域了。</li>
<li>安全区域是指一段代码片中，引用关系不会发生变化，在这个区域任何地方 GC 都是安全的，安全区域可以看做是安全点的一个扩展。</li>
<li>线程执行到安全区域的代码时，首先标识自己进入了安全区域，这样 GC 时就不用管进入安全区域的线程了.</li>
<li>线程要离开安全区域时就检查 JVM 是否完成了 GC Roots 枚举（或者整个 GC 过程），如果完成就继续执行，如果没有完成就等待直到收到可以安全离开的信号。</li>
</ul>
</li>
</ul>
</li>
<li><p>JVM 垃圾收集器有哪些？</p>
<ul>
<li>新生代收集器<ul>
<li>Serial 收集器</li>
<li>ParNew 收集器<blockquote>
<p>ParNew 收集器，是 Serial 收集器的多线程版。</p>
</blockquote>
</li>
<li>Parallel Scavenge 收集器</li>
</ul>
</li>
<li>老年代收集器<ul>
<li>Serial Old 收集器<blockquote>
<p>Serial Old 收集器，是 Serial 收集器的老年代版本。</p>
</blockquote>
</li>
<li>Parallel Old 收集器<blockquote>
<p>Parallel Old 收集器，是 Parallel Scavenge 收集器的老年代版本。</p>
</blockquote>
</li>
<li>CMS 收集器</li>
</ul>
</li>
<li>新生代 + 老年代收集器<ul>
<li>G1 收集器</li>
<li>ZGC 收集器</li>
</ul>
</li>
<li>对比<table>
<thead>
<tr>
<th>收集器</th>
<th>串行/并行/并发</th>
<th>新生代/老年代</th>
<th>算法</th>
<th>目标</th>
<th>适用场景</th>
</tr>
</thead>
<tbody><tr>
<td>Serial</td>
<td>串行</td>
<td>新生代</td>
<td>复制算法</td>
<td>响应速度</td>
<td>单CPU环境下的Client模式</td>
</tr>
<tr>
<td>Serial Old</td>
<td>串行</td>
<td>老年代</td>
<td>标记-整理</td>
<td>响应速度</td>
<td>单CPU环境下的Client模式、CMS的后备预案</td>
</tr>
<tr>
<td>ParNew</td>
<td>并行</td>
<td>新生代</td>
<td>复制算法</td>
<td>响应速度</td>
<td>多CPU环境时在Server模式下与CMS配合</td>
</tr>
<tr>
<td>Parallel Scavenge</td>
<td>并行</td>
<td>新生代</td>
<td>复制算法</td>
<td>吞吐量</td>
<td>在后台运算而不需要太多交互的任务</td>
</tr>
<tr>
<td>Parallel Old</td>
<td>并行</td>
<td>老年代</td>
<td>标记-整理</td>
<td>吞吐量</td>
<td>在后台运算而不需要太多交互的任务</td>
</tr>
<tr>
<td>CMS</td>
<td>并发</td>
<td>老年代</td>
<td>标记-清除</td>
<td>响应速度</td>
<td>集中在互联网站或B/S系统服务端上的Java应用</td>
</tr>
<tr>
<td>G1</td>
<td>并发</td>
<td>both</td>
<td>标记-整理+复制算法</td>
<td>响应速度</td>
<td>面向服务端应用，将来替换CMS</td>
</tr>
</tbody></table>
</li>
</ul>
</li>
<li><p>JDK默认的垃圾回收器?</p>
<ul>
<li>JDK1.7: Parallel Scavenge（新生代）+Parallel Old（老年代）</li>
<li>JDK1.8: Parallel Scavenge（新生代）+Parallel Old（老年代）</li>
<li>JDK1.9: G1</li>
<li>-XX:+PrintCommandLineFlagsjvm 看默认设置收集器类型</li>
<li>-XX:+PrintGCDetails 通过打印的GC日志的新生代、老年代名称判断</li>
</ul>
</li>
<li><p>G1 和 CMS 的区别？</p>
<ul>
<li>CMS ：并发标记清除。他的主要步骤有：初始收集，并发标记，重新标记，并发清除（删除）、重置。</li>
<li>G1：主要步骤：初始标记，并发标记，重新标记，复制清除（整理）</li>
<li>CMS 的缺点是对 CPU 的要求比较高。G1是将内存化成了多块，所有对内存的大小有很大的要求。</li>
<li>CMS是清除，所以会存在很多的内存碎片。G1是整理，所以碎片空间较小。</li>
<li>G1 和 CMS 都是响应优先，他们的目的都是尽量控制 STW 时间。</li>
<li>G1 和 CMS 的 Full GC 都是单线程 mark sweep compact 算法，直到 JDK10 才优化为并行的。</li>
</ul>
</li>
<li><p>CMS 算法回收过程中 JVM 是否需要暂停？</p>
<ul>
<li>会有短暂的停顿</li>
</ul>
</li>
<li><p>如何使用指定的垃圾收集器</p>
<table>
<thead>
<tr>
<th>配置</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>-XX:+UserSerialGC</td>
<td>串行垃圾收集器</td>
</tr>
<tr>
<td>-XX:+UserParrallelGC</td>
<td>并行垃圾收集器</td>
</tr>
<tr>
<td>-XX:+UseConcMarkSweepGC</td>
<td>并发标记扫描垃圾回收器</td>
</tr>
<tr>
<td>-XX:ParallelCMSThreads</td>
<td>并发标记扫描垃圾回收器 =为使用的线程数量</td>
</tr>
<tr>
<td>-XX:+UseG1GC</td>
<td>G1垃圾回收器</td>
</tr>
</tbody></table>
</li>
<li><p>对象分配规则是什么？</p>
<ul>
<li><p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/15847831931490.png?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10" alt="-w655"></p>
</li>
<li><p>对象优先分配在 Eden 区。</p>
<ul>
<li>如果 Eden 区无法分配，那么尝试把活着的对象放到 Survivor0 中去（Minor GC）<ul>
<li>如果 Survivor0 可以放入，那么放入之后清除 Eden 区。</li>
<li>如果 Survivor0 不可以放入，那么尝试把 Eden 和 Survivor0 的存活对象放到 Survivor1 中。<ul>
<li>如果 Survivor1 可以放入，那么放入 Survivor1 之后清除 Eden 和 Survivor0 ，之后再把 Survivor1 中的对象复制到 Survivor0 中，保持 Survivor1 一直为空。</li>
<li>如果 Survivor1 不可以放入，那么直接把它们放入到老年代中，并清除 Eden 和 Survivor0 ，这个过程也称为分配担保。</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>大对象直接进入老年代（大对象是指需要大量连续内存空间的对象）。</p>
<ul>
<li>这样做的目的是，避免在 Eden 区和两个 Survivor 区之间发生大量的内存拷贝（新生代采用复制算法收集内存）。</li>
</ul>
</li>
<li><p>长期存活的对象进入老年代。</p>
<ul>
<li>虚拟机为每个对象定义了一个年龄计数器，如果对象经过了 1 次 Minor GC 那么对象会进入 Survivor 区，之后每经过一次 Minor GC 那么对象的年龄加 1 ，知道达到阀值对象进入老年区。</li>
</ul>
</li>
<li><p>动态判断对象的年龄。</p>
<ul>
<li>为了更好的适用不同程序的内存情况，虚拟机并不是永远要求对象的年龄必须达到 MaxTenuringThreshold 才能晋升老年代。</li>
<li>如果 Survivor 区中相同年龄的所有对象大小的总和大于 Survivor 空间的一半，年龄大于或等于该年龄的对象可以直接进入老年代。</li>
</ul>
</li>
<li><p>空间分配担保。</p>
<ul>
<li>每次进行 Minor GC 时，JVM 会计算 Survivor 区移至老年区的对象的平均大小，如果这个值大于老年区的剩余值大小则进行一次 Full GC ，如果小于检查 HandlePromotionFailure 设置，如果 true 则只进行 or GC ，如果 false 则进行 Full GC 。</li>
</ul>
</li>
</ul>
</li>
<li><p>为什么新生代内存需要有两个 Survivor 区？</p>
<ul>
<li>解决了碎片化</li>
<li>减少被送到老年代的对象，进而减少Full GC的发生，Survivor的预筛选保证，只有经历16次Minor GC还能在新生代中存活的对象，才会被送到老年代。</li>
</ul>
</li>
<li><p>什么是新生代 GC 和老年代 GC？</p>
<ul>
<li><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/15847835594680.png?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10" alt="-w435"></li>
<li>默认新生代(Young)与老年代(Old)的比例的值为 1:2 (该值可以通过参数 –XX:NewRatio 来指定)。</li>
<li>默认的 Eden:from:to=8:1:1 (可以通过参数 –XX:SurvivorRatio 来设定)。</li>
<li>新生代GC（MinorGC/YoungGC）：指发生在新生代的垃圾收集动作，因为 Java 对象大多都具备朝生夕灭的特性，所以 MinorGC 非常频繁，一般回收速度也比较快。</li>
<li>老年代GC（MajorGC/FullGC）：指发生在老年代的 GC，出现了 MajorGC，经常会伴随至少一次的 MinorGC（但非绝对的，在 Parallel Scavenge 收集器的收集策略里就有直接进行 MajorGC 的策略选择过程）。MajorGC 的速度一般会比 MinorGC 慢 10 倍以上。</li>
</ul>
</li>
<li><p>什么情况下会出现 Young GC？</p>
<ul>
<li>对象优先在新生代 Eden 区中分配，如果 Eden 区没有足够的空间时，就会触发一次 Young GC 。</li>
</ul>
</li>
<li><p>什么情况下回出现 Full GC？</p>
<ul>
<li>Full GC 的触发条件有多个，FULL GC 的时候会 STOP THE WORD 。<ul>
<li>在执行 Young GC 之前，JVM 会进行空间分配担保——如果老年代的连续空间小于新生代对象的总大小（或历次晋升的平均大小），则触发一次 Full GC 。</li>
<li>大对象直接进入老年代，从年轻代晋升上来的老对象，尝试在老年代分配内存时，但是老年代内存空间不够。</li>
<li>显式调用 <code>System.gc()</code> 方法时。</li>
</ul>
</li>
</ul>
</li>
</ol>
<h2 id="虚拟机性能监控与故障处理工具"><a href="#虚拟机性能监控与故障处理工具" class="headerlink" title="虚拟机性能监控与故障处理工具"></a>虚拟机性能监控与故障处理工具</h2><ol>
<li><p>JDK 的命令行工具有哪些可以监控虚拟机？</p>
<ul>
<li>jps ：虚拟机进程状况工具<ul>
<li>JVM Process Status Tool ，显示系统内所有的HotSpot虚拟机进程。</li>
</ul>
</li>
<li>jstat ：虚拟机统计信息监控工具<ul>
<li>JVM statistics Monitoring ，是用于监视虚拟机运行时状态信息的命令，它可以显示出虚拟机进程中的类装载、内存、垃圾收集、JIT编译等运行数据。</li>
<li><code>jstat -gccause PID 1000</code></li>
</ul>
</li>
<li>jinfo ：Java 配置信息工具<ul>
<li>JVM Configuration info ，这个命令作用是实时查看和调整虚拟机运行参数。</li>
</ul>
</li>
</ul>
</li>
<li><p>JDK 的可视化工具有哪些可以监控虚拟机？</p>
<ul>
<li>JConsole对 JVM 中内存，线程和类等的监控。</li>
<li>VisualVM 可以分析内存快照、线程快照、监控内存变化、GC变化等。</li>
<li>JProfile</li>
<li>GC日志分析工具</li>
</ul>
</li>
<li><p>怎么获取 Java 程序使用的内存？</p>
<ul>
<li>可以通过 java.lang.Runtime 类中与内存相关方法来获取剩余的内存，总内存及最大堆内存。通过这些方法你也可以获取到堆使用的百分比及堆内存的剩余空间。<ul>
<li>Runtime.freeMemory() 方法，返回剩余空间的字节数。</li>
<li>Runtime.totalMemory() 方法，总内存的字节数。</li>
<li>Runtime.maxMemory() 方法，返回最大内存的字节数。</li>
</ul>
</li>
</ul>
</li>
<li><p>常见 GC 的优化配置？</p>
<table>
<thead>
<tr>
<th>配置</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>-Xms</td>
<td>初始化堆内存大小</td>
</tr>
<tr>
<td>-Xmx</td>
<td>堆内存最大值</td>
</tr>
<tr>
<td>-Xmn</td>
<td>新生代大小</td>
</tr>
<tr>
<td>-XX:PermSize</td>
<td>初始化永久代大小</td>
</tr>
<tr>
<td>-XX:MaxPermSize</td>
<td>永久代最大容量</td>
</tr>
<tr>
<td>-XX:SurvivorRatio</td>
<td>设置年轻代中 Eden 区与 Survivor 区的比值</td>
</tr>
<tr>
<td>-XX:Xmn</td>
<td>设置年轻代大小</td>
</tr>
</tbody></table>
</li>
<li><p>如何排查线程 Full GC 频繁的问题</p>
<ul>
<li>System.gc()方法的调用</li>
<li>老年代代空间不足</li>
<li>永生区空间不足</li>
<li>统计得到的Minor GC晋升到旧生代的平均大小大于老年代的剩余空间</li>
<li>堆中分配很大的对象</li>
</ul>
</li>
<li><p>类加载器是有了解吗？</p>
<ul>
<li><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/15847900544255.png?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></li>
</ul>
</li>
<li><p>什么是双亲委派模型（Parent Delegation Model）？</p>
<ul>
<li><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/15847901946443.png?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></li>
<li>类加载器 ClassLoader 是具有层次结构的，也就是父子关系</li>
<li>Bootstrap ClassLoader ：根类加载器，负责加载 Java 的核心类，它不是 java.lang.ClassLoader 的子类，而是由 JVM 自身实现。</li>
<li>Extension ClassLoader ：扩展类加载器</li>
<li>Application ClassLoader ：系统(应用)类加载器</li>
<li>该模型要求除了顶层的 Bootstrap 启动类加载器外，其余的类加载器都应当有自己的父类加载器。子类加载器和父类加载器不是以继承（Inheritance）的关系来实现，而是通过组合（Composition）关系来复用父加载器的代码</li>
</ul>
</li>
<li><p>Java 虚拟机是如何判定两个 Java 类是相同的？</p>
<ul>
<li>Java 虚拟机不仅要看类的全名是否相同，还要看加载此类的类加载器是否一样。只有两者都相同的情况，才认为两个类是相同的。即便是同样的字节代码，被不同的类加载器加载之后所得到的类，也是不同的。</li>
</ul>
</li>
<li><p>双亲委派模型的工作过程？</p>
<ul>
<li>当前 ClassLoader 首先从自己已经加载的类中，查询是否此类已经加载，如果已经加载则直接返回原来已经加载的类。</li>
<li>当前 ClassLoader 的缓存中没有找到被加载的类的时候<ul>
<li>委托父类加载器去加载，父类加载器采用同样的策略，首先查看自己的缓存，然后委托父类的父类去加载，一直到 bootstrap ClassLoader。</li>
<li>当所有的父类加载器都没有加载的时候，再由当前的类加载器加载，并将其放入它自己的缓存中，以便下次有加载请求的时候直接返回。</li>
</ul>
</li>
</ul>
</li>
<li><p>为什么优先使用父 ClassLoader 加载类？</p>
<ul>
<li>共享功能：可以避免重复加载，当父亲已经加载了该类的时候，子类不需要再次加载，一些 Framework 层级的类一旦被顶层的 ClassLoader 加载过就缓存在内存里面，以后任何地方用到都不需要重新加载。</li>
<li>隔离功能：主要是为了安全性，避免用户自己编写的类动态替换 Java 的一些核心类，比如 String ，同时也避免了重复加载，因为 JVM 中区分不同类，不仅仅是根据类名，相同的 class 文件被不同的 ClassLoader 加载就是不同的两个类，如果相互转型的话会抛 java.lang.ClassCaseException 。  </li>
</ul>
</li>
</ol>
]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>interview</tag>
      </tags>
  </entry>
  <entry>
    <title>设计模式</title>
    <url>/2021/11/07/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="设计模式-TODO"><a href="#设计模式-TODO" class="headerlink" title="设计模式(TODO)"></a>设计模式(TODO)</h1><h3 id="单例模式"><a href="#单例模式" class="headerlink" title="单例模式"></a>单例模式</h3><ol>
<li>饿汉 <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Singleton_Hunger</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> Singleton_Hunger SINGLETON_HUNGER = <span class="keyword">new</span> Singleton_Hunger();</span><br><span class="line">    <span class="comment">//限制产生多个对象</span></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="title">Singleton_Hunger</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//通过该方法获得实例对象</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> Singleton_Hunger <span class="title">getSingletonHunger</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> SINGLETON_HUNGER;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//类中其他方法，尽量是static</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">doSomething</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>懒汉 <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Singleton_Lazy</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 静态实例变量加上volatile</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">volatile</span> Singleton_Lazy instance;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 私有化构造函数</span></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="title">Singleton_Lazy</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 双重检查锁</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> Singleton_Lazy <span class="title">getInstance</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (instance == <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="keyword">synchronized</span>(Singleton_Lazy.class)&#123;</span><br><span class="line">                <span class="keyword">if</span>(instance == <span class="keyword">null</span>)&#123;</span><br><span class="line">                    instance = <span class="keyword">new</span> Singleton_Lazy();</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> instance;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>静态内部类(懒汉) <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 利用静态内部类特性实现外部类的单例</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SingleTon</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 利用静态内部类特性实现外部类的单例</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">SingleTonBuilder</span> </span>&#123;</span><br><span class="line">        <span class="keyword">private</span> <span class="keyword">static</span> SingleTon singleTon = <span class="keyword">new</span> SingleTon();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 私有化构造函数</span></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="title">SingleTon</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> SingleTon <span class="title">getInstance</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> SingleTonBuilder.singleTon;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        SingleTon instance = getInstance();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>枚举实现 <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 利用静态内部类特性实现外部类的单例</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">enum</span> <span class="title">SingleTon</span> </span>&#123;</span><br><span class="line">    ONE , TWO</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="工厂模式"><a href="#工厂模式" class="headerlink" title="工厂模式"></a>工厂模式</h2><ul>
<li><p>定义一个创建对象的接口，让其子类自己决定实例化哪一个工厂类，工厂模式使其创建过程延迟到子类进行。</p>
</li>
<li><p>主要解决：主要解决接口选择的问题。</p>
</li>
<li><p>实现</p>
  <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 定义接口</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">Shape</span> </span>&#123;</span><br><span class="line">   <span class="function"><span class="keyword">void</span> <span class="title">draw</span><span class="params">()</span></span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 实现接口</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Rectangle</span> <span class="keyword">implements</span> <span class="title">Shape</span> </span>&#123;</span><br><span class="line">   <span class="meta">@Override</span></span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">draw</span><span class="params">()</span> </span>&#123;</span><br><span class="line">      System.out.println(<span class="string">&quot;Inside Rectangle::draw() method.&quot;</span>);</span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Square</span> <span class="keyword">implements</span> <span class="title">Shape</span> </span>&#123;</span><br><span class="line">   <span class="meta">@Override</span></span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">draw</span><span class="params">()</span> </span>&#123;</span><br><span class="line">      System.out.println(<span class="string">&quot;Inside Square::draw() method.&quot;</span>);</span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Circle</span> <span class="keyword">implements</span> <span class="title">Shape</span> </span>&#123;</span><br><span class="line">   <span class="meta">@Override</span></span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">draw</span><span class="params">()</span> </span>&#123;</span><br><span class="line">      System.out.println(<span class="string">&quot;Inside Circle::draw() method.&quot;</span>);</span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 定义工厂</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ShapeFactory</span> </span>&#123;</span><br><span class="line">   <span class="function"><span class="keyword">public</span> Shape <span class="title">getShape</span><span class="params">(String shapeType)</span></span>&#123;</span><br><span class="line">      <span class="keyword">if</span>(shapeType == <span class="keyword">null</span>)&#123;</span><br><span class="line">         <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">      &#125;        </span><br><span class="line">      <span class="keyword">if</span>(shapeType.equalsIgnoreCase(<span class="string">&quot;CIRCLE&quot;</span>))&#123;</span><br><span class="line">         <span class="keyword">return</span> <span class="keyword">new</span> Circle();</span><br><span class="line">      &#125; <span class="keyword">else</span> <span class="keyword">if</span>(shapeType.equalsIgnoreCase(<span class="string">&quot;RECTANGLE&quot;</span>))&#123;</span><br><span class="line">         <span class="keyword">return</span> <span class="keyword">new</span> Rectangle();</span><br><span class="line">      &#125; <span class="keyword">else</span> <span class="keyword">if</span>(shapeType.equalsIgnoreCase(<span class="string">&quot;SQUARE&quot;</span>))&#123;</span><br><span class="line">         <span class="keyword">return</span> <span class="keyword">new</span> Square();</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul>
<h2 id="抽象工厂模式"><a href="#抽象工厂模式" class="headerlink" title="抽象工厂模式"></a>抽象工厂模式</h2><h2 id="建造者模式"><a href="#建造者模式" class="headerlink" title="建造者模式"></a>建造者模式</h2><h2 id="原型模式"><a href="#原型模式" class="headerlink" title="原型模式"></a>原型模式</h2><h2 id="适配器模式"><a href="#适配器模式" class="headerlink" title="适配器模式"></a>适配器模式</h2><h2 id="桥接模式"><a href="#桥接模式" class="headerlink" title="桥接模式"></a>桥接模式</h2><h2 id="过滤器模式"><a href="#过滤器模式" class="headerlink" title="过滤器模式"></a>过滤器模式</h2><h2 id="组合模式"><a href="#组合模式" class="headerlink" title="组合模式"></a>组合模式</h2><h2 id="装饰器模式"><a href="#装饰器模式" class="headerlink" title="装饰器模式"></a>装饰器模式</h2><h2 id="外观模式"><a href="#外观模式" class="headerlink" title="外观模式"></a>外观模式</h2><h2 id="享元模式"><a href="#享元模式" class="headerlink" title="享元模式"></a>享元模式</h2><h2 id="代理模式"><a href="#代理模式" class="headerlink" title="代理模式"></a>代理模式</h2><h2 id="责任链模式"><a href="#责任链模式" class="headerlink" title="责任链模式"></a>责任链模式</h2><h2 id="命令模式"><a href="#命令模式" class="headerlink" title="命令模式"></a>命令模式</h2><h2 id="解释器模式"><a href="#解释器模式" class="headerlink" title="解释器模式"></a>解释器模式</h2><h2 id="迭代器模式"><a href="#迭代器模式" class="headerlink" title="迭代器模式"></a>迭代器模式</h2><h2 id="中介者模式"><a href="#中介者模式" class="headerlink" title="中介者模式"></a>中介者模式</h2><h2 id="备忘录模式"><a href="#备忘录模式" class="headerlink" title="备忘录模式"></a>备忘录模式</h2><h2 id="观察者模式"><a href="#观察者模式" class="headerlink" title="观察者模式"></a>观察者模式</h2><h2 id="状态模式"><a href="#状态模式" class="headerlink" title="状态模式"></a>状态模式</h2><h2 id="空对象模式"><a href="#空对象模式" class="headerlink" title="空对象模式"></a>空对象模式</h2><h2 id="策略模式"><a href="#策略模式" class="headerlink" title="策略模式"></a>策略模式</h2><h2 id="模板模式"><a href="#模板模式" class="headerlink" title="模板模式"></a>模板模式</h2><h2 id="访问者模式"><a href="#访问者模式" class="headerlink" title="访问者模式"></a>访问者模式</h2><h2 id="MVC-模式"><a href="#MVC-模式" class="headerlink" title="MVC 模式"></a>MVC 模式</h2><h2 id="业务代表模式"><a href="#业务代表模式" class="headerlink" title="业务代表模式"></a>业务代表模式</h2><h2 id="组合实体模式"><a href="#组合实体模式" class="headerlink" title="组合实体模式"></a>组合实体模式</h2><h2 id="数据访问对象模式"><a href="#数据访问对象模式" class="headerlink" title="数据访问对象模式"></a>数据访问对象模式</h2><h2 id="前端控制器模式"><a href="#前端控制器模式" class="headerlink" title="前端控制器模式"></a>前端控制器模式</h2><h2 id="拦截过滤器模式"><a href="#拦截过滤器模式" class="headerlink" title="拦截过滤器模式"></a>拦截过滤器模式</h2><h2 id="服务定位器模式"><a href="#服务定位器模式" class="headerlink" title="服务定位器模式"></a>服务定位器模式</h2><h2 id="传输对象模式"><a href="#传输对象模式" class="headerlink" title="传输对象模式"></a>传输对象模式</h2>]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>interview</tag>
      </tags>
  </entry>
  <entry>
    <title>Redis面试题</title>
    <url>/2021/11/07/Redis%E9%9D%A2%E8%AF%95%E9%A2%98/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="Redis"><a href="#Redis" class="headerlink" title="Redis"></a>Redis</h1><ol>
<li>什么是 Redis ？<ul>
<li>Redis是一个基于内存的高性能 Key-Value 数据库</li>
</ul>
</li>
<li>Redis 有什么优点？<ul>
<li>速度快:数据存在内存中，类似于 HashMap ，HashMap 的优势就是查找和操作的时间复杂度都是O (1) 每秒可以处理超过 10 万次读写操作，是已知性能最快的 Key-Value 数据库。</li>
<li>支持丰富数据类型:String ，List，Set，Sorted Set，Hash 五种基础的数据结构。</li>
<li>丰富的特性: 订阅发布 Pub / Sub 功能,Key 过期策略,事务,支持多个 DB,计数</li>
<li>持久化存储: Redis 提供 RDB 和 AOF 两种数据的持久化存储方案，解决内存数据库最担心的万一 Redis 挂掉，数据会消失掉。</li>
<li>高可用:内置 Redis Sentinel ，提供高可用方案，实现主从故障自动转移;内置 Redis Cluster ，提供集群方案，实现基于槽的分片方案，从而支持更大的 Redis 规模。</li>
</ul>
</li>
<li>Redis 有什么缺点？<ul>
<li>由于 Redis 是内存数据库，所以，单台机器，存储的数据量，跟机器本身的内存大小。虽然 Redis 本身有 Key 过期策略，但是还是需要提前预估和节约内存。如果内存增长过快，需要定期删除数据。(可使用 Redis Cluster、Codis 等方案，对 Redis 进行分区，从单机 Redis 变成集群 Redis 。)</li>
<li>如果进行完整重同步，由于需要生成 RDB 文件，并进行传输，会占用主机的 CPU ，并会消耗现网的带宽。不过 Redis2.8 版本，已经有部分重同步的功能，但是还是有可能有完整重同步的。比如，新上线的备机。</li>
<li>修改配置文件，进行重启，将硬盘中的数据加载进内存，时间比较久。在这个过程中，Redis 不能提供服务。</li>
</ul>
</li>
<li>请说说 Redis 的线程模型？<ul>
<li>Redis 是非阻塞 IO ，多路复用。</li>
</ul>
</li>
<li>为什么 Redis 单线程模型也能效率这么高？<ul>
<li> C 语言实现。</li>
<li> 纯内存操作。</li>
<li> 基于非阻塞的 IO 多路复用机制。</li>
<li> 单线程，避免了多线程的频繁上下文切换问题。</li>
</ul>
</li>
<li>Redis 是单线程的，如何提高多核 CPU 的利用率？<ul>
<li>可以在同一个服务器部署多个 Redis 的实例，并把他们当作不同的服务器来使用，在某些时候，无论如何一个服务器是不够的， 所以，如果你想使用多个 CPU ，你可以考虑一下分区。</li>
</ul>
</li>
<li>Redis 有几种持久化方式？<ul>
<li>【全量】RDB 持久化<ul>
<li>在指定的时间间隔内将内存中的数据集快照写入磁盘。实际操作过程是，fork 一个子进程，先将数据集写入临时文件，写入成功后，再替换之前的文件，用二进制压缩存储</li>
<li>RDB 优点：<ul>
<li>灵活设置备份频率和周期。你可能打算每个小时归档一次最近 24 小时的数据，同时还要每天归档一次最近 30 天的数据。通过这样的备份策略，一旦系统出现灾难性故障，我们可以非常容易的进行恢复</li>
<li>非常适合冷备份，对于灾难恢复而言，RDB 是非常不错的选择。因为我们可以非常轻松的将一个单独的文件压缩后再转移到其它存储介质上。推荐，可以将这种完整的数据文件发送到一些远程的安全存储上去，</li>
<li>性能最大化。对于 Redis 的服务进程而言，在开始持久化时，它唯一需要做的只是 fork 出子进程，之后再由子进程完成这些持久化的工作，这样就可以极大的避免服务进程执行 IO 操作了。也就是说，RDB 对 Redis 对外提供的读写服务，影响非常小，可以让 Redis 保持高性能。</li>
<li>恢复更快。相比于 AOF 机制，RDB 的恢复速度更更快，更适合恢复数据，特别是在数据集非常大的情况</li>
</ul>
</li>
<li>RDB 缺点：<ul>
<li>如果想保证数据的高可用性，即最大限度的避免数据丢失，那么 RDB 将不是一个很好的选择。因为系统一旦在定时持久化之前出现宕机现象，此前没有来得及写入磁盘的数据都将丢失。</li>
<li>由于 RDB 是通过 fork 子进程来协助完成数据持久化工作的，因此，如果当数据集较大时，可能会导致整个服务器停止服务几百毫秒，甚至是 1 秒钟。</li>
</ul>
</li>
</ul>
</li>
<li>【增量】AOF持久化<ul>
<li>以日志的形式记录服务器所处理的每一个写、删除操作，查询操作不会记录，以文本的方式记录，可以打开文件看到详细的操作记录。</li>
<li>AOF 优点<ul>
<li>该机制可以带来更高的数据安全性，即数据持久性。Redis 中提供了 3 种同步策略，即每秒同步、每修改(执行一个命令)同步和不同步。</li>
<li>由于该机制对日志文件的写入操作采用的是 append 模式，因此在写入过程中即使出现宕机现象，也不会破坏日志文件中已经存在的内容。（redis-check-aof）</li>
<li>如果 AOF 日志过大，Redis 可以自动启用 rewrite 机制。即使出现后台重写操作，也不会影响客户端的读写。因为在 rewrite log 的时候，会对其中的指令进行压缩，创建出一份需要恢复数据的最小日志出来。再创建新日志文件的时候，老的日志文件还是照常写入。当新的 merge 后的日志文件 ready 的时候，再交换新老日志文件即可。</li>
</ul>
</li>
<li>AOF 缺点<ul>
<li>对于相同数量的数据集而言，AOF 文件通常要大于 RDB 文件。RDB 在恢复大数据集时的速度比 AOF 的恢复速度要快。</li>
<li>根据同步策略的不同，AOF 在运行效率上往往会慢于 RDB 。总之，每秒同步策略的效率是比较高的，同步禁用策略的效率和 RDB 一样高效。</li>
<li>以前 AOF 发生过 bug ，就是通过 AOF 记录的日志，进行数据恢复的时候，没有恢复一模一样的数据出来。所以说，类似 AOF 这种较为复杂的基于命令日志/merge/回放的方式，比基于 RDB 每次持久化一份完整的数据快照文件的方式，更加脆弱一些，容易有 bug 。不过 AOF 就是为了避免 rewrite 过程导致的 bug ，因此每次 rewrite 并不是基于旧的指令日志进行 merge 的，而是基于当时内存中的数据进行指令的重新构建，这样健壮性会好很多。</li>
</ul>
</li>
</ul>
</li>
<li>如何选择：<ul>
<li>不要仅仅使用 RDB，因为那样会导致你丢失很多数据。</li>
<li>也不要仅仅使用 AOF，因为那样有两个问题，第一，你通过 AOF 做冷备，没有 RDB 做冷备，来的恢复速度更快; 第二，RDB 每次简单粗暴生成数据快照，更加健壮，可以避免 AOF 这种复杂的备份和恢复机制的 bug 。</li>
<li>Redis 支持同时开启开启两种持久化方式，我们可以综合使用 AOF 和 RDB 两种持久化机制，用 AOF 来保证数据不丢失，作为数据恢复的第一选择; 用 RDB 来做不同程度的冷备，在 AOF 文件都丢失或损坏不可用的时候，还可以使用 RDB 来进行快速的数据恢复。</li>
</ul>
</li>
<li>AOF rewrite 机制，和 RDB 一样，也需要 fork 出一次子进程，如果 Redis 内存比较大，可能会因为 fork 阻塞下主进程。</li>
</ul>
</li>
<li>Redis 有哪几种数据“淘汰”策略？<ul>
<li>Redis 内存数据集大小上升到一定大小的时候，就会进行数据淘汰策略。Redis 提供了 6 种数据淘汰策略：<ul>
<li>volatile-lru</li>
<li>volatile-ttl</li>
<li>volatile-random</li>
<li>allkeys-lru</li>
<li>allkeys-random</li>
<li>【默认策略】no-enviction</li>
</ul>
</li>
</ul>
</li>
<li>Redis LRU 算法<ul>
<li> Redis 的 LRU 算法，并不是一个严格的 LRU 实现。这意味着 Redis 不能选择最佳候选键来回收，也就是最久未被访问的那些键。相反，Redis 会尝试执行一个近似的 LRU 算法，通过采样一小部分键，然后在采样键中回收最适合(拥有最久未被访问时间)的那个。</li>
<li> Redis 没有使用真正实现严格的 LRU 算是的原因是，因为消耗更多的内存。然而对于使用 Redis 的应用来说，使用近似的 LRU 算法，事实上是等价的。</li>
</ul>
</li>
<li>MySQL 里有 2000w 数据，Redis 中只存 20w 的数据，如何保证 Redis 中的数据都是热点数据？<ul>
<li>选择 volatile-lru 或 allkeys-lru 这两个基于 LRU 算法的淘汰策略。</li>
</ul>
</li>
<li>Redis 回收进程如何工作的？<ul>
<li>一个客户端运行了新的写命令，添加了新的数据。</li>
<li>Redis 检查内存使用情况，如果大于 maxmemory 的限制, 则根据设定好的策略进行回收。</li>
<li>Redis 执行新命令。</li>
<li>不断地穿越内存限制的边界，通过不断达到边界然后不断地回收回到边界以下（跌宕起伏）。</li>
</ul>
</li>
<li>如果有大量的 key 需要设置同一时间过期，一般需要注意什么？<ul>
<li>如果大量的 key 过期时间设置的过于集中，到过期的那个时间点，Redis可能会出现短暂的卡顿现象。</li>
<li>调大 hz 参数，每次过期的 key 更多，从而最终达到避免一次过期过多。<ul>
<li> hz 参数代表了一秒钟内，后台任务期望被调用的次数,hz 调大将会提高 Redis 主动淘汰的频率</li>
</ul>
</li>
<li>一般需要在时间上加一个随机值，使得过期时间分散一些。</li>
</ul>
</li>
<li>Redis 有哪些数据结构？<ul>
<li>初级<ul>
<li>字符串 String</li>
<li>字典Hash</li>
<li>列表List</li>
<li>集合Set</li>
<li>有序集合 SortedSet</li>
</ul>
</li>
<li>中级<ul>
<li>HyperLogLog</li>
<li>Geo</li>
<li>Bitmap</li>
</ul>
</li>
<li>高级<ul>
<li>BloomFilter</li>
<li>RedisSearch</li>
<li>Redis-ML</li>
<li>JSON</li>
</ul>
</li>
</ul>
</li>
<li>聊聊 Redis 使用场景/为什么使用redis<ul>
<li>数据缓存</li>
<li>会话缓存</li>
<li>时效性数据</li>
<li>访问频率</li>
<li>计数器</li>
<li>社交列表</li>
<li>记录用户判定信息</li>
<li>交集、并集和差集</li>
<li>热门列表与排行榜</li>
<li>最新动态</li>
<li>消息队列</li>
<li>分布式锁</li>
</ul>
</li>
<li>Redis 支持的 Java 客户端都有哪些？<ol>
<li>Redisson：封装好</li>
<li>Jedis：命令全</li>
<li>Lettuce：是一个可伸缩线程安全的 Redis 客户端。多个线程可以共享同一个 RedisConnection 。它利用优秀 Netty NIO 框架来高效地管理多个连接。</li>
</ol>
</li>
<li>如何使用 Redis 实现分布式锁？<ul>
<li>正确的获得锁：set 指令附带 nx 参数，保证有且只有一个进程获得到。</li>
<li>正确的释放锁：使用 Lua 脚本，比对锁持有的是不是自己。如果是，则进行删除来释放。</li>
<li>超时的自动释放锁：set 指令附带 expire 参数，通过过期机制来实现超时释放。</li>
<li>未获得到锁的等待机制：sleep 或者基于 Redis 的订阅 Pub/Sub 机制。一些业务场景，可能需要支持获得不到锁，直接返回 false ，不等待。</li>
<li>锁超时的处理：告警 + 后台线程自动续锁的超时时间。通过这样的机制，保证有且仅有一个线程，正在持有锁。</li>
<li>set 指令：<code>SET key value [EX seconds] [PX milliseconds] [NX|XX]</code></li>
<li>Redlock: Redisson 实现，所有master超过半数</li>
</ul>
</li>
<li>Redis 分布式锁 对比 Zookeeper 分布式锁<ul>
<li>从可靠性上来说，Zookeeper 分布式锁好于 Redis 分布式锁。</li>
<li>从性能上来说，Redis 分布式锁好于 Zookeeper 分布式锁。</li>
</ul>
</li>
<li>如何使用 Redis 实现消息队列？<ul>
<li>使用 list 结构作为队列，rpush 生产消息，lpop 消费消息。当 lpop 没有消息的时候，要适当 sleep 一会再重试。</li>
<li>如果对方追问可不可以不用 sleep 呢？list 还有个指令叫 blpop ，在没有消息的时候，它会阻塞住直到消息到来。</li>
<li>如果对方追问能不能生产一次消费多次呢？使用 pub / sub 主题订阅者模式，可以实现 1:N 的消息队列。</li>
<li>如果对方追问 pub / sub 有什么缺点？在消费者下线的情况下，生产的消息会丢失，得使用专业的消息队列如 rabbitmq 等。</li>
<li>果对方追问 redis 如何实现延时队列？使用 sortedset ，拿时间戳作为 score ，消息内容作为 key 调用 zadd 来生产消息，消费者用 zrangebyscore 指令获取 N 秒之前的数据轮询进行处理。</li>
</ul>
</li>
<li>Redis 如何做大量数据插入？<ul>
<li>Redis-cli 支持一种新的被称之为 pipe mode 的新模式用于执行大量数据插入工作。</li>
</ul>
</li>
<li>什么是 Redis 事务？<ul>
<li>MULTI / EXEC / DISCARD / WATCH 这四个命令是我们实现事务的基石<ul>
<li>在事务中的所有命令都将会被串行化的顺序执行，事务执行期间，Redis 不会再为其它客户端的请求提供任何服务，从而保证了事物中的所有命令被原子的执行。</li>
<li>和关系型数据库中的事务相比，在 Redis 事务中如果有某一条命令执行失败，其后的命令仍然会被继续执行。</li>
<li>我们可以通过 MULTI 命令开启一个事务，有关系型数据库开发经验的人可以将其理解为 “BEGIN TRANSACTION” 语句。在该语句之后执行的命令，都将被视为事务之内的操作，最后我们可以通过执行 EXEC / DISCARD 命令来提交 / 回滚该事务内的所有操作。这两个 Redis 命令，可被视为等同于关系型数据库中的 COMMIT / ROLLBACK 语句。(开启事务后，所有语句，发送给 Redis Server ，都会暂存在 Server 中。)</li>
<li>在事务开启之前，如果客户端与服务器之间出现通讯故障并导致网络断开，其后所有待执行的语句都将不会被服务器执行。然而如果网络中断事件是发生在客户端执行 EXEC 命令之后，那么该事务中的所有命令都会被服务器执行。</li>
</ul>
</li>
</ul>
</li>
<li>如何实现 Redis CAS 操作？<ul>
<li>在 Redis 的事务中，WATCH 命令可用于提供 CAS(check-and-set) 功能。</li>
<li>假设我们通过 WATCH 命令在事务执行之前监控了多个 keys ，倘若在 WATCH 之后有任何 Key 的值发生了变化，EXEC 命令执行的事务都将被放弃，同时返回 nil 应答以通知调用者事务执行失败</li>
</ul>
</li>
<li>Redis 集群都有哪些方案？<ul>
<li>Redis Sentinel<ul>
<li>体量较小时，选择 Redis Sentinel ，单主 Redis 足以支撑业务。</li>
</ul>
</li>
<li>Redis Cluster<ul>
<li>体量较大时，选择 Redis Cluster ，通过分片，使用更多内存。</li>
</ul>
</li>
<li>多大体量需要使用 Redis Cluster 呢<ol>
<li>一次 RDB 时间随着内存越大，会变大越来越久。同时，一次 fork 的时间也会变久。还有，重启通过 RDB 文件，或者 AOF 日志，恢复时间都会变长。</li>
<li>体量大之后，读写的 QPS 势必比体量小的时候打的多，那么使用 Redis Cluster 相比 Redis Sentinel ，可以分散读写压力到不同的集群中。</li>
</ol>
</li>
</ul>
</li>
<li>什么是 Redis 主从同步？<ul>
<li>Redis 的主从同步(replication)机制，允许 Slave 从 Master 那里，通过网络传输拷贝到完整的数据备份，从而达到主从机制。</li>
<li>主数据库可以进行读写操作，当发生写操作的时候自动将数据同步到从数据库，而从数据库一般是只读的，并接收主数据库同步过来的数据。</li>
<li>一个主数据库可以有多个从数据库，而一个从数据库只能有一个主数据库。</li>
<li>第一次同步时，主节点做一次 bgsave 操作，并同时将后续修改操作记录到内存 buffer ，待完成后将 RDB 文件全量同步到复制节点，复制节点接受完成后将 RDB 镜像加载到内存。加载完成后，再通知主节点将期间修改的操作记录同步到复制节点进行重放就完成了同步过程。</li>
</ul>
</li>
<li>Redis Cluster 的主从复制模型是怎样的？<ul>
<li>为了使在部分节点失败或者大部分节点无法通信的情况下集群仍然可用，所以集群使用了主从复制模型，每个节点都会有 N-1 个复制节点。</li>
<li>所以，Redis Cluster 可以说是 Redis Sentinel 带分片的加强版。也可以说：<ul>
<li>Redis Sentinel 着眼于高可用，在 master 宕机时会自动将 slave 提升为 master ，继续提供服务。</li>
<li>Redis Cluster 着眼于扩展性，在单个 Redis 内存不足时，使用 Cluster 进行分片存储。</li>
</ul>
</li>
</ul>
</li>
<li>Redis Cluster 方案什么情况下会导致整个集群不可用？<ul>
<li>有 A，B，C 三个节点的集群，在没有复制模型的情况下，如果节点 B 宕机了，那么整个集群就会以为缺少 5501-11000 这个范围的槽而不可用。当然，这种情况也可以配置 cluster-require-full-coverage=no ，整个集群无需所有槽位覆盖。</li>
</ul>
</li>
<li>Redis Cluster 会有写操作丢失吗？为什么？<ul>
<li>Redis 并不能保证数据的强一致性，而是【异步复制】，这意味这在实际中集群在特定的条件下可能会丢失写操作。</li>
<li>无论对于 Redis Sentinel 还是 Redis Cluster 方案，都是通过主从复制，所以在数据的复制方面，都存在相同的情况。</li>
</ul>
</li>
<li>Redis 集群如何选择数据库？<ul>
<li>Redis 集群目前无法做数据库选择，默认在 0 数据库。</li>
</ul>
</li>
<li>请说说生产环境中的 Redis 是怎么部署的？<ul>
<li>Redis Cluster ，10 台机器，5 台机器部署了 Redis 主实例，另外 5 台机器部署了 Redis 的从实例，每个主实例挂了一个从实例，5 个节点对外提供读写服务，每个节点的读写高峰 qps 可能可以达到每秒 5 万，5 台机器最多是 25 万读写请求每秒。</li>
<li>机器是什么配置？32G 内存 + 8 核 CPU + 1T 磁盘，但是分配给 Redis 进程的是 10G 内存，一般线上生产环境，Redis 的内存尽量不要超过 10G，超过 10G 可能会有问题。那么，5 台机器对外提供读写，一共有 50G 内存。</li>
<li>因为每个主实例都挂了一个从实例，所以是高可用的，任何一个主实例宕机，都会自动故障迁移，Redis 从实例会自动变成主实例继续提供读写服务。</li>
<li>你往内存里写的是什么数据？每条数据的大小是多少？商品数据，每条数据是 10kb 。100 条数据是 1mb ，10 万条数据是 1G 。常驻内存的是 200 万条商品数据，占用内存是 20G ，仅仅不到总内存的 50% 。目前高峰期每秒就是 3500 左右的请求量。</li>
<li>公司体量大了之后，建议是一个业务线独占一个或多个 Redis Cluster 集群，实现好业务线与业务线之间的隔离。</li>
</ul>
</li>
<li>什么是 Redis 分区？<ul>
<li>分区是分割数据到多个Redis实例的处理过程，因此每个实例只保存key的一个子集。</li>
<li>分区的优势:通过利用多台计算机内存的和值，允许我们构造更大的数据库。通过多核和多台计算机，允许我们扩展计算能力；通过多台计算机和网络适配器，允许我们扩展网络带宽。</li>
<li>分区的不足:不支持涉及多个key的操作通</li>
<li>分区类型:范围分区 &amp;&amp; 哈希分区</li>
</ul>
</li>
<li>Redis 有哪些重要的健康指标？<ul>
<li>存活情况</li>
<li>连接数</li>
<li>阻塞客户端数量</li>
<li>使用内存峰值</li>
<li>内存碎片率</li>
<li>缓存命中率</li>
<li>OPS</li>
<li>持久化</li>
<li>失效KEY</li>
<li>慢日志</li>
</ul>
</li>
<li>一个 Redis 实例最多能存放多少的 keys？List、Set、Sorted Set 他们最多能存放多少元素？<ul>
<li>Redis 可以处理多达 2^32 的 keys ，并且在实际中进行了测试，每个实例至少存放了 2 亿 5 千万的 keys。</li>
<li>任何 list、set、和 sorted set 都可以放 2^32 </li>
</ul>
</li>
<li>假如 Redis 里面有 1 亿个 key，其中有 10w 个 key 是以某个固定的已知的前缀开头的，如果将它们全部找出来？<ul>
<li>keys 指令可以扫出指定模式的 key 列表。</li>
<li>对方接着追问：如果这个 Redis 正在给线上的业务提供服务，那使用 keys 指令会有什么问题？</li>
<li>这个时候你要回答 Redis 关键的一个特性：Redis 的单线程的。keys 指令会导致线程阻塞一段时间，线上服务会停顿，直到指令执行完毕，服务才能恢复。这个时候可以使用 scan 指令，scan 指令可以无阻塞的提取出指定模式的 key 列表，但是会有一定的重复概率，在客户端做一次去重就可以了，但是整体所花费的时间会比直接用 keys 指令长。</li>
</ul>
</li>
<li>Redis 常见的性能问题都有哪些？如何解决？<ul>
<li>Master 最好不要做任何持久化工作，如 RDB 内存快照和 AOF 日志文件。</li>
<li></li>
</ul>
</li>
<li>Redis雪崩<ul>
<li><ul>
<li><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/15849210200503.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></li>
</ul>
</li>
<li>同一时间大量缓存失效，请求直接落到数据库，造成数据库崩溃</li>
<li>解决方式：<ul>
<li>失效期设置永不过期</li>
<li>失效期添加随机数</li>
<li>设置多层缓存，redis失效的情况下，使用内部缓存</li>
</ul>
</li>
</ul>
</li>
</ol>
<ul>
<li>缓存穿透，缓存击穿,雪崩的区别<ul>
<li>缓存穿透是指缓存和数据库中都没有的数据，比如id=-1导致数据库压力过大<ul>
<li>参数校验保证数据合法性</li>
<li>缓存假结果，添加失效期</li>
<li><ul>
<li><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/15849213541298.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></li>
</ul>
</li>
</ul>
</li>
<li>缓存击穿是指一个Key非常热点，在不停的扛着大并发，大并发集中对这一个点进行访问，当这个Key在失效的瞬间，持续的大并发就穿破缓存，直接请求数据库<ul>
<li>设置热点数据永远不过期</li>
</ul>
</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>interview</tag>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title>Dubbo面试题</title>
    <url>/2021/11/07/Dubbo%E9%9D%A2%E8%AF%95%E9%A2%98/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="Dubbo"><a href="#Dubbo" class="headerlink" title="Dubbo"></a>Dubbo</h1><ol>
<li><p>Dubbo 有几种配置方式？</p>
<ul>
<li>XML 配置</li>
<li>注解配置</li>
<li>Java API 配置</li>
<li>属性配置</li>
</ul>
</li>
<li><p>Dubbo 如何和 Spring Boot 进行集成？</p>
<ul>
<li>官方提供提供了集成库 dubbo-spring-boot</li>
</ul>
</li>
<li><p>Dubbo 框架的分层设计</p>
<ul>
<li><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/15850280657994.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></li>
<li>总体分成 Business、RPC、Remoting 三大层<ul>
<li>Service 业务层：业务代码的接口与实现。我们实际使用 Dubbo 的业务层级。接口层，给服务提供者和消费者来实现的。</li>
<li>RPC层：<ul>
<li>config 配置层：主要是对 Dubbo 进行各种配置的。</li>
<li>proxy 服务代理层：服务代理层，无论是 consumer 还是 provider，Dubbo 都会给你生成代理，代理之间进行网络通信。（ 对比Spring Cloud 体系，可以类比成 Feign 对于 consumer ，Spring MVC 对于 provider 。）</li>
<li>registry 注册中心层：服务注册层，负责服务的注册与发现。（对比 Spring Cloud 体系，可以类比成 Eureka Client ）</li>
<li>cluster 路由层：封装多个服务提供者的路由以及负载均衡，将多个实例组合成一个服务。（对比Spring Cloud 体系，可以类比成 Ribbon ）</li>
<li>monitor 监控层：对 rpc 接口的调用次数和调用时间进行监控。</li>
</ul>
</li>
<li>Remoting：<ul>
<li>protocol 远程调用层：远程调用层，封装 rpc 调用。</li>
<li>exchange 信息交换层：信息交换层，封装请求响应模式，同步转异步。</li>
<li>transport 网络传输层：抽象 mina 和 netty 为统一接口。</li>
<li>serialize 数据序列化层：可复用的一些工具，扩展接口为 Serialization, ObjectInput, ObjectOutput</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>Dubbo 调用流程</p>
<ul>
<li><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/15850284072280.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></li>
<li><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/15850285070885.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></li>
<li>Provider<ul>
<li>第 0 步，start 启动服务。</li>
<li>第 1 步，register 注册服务到注册中心。</li>
</ul>
</li>
<li>Consumer<ul>
<li>第 2 步，subscribe 向注册中心订阅服务。<ul>
<li>注意，只订阅使用到的服务。</li>
<li>再注意，首次会拉取订阅的服务列表，缓存在本地。</li>
<li>【异步】第 3 步，notify 当服务发生变化时，获取最新的服务列表，更新本地缓存。</li>
</ul>
</li>
</ul>
</li>
<li>invoke 调用<ul>
<li>Consumer 直接发起对 Provider 的调用，无需经过注册中心。而对多个 Provider 的负载均衡，Consumer 通过 cluster 组件实现。</li>
</ul>
</li>
<li>count 监控<ul>
<li>【异步】Consumer 和 Provider 都异步通知监控中心。</li>
</ul>
</li>
</ul>
</li>
<li><p>Dubbo 调用是同步的吗？</p>
<ul>
<li>默认情况下，调用是同步的方式。</li>
</ul>
</li>
<li><p>谈谈对 Dubbo 的异常处理机制？</p>
<ul>
<li>dubbo的异常处理类是com.alibaba.dubbo.rpc.filter.ExceptionFilter 类,源码这里就不贴了.归纳下对异常的处理分为下面几类:<ul>
<li>1)如果provider实现了GenericService接口,直接抛出</li>
<li>2)如果是checked异常，直接抛出</li>
<li>3)在方法签名上有声明，直接抛出</li>
<li>4)异常类和接口类在同一jar包里，直接抛出</li>
<li>5)是JDK自带的异常，直接抛出</li>
<li>6)是Dubbo本身的异常，直接抛出</li>
<li>7)否则，包装成RuntimeException抛给客户端（防止客户端反序列化失败.前面几种情况都能保证反序列化正常.）</li>
</ul>
</li>
<li><a href="https://blog.csdn.net/qq315737546/article/details/53915067">dubbo异常处理</a></li>
</ul>
</li>
<li><p>Dubbo 如何做参数校验？</p>
<ul>
<li>参数校验功能，通过参数校验过滤器 ValidationFilter 来实现。</li>
<li>ValidationFilter 在 Dubbo Provider 和 Consumer 都可生效。<ul>
<li>如果我们将校验注解写在 Service 接口的方法上，那么 Consumer 在本地就会校验。如果校验不通过，直接抛出校验失败的异常，不会发起 Dubbo 调用。</li>
<li>如果我们将校验注解写在 Service 实现的方法上，那么 Consumer 在本地不会校验，而是由 Provider 校验。</li>
</ul>
</li>
</ul>
</li>
<li><p>Dubbo 可以对调用结果进行缓存吗?</p>
<ul>
<li>Dubbo 通过 CacheFilter 过滤器，提供结果缓存的功能，且既可以适用于 Consumer 也可以适用于 Provider 。</li>
<li>通过结果缓存，用于加速热门数据的访问速度，Dubbo 提供声明式缓存，以减少用户加缓存的工作量。</li>
<li>Dubbo 目前提供三种实现：<ul>
<li>lru ：基于最近最少使用原则删除多余缓存，保持最热的数据被缓存。</li>
<li>threadlocal ：当前线程缓存，比如一个页面渲染，用到很多 portal，每个 portal 都要去查用户信息，通过线程缓存，可以减少这种多余访问。</li>
<li>jcache ：与 JSR107 集成，可以桥接各种缓存实现。</li>
</ul>
</li>
</ul>
</li>
<li><p>注册中心挂了还可以通信吗？</p>
<ul>
<li>可以。对于正在运行的 Consumer 调用 Provider 是不需要经过注册中心，所以不受影响。并且，Consumer 进程中，内存已经缓存了 Provider 列表。</li>
<li>此时 Provider 如果下线呢？<ul>
<li>如果 Provider 是正常关闭，它会主动且直接对和其处于连接中的 Consumer 们，发送一条“我要关闭”了的消息。那么，Consumer 们就不会调用该 Provider ，而调用其它的 Provider 。</li>
<li>因为 Consumer 也会持久化 Provider 列表到本地文件。所以，此处如果 Consumer 重启，依然能够通过本地缓存的文件，获得到 Provider 列表。</li>
<li>一般情况下，注册中心是一个集群，如果一个节点挂了，Dubbo Consumer 和 Provider 将自动切换到集群的另外一个节点上。</li>
</ul>
</li>
</ul>
</li>
<li><p>Dubbo 在 Zookeeper 存储了哪些信息？</p>
<pre><code> - ![](https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/15850292909765.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10)
</code></pre>
<ul>
<li>服务提供者启动时: 向 /dubbo/com.foo.BarService/providers 目录下写入自己的 URL 地址</li>
<li>服务消费者启动时: 订阅 /dubbo/com.foo.BarService/providers 目录下的提供者 URL 地址。并向 /dubbo/com.foo.BarService/consumers 目录下写入自己的 URL 地址（服务消费者启动后，不仅仅订阅了 “providers” 分类，也订阅了 “routes” “configurations” 分类。）</li>
<li>监控中心启动时: 订阅 /dubbo/com.foo.BarService 目录下的所有提供者和消费者 URL 地址。</li>
<li>Zookeeper 的节点层级，自上而下是：<ul>
<li> Root 层：根目录，可通过 &lt;dubbo:registry group=”dubbo” /&gt; 的 “group” 设置 Zookeeper 的根节点，缺省使用 “dubbo” 。</li>
<li> Service 层：服务接口全名。</li>
<li> Type 层：分类。目前除了我们在图中看到的 “providers”( 服务提供者列表 ) “consumers”( 服务消费者列表 ) 外，还有 “routes”( 路由规则列表 ) 和 “configurations”( 配置规则列表 )。</li>
<li> URL 层：URL ，根据不同 Type 目录，下面可以是服务提供者 URL 、服务消费者 URL 、路由规则 URL 、配置规则 URL 。</li>
<li> 实际上 URL 上带有 “category” 参数，已经能判断每个 URL 的分类，但是 Zookeeper 是基于节点目录订阅的，所以增加了 Type 层。</li>
</ul>
</li>
</ul>
</li>
<li><p>Dubbo Provider 如何实现优雅停机？</p>
<ul>
<li>Dubbo 是通过 JDK 的 ShutdownHook 来完成优雅停机的，所以如果用户使用 kill -9 PID 等强制关闭指令，是不会执行优雅停机的，只有通过 kill PID 时，才会执行。</li>
<li>因为大多数情况下，Dubbo 的声明周期是交给 Spring 进行管理，所以在最新的 Dubbo 版本中，增加了对 Spring 关闭事件的监听，从而关闭 Dubbo 服务</li>
<li>服务提供方的优雅停机过程<ul>
<li>首先，从注册中心中取消注册自己，从而使消费者不要再拉取到它。</li>
<li>然后，sleep 10 秒( 可配 )，等到服务消费，接收到注册中心通知到该服务提供者已经下线，加大了在不重试情况下优雅停机的成功率。</li>
<li>之后，广播 READONLY 事件给所有 Consumer 们，告诉它们不要在调用我了！！！如果此处注册中心挂掉的情况，依然能达到告诉 Consumer ，我要下线了的功能。</li>
<li>再之后，sleep 10 毫秒，保证 Consumer 们，尽可能接收到该消息。</li>
<li>再再之后，先标记为不接收新请求，新请求过来时直接报错，让客户端重试其它机器。</li>
<li>再再再之后，关闭心跳线程。</li>
<li>最后，检测线程池中的线程是否正在运行，如果有，等待所有线程执行完成，除非超时，则强制关闭。</li>
<li>最最后，关闭服务器。</li>
</ul>
</li>
<li>服务消费方的优雅停机过程<ul>
<li>停止时，不再发起新的调用请求，所有新的调用在客户端即报错。</li>
<li>然后，检测有没有请求的响应还没有返回，等待响应返回，除非超时，则强制关闭。</li>
</ul>
</li>
</ul>
</li>
<li><p>Dubbo Provider 异步关闭时，如何从注册中心下线？</p>
<ul>
<li>服务提供者，注册到 Zookeeper 上时，创建的是 EPHEMERAL 临时节点。所以在服务提供者异常关闭时，等待 Zookeeper 会话超时，那么该临时节点就会自动删除。</li>
</ul>
</li>
<li><p>Dubbo Consumer 只能调用从注册中心获取的 Provider 么？</p>
<ul>
<li>不是，Consumer 可以强制直连 Provider 。</li>
<li>在开发及测试环境下，经常需要绕过注册中心，只测试指定服务提供者，这时候可能需要点对点直连，点对点直连方式，将以服务接口为单位，忽略注册中心的提供者列表，A 接口配置点对点，不影响 B 接口从注册中心获取列表。</li>
<li>另外，直连 Dubbo Provider 时，如果要 Debug 调试 Dubbo Provider ，可以通过配置，禁用该 Provider 注册到注册中心。否则，会被其它 Consumer 调用到</li>
</ul>
</li>
<li><p>Dubbo 支持哪些通信协议？对应【protocol 远程调用层】。</p>
<ul>
<li>dubbo://</li>
<li>rest://</li>
<li>rmi://</li>
<li>webservice://</li>
<li>hessian://</li>
<li>thrift://</li>
<li>memcached://</li>
<li>redis://</li>
<li>http://</li>
</ul>
</li>
<li><p>什么是本地暴露和远程暴露，他们的区别？</p>
<ul>
<li>远程暴露:每次 Consumer 调用 Provider 都是跨进程，需要进行网络通信。</li>
<li>本地暴露:使用了 injvm:// 协议，是一个伪协议，它不开启端口，不发起远程调用，只在 JVM 内直接关联，但执行 Dubbo 的 Filter 链。</li>
</ul>
</li>
<li><p>Dubbo 使用什么通信框架？对应【transport 网络传输层】。</p>
<ul>
<li>Netty3</li>
<li>Netty4</li>
<li>Mina</li>
<li>Grizzly</li>
<li>在 Dubbo 的最新版本，默认使用 Netty4 的版本</li>
</ul>
</li>
<li><p>Dubbo 支持哪些序列化方式？对应【serialize 数据序列化层】。</p>
<ul>
<li>Dubbo 目前支付如下 7 种序列化方式：<ul>
<li>【重要】Hessian2 ：基于 Hessian 实现的序列化拓展。dubbo:// 协议的默认序列化方案。<ul>
<li>Hessian 除了是 Web 服务，也提供了其序列化实现，因此 Dubbo 基于它实现了序列化拓展。</li>
<li>另外，Dubbo 维护了自己的 hessian-lite ，对 Hessian 2 的 序列化 部分的精简、改进、BugFix 。</li>
</ul>
</li>
<li>Dubbo ：Dubbo 自己实现的序列化拓展。</li>
<li>Kryo ：基于 Kryo 实现的序列化拓展。</li>
<li>FST ：基于 FST 实现的序列化拓展。</li>
<li>JSON ：基于 Fastjson 实现的序列化拓展。</li>
<li>NativeJava ：基于 Java 原生的序列化拓展。</li>
<li>CompactedJava ：在 NativeJava 的基础上，实现了对 ClassDescriptor 的处理。</li>
</ul>
</li>
</ul>
</li>
<li><p>Dubbo 有哪些负载均衡策略？对应【cluster 路由层】的 LoadBalance 组件。</p>
<ul>
<li>Random LoadBalance <ul>
<li>随机，按权重设置随机概率。</li>
<li>在一个截面上碰撞的概率高，但调用量越大分布越均匀，而且按概率使用权重后也比较均匀，有利于动态调整提供者权重。</li>
</ul>
</li>
<li>RoundRobin LoadBalance<ul>
<li>轮询，按公约后的权重设置轮询比率。</li>
<li>存在慢的提供者累积请求的问题，比如</li>
</ul>
</li>
<li>LeastActive LoadBalance<ul>
<li>最少活跃调用数，相同活跃数的随机，活跃数指调用前后计数差。</li>
<li>使慢的提供者收到更少请求，因为越慢的提供者的调用前后计数差会越大。</li>
</ul>
</li>
<li>ConsistentHash LoadBalance<ul>
<li>一致性 Hash，相同参数的请求总是发到同一提供者。</li>
<li>当某一台提供者挂时，原本发往该提供者的请求，基于虚拟节点，平摊到其它提供者，不会引起剧烈变动。</li>
</ul>
</li>
</ul>
</li>
<li><p>Dubbo 有哪些集群容错策略？对应【cluster 路由层】的 Cluster 组件。</p>
<ul>
<li><p>Consumer 仅仅引用服务 ***-api.jar 包，那么可以获得到需要服务的 XXXService 接口。那么，通过动态创建对应调用 Dubbo 服务的实现类。简化代码如下：</p>
  <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// ProxyFactory.java</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * create proxy.</span></span><br><span class="line"><span class="comment"> * 创建 Proxy ，在引用服务调用。</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> invoker Invoker 对象</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span> proxy</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Adaptive(&#123;Constants.PROXY_KEY&#125;)</span></span><br><span class="line">&lt;T&gt; <span class="function">T <span class="title">getProxy</span><span class="params">(Invoker&lt;T&gt; invoker)</span> <span class="keyword">throws</span> RpcException</span>;</span><br></pre></td></tr></table></figure>
<ul>
<li>方法参数 invoker ，实现了调用 Dubbo 服务的逻辑。</li>
<li>返回的 <T> 结果，就是 XXXService 的实现类，而这个实现类，就是通过动态代理的工具类进行生成。</li>
</ul>
</li>
</ul>
</li>
<li><p>Dubbo SPI 的设计思想是什么？</p>
<ul>
<li>？？？？？？？？？？？？？？？？？</li>
</ul>
</li>
<li><p>Dubbo 服务如何监控和管理？</p>
<ul>
<li>Dubbo 管理平台 + 监控平台<ul>
<li>dubbo-monitor 监控平台，基于 Dubbo 的【monitor 监控层】，实现相应的监控数据的收集到监控平台。</li>
<li>dubbo-admin 管理平台，基于注册中心，可以获取到服务相关的信息。</li>
</ul>
</li>
<li>链路追踪<ul>
<li>目前能够实现链路追踪的组件还是比较多的，如下：<ul>
<li>Apache SkyWalking 【推荐】</li>
<li>Zipkin</li>
<li>Cat</li>
<li>PinPoint</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>Dubbo 服务如何做降级？比如说服务 A 调用服务 B，结果服务 B 挂掉了。服务 A 再重试几次调用服务 B，还是不行，那么直接降级，走一个备用的逻辑，给用户返回响应。</p>
<ul>
<li>Dubbo 原生自带的服务降级功能：不能实现现代微服务的熔断器的功能</li>
<li>引入支持服务降级的组件<ul>
<li>目前开源社区常用的有两种组件支持服务降级的功能，分别是：<ul>
<li>Alibaba Sentinel</li>
<li>Netflix Hystrix</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>Dubbo 如何做限流？</p>
<ul>
<li>Dubbo 原生自带的限流功能：通过 TpsLimitFilter 实现，仅适用于服务提供者</li>
<li>引入支持限流的组件：推荐集成 Sentinel 组件。</li>
</ul>
</li>
<li><p>Dubbo 的失败重试是什么？</p>
<ul>
<li>所谓失败重试，就是 consumer 调用 provider 要是失败了，比如抛异常了，此时应该是可以重试的，或者调用超时了也可以重试。</li>
<li>实际场景下，我们一般会禁用掉重试。因为，因为超时后重试会有问题，超时你不知道是成功还是失败。例如，可能会导致两次扣款的问题。</li>
<li>所以，我们一般使用 failfast 集群容错策略，而不是 failover 策略。配置如下：  <figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dubbo:service</span> <span class="attr">cluster</span>=<span class="string">&quot;failfast&quot;</span> <span class="attr">timeout</span>=<span class="string">&quot;2000&quot;</span> /&gt;</span></span><br></pre></td></tr></table></figure></li>
<li>一定一定一定要配置适合自己业务的超时时间。</li>
<li>当然，可以将操作分成读和写两种，前者支持重试，后者不支持重试。因为，读操作天然具有幂等性。</li>
</ul>
</li>
<li><p>Dubbo 支持哪些注册中心？</p>
<ul>
<li>【默认】Zookeeper </li>
<li>Redis </li>
<li>Multicast</li>
<li>Simple 注册中心</li>
<li>Nacos </li>
</ul>
</li>
<li><p>Dubbo 如何升级接口？</p>
<ul>
<li>当一个接口实现，出现不兼容升级时，可以用版本号过渡，版本号不同的服务相互间不引用。</li>
<li>可以按照以下的步骤进行版本迁移：<ul>
<li>在低压力时间段，先升级一半提供者为新版本。</li>
<li>再将所有消费者升级为新版本。</li>
<li>然后将剩下的一半提供者升级为新版本。</li>
</ul>
</li>
</ul>
</li>
<li><p>Dubbo 在安全机制方面是如何解决的？</p>
<pre><code> - ![](https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/15850320360538.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10)
</code></pre>
<ul>
<li>通过令牌验证在注册中心控制权限，以决定要不要下发令牌给消费者，可以防止消费者绕过注册中心访问提供者。</li>
<li>另外通过注册中心可灵活改变授权方式，而不需修改或升级提供者。</li>
</ul>
</li>
<li><p>Dubbo 需要 Web 容器吗？Dubbo 服务启动是否需要启动类似 Tomcat、Jetty 等服务器。</p>
<ul>
<li>这个答案可以是，也可以是不是。为什么呢？根据协议的不同，Provider 会启动不同的服务器。<ul>
<li>在使用 dubbo:// 协议时，答案是否，因为 Provider 启动 Netty、Mina 等 NIO Server 。</li>
<li>在使用 rest:// 协议时，答案是是，Provider 启动 Tomcat、Jetty 等 HTTP 服务器，或者也可以使用 Netty 封装的 HTTP 服务器。</li>
<li>在使用 hessian:// 协议时，答案是是，Provider 启动 Jetty、Tomcat 等 HTTP 服务器。</li>
</ul>
</li>
</ul>
</li>
<li><p>为什么要将系统进行拆分？SOA?微服务?</p>
<ul>
<li>维护成本</li>
<li>分布式挑战</li>
</ul>
</li>
<li><p>Dubbo 如何集成配置中心？</p>
<ul>
<li>对于使用了 Dubbo 的系统，配置分成两类：<ul>
<li>① Dubbo 自身配置。如：Dubbo 请求超时，Dubbo 重试次数等等。</li>
<li>② 非 Dubbo 自身配置<ul>
<li>基建配置，例如：数据库、Redis 等配置。</li>
<li>业务配置，例如：订单超时时间，下单频率等等配置。</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>Dubbo 如何实现分布式事务？</p>
<ul>
<li>？？？？？？？？？？？？？？？？？？</li>
</ul>
</li>
<li><p>Spring Cloud 与 Dubbo 怎么选择？</p>
<ul>
<li>？？？？？？？？？？？？</li>
</ul>
</li>
<li><p>如何自己设计一个类似 Dubbo 的 RPC 框架？</p>
<ul>
<li>服务提供者在注册中心服务发布</li>
<li>消费者去注册中心拿对应的服务信息</li>
<li>基于动态代理发起请求</li>
<li>负载均衡算法</li>
<li>通信方式，序列化方式</li>
<li>服务提供者生成一个动态代理，监听某个网络端口，然后代理本地的服务代码。接收到请求的时候，就调用对应的服务代码</li>
</ul>
</li>
</ol>
]]></content>
      <categories>
        <category>Dubbo</category>
      </categories>
      <tags>
        <tag>面试宝典</tag>
        <tag>Dubbo</tag>
        <tag>RPC</tag>
      </tags>
  </entry>
  <entry>
    <title>Hive进阶</title>
    <url>/2021/11/07/Hive%E8%BF%9B%E9%98%B6/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="Hive进阶"><a href="#Hive进阶" class="headerlink" title="Hive进阶"></a>Hive进阶</h1><h1 id="第1章-Explain-查看执行计划"><a href="#第1章-Explain-查看执行计划" class="headerlink" title="第1章 Explain 查看执行计划"></a>第1章 Explain 查看执行计划</h1><h2 id="1-1-创建测试用表"><a href="#1-1-创建测试用表" class="headerlink" title="1.1 创建测试用表"></a>1.1 创建测试用表</h2><ol>
<li><p>建大表、小表和 JOIN 后表的语句</p>
 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 创建大表</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> bigtable (</span><br><span class="line">    id        <span class="type">bigint</span>,</span><br><span class="line">    t         <span class="type">bigint</span>,</span><br><span class="line">    uid       string,</span><br><span class="line">    keyword   string,</span><br><span class="line">    url_rank  <span class="type">int</span>,</span><br><span class="line">    click_num <span class="type">int</span>,</span><br><span class="line">    click_url string</span><br><span class="line">) <span class="type">row</span> format delimited</span><br><span class="line">    fields terminated <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 创建小表</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> smalltable</span><br><span class="line">(</span><br><span class="line">    id        <span class="type">bigint</span>,</span><br><span class="line">    t         <span class="type">bigint</span>,</span><br><span class="line">    uid       string,</span><br><span class="line">    keyword   string,</span><br><span class="line">    url_rank  <span class="type">int</span>,</span><br><span class="line">    click_num <span class="type">int</span>,</span><br><span class="line">    click_url string</span><br><span class="line">) <span class="type">row</span> format delimited</span><br><span class="line">    fields terminated <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 创建 JOIN 后表</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> jointable</span><br><span class="line">(</span><br><span class="line">    id        <span class="type">bigint</span>,</span><br><span class="line">    t         <span class="type">bigint</span>,</span><br><span class="line">    uid       string,</span><br><span class="line">    keyword   string,</span><br><span class="line">    url_rank  <span class="type">int</span>,</span><br><span class="line">    click_num <span class="type">int</span>,</span><br><span class="line">    click_url string</span><br><span class="line">) <span class="type">row</span> format delimited</span><br><span class="line">    fields terminated <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span>;</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
<li><p>分别向大表和小表中导入数据</p>
 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> load data <span class="keyword">local</span> inpath <span class="string">&#x27;/opt/module/data/bigtable&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span> bigtable;</span><br><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> load data <span class="keyword">local</span> inpath <span class="string">&#x27;/opt/module/data/smalltable&#x27;</span> <span class="keyword">into</span>  <span class="keyword">table</span> smalltable;</span><br></pre></td></tr></table></figure>
<h2 id="1-2-基本语法"><a href="#1-2-基本语法" class="headerlink" title="1.2 基本语法"></a>1.2 基本语法</h2><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">EXPLAIN [EXTENDED <span class="operator">|</span> DEPENDENCY <span class="operator">|</span> <span class="keyword">AUTHORIZATION</span>] query<span class="operator">-</span><span class="keyword">sql</span></span><br></pre></td></tr></table></figure>
<h2 id="1-3-案例实操"><a href="#1-3-案例实操" class="headerlink" title="1.3 案例实操"></a>1.3 案例实操</h2></li>
<li><p>查看下面这条语句的执行计划</p>
 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> explain <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> bigtable;</span><br><span class="line"><span class="operator">+</span><span class="comment">----------------------------------------------------+</span></span><br><span class="line"><span class="operator">|</span>                      Explain                       <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------------------------------------------------+</span></span><br><span class="line"><span class="operator">|</span> STAGE DEPENDENCIES:                                <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>   Stage<span class="number">-0</span> <span class="keyword">is</span> a root stage                          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>                                                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> STAGE PLANS:                                       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>   Stage: Stage<span class="number">-0</span>                                   <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>     <span class="keyword">Fetch</span> Operator                                 <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>       limit: <span class="number">-1</span>                                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>       Processor Tree:                              <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>         TableScan                                  <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>           alias: bigtable                          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>           Statistics: Num <span class="keyword">rows</span>: <span class="number">1</span> Data size: <span class="number">1291573248</span> Basic stats: COMPLETE <span class="keyword">Column</span> stats: <span class="keyword">NONE</span> <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>           <span class="keyword">Select</span> Operator                          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>             expressions: id (type: <span class="type">bigint</span>), t (type: <span class="type">bigint</span>), uid (type: string), keyword (type: string), url_rank (type: <span class="type">int</span>), click_num (type: <span class="type">int</span>), click_url (type: string) <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>             outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6 <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>             Statistics: Num <span class="keyword">rows</span>: <span class="number">1</span> Data size: <span class="number">1291573248</span> Basic stats: COMPLETE <span class="keyword">Column</span> stats: <span class="keyword">NONE</span> <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>             ListSink                               <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>                                                    <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------------------------------------------------+</span></span><br><span class="line"><span class="number">17</span> <span class="keyword">rows</span> selected (<span class="number">0.112</span> seconds)</span><br><span class="line"></span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span>  explain <span class="keyword">select</span> click_url, <span class="built_in">count</span>(<span class="operator">*</span>) ct <span class="keyword">from</span> bigtable <span class="keyword">group</span> <span class="keyword">by</span> click_url;</span><br><span class="line"><span class="operator">+</span><span class="comment">----------------------------------------------------+</span></span><br><span class="line"><span class="operator">|</span>                      Explain                       <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------------------------------------------------+</span></span><br><span class="line"><span class="operator">|</span> STAGE DEPENDENCIES:                                <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>   Stage<span class="number">-1</span> <span class="keyword">is</span> a root stage                          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>   Stage<span class="number">-0</span> depends <span class="keyword">on</span> stages: Stage<span class="number">-1</span>               <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>                                                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> STAGE PLANS:                                       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>   Stage: Stage<span class="number">-1</span>                                   <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>     Map Reduce                                     <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>       Map Operator Tree:                           <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>           TableScan                                <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>             alias: bigtable                        <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>             Statistics: Num <span class="keyword">rows</span>: <span class="number">1</span> Data size: <span class="number">1291573248</span> Basic stats: COMPLETE <span class="keyword">Column</span> stats: <span class="keyword">NONE</span> <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>             <span class="keyword">Select</span> Operator                        <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>               expressions: click_url (type: string) <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>               outputColumnNames: click_url         <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>               Statistics: Num <span class="keyword">rows</span>: <span class="number">1</span> Data size: <span class="number">1291573248</span> Basic stats: COMPLETE <span class="keyword">Column</span> stats: <span class="keyword">NONE</span> <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>               <span class="keyword">Group</span> <span class="keyword">By</span> Operator                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>                 aggregations: <span class="built_in">count</span>()              <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>                 keys: click_url (type: string)     <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>                 mode: hash                         <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>                 outputColumnNames: _col0, _col1    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>                 Statistics: Num <span class="keyword">rows</span>: <span class="number">1</span> Data size: <span class="number">1291573248</span> Basic stats: COMPLETE <span class="keyword">Column</span> stats: <span class="keyword">NONE</span> <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>                 Reduce Output Operator             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>                   key expressions: _col0 (type: string) <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>                   sort <span class="keyword">order</span>: <span class="operator">+</span>                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>                   Map<span class="operator">-</span>reduce <span class="keyword">partition</span> columns: _col0 (type: string) <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>                   Statistics: Num <span class="keyword">rows</span>: <span class="number">1</span> Data size: <span class="number">1291573248</span> Basic stats: COMPLETE <span class="keyword">Column</span> stats: <span class="keyword">NONE</span> <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>                   <span class="keyword">value</span> expressions: _col1 (type: <span class="type">bigint</span>) <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>       Execution mode: vectorized                   <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>       Reduce Operator Tree:                        <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>         <span class="keyword">Group</span> <span class="keyword">By</span> Operator                          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>           aggregations: <span class="built_in">count</span>(VALUE._col0)         <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>           keys: KEY._col0 (type: string)           <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>           mode: mergepartial                       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>           outputColumnNames: _col0, _col1          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>           Statistics: Num <span class="keyword">rows</span>: <span class="number">1</span> Data size: <span class="number">1291573248</span> Basic stats: COMPLETE <span class="keyword">Column</span> stats: <span class="keyword">NONE</span> <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>           File Output Operator                     <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>             compressed: <span class="literal">false</span>                      <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>             Statistics: Num <span class="keyword">rows</span>: <span class="number">1</span> Data size: <span class="number">1291573248</span> Basic stats: COMPLETE <span class="keyword">Column</span> stats: <span class="keyword">NONE</span> <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>             <span class="keyword">table</span>:                                 <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>                 input format: org.apache.hadoop.mapred.SequenceFileInputFormat <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>                 output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>                 serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>                                                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>   Stage: Stage<span class="number">-0</span>                                   <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>     <span class="keyword">Fetch</span> Operator                                 <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>       limit: <span class="number">-1</span>                                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>       Processor Tree:                              <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>         ListSink                                   <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>                                                    <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------------------------------------------------+</span></span><br><span class="line"><span class="number">49</span> <span class="keyword">rows</span> selected (<span class="number">0.072</span> seconds)</span><br></pre></td></tr></table></figure>
<ul>
<li>STAGE DEPENDENCIES: 各个stage之间的依赖性</li>
<li>STAGE PLANS: 各个stage的执行计划</li>
<li>Map Operator Tree: MAP端的执行计划树</li>
<li>Reduce Operator Tree: Reduce端的执行计划树</li>
<li>TableScan: 表扫描操作，常见的属性：<ul>
<li>alias： 表名称</li>
<li>Statistics： 表统计信息，包含表中数据条数，数据大小等</li>
<li>Select Operator： 选取操作，常见的属性 ：</li>
<li>expressions：需要的字段名称及字段类型</li>
<li>outputColumnNames：输出的列名称</li>
</ul>
</li>
<li>Group By Operator：分组聚合操作，常见的属性：<ul>
<li>aggregations：显示聚合函数信息</li>
<li>mode：聚合模式，值有 hash：随机聚合，就是hash partition；partial：局部聚合；final：最终聚合</li>
<li>keys：分组的字段，如果没有分组，则没有此字段</li>
<li>outputColumnNames：聚合之后输出列名</li>
<li>Statistics： 表统计信息，包含分组聚合之后的数据条数，数据大小等</li>
</ul>
</li>
<li>Reduce Output Operator：输出到reduce操作，常见属性：<ul>
<li>sort order：值为空 不排序；值为 + 正序排序，值为 - 倒序排序；值为 +- 排序的列为两列，第一列为正序，第二列为倒序</li>
</ul>
</li>
<li>Filter Operator：过滤操作，常见的属性：<ul>
<li>predicate：过滤条件，如sql语句中的where id&gt;=1，则此处显示(id &gt;= 1)</li>
</ul>
</li>
<li>Map Join Operator：join 操作，常见的属性：<ul>
<li>condition map：join方式 ，如Inner Join 0 to 1 Left Outer Join0 to 2</li>
<li>keys: join 的条件字段</li>
<li>outputColumnNames： join 完成之后输出的字段</li>
<li>Statistics： join 完成之后生成的数据条数，大小等</li>
</ul>
</li>
<li>File Output Operator：文件输出操作，常见的属性<ul>
<li>compressed：是否压缩</li>
<li>table：表的信息，包含输入输出文件格式化方式，序列化方式等</li>
</ul>
</li>
<li>Fetch Operator 客户端获取数据操作，常见的属性：<ul>
<li>limit，值为 -1 表示不限制条数，其他值为限制的条数</li>
</ul>
</li>
</ul>
</li>
<li><p>查看详细执行计划</p>
 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> explain extended <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> bigtable;</span><br><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> explain extended <span class="keyword">select</span> click_url, <span class="built_in">count</span>(<span class="operator">*</span>) ct <span class="keyword">from</span> bigtable <span class="keyword">group</span> <span class="keyword">by</span> click_url;</span><br></pre></td></tr></table></figure>
<h1 id="第2章-Hive-建表优化"><a href="#第2章-Hive-建表优化" class="headerlink" title="第2章 Hive 建表优化"></a>第2章 Hive 建表优化</h1><h2 id="2-1-分区表"><a href="#2-1-分区表" class="headerlink" title="2.1 分区表"></a>2.1 分区表</h2><h2 id="2-2-分桶表"><a href="#2-2-分桶表" class="headerlink" title="2.2 分桶表"></a>2.2 分桶表</h2><h2 id="2-3-合适的文件格式"><a href="#2-3-合适的文件格式" class="headerlink" title="2.3 合适的文件格式"></a>2.3 合适的文件格式</h2><h2 id="2-4-合适的压缩格式"><a href="#2-4-合适的压缩格式" class="headerlink" title="2.4 合适的压缩格式"></a>2.4 合适的压缩格式</h2></li>
</ol>
<h1 id="第3章-HQL-语法优化"><a href="#第3章-HQL-语法优化" class="headerlink" title="第3章 HQL 语法优化"></a>第3章 HQL 语法优化</h1><h2 id="3-1-列裁剪与分区裁剪"><a href="#3-1-列裁剪与分区裁剪" class="headerlink" title="3.1 列裁剪与分区裁剪"></a>3.1 列裁剪与分区裁剪</h2><ul>
<li>列裁剪就是在查询时只读取需要的列，分区裁剪就是只读取需要的分区。当列很多或者<br>数据量很大时，如果 select * 或者不指定分区，全列扫描和全表扫描效率都很低。</li>
<li>Hive 在读数据的时候，可以只读取查询中所需要用到的列，而忽略其他的列。这样做<br>可以节省读取开销：中间表存储开销和数据整合开销。</li>
</ul>
<h2 id="3-2-Group-By"><a href="#3-2-Group-By" class="headerlink" title="3.2 Group By"></a>3.2 Group By</h2><ol>
<li>开启 Map 端聚合参数设置<ol>
<li>是否在 Map 端进行聚合，默认为 True<br> <code>set hive.map.aggr = true; </code></li>
<li>在 Map 端进行聚合操作的条目数目<br> <code>set hive.groupby.mapaggr.checkinterval = 100000;</code></li>
<li>有数据倾斜的时候进行负载均衡（默认是 false）<br> <code>set hive.groupby.skewindata = true;</code><br> 当选项设定为 true，生成的查询计划会有两个 MR Job。<ul>
<li>第一个 MR Job 中，Map 的输出结果会随机分布到 Reduce 中，每个 Reduce 做部分聚合操作，并输出结果，这样处理的结果是相同的 Group By Key 有可能被分发到不同的 Reduce中，从而达到负载均衡的目的；</li>
<li>第二个 MR Job 再根据预处理的数据结果按照 Group By Key 分布到 Reduce 中（这个过程可以保证相同的 Group By Key 被分布到同一个 Reduce 中），最后完成最终的聚合操作（虽然能解决数据倾斜，但是不能让运行速度的更快）。<h2 id="3-3-Vectorization"><a href="#3-3-Vectorization" class="headerlink" title="3.3 Vectorization"></a>3.3 Vectorization</h2>vectorization : 矢量计算的技术，在计算类似scan, filter, aggregation的时候，vectorization技术以设置批处理的增量大小为 1024 行单次来达到比单条记录单次获得更高的效率。<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.vectorized.execution.enabled <span class="operator">=</span> <span class="literal">true</span>;</span><br><span class="line"><span class="keyword">set</span> hive.vectorized.execution.reduce.enabled <span class="operator">=</span> <span class="literal">true</span>;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ol>
</li>
</ol>
<h2 id="3-4-多重模式"><a href="#3-4-多重模式" class="headerlink" title="3.4 多重模式"></a>3.4 多重模式</h2><ul>
<li>如果碰到一堆 SQL，并且这一堆 SQL 的模式还一样。都是从同一个表进行扫描，做不<br>同的逻辑。</li>
<li>可优化的地方：如果有 n 条 SQL，每个 SQL 执行都会扫描一次这张表。  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">insert</span> .... <span class="keyword">select</span> id,name,sex, age <span class="keyword">from</span> student <span class="keyword">where</span> age <span class="operator">&gt;</span> <span class="number">17</span>;</span><br><span class="line"><span class="keyword">insert</span> .... <span class="keyword">select</span> id,name,sex, age <span class="keyword">from</span> student <span class="keyword">where</span> age <span class="operator">&gt;</span> <span class="number">18</span>;</span><br><span class="line"><span class="keyword">insert</span> .... <span class="keyword">select</span> id,name,sex, age <span class="keyword">from</span> student <span class="keyword">where</span> age <span class="operator">&gt;</span> <span class="number">19</span>;</span><br></pre></td></tr></table></figure></li>
<li>隐藏了一个问题：这种类型的 SQL 有多少个，那么最终。这张表就被全表扫描了多少次  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="type">int</span> t_ptn <span class="keyword">partition</span>(city<span class="operator">=</span>A). <span class="keyword">select</span> id,name,sex, age <span class="keyword">from</span> student <span class="keyword">where</span> city<span class="operator">=</span> A;</span><br><span class="line"><span class="keyword">insert</span> <span class="type">int</span> t_ptn <span class="keyword">partition</span>(city<span class="operator">=</span>B). <span class="keyword">select</span> id,name,sex, age <span class="keyword">from</span> student <span class="keyword">where</span> city<span class="operator">=</span> B;</span><br><span class="line"><span class="keyword">insert</span> <span class="type">int</span> t_ptn <span class="keyword">partition</span>(city<span class="operator">=</span>c). <span class="keyword">select</span> id,name,sex, age <span class="keyword">from</span> student <span class="keyword">where</span> city<span class="operator">=</span> c;</span><br><span class="line"><span class="comment">-- 修改为：</span></span><br><span class="line"><span class="keyword">from</span> student</span><br><span class="line"><span class="keyword">insert</span> <span class="type">int</span> t_ptn <span class="keyword">partition</span>(city<span class="operator">=</span>A) <span class="keyword">select</span> id,name,sex, age <span class="keyword">where</span> city<span class="operator">=</span> A</span><br><span class="line"><span class="keyword">insert</span> <span class="type">int</span> t_ptn <span class="keyword">partition</span>(city<span class="operator">=</span>B) <span class="keyword">select</span> id,name,sex, age <span class="keyword">where</span> city<span class="operator">=</span> B</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<ul>
<li>如果一个 HQL 底层要执行 10 个 Job，那么能优化成 8 个一般来说，肯定能有所提高，多重插入就是一个非常实用的技能。一次读取，多次插入，有些场景是从一张表读取数据后，要多次利用。</li>
</ul>
</li>
</ul>
<h2 id="3-5-in-exists-语句"><a href="#3-5-in-exists-语句" class="headerlink" title="3.5 in/exists 语句"></a>3.5 in/exists 语句</h2><ul>
<li>使用 Hive 的一个高效替代方案：left semi join</li>
<li>比如：– in / exists 实现  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> a.id, a.name <span class="keyword">from</span> a <span class="keyword">where</span> a.id <span class="keyword">in</span> (<span class="keyword">select</span> b.id <span class="keyword">from</span> b);</span><br><span class="line"><span class="keyword">select</span> a.id, a.name <span class="keyword">from</span> a <span class="keyword">where</span> <span class="keyword">exists</span> (<span class="keyword">select</span> id <span class="keyword">from</span> b <span class="keyword">where</span> a.id <span class="operator">=</span> b.id);</span><br></pre></td></tr></table></figure></li>
<li>可以使用 join 来改写：  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> a.id, a.name <span class="keyword">from</span> a <span class="keyword">join</span> b <span class="keyword">on</span> a.id <span class="operator">=</span> b.id;</span><br></pre></td></tr></table></figure></li>
<li>应该转换成left semi join 实现  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> a.id, a.name <span class="keyword">from</span> a <span class="keyword">left</span> semi <span class="keyword">join</span> b <span class="keyword">on</span> a.id <span class="operator">=</span> b.id;</span><br></pre></td></tr></table></figure></li>
</ul>
<h2 id="3-6-CBO-优化"><a href="#3-6-CBO-优化" class="headerlink" title="3.6 CBO 优化"></a>3.6 CBO 优化</h2><ul>
<li>CBO 优化可以自动优化 HQL 中多个 Join 的顺序，并选择合适的 Join 算法。代价最小的执行计划就是最好的执行计划</li>
<li>要使用基于成本的优化（也称为 CBO），请在查询开始设置以下参数：<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.cbo.enable<span class="operator">=</span><span class="literal">true</span>;</span><br><span class="line"><span class="keyword">set</span> hive.compute.query.using.stats<span class="operator">=</span><span class="literal">true</span>;</span><br><span class="line"><span class="keyword">set</span> hive.stats.fetch.column.stats<span class="operator">=</span><span class="literal">true</span>;</span><br><span class="line"><span class="keyword">set</span> hive.stats.fetch.partition.stats<span class="operator">=</span><span class="literal">true</span>;</span><br></pre></td></tr></table></figure></li>
</ul>
<h2 id="3-7-谓词下推"><a href="#3-7-谓词下推" class="headerlink" title="3.7 谓词下推"></a>3.7 谓词下推</h2><ul>
<li>将 SQL 语句中的 where 谓词逻辑都尽可能提前执行，减少下游处理的数据量。对应逻辑优化器是 PredicatePushDown，配置项为 hive.optimize.ppd，默认为 true。</li>
<li>案例实操：</li>
</ul>
<ol>
<li><p>打开谓词下推优化属性</p>
 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 谓词下推，默认是 true</span></span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">set</span> hive.optimize.ppd;</span><br><span class="line"><span class="operator">+</span><span class="comment">-------------------------+</span></span><br><span class="line"><span class="operator">|</span>           <span class="keyword">set</span>           <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-------------------------+</span></span><br><span class="line"><span class="operator">|</span> hive.optimize.ppd<span class="operator">=</span><span class="literal">true</span>  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-------------------------+</span></span><br><span class="line"><span class="number">1</span> <span class="type">row</span> selected (<span class="number">0.01</span> seconds) </span><br></pre></td></tr></table></figure></li>
<li><p>查看先关联两张表，再用 where 条件过滤的执行计划</p>
 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> explain <span class="keyword">select</span> o.id <span class="keyword">from</span> bigtable b <span class="keyword">join</span> bigtable o <span class="keyword">on</span> o.id <span class="operator">=</span> b.id <span class="keyword">where</span> o.id <span class="operator">&lt;=</span> <span class="number">10</span>;</span><br></pre></td></tr></table></figure></li>
<li><p>查看子查询后，再关联表的执行计划</p>
 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> explain <span class="keyword">select</span> b.id <span class="keyword">from</span> bigtable b <span class="keyword">join</span> (<span class="keyword">select</span> id <span class="keyword">from</span> bigtable <span class="keyword">where</span> id <span class="operator">&lt;=</span> <span class="number">10</span>) o <span class="keyword">on</span> b.id <span class="operator">=</span> o.id;</span><br></pre></td></tr></table></figure>
<h2 id="3-8-MapJoin"><a href="#3-8-MapJoin" class="headerlink" title="3.8 MapJoin"></a>3.8 MapJoin</h2><p>MapJoin 是将 Join 双方比较小的表直接分发到各个 Map 进程的内存中，在 Map 进程中进行 Join 操 作，这样就不用进行 Reduce 步骤，从而提高了速度。如果不指定 MapJoin或者不符合 MapJoin 的条件，那么 Hive 解析器会将 Join 操作转换成 Common Join，即：在Reduce 阶段完成 Join。容易发生数据倾斜。可以用 MapJoin 把小表全部加载到内存在 Map端进行 Join，避免 Reducer 处理。</p>
</li>
<li><p>开启 MapJoin 参数设置</p>
<ol>
<li>设置自动选择 MapJoin <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 默认为 true</span></span><br><span class="line"><span class="keyword">set</span> hive.auto.convert.join<span class="operator">=</span><span class="literal">true</span>; </span><br></pre></td></tr></table></figure></li>
<li>大表小表的阈值设置（默认 25M 以下认为是小表）： <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.mapjoin.smalltable.filesize<span class="operator">=</span><span class="number">25000000</span>;</span><br></pre></td></tr></table></figure></li>
</ol>
</li>
<li><p>MapJoin 工作机制</p>
<ul>
<li>MapJoin 是将 Join 双方比较小的表直接分发到各个 Map 进程的内存中，在 Map 进程中进行 Join 操作，这样就不用进行 Reduce 步骤，从而提高了速度。</li>
</ul>
</li>
<li><p>案例实操：</p>
<ol>
<li>开启 MapJoin 功能 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 默认为 true</span></span><br><span class="line"><span class="keyword">set</span> hive.auto.convert.join <span class="operator">=</span> <span class="literal">true</span>; </span><br></pre></td></tr></table></figure></li>
<li>执行小表 JOIN 大表语句<ul>
<li>注意：此时小表(左连接)作为主表，所有数据都要写出去，因此此时会走 reduce，mapjoin失效<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">Explain <span class="keyword">insert</span> overwrite <span class="keyword">table</span> jointable</span><br><span class="line"><span class="keyword">select</span> b.id, b.t, b.uid, b.keyword, b.url_rank, b.click_num, b.click_url</span><br><span class="line"><span class="keyword">from</span> smalltable s</span><br><span class="line"><span class="keyword">left</span> <span class="keyword">join</span> bigtable b</span><br><span class="line"><span class="keyword">on</span> s.id <span class="operator">=</span> b.id;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>执行大表 JOIN 小表语句 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">Explain <span class="keyword">insert</span> overwrite <span class="keyword">table</span> jointable</span><br><span class="line"><span class="keyword">select</span> b.id, b.t, b.uid, b.keyword, b.url_rank, b.click_num, b.click_url</span><br><span class="line"><span class="keyword">from</span> bigtable b</span><br><span class="line"><span class="keyword">left</span> <span class="keyword">join</span> smalltable s</span><br><span class="line"><span class="keyword">on</span> s.id <span class="operator">=</span> b.id;</span><br></pre></td></tr></table></figure>
<h2 id="3-9-大表、大表-SMB-Join"><a href="#3-9-大表、大表-SMB-Join" class="headerlink" title="3.9 大表、大表 SMB Join"></a>3.9 大表、大表 SMB Join</h2></li>
</ol>
</li>
</ol>
<ul>
<li>SMB Join ：Sort Merge Bucket Join</li>
</ul>
<h2 id="3-10-笛卡尔积"><a href="#3-10-笛卡尔积" class="headerlink" title="3.10 笛卡尔积"></a>3.10 笛卡尔积</h2><p>Join 的时候不加 on 条件，或者无效的 on 条件，因为找不到 Join key，Hive 只能使用1 个 Reducer 来完成笛卡尔积。当 Hive 设定为严格模式（hive.mapred.mode=strictnonstrict） 时，不允许在 HQL 语句中出现笛卡尔积。</p>
<h1 id="第4章-数据倾斜"><a href="#第4章-数据倾斜" class="headerlink" title="第4章 数据倾斜"></a>第4章 数据倾斜</h1><h2 id="4-1-单表数据倾斜优化"><a href="#4-1-单表数据倾斜优化" class="headerlink" title="4.1 单表数据倾斜优化"></a>4.1 单表数据倾斜优化</h2><h3 id="4-1-1-使用参数"><a href="#4-1-1-使用参数" class="headerlink" title="4.1.1 使用参数"></a>4.1.1 使用参数</h3><p>当任务中存在 GroupBy 操作同时聚合函数为 count 或者 sum 可以设置参数来处理数据<br>倾斜问题。</p>
<ul>
<li>是否在 Map 端进行聚合，默认为 True  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.map.aggr <span class="operator">=</span> <span class="literal">true</span>;</span><br></pre></td></tr></table></figure></li>
<li>在 Map 端进行聚合操作的条目数目  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.groupby.mapaggr.checkinterval <span class="operator">=</span> <span class="number">100000</span>;</span><br></pre></td></tr></table></figure></li>
<li>有数据倾斜的时候进行负载均衡（默认是 false）  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.groupby.skewindata <span class="operator">=</span> <span class="literal">true</span>;</span><br></pre></td></tr></table></figure>
<ul>
<li>当选项设定为 true，生成的查询计划会有两个 MR Job。</li>
</ul>
</li>
</ul>
<h3 id="4-1-2-增加-Reduce-数量"><a href="#4-1-2-增加-Reduce-数量" class="headerlink" title="4.1.2 增加 Reduce 数量"></a>4.1.2 增加 Reduce 数量</h3><ol>
<li>调整 reduce 个数方法一<ol>
<li>每个 Reduce 处理的数据量默认是 256MB <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.exec.reducers.bytes.per.reducer <span class="operator">=</span> <span class="number">256000000</span></span><br></pre></td></tr></table></figure></li>
<li>每个任务最大的 reduce 数，默认为 1009 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.exec.reducers.max <span class="operator">=</span> <span class="number">1009</span></span><br></pre></td></tr></table></figure></li>
<li>计算 reducer 数的公式 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">N<span class="operator">=</span><span class="built_in">min</span>(参数 <span class="number">2</span>，总输入数据量<span class="operator">/</span>参数 <span class="number">1</span>)(参数 <span class="number">2</span> 指的是上面的 <span class="number">1009</span>，参数 <span class="number">1</span> 值得是 <span class="number">256</span>M)</span><br></pre></td></tr></table></figure></li>
</ol>
</li>
<li>调整 reduce 个数方法二<ol>
<li>在 hadoop 的 mapred-default.xml 文件中修改设置每个 job 的 Reduce 个数 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> mapreduce.job.reduces <span class="operator">=</span> <span class="number">15</span>;</span><br></pre></td></tr></table></figure>
<h2 id="4-2-Join-数据倾斜优化"><a href="#4-2-Join-数据倾斜优化" class="headerlink" title="4.2 Join 数据倾斜优化"></a>4.2 Join 数据倾斜优化</h2><h3 id="4-2-1-使用参数"><a href="#4-2-1-使用参数" class="headerlink" title="4.2.1 使用参数"></a>4.2.1 使用参数</h3></li>
</ol>
</li>
</ol>
<ul>
<li>在编写 Join 查询语句时，如果确定是由于 join 出现的数据倾斜，那么请做如下设置：  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"># <span class="keyword">join</span> 的键对应的记录条数超过这个值则会进行分拆，值根据具体数据量设置</span><br><span class="line"><span class="keyword">set</span> hive.skewjoin.key<span class="operator">=</span><span class="number">100000</span>;</span><br><span class="line"># 如果是 <span class="keyword">join</span> 过程出现倾斜应该设置为 <span class="literal">true</span></span><br><span class="line"><span class="keyword">set</span> hive.optimize.skewjoin<span class="operator">=</span><span class="literal">false</span>;</span><br></pre></td></tr></table></figure></li>
<li>如果开启了，在 Join 过程中 Hive 会将计数超过阈值 hive.skewjoin.key（默认 100000）的倾斜 key 对应的行临时写进文件中，然后再启动另一个 job 做 map join 生成结果。通过hive.skewjoin.mapjoin.map.tasks 参数还可以控制第二个 job 的 mapper 数量，默认 10000。  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.skewjoin.mapjoin.map.tasks<span class="operator">=</span><span class="number">10000</span>;</span><br></pre></td></tr></table></figure></li>
</ul>
<h2 id="4-2-2-MapJoin"><a href="#4-2-2-MapJoin" class="headerlink" title="4.2.2 MapJoin"></a>4.2.2 MapJoin</h2><h1 id="第5章-Hive-Job-优化"><a href="#第5章-Hive-Job-优化" class="headerlink" title="第5章 Hive Job 优化"></a>第5章 Hive Job 优化</h1><h2 id="5-1-Hive-Map-优化"><a href="#5-1-Hive-Map-优化" class="headerlink" title="5.1 Hive Map 优化"></a>5.1 Hive Map 优化</h2><h3 id="5-1-1-复杂文件增加-Map-数"><a href="#5-1-1-复杂文件增加-Map-数" class="headerlink" title="5.1.1 复杂文件增加 Map 数"></a>5.1.1 复杂文件增加 Map 数</h3><ul>
<li><p>当 input 的文件都很大，任务逻辑复杂，map 执行非常慢的时候，可以考虑增加 Map 数，来使得每个 map 处理的数据量减少，从而提高任务的执行效率。增加 map 的方法为：根据<code>computeSliteSize(Math.max(minSize,Math.min(maxSize,blocksize)))=blocksize=128M </code>公式，调整 maxSize 最大值。让 maxSize 最大值低于 blocksize 就可以增加 map 的个数。</p>
</li>
<li><p>案例实操：</p>
<ol>
<li>执行查询 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="built_in">count</span>(<span class="operator">*</span>) <span class="keyword">from</span> emp;</span><br><span class="line">Hadoop job information <span class="keyword">for</span> Stage<span class="number">-1</span>: number <span class="keyword">of</span> mappers: <span class="number">1</span>; number <span class="keyword">of</span> reducers: <span class="number">1</span></span><br></pre></td></tr></table></figure></li>
<li>设置最大切片值为 100 个字节 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">set</span> mapreduce.input.fileinputformat.split.maxsize<span class="operator">=</span><span class="number">100</span>;</span><br><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="built_in">count</span>(<span class="operator">*</span>) <span class="keyword">from</span> emp;</span><br><span class="line">Hadoop job information <span class="keyword">for</span> Stage<span class="number">-1</span>: number <span class="keyword">of</span> mappers: <span class="number">6</span>; number <span class="keyword">of</span> reducers: <span class="number">1</span></span><br></pre></td></tr></table></figure>
<h2 id="5-1-2-小文件进行合并"><a href="#5-1-2-小文件进行合并" class="headerlink" title="5.1.2 小文件进行合并"></a>5.1.2 小文件进行合并</h2></li>
</ol>
</li>
</ul>
<ol>
<li>在 map 执行前合并小文件，减少 map 数：CombineHiveInputFormat 具有对小文件进行合并的功能（系统默认的格式）。HiveInputFormat 没有对小文件合并功能。 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.input.format<span class="operator">=</span>org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;</span><br></pre></td></tr></table></figure></li>
<li>在 Map-Reduce 的任务结束时合并小文件的设置： <ul>
<li>在 map-only 任务结束时合并小文件，默认 true  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.merge.mapfiles <span class="operator">=</span> <span class="literal">true</span>;</span><br></pre></td></tr></table></figure></li>
<li>在 map-reduce 任务结束时合并小文件，默认 false  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.merge.mapredfiles <span class="operator">=</span> <span class="literal">true</span>;</span><br></pre></td></tr></table></figure></li>
<li>合并文件的大小，默认 256M  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.merge.size.per.task <span class="operator">=</span> <span class="number">268435456</span>;</span><br></pre></td></tr></table></figure></li>
<li>当输出文件的平均大小小于该值时，启动一个独立的 map-reduce 任务进行文件 merge  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.merge.smallfiles.avgsize <span class="operator">=</span> <span class="number">16777216</span>;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ol>
<h3 id="5-1-3-Map-端聚合"><a href="#5-1-3-Map-端聚合" class="headerlink" title="5.1.3 Map 端聚合"></a>5.1.3 Map 端聚合</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 相当于 map 端执行 combiner</span></span><br><span class="line"><span class="keyword">set</span> hive.map.aggr<span class="operator">=</span><span class="literal">true</span>;</span><br></pre></td></tr></table></figure>
<h3 id="5-1-4-推测执行"><a href="#5-1-4-推测执行" class="headerlink" title="5.1.4 推测执行"></a>5.1.4 推测执行</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">#默认是 <span class="literal">true</span></span><br><span class="line"><span class="keyword">set</span> mapred.map.tasks.speculative.execution <span class="operator">=</span> <span class="literal">true</span></span><br></pre></td></tr></table></figure>

<h2 id="5-2-Hive-Reduce-优化"><a href="#5-2-Hive-Reduce-优化" class="headerlink" title="5.2 Hive Reduce 优化"></a>5.2 Hive Reduce 优化</h2><h3 id="5-2-1-合理设置-Reduce-数"><a href="#5-2-1-合理设置-Reduce-数" class="headerlink" title="5.2.1 合理设置 Reduce 数"></a>5.2.1 合理设置 Reduce 数</h3><ol>
<li>调整 reduce 个数方法一<ol>
<li>每个 Reduce 处理的数据量默认是 256MB <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.exec.reducers.bytes.per.reducer <span class="operator">=</span> <span class="number">256000000</span></span><br></pre></td></tr></table></figure></li>
<li>每个任务最大的 reduce 数，默认为 1009 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.exec.reducers.max <span class="operator">=</span> <span class="number">1009</span></span><br></pre></td></tr></table></figure></li>
<li>计算 reducer 数的公式 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">N=min(参数 2，总输入数据量/参数 1)(参数 2 指的是上面的 1009，参数 1 值得是 256M)</span><br></pre></td></tr></table></figure></li>
</ol>
</li>
<li>调整 reduce 个数方法二<ul>
<li>在 hadoop 的 mapred-default.xml 文件中修改,设置每个 job 的 Reduce 个数  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> mapreduce.job.reduces <span class="operator">=</span> <span class="number">15</span>;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>reduce 个数并不是越多越好<ol>
<li>过多的启动和初始化 reduce 也会消耗时间和资源；</li>
<li>另外，有多少个 reduce，就会有多少个输出文件，如果生成了很多个小文件，那么如果这些小文件作为下一个任务的输入，则也会出现小文件过多的问题；</li>
<li>在设置 reduce 个数的时候也需要考虑这两个原则：<ol>
<li>处理大数据量利用合适的 reduce 数；</li>
<li>使单个 reduce 任务处理数据量大小要合适；</li>
</ol>
</li>
</ol>
</li>
</ol>
<h3 id="5-3-2-推测执行"><a href="#5-3-2-推测执行" class="headerlink" title="5.3.2 推测执行"></a>5.3.2 推测执行</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># hadoop 里面的</span><br><span class="line">mapred.reduce.tasks.speculative.execution</span><br><span class="line"># hive 里面相同的参数，效果和hadoop 里面的一样两个随便哪个都行</span><br><span class="line">hive.mapred.reduce.tasks.speculative.execution</span><br></pre></td></tr></table></figure>

<h2 id="5-3-Hive-任务整体优化"><a href="#5-3-Hive-任务整体优化" class="headerlink" title="5.3 Hive 任务整体优化"></a>5.3 Hive 任务整体优化</h2><h3 id="5-3-1-Fetch-抓取"><a href="#5-3-1-Fetch-抓取" class="headerlink" title="5.3.1 Fetch 抓取"></a>5.3.1 Fetch 抓取</h3><ul>
<li>Fetch 抓取是指，Hive 中对某些情况的查询可以不必使用 MapReduce 计算。例如：<code>SELECT * FROM emp</code></li>
<li>在这种情况下，Hive 可以简单地读取 emp 对应的存储目录下的文件，然后输出<br>查询结果到控制台。</li>
<li>在 hive-default.xml.template 文件中 hive.fetch.task.conversion 默认是 more，老版本 hive默认是 minimal，该属性修改为 more 以后，在全局查找、字段查找、limit 查找等都不走mapreduce。  <figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.fetch.task.conversion<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>more<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span></span><br><span class="line">        Expects one of [none, minimal, more].</span><br><span class="line">        Some select queries can be converted to single FETCH task minimizing latency.</span><br><span class="line">        Currently the query should be single sourced not having any subquery and should not have any aggregations or distincts (which incurs RS), lateral views and joins.</span><br><span class="line">        0. none : disable hive.fetch.task.conversion</span><br><span class="line">        1. minimal : SELECT STAR, FILTER on partition columns, LIMIT only</span><br><span class="line">        2. more : SELECT, FILTER, LIMIT only (support TABLESAMPLE and </span><br><span class="line">        virtual columns)</span><br><span class="line">    <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li>
</ul>
]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title>Hadoop压缩、优化、高可用</title>
    <url>/2021/11/07/Hadoop%E5%8E%8B%E7%BC%A9%E3%80%81%E4%BC%98%E5%8C%96%E3%80%81%E9%AB%98%E5%8F%AF%E7%94%A8/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="Hadoop压缩、优化、高可用"><a href="#Hadoop压缩、优化、高可用" class="headerlink" title="Hadoop压缩、优化、高可用"></a>Hadoop压缩、优化、高可用</h1><h1 id="一、Hadoop数据压缩"><a href="#一、Hadoop数据压缩" class="headerlink" title="一、Hadoop数据压缩"></a>一、Hadoop数据压缩</h1><h2 id="1-1-概述"><a href="#1-1-概述" class="headerlink" title="1.1 概述"></a>1.1 概述</h2><h3 id="1-1-1-压缩概述"><a href="#1-1-1-压缩概述" class="headerlink" title="1.1.1 压缩概述"></a>1.1.1 压缩概述</h3><ul>
<li>压缩技术能够有效减少底层存储系统（HDFS）读写字节数。压缩提高了网络带宽和磁盘空间的效率。在运行MR程序时，I/O操作、网络数据传输、 Shuffle和Merge要花大量的时间，尤其是数据规模很大和工作负载密集的情况下，因此，使用数据压缩显得非常重要。</li>
<li>鉴于磁盘I/O和网络带宽是Hadoop的宝贵资源，数据压缩对于节省资源、最小化磁盘I/O和网络传输非常有帮助。可以在任意MapReduce阶段启用压缩。不过，尽管压缩与解压操作的CPU开销不高，其性能的提升和资源的节省并非没有代价。</li>
</ul>
<h2 id="1-1-2-压缩策略和原则"><a href="#1-1-2-压缩策略和原则" class="headerlink" title="1.1.2 压缩策略和原则"></a>1.1.2 压缩策略和原则</h2><ul>
<li>压缩是提高Hadoop运行效率的一种优化策略</li>
<li>通过对Mapper、Reducer运行过程的数据进行压缩，以减少磁盘IO，提高MR程序运行速度</li>
<li>注意：采用压缩技术减少了磁盘IO，但同时增加了CPU运算负担。所以，压缩特性运用得当能提高性能，但运用不当也可能降低性能</li>
<li>压缩基本原则<ul>
<li>运算密集型的job，少用压缩</li>
<li>IO密集型的job，多用压缩</li>
</ul>
</li>
</ul>
<hr>
<h2 id="1-2-MR支持的压缩编码"><a href="#1-2-MR支持的压缩编码" class="headerlink" title="1.2 MR支持的压缩编码"></a>1.2 MR支持的压缩编码</h2><ul>
<li>支持的压缩编码<table>
<thead>
<tr>
<th>压缩格式</th>
<th>hadoop自带？</th>
<th>算法</th>
<th>文件扩展名</th>
<th>是否可切分</th>
<th>换成压缩格式后，原来的程序是否需要修改</th>
</tr>
</thead>
<tbody><tr>
<td>DEFLATE</td>
<td>是，直接使用</td>
<td>DEFLATE</td>
<td>.deflate</td>
<td>否</td>
<td>和文本处理一样，不需要修改</td>
</tr>
<tr>
<td>Gzip</td>
<td>是，直接使用</td>
<td>DEFLATE</td>
<td>.gz</td>
<td>否</td>
<td>和文本处理一样，不需要修改</td>
</tr>
<tr>
<td>bzip2</td>
<td>是，直接使用</td>
<td>bzip2</td>
<td>.bz2</td>
<td>是</td>
<td>和文本处理一样，不需要修改</td>
</tr>
<tr>
<td>LZO</td>
<td>否，需要安装</td>
<td>LZO</td>
<td>.lzo</td>
<td>是</td>
<td>需要建索引，还需要指定输入格式</td>
</tr>
<tr>
<td>Snappy</td>
<td>是，直接使用</td>
<td>Snappy</td>
<td>.snappy</td>
<td>否</td>
<td>和文本处理一样，不需要修改</td>
</tr>
</tbody></table>
</li>
<li>为了支持多种压缩/解压缩算法，Hadoop引入了编码/解码器，如下表所示。<table>
<thead>
<tr>
<th>压缩格式</th>
<th>对应的编码/解码器</th>
</tr>
</thead>
<tbody><tr>
<td>DEFLATE</td>
<td>org.apache.hadoop.io.compress.DefaultCodec</td>
</tr>
<tr>
<td>gzip</td>
<td>org.apache.hadoop.io.compress.GzipCodec</td>
</tr>
<tr>
<td>bzip2</td>
<td>org.apache.hadoop.io.compress.BZip2Codec</td>
</tr>
<tr>
<td>LZO</td>
<td>com.hadoop.compression.lzo.LzopCodec</td>
</tr>
<tr>
<td>Snappy</td>
<td>org.apache.hadoop.io.compress.SnappyCodec</td>
</tr>
</tbody></table>
</li>
<li>压缩性能的比较<table>
<thead>
<tr>
<th>压缩算法</th>
<th>原始文件大小</th>
<th>压缩文件大小</th>
<th>压缩速度</th>
<th>解压速度</th>
</tr>
</thead>
<tbody><tr>
<td>gzip</td>
<td>8.3GB</td>
<td>1.8GB</td>
<td>17.5MB/s</td>
<td>58MB/s</td>
</tr>
<tr>
<td>bzip2</td>
<td>8.3GB</td>
<td>1.1GB</td>
<td>2.4MB/s</td>
<td>9.5MB/s</td>
</tr>
<tr>
<td>LZO</td>
<td>8.3GB</td>
<td>2.9GB</td>
<td>49.3MB/s</td>
<td>74.6MB/s</td>
</tr>
</tbody></table>
</li>
</ul>
<hr>
<h2 id="1-3-压缩方式选择"><a href="#1-3-压缩方式选择" class="headerlink" title="1.3 压缩方式选择"></a>1.3 压缩方式选择</h2><h3 id="1-3-1-Gzip"><a href="#1-3-1-Gzip" class="headerlink" title="1.3.1 Gzip"></a>1.3.1 Gzip</h3><ul>
<li>优点：压缩率比较高，而且压缩/解压速度也比较快；Hadoop本身支持，在应用中处理Gzip格式的文件就和直接处理文本一样；大部分Linux系统都自带Gzip命令，使用方便。</li>
<li>缺点：不支持Split。</li>
<li>应用场景：当每个文件压缩之后在130M以内的（1个块大小内），都可以考虑用Gzip压缩格式。例如说一天或者一个小时的日志压缩成一个Gzip文件。</li>
</ul>
<h3 id="1-3-2-Bzip2"><a href="#1-3-2-Bzip2" class="headerlink" title="1.3.2 Bzip2"></a>1.3.2 Bzip2</h3><ul>
<li>优点：支持Split；具有很高的压缩率，比Gzip压缩率都高；Hadoop本身自带，使用方便</li>
<li>缺点：压缩/解压速度慢</li>
<li>应用场景：适合对速度要求不高，但需要较高的压缩率的时候；或者输出之后的数据比较大，处理之后的数据需要压缩存档减少磁盘空间并且以后数据用得比较少的情况；或者对单个很大的文本文件想压缩减少存储空间，同时又需要支持Split，而且兼容之前的应用程序的情况</li>
</ul>
<h3 id="1-3-3-Lzo压缩"><a href="#1-3-3-Lzo压缩" class="headerlink" title="1.3.3 Lzo压缩"></a>1.3.3 Lzo压缩</h3><ul>
<li>优点：压缩/解压速度也比较快，合理的压缩率；支持Split，是Hadoop中最流行的压缩格式；可以在Linux系统下安装lzop命令，使用方便</li>
<li>缺点：压缩率比Gzip要低一些；Hadoop本身不支持，需要安装；在应用中对Lzo格式的文件需要做一些特殊处理（为了支持Split需要建索引，还需要指定InputFormat为Lzo格式）</li>
<li>应用场景：一个很大的文本文件，压缩之后还大于200M以上的可以考虑，而且单个文件越大，Lzo优点越越明显</li>
</ul>
<h3 id="1-3-4-Snappy压缩"><a href="#1-3-4-Snappy压缩" class="headerlink" title="1.3.4 Snappy压缩"></a>1.3.4 Snappy压缩</h3><ul>
<li>优点：高速压缩速度和合理的压缩率</li>
<li>缺点：不支持Split；压缩率比Gzip要低；Hadoop本身不支持，需要安装</li>
<li>应用场景：当MapReduce作业的Map输出的数据比较大的时候，作为Map到Reduce的中间数据的压缩格式；或者作为一个MapReduce作业的输出和另外一个MapReduce作业的输入</li>
</ul>
<hr>
<h2 id="1-4-压缩位置选择"><a href="#1-4-压缩位置选择" class="headerlink" title="1.4 压缩位置选择"></a>1.4 压缩位置选择</h2><p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16345428872480.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
<hr>
<h2 id="1-5-压缩参数配置"><a href="#1-5-压缩参数配置" class="headerlink" title="1.5 压缩参数配置"></a>1.5 压缩参数配置</h2><p>要在Hadoop中启用压缩，可以配置如下参数：<br>|参数    |默认值    |阶段    |建议|<br>|——|——|——|—-|<br>|io.compression.codecs（在core-site.xml中配置）    |无，这个需要在命令行输入hadoop checknative查看    |输入压缩    |Hadoop使用文件扩展名判断是否支持某种编解码器|<br>|mapreduce.map.output.compress（在mapred-site.xml中配置）    |false    |mapper输出    |这个参数设为true启用压缩|<br>|mapreduce.map.output.compress.codec（在mapred-site.xml中配置）    |org.apache.hadoop.io.compress.DefaultCodec    |mapper输出    |企业多使用LZO或Snappy编解码器在此阶段压缩数据|<br>|mapreduce.output.fileoutputformat.compress（在mapred-site.xml中配置）    |false    |reducer输出    |这个参数设为true启用压缩|<br>|mapreduce.output.fileoutputformat.compress.codec（在mapred-site.xml中配置）    |org.apache.hadoop.io.compress.DefaultCodec    |reducer输出    |使用标准工具或者编解码器，如gzip和bzip2|<br>|mapreduce.output.fileoutputformat.compress.type（在mapred-site.xml中配置）    |RECORD    |reducer输出    |SequenceFile输出使用的压缩类型：NONE和BLOCK|</p>
<hr>
<h2 id="1-6-压缩实操案例"><a href="#1-6-压缩实操案例" class="headerlink" title="1.6 压缩实操案例"></a>1.6 压缩实操案例</h2><h3 id="1-6-1-数据流的压缩和解压缩"><a href="#1-6-1-数据流的压缩和解压缩" class="headerlink" title="1.6.1 数据流的压缩和解压缩"></a>1.6.1 数据流的压缩和解压缩</h3><ul>
<li>使用createOutputStream(OutputStreamout)方法创建一个CompressionOutputStream，将其以压缩格式写入底层的流</li>
<li>调用createInputStream(InputStreamin)函数，获得一个CompressionInputStream，从底层的流读取未压缩的数据</li>
</ul>
<h3 id="1-6-2-Map输出端压缩"><a href="#1-6-2-Map输出端压缩" class="headerlink" title="1.6.2 Map输出端压缩"></a>1.6.2 Map输出端压缩</h3><ul>
<li>即使MapReduce的输入输出文件都是未压缩的文件，仍然可以对Map任务的中间结果输出做压缩，因为它要写在硬盘并且通过网络传输到Reduce节点，对其压缩可以提高很多性能，这些工作只要设置两个属性即可。  <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 开启map端输出压缩</span></span><br><span class="line">conf.setBoolean(<span class="string">&quot;mapreduce.map.output.compress&quot;</span>, <span class="keyword">true</span>);</span><br><span class="line"><span class="comment">// 设置map端输出压缩方式</span></span><br><span class="line">conf.setClass(<span class="string">&quot;mapreduce.map.output.compress.codec&quot;</span>, BZip2Codec.class,CompressionCodec.class);</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="1-6-3-Reduce输出端压缩"><a href="#1-6-3-Reduce输出端压缩" class="headerlink" title="1.6.3 Reduce输出端压缩"></a>1.6.3 Reduce输出端压缩</h3><pre><code><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 设置reduce端输出压缩开启</span></span><br><span class="line">FileOutputFormat.setCompressOutput(job, <span class="keyword">true</span>);</span><br><span class="line"><span class="comment">// 设置压缩的方式</span></span><br><span class="line">FileOutputFormat.setOutputCompressorClass(job, BZip2Codec.class); </span><br></pre></td></tr></table></figure>
</code></pre>
<h1 id="二、Hadoop性能优化"><a href="#二、Hadoop性能优化" class="headerlink" title="二、Hadoop性能优化"></a>二、Hadoop性能优化</h1><h2 id="2-1-MapReduce跑得慢的原因"><a href="#2-1-MapReduce跑得慢的原因" class="headerlink" title="2.1 MapReduce跑得慢的原因"></a>2.1 MapReduce跑得慢的原因</h2><ol>
<li>计算机性能<ul>
<li>CPU，内存，磁盘，网络···</li>
</ul>
</li>
<li>I/O操作优化<ol>
<li>数据倾斜</li>
<li>Map和Reduce数设置不合理</li>
<li>Map运行时间太长，导致Reduce等待过久</li>
<li>小文件过多</li>
<li>大量不可切片的超大压缩文件</li>
<li>Spill次数过多</li>
<li>Merge次数过多</li>
</ol>
</li>
</ol>
<h2 id="2-2-MapReduce优化"><a href="#2-2-MapReduce优化" class="headerlink" title="2.2 MapReduce优化"></a>2.2 MapReduce优化</h2><ul>
<li>MapReduce优化方法主要从六个方面考虑：数据输入、Map阶段、Reduce阶段、IO传输、数据倾斜问题、常用的调优参数</li>
</ul>
<h3 id="2-2-1-数据输入"><a href="#2-2-1-数据输入" class="headerlink" title="2.2.1 数据输入"></a>2.2.1 数据输入</h3><ol>
<li>合并小文件：执行MR任务前将小文件合并，大量小文件会产生大量Map任务，增大Map任务装载次数，而任务的装载比较耗时，从而导致MR运行较慢</li>
<li>采用CombineTextInputFormat来作为输入，解决输入端大量小文件场景</li>
</ol>
<h3 id="2-2-2-Map阶段"><a href="#2-2-2-Map阶段" class="headerlink" title="2.2.2 Map阶段"></a>2.2.2 Map阶段</h3><ol>
<li>减少溢写（Spill）次数：通过调整mapreduce.task.io.sort.mb及mapreduce.map.sort.spill.percent参数值，增大触发Spill的内存上限，减少Spill次数，从而减少磁盘IO</li>
<li>减少合并（Merge）次数：通过调整mapreduce.task.io.sort.factor参数，增大Merge的文件数目，减少Merge的次数，从而缩短MR处理时间</li>
<li>在Map之后，不影响业务逻辑前提下，先进行Combine处理，减少 I/O</li>
</ol>
<h3 id="2-2-3-Reduce阶段"><a href="#2-2-3-Reduce阶段" class="headerlink" title="2.2.3 Reduce阶段"></a>2.2.3 Reduce阶段</h3><ol>
<li>合理设置Map和Reduce数：两个都不能设置太少，也不能设置太多。太少，会导致Task等待，延长处理时间；太多，会导致Map、Reduce任务间竞争资源，造成处理超时等错误</li>
<li>设置Map、Reduce共存：调整mapreduce.job.reduce.slowstart.completedmaps参数，使Map运行到一定程度后，Reduce也开始运行，减少Reduce的等待时间</li>
<li>规避使用Reduce：因为Reduce在用于连接数据集的时候将会产生大量的网络消耗</li>
<li>合理设置Reduce端的Buffer：默认情况下，数据达到一个阈值的时候，Buffer中的数据就会写入磁盘，然后Reduce会从磁盘中获得所有的数据。也就是说，Buffer和Reduce是没有直接关联的，中间多次写磁盘-&gt;读磁盘的过程，既然有这个弊端，那么就可以通过参数来配置，使得Buffer中的一部分数据可以直接输送到Reduce，从而减少IO开销：<font color ='red' >mapreduce.reduce.input.buffer.percent，默认为0.0。当值大于0的时候，会保留指定比例的内存读Buffer中的数据直接拿给Reduce使用。这样一来，设置Buffer需要内存，读取数据需要内存，Reduce计算也要内存，所以要根据作业的运行情况进行调整</font></li>
</ol>
<h3 id="2-2-4-I-O传输"><a href="#2-2-4-I-O传输" class="headerlink" title="2.2.4 I/O传输"></a>2.2.4 I/O传输</h3><ol>
<li>采用数据压缩的方式，减少网络I/O传输的数据量，从而减少I/O传输时间，安装Snappy和LZO压缩编码器。</li>
<li>使用SequenceFile二进制文件</li>
</ol>
<h3 id="2-2-5-数据倾斜问题"><a href="#2-2-5-数据倾斜问题" class="headerlink" title="2.2.5 数据倾斜问题"></a>2.2.5 数据倾斜问题</h3><ol>
<li>数据倾斜现象<ul>
<li>数据频率倾斜——某一个区域的数据量要远远大于其他区域</li>
<li>数据大小倾斜——部分记录的大小远远大于平均值</li>
</ul>
</li>
<li>减少数据倾斜的方法<ul>
<li>方法1：抽样和范围分区<ol>
<li>可以通过对原始数据进行抽样得到的结果集来预设分区边界值</li>
</ol>
</li>
<li>方法2：自定义分区<ol>
<li>基于输出键的背景知识进行自定义分区。例如，如果Map输出键的单词来源于一本书。且其中某几个专业词汇较多。那么就可以自定义分区将这这些专业词汇发送给固定的一部分Reduce实例。而将其他的都发送给剩余的Reduce实例</li>
</ol>
</li>
<li>方法3：Combiner<ol>
<li>使用Combiner可以大量地减小数据倾斜。在可能的情况下，Combine的目的就是聚合并精简数据</li>
</ol>
</li>
<li>方法4：采用Map Join，尽量避免Reduce Join</li>
</ul>
</li>
</ol>
<h2 id="2-3常用的调优参数"><a href="#2-3常用的调优参数" class="headerlink" title="2.3常用的调优参数"></a>2.3常用的调优参数</h2><ol>
<li><p>资源相关参数：<br> 在MR应用程序中配置就可以生效（mapred-default.xml）</p>
<table>
<thead>
<tr>
<th>配置参数</th>
<th>参数说明</th>
</tr>
</thead>
<tbody><tr>
<td>mapreduce.map.memory.mb</td>
<td>一个MapTask可使用的资源上限（单位:MB），默认为1024。如果MapTask实际使用的资源量超过该值，则会被强制杀死。</td>
</tr>
<tr>
<td>mapreduce.reduce.memory.mb</td>
<td>一个ReduceTask可使用的资源上限（单位:MB），默认为1024。如果ReduceTask实际使用的资源量超过该值，则会被强制杀死。</td>
</tr>
<tr>
<td>mapreduce.map.cpu.vcores</td>
<td>每个MapTask可使用的最多cpu core数目，默认值: 1</td>
</tr>
<tr>
<td>mapreduce.reduce.cpu.vcores</td>
<td>每个ReduceTask可使用的最多cpu core数目，默认值: 1</td>
</tr>
<tr>
<td>mapreduce.reduce.shuffle.parallelcopies</td>
<td>每个Reduce去Map中取数据的并行数。默认值是5</td>
</tr>
<tr>
<td>mapreduce.reduce.shuffle.merge.percent</td>
<td>Buffer中的数据达到多少比例开始写入磁盘。默认值0.66</td>
</tr>
<tr>
<td>mapreduce.reduce.shuffle.input.buffer.percent</td>
<td>Buffer大小占Reduce可用内存的比例。默认值0.7</td>
</tr>
<tr>
<td>mapreduce.reduce.input.buffer.percent</td>
<td>指定多少比例的内存用来存放Buffer中的数据，默认值是0.0</td>
</tr>
</tbody></table>
</li>
<li><p>在YARN启动之前就配置在服务器的配置文件中才能生效（yarn-default.xml）</p>
<table>
<thead>
<tr>
<th>配置参数</th>
<th>参数说明</th>
</tr>
</thead>
<tbody><tr>
<td>yarn.scheduler.minimum-allocation-mb</td>
<td>给应用程序Container分配的最小内存，默认值：1024</td>
</tr>
<tr>
<td>yarn.scheduler.maximum-allocation-mb</td>
<td>给应用程序Container分配的最大内存，默认值：8192</td>
</tr>
<tr>
<td>yarn.scheduler.minimum-allocation-vcores</td>
<td>每个Container申请的最小CPU核数，默认值：1</td>
</tr>
<tr>
<td>yarn.scheduler.maximum-allocation-vcores</td>
<td>每个Container申请的最大CPU核数，默认值：32</td>
</tr>
<tr>
<td>yarn.nodemanager.resource.memory-mb</td>
<td>给Containers分配的最大物理内存，默认值：8192</td>
</tr>
</tbody></table>
</li>
<li><p>Shuffle性能优化的关键参数，应在YARN启动之前就配置好（mapred-default.xml）</p>
<table>
<thead>
<tr>
<th>配置参数</th>
<th>参数说明</th>
</tr>
</thead>
<tbody><tr>
<td>mapreduce.task.io.sort.mb</td>
<td>Shuffle的环形缓冲区大小，默认100m</td>
</tr>
<tr>
<td>mapreduce.map.sort.spill.percent</td>
<td>环形缓冲区溢出的阈值，默认80%</td>
</tr>
</tbody></table>
</li>
<li><p>容错相关参数（MapReduce性能优化）</p>
<table>
<thead>
<tr>
<th>配置参数</th>
<th>参数说明</th>
</tr>
</thead>
<tbody><tr>
<td>mapreduce.map.maxattempts</td>
<td>每个Map Task最大重试次数，一旦重试次数超过该值，则认为Map Task运行失败，默认值：4。</td>
</tr>
<tr>
<td>mapreduce.reduce.maxattempts</td>
<td>每个Reduce Task最大重试次数，一旦重试次数超过该值，则认为Map Task运行失败，默认值：4。</td>
</tr>
<tr>
<td>mapreduce.task.timeout</td>
<td>Task超时时间，经常需要设置的一个参数，该参数表达的意思为：如果一个Task在一定时间内没有任何进入，即不会读取新的数据，也没有输出数据，则认为该Task处于Block状态，可能是卡住了，也许永远会卡住，为了防止因为用户程序永远Block住不退出，则强制设置了一个该超时时间（单位毫秒），默认是600000（10分钟）。如果你的程序对每条输入数据的处理时间过长（比如会访问数据库，通过网络拉取数据等），建议将该参数调大，该参数过小常出现的错误提示是：“AttemptID:attempt_14267829456721_123456_m_000224_0 Timed out after 300 secsContainer killed by the ApplicationMaster.”。</td>
</tr>
</tbody></table>
</li>
</ol>
<h2 id="2-4-Hadoop小文件优化方法"><a href="#2-4-Hadoop小文件优化方法" class="headerlink" title="2.4 Hadoop小文件优化方法"></a>2.4 Hadoop小文件优化方法</h2><h3 id="2-4-1-Hadoop小文件弊端"><a href="#2-4-1-Hadoop小文件弊端" class="headerlink" title="2.4.1 Hadoop小文件弊端"></a>2.4.1 Hadoop小文件弊端</h3><ul>
<li>HDFS上每个文件都要在NameNode上创建对应的元数据，这个元数据的大小约为150byte，这样当小文件比较多的时候，就会产生很多的元数据文件，一方面会大量占用NameNode的内存空间，另一方面就是元数据文件过多，使得寻址索引速度变慢。</li>
<li>小文件过多，在进行MR计算时，会生成过多切片，需要启动过多的MapTask。每个MapTask处理的数据量小，导致MapTask的处理时间比启动时间还小，白白消耗资源。</li>
</ul>
<h3 id="2-4-2-Hadoop小文件解决方案"><a href="#2-4-2-Hadoop小文件解决方案" class="headerlink" title="2.4.2 Hadoop小文件解决方案"></a>2.4.2 Hadoop小文件解决方案</h3><ol>
<li>小文件优化的方向：<ul>
<li>（1）在数据采集的时候，就将小文件或小批数据合成大文件再上传HDFS。</li>
<li>（2）在业务处理之前，在HDFS上使用MapReduce程序对小文件进行合并。</li>
<li>（3）在MapReduce处理时，可采用CombineTextInputFormat提高效率。</li>
<li>（4）开启uber模式，实现jvm重用</li>
</ul>
</li>
<li>Hadoop Archive<ul>
<li>是一个高效的将小文件放入HDFS块中的文件存档工具，能够将多个小文件打包成一个HAR文件，从而达到减少NameNode的内存使用</li>
</ul>
</li>
<li>SequenceFile<ul>
<li>SequenceFile是由一系列的二进制k/v组成，如果为key为文件名，value为文件内容，可将大批小文件合并成一个大文件</li>
</ul>
</li>
<li>CombineTextInputFormat<ul>
<li>CombineTextInputFormat用于将多个小文件在切片过程中生成一个单独的切片或者少量的切片。 </li>
</ul>
</li>
<li>开启uber模式，实现jvm重用。默认情况下，每个Task任务都需要启动一个jvm来运行，如果Task任务计算的数据量很小，我们可以让同一个Job的多个Task运行在一个Jvm中，不必为每个Task都开启一个Jvm. <ul>
<li>开启uber模式，在mapred-site.xml中添加如下配置  <figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!--  开启uber模式 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.job.ubertask.enable<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- uber模式中最大的mapTask数量，可向下修改  --&gt;</span> </span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.job.ubertask.maxmaps<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>9<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- uber模式中最大的reduce数量，可向下修改 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.job.ubertask.maxreduces<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- uber模式中最大的输入数据量，默认使用dfs.blocksize 的值，可向下修改 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.job.ubertask.maxbytes<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span><span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ol>
<h1 id="三、Hadoop新特性"><a href="#三、Hadoop新特性" class="headerlink" title="三、Hadoop新特性"></a>三、Hadoop新特性</h1><h2 id="3-1-Hadoop2-x新特性"><a href="#3-1-Hadoop2-x新特性" class="headerlink" title="3.1 Hadoop2.x新特性"></a>3.1 Hadoop2.x新特性</h2><h3 id="3-1-1-集群间数据拷贝"><a href="#3-1-1-集群间数据拷贝" class="headerlink" title="3.1.1 集群间数据拷贝"></a>3.1.1 集群间数据拷贝</h3><ul>
<li>采用distcp命令实现两个Hadoop集群之间的递归数据复制<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 ~]$ hadoop distcp hdfs://hadoop002:9820/WeCom_3.1.18.90318.dmg hdfs://hadoop002:9820/testDistct</span><br><span class="line">···</span><br><span class="line">···</span><br><span class="line">2021-10-19 17:56:03,705 INFO mapreduce.Job: Job job_1634633871057_0003 completed successfully</span><br><span class="line">2021-10-19 17:56:03,754 INFO mapreduce.Job: Counters: 36</span><br><span class="line">	File System Counters</span><br><span class="line">		FILE: Number of bytes <span class="built_in">read</span>=0</span><br><span class="line">		FILE: Number of bytes written=227128</span><br><span class="line">		FILE: Number of <span class="built_in">read</span> operations=0</span><br><span class="line">		FILE: Number of large <span class="built_in">read</span> operations=0</span><br><span class="line">		FILE: Number of write operations=0</span><br><span class="line">		HDFS: Number of bytes <span class="built_in">read</span>=325449295</span><br><span class="line">		HDFS: Number of bytes written=325448910</span><br><span class="line">		HDFS: Number of <span class="built_in">read</span> operations=19</span><br><span class="line">		HDFS: Number of large <span class="built_in">read</span> operations=0</span><br><span class="line">		HDFS: Number of write operations=5</span><br><span class="line">	Job Counters</span><br><span class="line">		Launched map tasks=1</span><br><span class="line">		Other <span class="built_in">local</span> map tasks=1</span><br><span class="line">		Total time spent by all maps <span class="keyword">in</span> occupied slots (ms)=5517</span><br><span class="line">		Total time spent by all reduces <span class="keyword">in</span> occupied slots (ms)=0</span><br><span class="line">		Total time spent by all map tasks (ms)=5517</span><br><span class="line">		Total vcore-milliseconds taken by all map tasks=5517</span><br><span class="line">		Total megabyte-milliseconds taken by all map tasks=5649408</span><br><span class="line">	Map-Reduce Framework</span><br><span class="line">		Map input records=1</span><br><span class="line">		Map output records=0</span><br><span class="line">		Input split bytes=136</span><br><span class="line">		Spilled Records=0</span><br><span class="line">		Failed Shuffles=0</span><br><span class="line">		Merged Map outputs=0</span><br><span class="line">		GC time elapsed (ms)=52</span><br><span class="line">		CPU time spent (ms)=1730</span><br><span class="line">		Physical memory (bytes) snapshot=270876672</span><br><span class="line">		Virtual memory (bytes) snapshot=2578894848</span><br><span class="line">		Total committed heap usage (bytes)=217055232</span><br><span class="line">		Peak Map Physical memory (bytes)=270876672</span><br><span class="line">		Peak Map Virtual memory (bytes)=2578894848</span><br><span class="line">	File Input Format Counters</span><br><span class="line">		Bytes Read=249</span><br><span class="line">	File Output Format Counters</span><br><span class="line">		Bytes Written=0</span><br><span class="line">	DistCp Counters</span><br><span class="line">		Bandwidth <span class="keyword">in</span> Btyes=81362227</span><br><span class="line">		Bytes Copied=325448910</span><br><span class="line">		Bytes Expected=325448910</span><br><span class="line">		Files Copied=1</span><br><span class="line">[atguigu@hadoop001 ~]$</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="3-1-2-小文件存档"><a href="#3-1-2-小文件存档" class="headerlink" title="3.1.2 小文件存档"></a>3.1.2 小文件存档</h3><ol>
<li>HDFS存储小文件弊端<ul>
<li>每个文件均按块存储，每个块的元数据存储在NameNode的内存中，因此HDFS存储小文件会非常低效。因为大量的小文件会耗尽NameNode中的大部分内存。但注意，存储小文件所需要的磁盘容量和数据块的大小无关。例如，一个1MB的文件设置为128MB的块存储，实际使用的是1MB的磁盘空间，而不是128MB</li>
</ul>
</li>
<li>解决存储小文件办法之一<ul>
<li>HDFS存档文件或HAR文件，是一个更高效的文件存档工具，它将文件存入HDFS块，在减少NameNode内存使用的同时，允许对文件进行透明的访问。具体说来，HDFS存档文件对内还是一个一个独立文件，对NameNode而言却是一个整体，减少了NameNode的内存<br><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16346376074095.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></li>
</ul>
</li>
<li>案例实操<ul>
<li>归档文件:把/user/atguigu/input目录里面的所有文件归档成一个叫input.har的归档文件，并把归档后文件存储到/user/atguigu/output路径下。  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 hadoop-3.1.3]$ hadoop archive -archiveName input.har -p  /user/atguigu/input   /user/atguigu/output</span><br></pre></td></tr></table></figure></li>
<li>查看归档  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 hadoop-3.1.3]$ hadoop fs -ls /user/atguigu/output/input.har</span><br><span class="line">[atguigu@hadoop001 hadoop-3.1.3]$ hadoop fs -ls har:///user/atguigu/output/input.har</span><br></pre></td></tr></table></figure></li>
<li>解归档文件  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 hadoop-3.1.3]$ hadoop fs -cp har:/// user/atguigu/output/input.har/*    /user/atguigu</span><br></pre></td></tr></table></figure>
<h3 id="3-1-2-回收站"><a href="#3-1-2-回收站" class="headerlink" title="3.1.2 回收站"></a>3.1.2 回收站</h3></li>
</ul>
</li>
</ol>
<ul>
<li>开启回收站功能，可以将删除的文件在不超时的情况下，恢复原数据，起到防止误删除、备份等作用</li>
<li>开启回收站功能参数说明<ol>
<li>默认值fs.trash.interval=0，0表示禁用回收站;其他值表示设置文件的存活时间</li>
<li>默认值fs.trash.checkpoint.interval=0，检查回收站的间隔时间。如果该值为0，则该值设置和fs.trash.interval的参数值相等</li>
<li>要求fs.trash.checkpoint.interval&lt;=fs.trash.interval</li>
</ol>
</li>
<li>回收站工作机制<br>  <img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16346388522940.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></li>
<li>回收站使用<ol>
<li>启用回收站：修改core-site.xml，配置垃圾回收时间为1分钟。 <figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.trash.interval<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.trash.checkpoint.interval<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li>
<li>查看回收站 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">#回收站目录在hdfs集群中的路径：</span></span><br><span class="line">/user/atguigu/.Trash/</span><br></pre></td></tr></table></figure></li>
<li>通过程序删除的文件不会经过回收站，需要调用moveToTrash()才进入回收站 <figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Trash trash = <span class="function">New <span class="title">Trash</span><span class="params">(conf)</span></span>;</span><br><span class="line">trash.moveToTrash(path);</span><br></pre></td></tr></table></figure></li>
<li>通过网页上直接删除的文件也不会走回收站。</li>
<li>只有在命令行利用hadoop fs -rm命令删除的文件才会走回收站。 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 hadoop-3.1.3]$ hadoop fs -rm -r /user/atguigu/input</span><br><span class="line">2020-07-14 16:13:42,643 INFO fs.TrashPolicyDefault: Moved: <span class="string">&#x27;hdfs://hadoop001:9820/user/atguigu/input&#x27;</span> to trash at: hdfs://hadoop001:9820/user/atguigu/.Trash/Current/user/atguigu/input</span><br></pre></td></tr></table></figure></li>
<li>恢复回收站数据 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 hadoop-3.1.3]$ hadoop fs -mv /user/atguigu/.Trash/Current/user/atguigu/input    /user/atguigu/input</span><br></pre></td></tr></table></figure></li>
</ol>
</li>
</ul>
<h2 id="3-2-Hadoop3-x新特性"><a href="#3-2-Hadoop3-x新特性" class="headerlink" title="3.2 Hadoop3.x新特性"></a>3.2 Hadoop3.x新特性</h2><h3 id="3-2-1-多NN的HA架构"><a href="#3-2-1-多NN的HA架构" class="headerlink" title="3.2.1 多NN的HA架构"></a>3.2.1 多NN的HA架构</h3><ul>
<li>HDFS NameNode高可用性的初始实现为单个活动NameNode和单个备用NameNode，将edits复制到三个JournalNode。该体系结构能够容忍系统中一个NN或一个JN的故障。但是，某些部署需要更高程度的容错能力。</li>
<li>Hadoop3.x允许用户运行多个备用NameNode。例如，通过配置三个NameNode和五个JournalNode，群集能够容忍两个节点而不是一个节点的故障。</li>
</ul>
<h3 id="3-2-2-纠删码"><a href="#3-2-2-纠删码" class="headerlink" title="3.2.2 纠删码"></a>3.2.2 纠删码</h3><ul>
<li>HDFS中的默认3副本方案在存储空间和其他资源（例如，网络带宽）中具有200％的开销。但是，对于I / O活动相对较低暖和冷数据集，在正常操作期间很少访问其他块副本，但仍会消耗与第一个副本相同的资源量。</li>
<li>纠删码（Erasure Coding）能够在不到50% 的数据冗余情况下提供和3副本相同的容错能力，因此，使用纠删码作为副本机制的改进是自然而然的。</li>
<li>查看集群支持的纠删码策略：hdfs ec -listPolicies</li>
</ul>
<p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16346151537039.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
<h1 id="四、HadoopHA高可用"><a href="#四、HadoopHA高可用" class="headerlink" title="四、HadoopHA高可用"></a>四、HadoopHA高可用</h1><h2 id="4-1-现有集群存在哪些问题？"><a href="#4-1-现有集群存在哪些问题？" class="headerlink" title="4.1 现有集群存在哪些问题？"></a>4.1 现有集群存在哪些问题？</h2><ol>
<li>HDFS集群 单个NN场景下NN如果故障了，整个HDFS集群就不可用（中心化集群） <ul>
<li>解决方案：配置多个NN !!!</li>
</ul>
</li>
<li>多个NN的场景下由哪一台对外进行服务？<ul>
<li>当HDFS实现多NN的高可用后，但是只有一台 NN 对外提供服务(Active)，其他的NN都是替补（Standby），当正在提供服务的NN宕机故障，其他的NN自动切换成Active状态</li>
</ul>
</li>
<li>HA实现后，元数据的管理策略是否发生改变？<ul>
<li>不改变，但是在HA的HDFS中合并 fsimage和edits编辑日志的合并工作交给Standby状态的NN去完成！</li>
</ul>
</li>
<li>2NN 在高可用的集群中还要不要？<ul>
<li>不要了！2NN的工作有Standby状态的NN完成！</li>
</ul>
</li>
<li>为了保证整个集群中的所有NN 能够共享元数据信息，会新增一个 JournalNode 服务，在集群中我们会启动多个JournalNode服务 形成一个集群，每个JournalNode服务对应一个NN。进行数共享！</li>
<li>JournalNode如何实现元数据的共享？<ul>
<li>在集群状态下，当一个请求对元数据进行更改的时候，此时Active状态的NN会处理请求，会往磁盘上的编辑日志edits文件追加记录，并且会通过当前机器的JournalNode服务同步edits日志文件。接下来请求也会被转发到Standby状态的NN上，Standby状态的NN接收到请求后，只去读取自己的JournalNode服务中保存的最新的编辑日志信息，加载内存中形成最新的元数据映像，保证一旦Active状态的NN宕机，Standby自己马上顶上后能够展示最新的元数据。  </li>
<li>到达checkPoint之后，Standby都会尝试进行合并Edit和Fsimage，以接收到的第一个为准</li>
</ul>
</li>
<li>当一台NN故障后，其他NN如何争抢上位？<ul>
<li>采用高可用集群中的自动故障转移机制来完成切换。</li>
</ul>
</li>
<li>自动故障转移的机制如何实现？</li>
</ol>
<h2 id="4-2-HDFS-HA工作机制"><a href="#4-2-HDFS-HA工作机制" class="headerlink" title="4.2 HDFS-HA工作机制"></a>4.2 HDFS-HA工作机制</h2><ul>
<li>通过多个NameNode消除单点故障<h3 id="4-2-1-HDFS-HA工作要点"><a href="#4-2-1-HDFS-HA工作要点" class="headerlink" title="4.2.1 HDFS-HA工作要点"></a>4.2.1 HDFS-HA工作要点</h3></li>
</ul>
<ol>
<li>元数据管理方式需要改变<ul>
<li>内存中各自保存一份元数据；</li>
<li>Edits日志只有Active状态的NameNode节点可以做写操作；</li>
<li>所有的NameNode都可以读取Edits；</li>
<li>共享的Edits放在一个共享存储中管理（qjournal和NFS两个主流实现）；</li>
</ul>
</li>
<li>需要一个状态管理功能模块<ul>
<li>实现了一个zkfailover，常驻在每一个namenode所在的节点，每一个zkfailover负责监控自己所在NameNode节点，利用zk进行状态标识，当需要进行状态切换时，由zkfailover来负责切换，切换时需要防止brain split现象的发生。</li>
</ul>
</li>
<li>必须保证两个NameNode之间能够ssh无密码登录</li>
<li>隔离（Fence），即同一时刻仅仅有一个NameNode对外提供服务</li>
</ol>
<h3 id="4-2-2-HDFS-HA自动故障转移工作机制"><a href="#4-2-2-HDFS-HA自动故障转移工作机制" class="headerlink" title="4.2.2 HDFS-HA自动故障转移工作机制"></a>4.2.2 HDFS-HA自动故障转移工作机制</h3><p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16346403266012.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
<ul>
<li>自动故障转移为HDFS部署增加了两个新组件：ZooKeeper和ZKFailoverController（ZKFC）进程。</li>
<li>ZooKeeper是维护少量协调数据，通知客户端这些数据的改变和监视客户端故障的高可用服务。</li>
<li>HA的自动故障转移依赖于ZooKeeper的以下功能：<ol>
<li>故障检测<ul>
<li>集群中的每个NameNode在ZooKeeper中维护了一个会话，如果机器崩溃，ZooKeeper中的会话将终止，ZooKeeper通知另一个NameNode需要触发故障转移。</li>
</ul>
</li>
<li>现役NameNode选择<ul>
<li>ZooKeeper提供了一个简单的机制用于唯一的选择一个节点为active状态。如果目前现役NameNode崩溃，另一个节点可能从ZooKeeper获得特殊的排外锁以表明它应该成为现役NameNode。</li>
</ul>
</li>
</ol>
</li>
<li>ZKFC是自动故障转移中的另一个新组件，是ZooKeeper的客户端，也监视和管理NameNode的状态。每个运行NameNode的主机也运行了一个ZKFC进程，ZKFC负责：<ol>
<li>健康监测<ul>
<li>ZKFC使用一个健康检查命令定期地ping与之在相同主机的NameNode，只要该NameNode及时地回复健康状态，ZKFC认为该节点是健康的。如果该节点崩溃，冻结或进入不健康状态，健康监测器标识该节点为非健康的。</li>
</ul>
</li>
<li>ZooKeeper会话管理<ul>
<li>当本地NameNode是健康的，ZKFC保持一个在ZooKeeper中打开的会话。如果本地NameNode处于active状态，ZKFC也保持一个特殊的znode锁，该锁使用了ZooKeeper对短暂节点的支持，如果会话终止，锁节点将自动删除。</li>
</ul>
</li>
<li>基于ZooKeeper的选择<ul>
<li>如果本地NameNode是健康的，且ZKFC发现没有其它的节点当前持有znode锁，它将为自己获取该锁。如果成功，则它已经赢得了选择，并负责运行故障转移进程以使它的本地NameNode为Active。</li>
</ul>
</li>
</ol>
</li>
</ul>
<h2 id="4-3-HDFS-HA集群配置"><a href="#4-3-HDFS-HA集群配置" class="headerlink" title="4.3 HDFS-HA集群配置"></a>4.3 HDFS-HA集群配置</h2><h3 id="4-3-1-环境准备"><a href="#4-3-1-环境准备" class="headerlink" title="4.3.1 环境准备"></a>4.3.1 环境准备</h3><ul>
<li>准备三台服务器</li>
<li>JDK,Hadoop安装包</li>
<li>干净的集群</li>
<li>环境变量</li>
</ul>
<h3 id="4-3-2-规划集群"><a href="#4-3-2-规划集群" class="headerlink" title="4.3.2 规划集群"></a>4.3.2 规划集群</h3><table>
<thead>
<tr>
<th>hadoop001</th>
<th>hadoop002</th>
<th>hadoop003</th>
</tr>
</thead>
<tbody><tr>
<td>NameNode</td>
<td>NameNode</td>
<td>NameNode</td>
</tr>
<tr>
<td>ZKFC</td>
<td>ZKFC</td>
<td>ZKFC</td>
</tr>
<tr>
<td>JournalNode</td>
<td>JournalNode</td>
<td>JournalNode</td>
</tr>
<tr>
<td>DataNode</td>
<td>DataNode</td>
<td>DataNode</td>
</tr>
<tr>
<td>ZK</td>
<td>ZK</td>
<td>ZK</td>
</tr>
<tr>
<td>-</td>
<td>ResourceManager</td>
<td>-</td>
</tr>
<tr>
<td>NodeManager</td>
<td>NodeManager</td>
<td>NodeManager</td>
</tr>
</tbody></table>
<h3 id="4-3-3-配置HA集群"><a href="#4-3-3-配置HA集群" class="headerlink" title="4.3.3 配置HA集群"></a>4.3.3 配置HA集群</h3><ol>
<li>修改配置文件 core-site.xml  <figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">	<span class="comment">&lt;!-- 把多个NameNode的地址组装成一个集群mycluster --&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	  <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	  <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://mycluster<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">	<span class="comment">&lt;!-- 指定hadoop数据的存储目录 --&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/module/ha/hadoop-3.1.3/data<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">	<span class="comment">&lt;!-- 配置HDFS网页登录使用的静态用户为atguigu --&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.http.staticuser.user<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>atguigu<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">	<span class="comment">&lt;!-- 配置该atguigu(superUser)允许通过代理访问的主机节点 --&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.atguigu.hosts<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="comment">&lt;!-- 配置该atguigu(superUser)允许通过代理用户所属组 --&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.atguigu.groups<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="comment">&lt;!-- 配置该atguigu(superUser)允许通过代理的用户--&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.atguigu.users<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure></li>
<li>修改配置文件 hdfs-site.xml    <figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">	<span class="comment">&lt;!-- NameNode数据存储目录 --&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>file://$&#123;hadoop.tmp.dir&#125;/name<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="comment">&lt;!-- DataNode数据存储目录 --&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.datanode.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>file://$&#123;hadoop.tmp.dir&#125;/data<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="comment">&lt;!-- JournalNode数据存储目录 --&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.journalnode.edits.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>$&#123;hadoop.tmp.dir&#125;/jn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="comment">&lt;!-- 完全分布式HDFS集群名称 --&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.nameservices<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>mycluster<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="comment">&lt;!-- 集群中NameNode节点都有哪些 --&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.namenodes.mycluster<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>nn1,nn2,nn3<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="comment">&lt;!-- NameNode的RPC通信地址 --&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.rpc-address.mycluster.nn1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop001:9820<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.rpc-address.mycluster.nn2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop002:9820<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.rpc-address.mycluster.nn3<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop003:9820<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="comment">&lt;!-- NameNode的Web通信地址 --&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.http-address.mycluster.nn1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop001:9870<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.http-address.mycluster.nn2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop002:9870<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.http-address.mycluster.nn3<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop003:9870<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="comment">&lt;!-- 指定NameNode元数据在JournalNode上的存放位置 --&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.shared.edits.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>qjournal://hadoop001:8485;hadoop002:8485;hadoop003:8485/mycluster<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="comment">&lt;!-- 访问代理类：client用于确定哪个NameNode为Active --&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.client.failover.proxy.provider.mycluster<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="comment">&lt;!-- 配置隔离机制，即同一时刻只能有一台服务器对外响应 --&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.fencing.methods<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>sshfence<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="comment">&lt;!-- 使用隔离机制时需要ssh秘钥登录--&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.fencing.ssh.private-key-files<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/atguigu/.ssh/id_rsa<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	</span><br><span class="line">  <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure></li>
<li>修改每一台机器的HADOOP_HOME 的环境变量打开 /etc/profile.d/set_evn.sh 修改如下： <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">HADOOP_HOME=/opt/module/ha/hadoop-3.1.3</span><br></pre></td></tr></table></figure></li>
<li>将/opt/ha/hadoop-3.1.3 分发到103 和 104 并且修改103和104的 HADOOP_HOME=/opt/module/ha/hadoop-3.1.3<ul>
<li>注意：修改完环境变量后一定要重新加载 profile 文件</li>
</ul>
</li>
<li>在102、103、104 各个JournalNode节点上，输入以下命令启动journalnode服务 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hdfs --daemon start journalnode</span><br></pre></td></tr></table></figure></li>
<li>在 hadoop001的 nn1 上，对其进行格式化，并启动 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hdfs namenode -format</span><br><span class="line">hdfs --daemon start namenode</span><br></pre></td></tr></table></figure></li>
<li>分别在 hadoop002的nn2 和 hadoop003的nn3上，同步nn1的元数据信息 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hdfs namenode -bootstrapStandby</span><br></pre></td></tr></table></figure></li>
<li>分别在 hadoop002上启动nn2 和 hadoop003上启动nn3 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hdfs --daemon start namenode</span><br></pre></td></tr></table></figure></li>
<li>通过web地址访问nn1 nn2 nn3    <ul>
<li>nn1:<a href="http://hadoop001:9870/">http://hadoop001:9870</a></li>
<li>nn2:<a href="http://hadoop002:9870/">http://hadoop002:9870</a></li>
<li>nn3:<a href="http://hadoop003:9870/">http://hadoop003:9870</a></li>
</ul>
</li>
<li>在每台机器上启动DN <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">    hdfs --daemon start datanode</span><br><span class="line">    ```   </span><br><span class="line">9. 将其中的一个nn切换成Active状态</span><br><span class="line">    ```bash</span><br><span class="line">    hdfs haadmin -transitionToActive nn1</span><br></pre></td></tr></table></figure></li>
<li>查看是否Active<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hdfs haadmin -getServiceState nn1</span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="4-3-4-实现HA的故障自动转移"><a href="#4-3-4-实现HA的故障自动转移" class="headerlink" title="4.3.4 实现HA的故障自动转移"></a>4.3.4 实现HA的故障自动转移</h3><ol>
<li>在core-site.xml文件中增加 <figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 指定zkfc要连接的zkServer地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>ha.zookeeper.quorum<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop001:2181,hadoop002:2181,hadoop003:2181<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li>
<li>在hdfs-site.xml中增加 <figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 启用nn故障自动转移 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.automatic-failover.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li>
<li>修改后分发配置文件<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">   xsync /opt/module/ha/hadoop-3.1.3/etc/hadoop </span><br><span class="line">   ```   </span><br><span class="line">4. 关闭HDFS集群</span><br><span class="line">   ```bash</span><br><span class="line">   stop-dfs.sh</span><br></pre></td></tr></table></figure></li>
<li>启动Zookeeper集群<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">zk.sh start</span><br></pre></td></tr></table></figure></li>
<li>初始化HA在Zookeeper中状态<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hdfs zkfc -formatZK</span><br></pre></td></tr></table></figure></li>
<li>启动HDFS服务<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">start-dfs.sh</span><br></pre></td></tr></table></figure></li>
<li>可以去zkCli.sh客户端查看Namenode选举锁节点内容 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">get /hadoop-ha/mycluster/ActiveStandbyElectorLock</span><br></pre></td></tr></table></figure></li>
<li>测试故障自动转移<ul>
<li>将当前状态为Active的namenode 杀死</li>
<li>刷新另外两台namenode的web端，关注状态</li>
<li>最后可以到zk中验证锁内容的名称</li>
</ul>
</li>
</ol>
<h2 id="4-4-YARN-HA配置"><a href="#4-4-YARN-HA配置" class="headerlink" title="4.4 YARN-HA配置"></a>4.4 YARN-HA配置</h2><p>YARN HA 集群搭建步骤</p>
<ol>
<li>修改yarn-site.xml <figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 启用resourcemanager ha --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.ha.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 声明三台resourcemanager的地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.cluster-id<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>cluster-yarn1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--指定resourcemanager的逻辑列表--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.ha.rm-ids<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>rm1,rm2,rm3<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- ========== rm1的配置 ========== --&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 指定rm1的主机名 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname.rm1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop001<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 指定rm1的web端地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.webapp.address.rm1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop001:8088<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 指定rm1的 RPC 通信地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.address.rm1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop001:8032<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 指定AM向rm1申请资源的地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.scheduler.address.rm1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop001:8030<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 指定供NM连接的地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.resource-tracker.address.rm1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop001:8031<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- ========== rm2的配置 ========== --&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 指定rm2的主机名 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname.rm2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop002<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.webapp.address.rm2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop002:8088<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.address.rm2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop002:8032<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.scheduler.address.rm2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop002:8030<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.resource-tracker.address.rm2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop002:8031<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- ========== rm3的配置 ========== --&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 指定rm3的主机名 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname.rm3<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop003<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.webapp.address.rm3<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop003:8088<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.address.rm3<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop003:8032<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.scheduler.address.rm3<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop003:8030<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.resource-tracker.address.rm3<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop003:8031<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 指定zookeeper集群的地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.zk-address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop001:2181,hadoop002:2181,hadoop003:2181<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 启用自动恢复,启用自动故障转移 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.recovery.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 指定resourcemanager的状态信息存储在zookeeper集群 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.store.class<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li>
<li>将yarn-site.xml文件进行分发<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">xsync /opt/module/ha/hadoop-3.1.3/etc/hadoop/yarn-site.xml</span><br></pre></td></tr></table></figure></li>
<li>在任意的机器上启动yarn <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">start-yarn.sh</span><br></pre></td></tr></table></figure></li>
<li>通过访问web地址验证<br> <img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16346426629749.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"> </li>
<li>测试Yarn故障自动转移<ul>
<li>kill 当前active节点，会有另一个standby节点自动升级成active</li>
</ul>
</li>
</ol>
<h2 id="4-5-HDFS-Federation架构设计"><a href="#4-5-HDFS-Federation架构设计" class="headerlink" title="4.5 HDFS Federation架构设计"></a>4.5 HDFS Federation架构设计</h2><h3 id="4-5-1-NameNode架构的局限性"><a href="#4-5-1-NameNode架构的局限性" class="headerlink" title="4.5.1 NameNode架构的局限性"></a>4.5.1 NameNode架构的局限性</h3><ol>
<li>Namespace（命名空间）的限制<ul>
<li>由于NameNode在内存中存储所有的元数据（metadata），因此单个NameNode所能存储的对象（文件+块）数目受到NameNode所在JVM的heap size的限制。50G的heap能够存储20亿（200million）个对象，这20亿个对象支持4000个DataNode，12PB的存储（假设文件平均大小为40MB）。随着数据的飞速增长，存储的需求也随之增长。单个DataNode从4T增长到36T，集群的尺寸增长到8000个DataNode。存储的需求从12PB增长到大于100PB。</li>
</ul>
</li>
<li>隔离问题<ul>
<li>由于HDFS仅有一个NameNode，无法隔离各个程序，因此HDFS上的一个实验程序就很有可能影响整个HDFS上运行的程序。</li>
</ul>
</li>
<li>性能的瓶颈<ul>
<li>由于是单个NameNode的HDFS架构，因此整个HDFS文件系统的吞吐量受限于单个NameNode的吞吐量。</li>
</ul>
</li>
</ol>
<h3 id="4-5-2-HDFS-Federation架构设计"><a href="#4-5-2-HDFS-Federation架构设计" class="headerlink" title="4.5.2 HDFS Federation架构设计"></a>4.5.2 HDFS Federation架构设计</h3><p>多个NameNode集群管理不同业务线的元数据<br>| NameNode | NameNode | NameNode          |<br>| ——– | ——– | —————– |<br>| 元数据   | 元数据   | 元数据            |<br>| Log      | machine  | 电商数据/话单数据 |</p>
<p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16346432680052.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
<h3 id="4-5-3-HDFS-Federation应用思考"><a href="#4-5-3-HDFS-Federation应用思考" class="headerlink" title="4.5.3 HDFS Federation应用思考"></a>4.5.3 HDFS Federation应用思考</h3><p>不同应用可以使用不同NameNode进行数据管理图片业务、爬虫业务、日志审计业务。Hadoop生态系统中，不同的框架使用不同的NameNode进行管理NameSpace。（隔离性）<br><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16346433814694.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
<h3 id="4-5-4-联邦机制原理："><a href="#4-5-4-联邦机制原理：" class="headerlink" title="4.5.4 联邦机制原理："></a>4.5.4 联邦机制原理：</h3><ol>
<li>将NameNode划分成不同的命名空间并进行编号。不同的命名空间之间相互隔离互不干扰。</li>
<li>在DataNode中创建目录，此目录对应命名空间的编号。</li>
<li>由此，编号相同的数据由对应的命名空间进行管理</li>
<li>适用场景分析 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">128G(内存空间大小) * 1024(M) * 1024(KB) * 1024(bety) / 150 = xxx（元数据的数量）</span><br><span class="line">xxx * 256M（每一个文件大小） = yyy</span><br><span class="line">yyy / 1024(G) / 1024(TB) / 1024(PB) = 200 左右PB的数据</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h1 id="问答"><a href="#问答" class="headerlink" title="问答"></a>问答</h1><h3 id="一、Hadoop中的压缩作为一种常用的优化手段，经常被用在什么场景下？"><a href="#一、Hadoop中的压缩作为一种常用的优化手段，经常被用在什么场景下？" class="headerlink" title="一、Hadoop中的压缩作为一种常用的优化手段，经常被用在什么场景下？"></a>一、Hadoop中的压缩作为一种常用的优化手段，经常被用在什么场景下？</h3><ol>
<li>降低磁盘占用</li>
<li>减少网络IO</li>
</ol>
<h3 id="二、如果想要使用压缩，Hadoop如何对某一种压缩编码格式进行取舍？"><a href="#二、如果想要使用压缩，Hadoop如何对某一种压缩编码格式进行取舍？" class="headerlink" title="二、如果想要使用压缩，Hadoop如何对某一种压缩编码格式进行取舍？"></a>二、如果想要使用压缩，Hadoop如何对某一种压缩编码格式进行取舍？</h3><ol>
<li>压缩/解压的速度，资源占用</li>
<li>压缩率</li>
<li>压缩后是否支持切片</li>
</ol>
<h3 id="三、你们公司常用的压缩方式有哪些？"><a href="#三、你们公司常用的压缩方式有哪些？" class="headerlink" title="三、你们公司常用的压缩方式有哪些？"></a>三、你们公司常用的压缩方式有哪些？</h3><ol>
<li>单文件压缩后再130M以内使用gzip，如每天的日志文件，可以支持并行处理</li>
<li>单文件压缩后大于400M考虑支持切片的lzo 或者bzip</li>
</ol>
<h3 id="四、从哪些方面定位MR执行的效率（如何分析MR执行慢的原因）"><a href="#四、从哪些方面定位MR执行的效率（如何分析MR执行慢的原因）" class="headerlink" title="四、从哪些方面定位MR执行的效率（如何分析MR执行慢的原因）"></a>四、从哪些方面定位MR执行的效率（如何分析MR执行慢的原因）</h3><ol>
<li>计算机性能<ul>
<li>cpu，内存，磁盘，网络</li>
</ul>
</li>
<li>I/O操作优化<ol>
<li>数据倾斜</li>
<li>Map和Reduce数量设置不合理</li>
<li>Map运行时间过长，导致Reduce等待过久</li>
<li>小文件过多</li>
<li>大量不可切片的超大压缩文件</li>
<li>Spill次数过多</li>
<li>Merge次数过多</li>
</ol>
</li>
</ol>
<h3 id="五、如果想对MR程序进行优化，应该从哪些方面入手以及可能用到的优化手段？"><a href="#五、如果想对MR程序进行优化，应该从哪些方面入手以及可能用到的优化手段？" class="headerlink" title="五、如果想对MR程序进行优化，应该从哪些方面入手以及可能用到的优化手段？"></a>五、如果想对MR程序进行优化，应该从哪些方面入手以及可能用到的优化手段？</h3><ol>
<li>数据输入<ol>
<li>合并小文件</li>
<li>使用CombineTextInputFormat作为输入解决小文件场景</li>
</ol>
</li>
<li>Map阶段<ol>
<li>减少Spill溢写次数，调整环形缓冲区大小和触发溢写的内存上限，减少磁盘IO</li>
<li>减少Merge合并次数：调整一次merge合并的Spill溢写文件数量，减少Merge次数</li>
<li>Map之后合理使用Combine，减少网络IO</li>
</ol>
</li>
<li>Reduce阶段<ol>
<li>合理设置Map和Reduce数量</li>
<li>设置Map和Reduce并行处理，减少Reduce等待时间</li>
<li>规避使用Reduce，减少连接数据集产生的网络IO</li>
<li>合理设置Reduce端的Buffer</li>
</ol>
</li>
<li>IO传输<ol>
<li>选择合适的压缩算法，减少网络I\O传输的数据量</li>
<li>使用sequenceFile二进制文件</li>
</ol>
</li>
<li>数据倾斜<ol>
<li>对原始数据进行抽样得到结果集来预设分区边界值</li>
<li>根据业务分析自定义分区</li>
<li>使用combiner减少数据倾斜</li>
<li>尽量使用MapJoin，避免ReduceJoin</li>
</ol>
</li>
<li>参数调优<ol>
<li>MR源相关配置：MapTask和ReduceTask使用的内存，cpu</li>
<li>Yarn资源配置：Container使用的内存，cpu</li>
<li>Shuffer配置：环形缓冲区的大小，触发溢写的内存比例</li>
<li>容错配置：任务重试次数，超时时间</li>
</ol>
</li>
</ol>
<h3 id="六、在Hadoop针对小文件的处理方案有哪些？"><a href="#六、在Hadoop针对小文件的处理方案有哪些？" class="headerlink" title="六、在Hadoop针对小文件的处理方案有哪些？"></a>六、在Hadoop针对小文件的处理方案有哪些？</h3><ol>
<li>数据采集的时候，将小文件或小批数据合并成大文件在上传HDFS，从源头上避免小文件产生</li>
<li>业务处理之前，使用MapReduce程序对HDFS上的小文件进行合并</li>
<li>使用CombineTextInputFormat处理小文件的输入</li>
<li>开启uber模式，实现jvm重用</li>
</ol>
<h3 id="七、如何解决MR中Reduce的数据倾斜问题？"><a href="#七、如何解决MR中Reduce的数据倾斜问题？" class="headerlink" title="七、如何解决MR中Reduce的数据倾斜问题？"></a>七、如何解决MR中Reduce的数据倾斜问题？</h3><ol>
<li>对原始数据进行抽样得到结果集来预设分区边界值</li>
<li>根据业务分析自定义分区</li>
<li>使用combiner减少数据倾斜</li>
<li>尽量使用MapJoin，避免ReduceJoin</li>
</ol>
<h3 id="八、大概简述一下-Hadoop每一代版本的新特性？"><a href="#八、大概简述一下-Hadoop每一代版本的新特性？" class="headerlink" title="八、大概简述一下 Hadoop每一代版本的新特性？"></a>八、大概简述一下 Hadoop每一代版本的新特性？</h3><ol>
<li>Hadoop 2.x<ul>
<li>distcp命令实现两个Hadoop集群之间的递归数据复制</li>
<li>小文件存档</li>
<li>回收站</li>
</ul>
</li>
<li>Hadoop 3.x<ul>
<li>多NN的HA架构：提高集群的可用性</li>
<li>纠删码：降低磁盘占用</li>
</ul>
</li>
</ol>
<h3 id="九、什么是Hadoop的HA"><a href="#九、什么是Hadoop的HA" class="headerlink" title="九、什么是Hadoop的HA?"></a>九、什么是Hadoop的HA?</h3><ol>
<li>集群可实现7*24小时不中断服务</li>
<li>不存在单点故障</li>
<li>可以实现故障自动转移</li>
</ol>
<h3 id="十、描述一下HDFS-HA的工作机制？"><a href="#十、描述一下HDFS-HA的工作机制？" class="headerlink" title="十、描述一下HDFS-HA的工作机制？"></a>十、描述一下HDFS-HA的工作机制？</h3><ol>
<li>多NN消除单点故障</li>
<li>由Active状态的NN负责写操作，JournalNode负责同步Edits，Standby状态的NN读取自己的Edit，加载到内存形成完整元数据</li>
<li>Standby状态的NN负责合并Edit和FsImage</li>
<li>依赖Zookeeper实现故障自动转移</li>
</ol>
<h3 id="十一、如何实现HA的集群搭建-用话术描述即可！！！"><a href="#十一、如何实现HA的集群搭建-用话术描述即可！！！" class="headerlink" title="十一、如何实现HA的集群搭建?(用话术描述即可！！！)"></a>十一、如何实现HA的集群搭建?(用话术描述即可！！！)</h3><ol>
<li>配置集群名称</li>
<li>配置集群节点</li>
<li>配置JournalNode</li>
<li>配置zookeeper连接地址</li>
</ol>
<h3 id="十二、HDFS如何实现自动故障转移？"><a href="#十二、HDFS如何实现自动故障转移？" class="headerlink" title="十二、HDFS如何实现自动故障转移？"></a>十二、HDFS如何实现自动故障转移？</h3><ol>
<li>HDFS故障自动转移依赖zookeeper和zkfc进程</li>
<li>zookeeper实现功能<ol>
<li>故障检测：集群中的每个NameNode在ZooKeeper中维护了一个会话，如果机器崩溃，ZooKeeper中的会话将终止，ZooKeeper通知另一个NameNode触发故障转移</li>
<li>现役NameNode选择：ZooKeeper提供了一个简单的机制用于唯一的选择一个节点为active状态。如果目前现役NameNode崩溃，另一个节点可能从ZooKeeper获得特殊的排外锁以表明它应该成为现役NameNode</li>
</ol>
</li>
<li>zkfc进程是ZooKeeper的客户端，也监视和管理NameNode的状态。每个运行NameNode的主机也运行了一个ZKFC进程，ZKFC负责：<ol>
<li>健康监测<ul>
<li>ZKFC使用一个健康检查命令定期地ping与之在相同主机的NameNode，只要该NameNode及时地回复健康状态，ZKFC认为该节点是健康的。如果该节点崩溃，冻结或进入不健康状态，健康监测器标识该节点为非健康的。</li>
</ul>
</li>
<li>ZooKeeper会话管理<ul>
<li>当本地NameNode是健康的，ZKFC保持一个在ZooKeeper中打开的会话。如果本地NameNode处于active状态，ZKFC也保持一个特殊的znode锁，该锁使用了ZooKeeper对短暂节点的支持，如果会话终止，锁节点将自动删除。</li>
</ul>
</li>
<li>基于ZooKeeper的选择<ul>
<li>如果本地NameNode是健康的，且ZKFC发现没有其它的节点当前持有znode锁，它将为自己获取该锁。如果成功，则它已经赢得了选择，并负责运行故障转移进程以使它的本地NameNode为Active。</li>
</ul>
</li>
</ol>
</li>
</ol>
<h3 id="十三、什么是脑裂问题？HDFS-HA中如何解决的脑裂问题？"><a href="#十三、什么是脑裂问题？HDFS-HA中如何解决的脑裂问题？" class="headerlink" title="十三、什么是脑裂问题？HDFS-HA中如何解决的脑裂问题？"></a>十三、什么是脑裂问题？HDFS-HA中如何解决的脑裂问题？</h3><ol>
<li>active节点的zkfc进程检测到namenode异常，会通知另一台NameNode的zkfc</li>
<li>接收通知的zkf会通过ssh在异常namenode上执行kill命令，确保异常NameNode死透</li>
<li>避免了出现两个active节点，解决了脑裂问题</li>
</ol>
<h3 id="十四、YARN-HA-实现高可用的思路"><a href="#十四、YARN-HA-实现高可用的思路" class="headerlink" title="十四、YARN-HA 实现高可用的思路"></a>十四、YARN-HA 实现高可用的思路</h3><ol>
<li>ResourceManager启动时候会向ZK的/rmstore目录写lock文件，写成功就为active，否则standby.</li>
<li>ResourceManager节点zkfc会一直监控这个lock文件是否存在，假如不存在，就为active，否则为standby.</li>
<li>zookeeper存储RMStateStore。选举active RM。</li>
<li>RMStateStore: 存储在zk的/rmstore目录下。</li>
<li>activeRM会向这个目录写APP信息</li>
<li>当activeRM挂了，另外一个standby RM通过ZKFC选举成功为active，会从/rmstore读取相应的作业信息。重新构建作业的内存信息，启动内部的服务，开始接收NM的心跳，构建集群的资源信息，并且接收客户端的作业提交请求。</li>
</ol>
<h3 id="十五、简单说一下-联邦架构-HDFS-Federation-架构设计思想。-了解"><a href="#十五、简单说一下-联邦架构-HDFS-Federation-架构设计思想。-了解" class="headerlink" title="十五、简单说一下 联邦架构(HDFS Federation) 架构设计思想。(了解)"></a>十五、简单说一下 联邦架构(HDFS Federation) 架构设计思想。(了解)</h3><ol>
<li>解决Namespace（命名空间）的限制</li>
<li>解决隔离问题</li>
<li>解决性能的瓶颈</li>
<li>将NameNode划分成不同的命名空间并进行编号。不同的命名空间之间相互隔离互不干扰。</li>
<li>在DataNode中创建目录，此目录对应命名空间的编号。</li>
<li>由此，编号相同的数据由对应的命名空间进行管理</li>
</ol>
]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title>Zookeeper</title>
    <url>/2021/11/07/Zookeeper/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="Zookeeper"><a href="#Zookeeper" class="headerlink" title="Zookeeper"></a>Zookeeper</h1><h1 id="一、Zookeeper入门"><a href="#一、Zookeeper入门" class="headerlink" title="一、Zookeeper入门"></a>一、Zookeeper入门</h1><h2 id="1-1-概述"><a href="#1-1-概述" class="headerlink" title="1.1 概述"></a>1.1 概述</h2><ul>
<li>Zookeeper是一个开源的分布式的，为分布式应用提供协调服务的Apache项目。</li>
<li>Zookeeper从设计模式角度来理解，是一个基于<font color ='red' >观察者模式</font>设计的分布式服务管理框架，它负责存储和管理大家都关心的数据，然后接受观察者的注册，一旦这些数据的状态发生了变化，Zookeeper就负责通知已经在Zookeeper上注册的那些观察者做出相应的反应.</li>
<li>Zookeeper = 文件系统 + 通知机制</li>
</ul>
<hr>
<h2 id="1-2-特点"><a href="#1-2-特点" class="headerlink" title="1.2 特点"></a>1.2 特点</h2><p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16345175804501.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
<ul>
<li>Zookeeper：一个领导者（Leader），多个跟随者（Follower）组成的集群。</li>
<li>集群中只要有半数以上节点存活，Zookeeper集群就能正常服务</li>
<li>全局数据一致：每个Server保存一份相同的数据副本，Client无论连接到哪个Server，数据都是一致的</li>
<li>更新请求顺序进行，来自同一个Client的更新请求按其发送顺序依次执行</li>
<li>数据更新原子性，一次数据更新要么全部成功（超集群半数节点），要么所有节点全部失败</li>
<li>实时性，在一定时间范围内，Client能读到最新数据</li>
</ul>
<hr>
<h2 id="1-3-数据结构"><a href="#1-3-数据结构" class="headerlink" title="1.3 数据结构"></a>1.3 数据结构</h2><p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16345177645379.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
<ul>
<li>ZooKeeper数据模型的结构与Unix文件系统很类似，整体上可以看作是一棵树，每个节点称做一个ZNode。每一个ZNode默认能够存储1MB的数据，每个ZNode都可以通过其路径唯一标识。</li>
</ul>
<hr>
<h2 id="1-4-应用场景"><a href="#1-4-应用场景" class="headerlink" title="1.4 应用场景"></a>1.4 应用场景</h2><ul>
<li><p>提供的服务包括：统一命名服务、统一配置管理、统一集群管理、服务器节点动态上下线、软负载均衡等。</p>
</li>
<li><p>统一命名服务：在分布式环境下，经常需要对应用/服务进行统一命名，便于识别。<br>  <img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16345532077391.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
</li>
<li><p>统一配置管理<br>  <img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16345533954441.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
<ol>
<li>分布式环境下，配置文件同步非常常见，配置管理可交由ZooKeeper实现</li>
<li>一般要求一个集群中，所有节点的配置信息是一致的，比如 Kafka 集群</li>
<li>对配置文件修改后，希望能够快速同步到各个节点上</li>
<li>将配置信息写入ZooKeeper上的一个Znode</li>
<li>各个客户端服务器监听这个Znode</li>
<li>Znode中的数据被修改，ZooKeeper将通知各个客户端服务器</li>
</ol>
</li>
<li><p>统一集群管理<br>  <img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16345535437380.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
<ol>
<li>分布式环境中，实时掌握每个节点的状态是必要的。 <ol>
<li>可根据节点实时状态做出一些调整。 </li>
</ol>
</li>
<li>ZooKeeper可以实现实时监控节点状态变化<ol>
<li>可将节点信息写入ZooKeeper上的一个ZNode</li>
<li>监听这个ZNode可获取它的实时状态变化。</li>
</ol>
</li>
</ol>
</li>
<li><p>服务器动态上下线<br>  <img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16345535923783.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
</li>
<li><p>软负载均衡：在Zookeeper中记录每台服务器的访问数，让访问数最少的服务器去处理最新的客户端请求<br>  <img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16345536248922.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
</li>
</ul>
<hr>
<h1 id="二、Zookeeper-安装"><a href="#二、Zookeeper-安装" class="headerlink" title="二、Zookeeper 安装"></a>二、Zookeeper 安装</h1><h2 id="2-1-安装ZK"><a href="#2-1-安装ZK" class="headerlink" title="2.1 安装ZK:"></a>2.1 安装ZK:</h2><ol>
<li>把软件包上传的Linux的 /opt/software 下</li>
<li>加压ZK到 /opt/module 下</li>
<li>将加压后的目录名称修改一下（选做）</li>
<li>将zk的安装目录下 conf/zoo_sample.cfg 文件改名为 zoo.cfg</li>
<li>在ZK的安装目录下创建一个新的目录，作为zk的数据持久化目录</li>
<li>修改zoo.cfg配置文件<code>dataDir=/opt/module/zookeeper-3.5.7/zkData</code></li>
<li>配置ZK的环境变量 （选做）</li>
</ol>
<h2 id="2-2-单点模式的简单操作"><a href="#2-2-单点模式的简单操作" class="headerlink" title="2.2 单点模式的简单操作"></a>2.2 单点模式的简单操作</h2><ol>
<li>启停zk服务端 和 zk客户端<ul>
<li>启停 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">    [atguigu@hadoop001 zookeeper-3.5.7]$ bin/zkServer.sh start</span><br><span class="line">    ZooKeeper JMX enabled by default</span><br><span class="line">    Using config: /opt/module/zookeeper-3.5.7/bin/../conf/zoo.cfg</span><br><span class="line">    Starting zookeeper ... STARTED</span><br><span class="line">    [atguigu@hadoop001 zookeeper-3.5.7]$ jps</span><br><span class="line">    1456 NameNode</span><br><span class="line">    8225 QuorumPeerMain</span><br><span class="line">    1619 DataNode</span><br><span class="line">    2076 JobHistoryServer</span><br><span class="line">    1901 NodeManager</span><br><span class="line">    8269 Jps</span><br><span class="line">    [atguigu@hadoop001 zookeeper-3.5.7]$ bin/zkServer.sh stop</span><br><span class="line">    ZooKeeper JMX enabled by default</span><br><span class="line">    Using config: /opt/module/zookeeper-3.5.7/bin/../conf/zoo.cfg</span><br><span class="line">    Stopping zookeeper ... STOPPED</span><br><span class="line">    [atguigu@hadoop001 zookeeper-3.5.7]$ jps</span><br><span class="line">    1456 NameNode</span><br><span class="line">    1619 DataNode</span><br><span class="line">    2076 JobHistoryServer</span><br><span class="line">    1901 NodeManager</span><br><span class="line">    8302 Jps</span><br><span class="line">    [atguigu@hadoop001 zookeeper-3.5.7]$</span><br><span class="line">    ```   </span><br><span class="line">- 客户端连接</span><br><span class="line">    ```bash</span><br><span class="line">    [atguigu@hadoop001 zookeeper-3.5.7]$ bin/zkCli.sh -server hadoop001:2181</span><br><span class="line">    Connecting to hadoop001:2181</span><br><span class="line">    ········</span><br><span class="line"></span><br><span class="line">    2021-10-18 16:36:10,536 [myid:hadoop001:2181] - INFO  [main-SendThread(hadoop001:2181):ClientCnxn<span class="variable">$SendThread</span>@1394] - Session establishment complete on server hadoop001/192.168.2.6:2181, sessionid = 0x10000a6d44b0000, negotiated timeout = 30000</span><br><span class="line">    </span><br><span class="line">    WATCHER::</span><br><span class="line">    </span><br><span class="line">    WatchedEvent state:SyncConnected <span class="built_in">type</span>:None path:null</span><br><span class="line">    [zk: hadoop001:2181(CONNECTED) 0]</span><br></pre></td></tr></table></figure>
 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 ~]$ jps</span><br><span class="line">1456 NameNode</span><br><span class="line">1619 DataNode</span><br><span class="line">8483 ZooKeeperMain</span><br><span class="line">8532 Jps</span><br><span class="line">8378 QuorumPeerMain</span><br><span class="line">2076 JobHistoryServer</span><br><span class="line">1901 NodeManager</span><br><span class="line">[atguigu@hadoop001 ~]$</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>查看一下zk的服务端和客户端对应的进程 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 ~]$ jps</span><br><span class="line">1456 NameNode</span><br><span class="line">1619 DataNode</span><br><span class="line">8483 ZooKeeperMain      <span class="comment">#客户端</span></span><br><span class="line">8532 Jps</span><br><span class="line">8378 QuorumPeerMain     <span class="comment">#服务端</span></span><br><span class="line">2076 JobHistoryServer</span><br><span class="line">1901 NodeManager</span><br><span class="line">[atguigu@hadoop001 ~]$</span><br></pre></td></tr></table></figure></li>
<li>退出客户端 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[zk: hadoop001:2181(CONNECTED) 0] quit</span><br><span class="line"></span><br><span class="line">WATCHER::</span><br><span class="line"></span><br><span class="line">WatchedEvent state:Closed <span class="built_in">type</span>:None path:null</span><br><span class="line">2021-10-18 16:41:05,769 [myid:] - INFO  [main:ZooKeeper@1422] - Session: 0x10000a6d44b0001 closed</span><br><span class="line">2021-10-18 16:41:05,769 [myid:] - INFO  [main-EventThread:ClientCnxn<span class="variable">$EventThread</span>@524] - EventThread shut down <span class="keyword">for</span> session: 0x10000a6d44b0001</span><br><span class="line">[atguigu@hadoop001 zookeeper-3.5.7]$ jps</span><br><span class="line">1456 NameNode</span><br><span class="line">1619 DataNode</span><br><span class="line">8552 Jps</span><br><span class="line">8378 QuorumPeerMain</span><br><span class="line">2076 JobHistoryServer</span><br><span class="line">1901 NodeManager</span><br><span class="line">[atguigu@hadoop001 zookeeper-3.5.7]$</span><br></pre></td></tr></table></figure></li>
<li>当客户端启动连接后不能单独关闭，当服务端关闭后，客户端也就消失了。</li>
</ol>
<h2 id="2-2-配置参数解读"><a href="#2-2-配置参数解读" class="headerlink" title="2.2 配置参数解读"></a>2.2 配置参数解读</h2><p>Zookeeper中的配置文件zoo.cfg中参数含义解读如下：</p>
<ol>
<li>tickTime =2000：通信心跳数，Zookeeper服务器与客户端心跳时间，单位毫秒<ul>
<li>Zookeeper使用的基本时间，服务器之间或客户端与服务器之间维持心跳的时间间隔，也就是每个tickTime时间就会发送一个心跳，时间单位为毫秒。</li>
<li>它用于心跳检测机制，并且设置最小的session超时时间为两倍心跳时间。(session的最小超时时间是2*tickTime)</li>
</ul>
</li>
<li>initLimit =10：LF初始通信时限<ul>
<li>集群中的Follower跟随者服务器与Leader领导者服务器之间初始连接时能容忍的最多心跳数（tickTime的数量），用它来限定集群中的Zookeeper服务器连接到Leader的时限。</li>
</ul>
</li>
<li>syncLimit =5：LF同步通信时限<ul>
<li>集群中Leader与Follower之间的最大响应时间单位，假如响应超过syncLimit * tickTime，Leader认为Follwer死掉，从服务器列表中删除Follwer。</li>
</ul>
</li>
<li>dataDir：数据文件目录+数据持久化路径<ul>
<li>用于保存Zookeeper中的数据。</li>
</ul>
</li>
<li>clientPort=2181：客户端连接端口<ul>
<li>监听客户端连接的端口。</li>
</ul>
</li>
</ol>
<h1 id="三、Zookeeper-实战"><a href="#三、Zookeeper-实战" class="headerlink" title="三、Zookeeper 实战"></a>三、Zookeeper 实战</h1><h2 id="3-1-搭建ZK的集群"><a href="#3-1-搭建ZK的集群" class="headerlink" title="3.1 搭建ZK的集群"></a>3.1 搭建ZK的集群</h2><ol>
<li>集群规划：在hadoop102、hadoop103和hadoop104三个节点上部署Zookeeper </li>
<li>每个节点执行单节点部署的步骤</li>
<li>修改zoo.cfg 配置文件 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 数据存储路径</span><br><span class="line">dataDir=/opt/module/zookeeper-3.5.7/zkData</span><br><span class="line"></span><br><span class="line">#集群节点配置</span><br><span class="line">server.2=hadoop102:2888:3888</span><br><span class="line">server.3=hadoop103:2888:3888</span><br><span class="line">server.4=hadoop104:2888:3888</span><br></pre></td></tr></table></figure></li>
<li>zkData目录下创建myid的文件对应上述配置编号</li>
<li>配置参数解读<ul>
<li>server.A=B:C:D</li>
<li>A是一个数字，表示这个是第几号服务器，对应zkData/myid中的编号；Zookeeper启动时读取此文件，拿到里面的数据与zoo.cfg里面的配置信息比较从而判断到底是哪个server</li>
<li>B是Zookeeper节点的地址（域名/IP）</li>
<li>C是这个服务器Follower与集群中的Leader服务器交换信息的端口</li>
<li>D是集群中的Leader服务器挂掉之后，需要一个端口来重新进行选举，选出一个新的Leader，而这个端口就是用来执行选举时服务器相互通信的端口</li>
</ul>
</li>
<li>编写启动/停止Zookeeper集群脚本 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#检验参数</span></span><br><span class="line"><span class="keyword">if</span> [ <span class="variable">$#</span> -lt 1 ]; <span class="keyword">then</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="string">&#x27;参数不能为空！！！&#x27;</span></span><br><span class="line">    <span class="built_in">exit</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#循环遍历每一台机器，分别启动或者停止ZK服务</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> host <span class="keyword">in</span> hadoop001 hadoop002 hadoop003 hadoop004 hadoop005; <span class="keyword">do</span></span><br><span class="line">    <span class="keyword">case</span> <span class="variable">$1</span> <span class="keyword">in</span></span><br><span class="line">    <span class="string">&quot;start&quot;</span>)</span><br><span class="line">        <span class="built_in">echo</span> <span class="string">&quot;*****************<span class="variable">$1</span> <span class="variable">$host</span> zookeeper****************&quot;</span></span><br><span class="line">        ssh <span class="variable">$host</span> /opt/module/zookeeper-3.5.7/bin/zkServer.sh <span class="variable">$1</span></span><br><span class="line">        ;;</span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;stop&quot;</span>)</span><br><span class="line">        <span class="built_in">echo</span> <span class="string">&quot;*****************<span class="variable">$1</span> <span class="variable">$host</span> zookeeper****************&quot;</span></span><br><span class="line">        ssh <span class="variable">$host</span> /opt/module/zookeeper-3.5.7/bin/zkServer.sh <span class="variable">$1</span></span><br><span class="line">        ;;</span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;status&quot;</span>)</span><br><span class="line">        <span class="built_in">echo</span> <span class="string">&quot;*****************<span class="variable">$1</span> <span class="variable">$host</span> zookeeper****************&quot;</span></span><br><span class="line">        ssh <span class="variable">$host</span> /opt/module/zookeeper-3.5.7/bin/zkServer.sh <span class="variable">$1</span></span><br><span class="line">        ;;</span><br><span class="line"></span><br><span class="line">    *)</span><br><span class="line">        <span class="built_in">echo</span> <span class="string">&#x27;参数有误！！！&#x27;</span></span><br><span class="line">        <span class="built_in">exit</span></span><br><span class="line">        ;;</span><br><span class="line">    <span class="keyword">esac</span></span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure></li>
<li>执行脚本 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 bin]$ zookeeper_cluster.sh start</span><br><span class="line">*****************start hadoop001 zookeeper****************</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /opt/module/zookeeper-3.5.7/bin/../conf/zoo.cfg</span><br><span class="line">Starting zookeeper ... STARTED</span><br><span class="line">*****************start hadoop002 zookeeper****************</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /opt/module/zookeeper-3.5.7/bin/../conf/zoo.cfg</span><br><span class="line">Starting zookeeper ... STARTED</span><br><span class="line">*****************start hadoop003 zookeeper****************</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /opt/module/zookeeper-3.5.7/bin/../conf/zoo.cfg</span><br><span class="line">Starting zookeeper ... STARTED</span><br><span class="line">[atguigu@hadoop001 bin]$ zookeeper_cluster.sh status</span><br><span class="line">*****************status hadoop001 zookeeper****************</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /opt/module/zookeeper-3.5.7/bin/../conf/zoo.cfg</span><br><span class="line">Client port found: 2181. Client address: localhost.</span><br><span class="line">Mode: follower</span><br><span class="line">*****************status hadoop002 zookeeper****************</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /opt/module/zookeeper-3.5.7/bin/../conf/zoo.cfg</span><br><span class="line">Client port found: 2181. Client address: localhost.</span><br><span class="line">Mode: follower</span><br><span class="line">*****************status hadoop003 zookeeper****************</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /opt/module/zookeeper-3.5.7/bin/../conf/zoo.cfg</span><br><span class="line">Client port found: 2181. Client address: localhost.</span><br><span class="line">Mode: leader</span><br><span class="line">[atguigu@hadoop001 bin]$ zookeeper_cluster.sh stop</span><br><span class="line">*****************stop hadoop001 zookeeper****************</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /opt/module/zookeeper-3.5.7/bin/../conf/zoo.cfg</span><br><span class="line">Stopping zookeeper ... STOPPED</span><br><span class="line">*****************stop hadoop002 zookeeper****************</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /opt/module/zookeeper-3.5.7/bin/../conf/zoo.cfg</span><br><span class="line">Stopping zookeeper ... STOPPED</span><br><span class="line">*****************stop hadoop003 zookeeper****************</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /opt/module/zookeeper-3.5.7/bin/../conf/zoo.cfg</span><br><span class="line">Stopping zookeeper ... STOPPED</span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="3-2-客户端命令行操作"><a href="#3-2-客户端命令行操作" class="headerlink" title="3.2 客户端命令行操作"></a>3.2 客户端命令行操作</h2><h2 id="3-2-1-基本命令"><a href="#3-2-1-基本命令" class="headerlink" title="3.2.1 基本命令"></a>3.2.1 基本命令</h2><table>
<thead>
<tr>
<th>命令基本语法</th>
<th>功能描述</th>
</tr>
</thead>
<tbody><tr>
<td>help</td>
<td>显示所有操作命令</td>
</tr>
<tr>
<td>ls path</td>
<td>使用 ls 命令来查看当前znode的子节点 <br> -w  监听子节点变化 <br>-s   附加次级信息</td>
</tr>
<tr>
<td>create</td>
<td>普通创建 <br>-s  含有序列 <br>-e  临时（重启或者超时消失）</td>
</tr>
<tr>
<td>get path</td>
<td>获得节点的值<br>-w  监听节点内容变化<br>-s   附加次级信息</td>
</tr>
<tr>
<td>set</td>
<td>设置节点的具体值</td>
</tr>
<tr>
<td>stat</td>
<td>查看节点状态</td>
</tr>
<tr>
<td>delete</td>
<td>删除节点</td>
</tr>
<tr>
<td>deleteall</td>
<td>递归删除节点</td>
</tr>
</tbody></table>
<h2 id="3-2-2-具体操作"><a href="#3-2-2-具体操作" class="headerlink" title="3.2.2 具体操作"></a>3.2.2 具体操作</h2><ol>
<li><p>启动客户端</p>
 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 zookeeper-3.5.7]$ bin/zkCli.sh -server hadoop001:2181</span><br><span class="line">Connecting to hadoop001:2181</span><br><span class="line">·······</span><br><span class="line">WATCHER::</span><br><span class="line"></span><br><span class="line">WatchedEvent state:SyncConnected <span class="built_in">type</span>:None path:null</span><br><span class="line">[zk: hadoop001:2181(CONNECTED) 0]</span><br></pre></td></tr></table></figure></li>
<li><p>显示所有操作命令</p>
 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[zk: hadoop001:2181(CONNECTED) 0] <span class="built_in">help</span></span><br><span class="line">ZooKeeper -server host:port cmd args</span><br><span class="line">	addauth scheme auth</span><br><span class="line">	close</span><br><span class="line">	config [-c] [-w] [-s]</span><br><span class="line">	connect host:port</span><br><span class="line">	create [-s] [-e] [-c] [-t ttl] path [data] [acl]</span><br><span class="line">	delete [-v version] path</span><br><span class="line">	deleteall path</span><br><span class="line">	delquota [-n|-b] path</span><br><span class="line">	get [-s] [-w] path</span><br><span class="line">	getAcl [-s] path</span><br><span class="line">	<span class="built_in">history</span></span><br><span class="line">	listquota path</span><br><span class="line">	ls [-s] [-w] [-R] path</span><br><span class="line">	ls2 path [watch]</span><br><span class="line">	printwatches on|off</span><br><span class="line">	quit</span><br><span class="line">	reconfig [-s] [-v version] [[-file path] | [-members serverID=host:port1:port2;port3[,...]*]] | [-add serverId=host:port1:port2;port3[,...]]* [-remove serverId[,...]*]</span><br><span class="line">	redo cmdno</span><br><span class="line">	removewatches path [-c|-d|-a] [-l]</span><br><span class="line">	rmr path</span><br><span class="line">	<span class="built_in">set</span> [-s] [-v version] path data</span><br><span class="line">	setAcl [-s] [-v version] [-R] path acl</span><br><span class="line">	setquota -n|-b val path</span><br><span class="line">	<span class="built_in">stat</span> [-w] path</span><br><span class="line">	sync path</span><br></pre></td></tr></table></figure></li>
<li><p>查看当前znode中所包含的内容</p>
 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[zk: hadoop001:2181(CONNECTED) 2] ls /</span><br><span class="line">[zookeeper]</span><br><span class="line">[zk: hadoop001:2181(CONNECTED) 3]</span><br></pre></td></tr></table></figure></li>
<li><p>查看当前节点详细数据</p>
 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[zk: hadoop001:2181(CONNECTED) 3] ls -s /</span><br><span class="line">[zookeeper]cZxid = 0x0</span><br><span class="line">ctime = Thu Jan 01 08:00:00 CST 1970</span><br><span class="line">mZxid = 0x0</span><br><span class="line">mtime = Thu Jan 01 08:00:00 CST 1970</span><br><span class="line">pZxid = 0x100000011</span><br><span class="line">cversion = 1</span><br><span class="line">dataVersion = 0</span><br><span class="line">aclVersion = 0</span><br><span class="line">ephemeralOwner = 0x0</span><br><span class="line">dataLength = 0</span><br><span class="line">numChildren = 1</span><br><span class="line"></span><br><span class="line">[zk: hadoop001:2181(CONNECTED) 4]</span><br></pre></td></tr></table></figure></li>
<li><p>分别创建2个普通节点</p>
 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[zk: hadoop001:2181(CONNECTED) 4] create /sanguo <span class="string">&quot;diaochan&quot;</span></span><br><span class="line">Created /sanguo</span><br><span class="line">[zk: hadoop001:2181(CONNECTED) 6] create /sanguo/shuguo <span class="string">&quot;liubei&quot;</span></span><br><span class="line">Created /sanguo/shuguo</span><br><span class="line">[zk: hadoop001:2181(CONNECTED) 8] ls  /</span><br><span class="line">[sanguo, zookeeper]</span><br><span class="line">[zk: hadoop001:2181(CONNECTED) 9] ls  /sanguo</span><br><span class="line">[shuguo]</span><br></pre></td></tr></table></figure></li>
<li><p>获得节点的值</p>
 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[zk: hadoop001:2181(CONNECTED) 10] get /sanguo</span><br><span class="line">diaochan</span><br><span class="line">[zk: hadoop001:2181(CONNECTED) 11] get -s /sanguo</span><br><span class="line">diaochan</span><br><span class="line">cZxid = 0x300000004</span><br><span class="line">ctime = Mon Oct 18 17:31:28 CST 2021</span><br><span class="line">mZxid = 0x300000004</span><br><span class="line">mtime = Mon Oct 18 17:31:28 CST 2021</span><br><span class="line">pZxid = 0x300000006</span><br><span class="line">cversion = 1</span><br><span class="line">dataVersion = 0</span><br><span class="line">aclVersion = 0</span><br><span class="line">ephemeralOwner = 0x0</span><br><span class="line">dataLength = 8</span><br><span class="line">numChildren = 1</span><br><span class="line">[zk: hadoop001:2181(CONNECTED) 12] get -s /sanguo/shuguo</span><br><span class="line">liubei</span><br><span class="line">cZxid = 0x300000006</span><br><span class="line">ctime = Mon Oct 18 17:32:12 CST 2021</span><br><span class="line">mZxid = 0x300000006</span><br><span class="line">mtime = Mon Oct 18 17:32:12 CST 2021</span><br><span class="line">pZxid = 0x300000006</span><br><span class="line">cversion = 0</span><br><span class="line">dataVersion = 0</span><br><span class="line">aclVersion = 0</span><br><span class="line">ephemeralOwner = 0x0</span><br><span class="line">dataLength = 6</span><br><span class="line">numChildren = 0</span><br></pre></td></tr></table></figure></li>
<li><p>创建临时节点</p>
 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 创建临时节点</span></span><br><span class="line">[zk: hadoop001:2181(CONNECTED) 13] create -e /sanguo/wuguo <span class="string">&quot;zhouyu&quot;</span></span><br><span class="line">Created /sanguo/wuguo</span><br><span class="line"><span class="comment"># 查看节点存在</span></span><br><span class="line">[zk: hadoop001:2181(CONNECTED) 15] ls /sanguo</span><br><span class="line">[shuguo, wuguo]</span><br><span class="line"><span class="comment"># 退出客户端</span></span><br><span class="line">[zk: hadoop001:2181(CONNECTED) 16] quit</span><br><span class="line"></span><br><span class="line">WATCHER::</span><br><span class="line"></span><br><span class="line">WatchedEvent state:Closed <span class="built_in">type</span>:None path:null</span><br><span class="line">2021-10-18 17:35:37,751 [myid:] - INFO  [main:ZooKeeper@1422] - Session: 0x10000c30dbf0001 closed</span><br><span class="line">2021-10-18 17:35:37,752 [myid:] - INFO  [main-EventThread:ClientCnxn<span class="variable">$EventThread</span>@524] - EventThread shut down <span class="keyword">for</span> session: 0x10000c30dbf0001</span><br><span class="line"><span class="comment"># 重新连接客户端</span></span><br><span class="line">[atguigu@hadoop001 zookeeper-3.5.7]$ bin/zkCli.sh -server hadoop001:2181</span><br><span class="line">Connecting to hadoop001:2181</span><br><span class="line">···</span><br><span class="line">···</span><br><span class="line">WatchedEvent state:SyncConnected <span class="built_in">type</span>:None path:null</span><br><span class="line"><span class="comment"># 再次查看节点 发现临时节点被删除</span></span><br><span class="line">[zk: hadoop001:2181(CONNECTED) 0] ls /sanguo</span><br><span class="line">[shuguo]</span><br><span class="line">[zk: hadoop001:2181(CONNECTED) 1]</span><br></pre></td></tr></table></figure></li>
<li><p>创建带序号的节点</p>
 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[zk: hadoop001:2181(CONNECTED) 6] ls /sanguo/weiguo</span><br><span class="line">[]</span><br><span class="line"><span class="comment"># 创建普通节点xiaobing</span></span><br><span class="line">[zk: hadoop001:2181(CONNECTED) 7] create /sanguo/weiguo/xiaobing</span><br><span class="line">Created /sanguo/weiguo/xiaobing</span><br><span class="line"><span class="comment"># 再次创建普通节点xiaobing--Node already exists</span></span><br><span class="line">[zk: hadoop001:2181(CONNECTED) 8] create /sanguo/weiguo/xiaobing</span><br><span class="line">Node already exists: /sanguo/weiguo/xiaobing</span><br><span class="line"><span class="comment"># 创建普通节点xiaobing1</span></span><br><span class="line">[zk: hadoop001:2181(CONNECTED) 9] create /sanguo/weiguo/xiaobing1</span><br><span class="line">Created /sanguo/weiguo/xiaobing1</span><br><span class="line"><span class="comment"># 创建带序号的节点xiaobing</span></span><br><span class="line">[zk: hadoop001:2181(CONNECTED) 10] create -s /sanguo/weiguo/xiaobing</span><br><span class="line">Created /sanguo/weiguo/xiaobing0000000002</span><br><span class="line"><span class="comment"># 创建带序号的节点xiaobing</span></span><br><span class="line">[zk: hadoop001:2181(CONNECTED) 11] create -s /sanguo/weiguo/xiaobing</span><br><span class="line">Created /sanguo/weiguo/xiaobing0000000003</span><br><span class="line"><span class="comment"># 创建带序号的节点xiaobing</span></span><br><span class="line">[zk: hadoop001:2181(CONNECTED) 12] create -s /sanguo/weiguo/xiaobing</span><br><span class="line">Created /sanguo/weiguo/xiaobing0000000004</span><br><span class="line"><span class="comment"># 查看节点，如果节点下原来没有子节点，序号从0开始依次递增。如果原节点下已有2个节点，则再排序时从2开始，以此类推</span></span><br><span class="line">[zk: hadoop001:2181(CONNECTED) 13] ls /sanguo/weiguo</span><br><span class="line">[xiaobing, xiaobing0000000002, xiaobing0000000003, xiaobing0000000004, xiaobing1]</span><br><span class="line">[zk: hadoop001:2181(CONNECTED) 14]</span><br></pre></td></tr></table></figure></li>
<li><p>修改节点数据值</p>
 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[zk: hadoop001:2181(CONNECTED) 0] get /sanguo/weiguo</span><br><span class="line">caocao</span><br><span class="line">[zk: hadoop001:2181(CONNECTED) 1] <span class="built_in">set</span> /sanguo/weiguo <span class="string">&#x27;caopi&#x27;</span></span><br><span class="line">[zk: hadoop001:2181(CONNECTED) 2] get /sanguo/weiguo</span><br><span class="line">caopi</span><br><span class="line">[zk: hadoop001:2181(CONNECTED) 3]</span><br></pre></td></tr></table></figure></li>
<li><p>节点的值变化监听</p>
<ol>
<li>在hadoop002主机上注册监听/sanguo节点数据变化 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[zk: hadoop002:2181(CONNECTED) 0] get -w /sanguo</span><br><span class="line">null</span><br><span class="line">[zk: hadoop002:2181(CONNECTED) 1]</span><br></pre></td></tr></table></figure></li>
<li>在hadoop001主机上修改/sanguo节点的数据 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[zk: hadoop001:2181(CONNECTED) 5] <span class="built_in">set</span> /sanguo <span class="string">&quot;xishi&quot;</span></span><br></pre></td></tr></table></figure></li>
<li>观察hadoop002主机收到数据变化的监听 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">WATCHER::</span><br><span class="line"></span><br><span class="line">WatchedEvent state:SyncConnected <span class="built_in">type</span>:NodeDataChanged path:/sanguo</span><br></pre></td></tr></table></figure></li>
</ol>
</li>
<li><p>节点的子节点变化监听路径变化.</p>
<ol>
<li>在hadoop002主机上注册监听/sanguo节点的子节点变化 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[zk: hadoop002:2181(CONNECTED) 0] ls -w /sanguo</span><br><span class="line">[shuguo, weiguo, wuguo]</span><br></pre></td></tr></table></figure></li>
<li>在hadoop001主机/sanguo节点上创建子节点 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[zk: hadoop001:2181(CONNECTED) 4] create /sanguo/jin</span><br><span class="line">Created /sanguo/jin</span><br><span class="line">[zk: hadoop001:2181(CONNECTED) 5]</span><br></pre></td></tr></table></figure></li>
<li>观察hadoop002主机收到子节点变化的监听 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[zk: hadoop002:2181(CONNECTED) 1]</span><br><span class="line">WATCHER::</span><br><span class="line"></span><br><span class="line">WatchedEvent state:SyncConnected <span class="built_in">type</span>:NodeChildrenChanged path:/sanguo</span><br></pre></td></tr></table></figure></li>
</ol>
</li>
<li><p>删除节点</p>
 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[zk: hadoop001:2181(CONNECTED) 7] ls /sanguo</span><br><span class="line">[jin, shuguo, weiguo, wuguo]</span><br><span class="line">[zk: hadoop001:2181(CONNECTED) 8] delete /sanguo/jin</span><br><span class="line">[zk: hadoop001:2181(CONNECTED) 9] ls /sanguo</span><br><span class="line">[shuguo, weiguo, wuguo]</span><br><span class="line">[zk: hadoop001:2181(CONNECTED) 10]</span><br></pre></td></tr></table></figure></li>
<li><p>递归删除节点</p>
 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[zk: hadoop001:2181(CONNECTED) 12] ls /sanguo/weiguo</span><br><span class="line">[xiaobing, xiaobing0000000002, xiaobing0000000003, xiaobing0000000004, xiaobing1]</span><br><span class="line">[zk: hadoop001:2181(CONNECTED) 13] deleteall /sanguo/weiguo</span><br><span class="line">[zk: hadoop001:2181(CONNECTED) 14] ls /sanguo/weiguo</span><br><span class="line">Node does not exist: /sanguo/weiguo</span><br><span class="line">[zk: hadoop001:2181(CONNECTED) 15]</span><br></pre></td></tr></table></figure></li>
<li><p>查看节点状态</p>
 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[zk: hadoop001:2181(CONNECTED) 15] <span class="built_in">stat</span> /sanguo</span><br><span class="line">cZxid = 0x300000016</span><br><span class="line">ctime = Mon Oct 18 17:41:54 CST 2021</span><br><span class="line">mZxid = 0x300000028</span><br><span class="line">mtime = Mon Oct 18 17:58:53 CST 2021</span><br><span class="line">pZxid = 0x300000030</span><br><span class="line">cversion = 6</span><br><span class="line">dataVersion = 1</span><br><span class="line">aclVersion = 0</span><br><span class="line">ephemeralOwner = 0x0</span><br><span class="line">dataLength = 5</span><br><span class="line">numChildren = 2</span><br><span class="line">[zk: hadoop001:2181(CONNECTED) 16]</span><br></pre></td></tr></table></figure></li>
</ol>
<h1 id="四、Zookeeper-原理"><a href="#四、Zookeeper-原理" class="headerlink" title="四、Zookeeper 原理"></a>四、Zookeeper 原理</h1><h2 id="4-1-节点类型"><a href="#4-1-节点类型" class="headerlink" title="4.1 节点类型"></a>4.1 节点类型</h2><p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16345542078737.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
<ul>
<li>持久（Persistent）：客户端和服务器端断开连接后，创建的节点不删除</li>
<li>短暂（Ephemeral）：客户端和服务器端断开连接后，创建的节点自己删除</li>
<li>在分布式系统中，顺序号可以被用于为所有的事件进行全局排序，这样客户端可以通过顺序号推断事件的顺序</li>
<li>创建znode时设置顺序标识，znode名称后会附加一个值，顺序号是一个单调递增的计数器，由父节点维护</li>
</ul>
<h2 id="4-2-Stat结构体"><a href="#4-2-Stat结构体" class="headerlink" title="4.2 Stat结构体"></a>4.2 Stat结构体</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[zk: hadoop001:2181(CONNECTED) 15] <span class="built_in">stat</span> /sanguo</span><br><span class="line"><span class="comment"># czxid-创建节点的事务zxid</span></span><br><span class="line"><span class="comment">#   每次修改ZooKeeper状态都会收到一个zxid形式的时间戳，也就是ZooKeeper事务ID。</span></span><br><span class="line"><span class="comment">#   事务ID是ZooKeeper中所有修改总的次序。每个修改都有唯一的zxid，如果zxid1小于zxid2，那么zxid1在zxid2之前发生。</span></span><br><span class="line">cZxid = 0x300000016</span><br><span class="line"><span class="comment"># ctime - znode被创建的毫秒数(从1970年开始)</span></span><br><span class="line">ctime = Mon Oct 18 17:41:54 CST 2021</span><br><span class="line"><span class="comment"># mzxid - znode最后更新的事务zxid</span></span><br><span class="line">mZxid = 0x300000028</span><br><span class="line"><span class="comment"># mtime - znode最后修改的毫秒数(从1970年开始)</span></span><br><span class="line">mtime = Mon Oct 18 17:58:53 CST 2021</span><br><span class="line"><span class="comment"># pZxid - znode最后更新的子节点zxid</span></span><br><span class="line">pZxid = 0x300000030</span><br><span class="line"><span class="comment"># cversion - znode子节点变化号，znode子节点修改次数</span></span><br><span class="line">cversion = 6</span><br><span class="line"><span class="comment"># dataversion - znode数据变化号</span></span><br><span class="line">dataVersion = 1</span><br><span class="line"><span class="comment"># aclVersion - znode访问控制列表的变化号</span></span><br><span class="line">aclVersion = 0</span><br><span class="line"><span class="comment"># ephemeralOwner- 如果是临时节点，这个是znode拥有者的session id。如果不是临时节点则是0。</span></span><br><span class="line">ephemeralOwner = 0x0</span><br><span class="line"><span class="comment"># dataLength - znode的数据长度</span></span><br><span class="line">dataLength = 5</span><br><span class="line"><span class="comment"># numChildren - znode子节点数量</span></span><br><span class="line">numChildren = 2</span><br><span class="line">[zk: hadoop001:2181(CONNECTED) 16]</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="4-3-监听器原理"><a href="#4-3-监听器原理" class="headerlink" title="4.3 监听器原理"></a>4.3 监听器原理</h2><p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16345549940089.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
<h3 id="4-3-1-监听原理详解"><a href="#4-3-1-监听原理详解" class="headerlink" title="4.3.1 监听原理详解"></a>4.3.1 监听原理详解</h3><ol>
<li>首先要有一个main()线程</li>
<li>在main线程中创建Zookeeper客户端，这时就会创建两个线程，一个负责网络连接通信（connet），一个负责监听（listener）。 </li>
<li>通过connect线程将注册的监听事件发送给Zookeeper。 </li>
<li>在Zookeeper的注册监听器列表中将注册的监听事件添加到列表中。 </li>
<li>Zookeeper监听到有数据或路径变化，就会将这个消息发送给listener线程。 </li>
<li>listener线程内部调用了process()方法。 <h3 id="4-3-2-常见的监听"><a href="#4-3-2-常见的监听" class="headerlink" title="4.3.2 常见的监听"></a>4.3.2 常见的监听</h3></li>
<li>监听节点数据的变化<ul>
<li><code>get path [watch]</code></li>
</ul>
</li>
<li>监听子节点增减的变化<ul>
<li><code>ls path [watch]</code></li>
</ul>
</li>
</ol>
<h2 id="4-4-选举机制"><a href="#4-4-选举机制" class="headerlink" title="4.4 选举机制"></a>4.4 选举机制</h2><ol>
<li>半数机制：集群中半数以上机器存活，集群可用。所以<font color ='red' >Zookeeper适合安装奇数台服务器</font>。</li>
<li>Zookeeper虽然在配置文件中并没有指定Master和Slave。但是，Zookeeper工作时，是有一个节点为Leader，其他则为Follower，Leader是通过内部的选举机制临时产生的。</li>
<li>以一个简单的例子来说明整个选举的过程。<br> <img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16345388234410.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"><ul>
<li>假设有五台服务器组成的Zookeeper集群，它们的id从1-5，同时它们都是最新启动的，也就是没有历史数据，在存放数据量这一点上，都是一样的。假设这些服务器依序启动，来看看会发生什么。</li>
<li>Zookeeper的选举机制<ol>
<li>服务器1启动，发起一次选举。服务器1投自己一票。此时服务器1票数一票，不够半数以上（3票），选举无法完成，服务器1状态保持为LOCKING；</li>
<li>服务器2启动，再发起一次选举。服务器1和2分别投自己一票并交换选票信息：此时服务器1发现服务器2的ID比自己目前投票推举的（服务器1）大，更改选票为推举服务器2。此时服务器1票数0票，服务器2票数2票，没有半数以上结果，选举无法完成，服务器1，2状态保持LOCKING</li>
<li>服务器3启动，发起一次选举。此时服务器1和2都会更改选票为服务器3。此次投票结果：服务器1为0票，服务器2为0票，服务器3为3票。此时服务器3的票数已经超过半数，服务器3当选Leader。服务器1，2更改状态为FOLLOWING，服务器3更改状态为LEADING；</li>
<li>服务器4启动，发起一次选举。此时服务器1，2，3已经不是LOOKING状态，不会更改选票信息。交换选票信息结果：服务器3为3票，服务器4为1票。此时服务器4服从多数，更改选票信息为服务器3，并更改状态为FOLLOWING；</li>
<li>服务器5启动，同4一样当小弟。</li>
</ol>
</li>
<li>举例1<ul>
<li>场景：以5台机器为例，集群的机器顺时启动，当前集群中没有任何数据。<ol>
<li>server1 启动，首先server1给自己投一票，然后看当前票数是否超过半数，结果没有超过，这时候leader就没选出来，当前选举状态是Locking状态。</li>
<li>server2 启动，首先server2先给自己投一票，因为当前集群已经有两台机器已启动，所以server1</li>
<li>server2会交换选票，交换后发现各自有一票，接下来比较 myid 发现server2的myid值 &gt; server1的myid值此时server2胜出，最后server2有两票。最后再看当前票数是否半，发现未过半，集群的选举状态集训保持locking状态。</li>
<li>server3启动， 首先自己投自己一票，server1和server2也会投自己一票，然后交换选票发现都一样，接着比较myid 最后server3胜出，此时server3就有3票，同时server3的票数超过半数。所以server3成为leader。</li>
<li>server4启动，发现当前集群已经有leader 它自己自动成为follower</li>
<li>server5启动，发现当前集群已经有leader 它自己自动成为follower</li>
</ol>
</li>
</ul>
</li>
<li>举例说明2<ul>
<li>场景：以5台机器为例，当前集群正在使用（有数据/没数据），leader突然宕机的情况。</li>
<li>当集群中的leader挂掉，集群会重新选出一个leader，此时首先会比较每一台机器的mzxid,mzxid最大的被选为leader。极端情况，mzxid都相等的情况，那么就会直接比较myid。</li>
</ul>
</li>
<li>举例说明3<ul>
<li>场景：集群中有数据，重启的时候Leader该如何选？</li>
<li>和场景一选举机制是一样！</li>
</ul>
</li>
</ul>
</li>
<li>一般情况下ZK集群更推荐使用奇数台机器原因？<ul>
<li>在ZK集群中 奇数台 和 偶数台（接近的台数） 机器的容错能力是一样的，所以在考虑资源节省的情况我们推荐使用奇数台方案</li>
</ul>
</li>
</ol>
<h2 id="4-5-写数据流程"><a href="#4-5-写数据流程" class="headerlink" title="4.5 写数据流程"></a>4.5 写数据流程</h2><p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16345398429599.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
<ol>
<li>客户端会向ZK集群中的一台机器server1发送写数据的请求。</li>
<li>server1接收到请求后，马上会通知leader 有写数据的请求来了</li>
<li>leader拿到请求后，进行广播，让集群每一台机器都准备要写数据</li>
<li>集群中的所有机机器接收到leader广播后都回应一下leader</li>
<li>leader接收机器数过半的机器回应后，再次进行广播 开始写数据，<br>其他机器接收到广播后也开始写数据</li>
<li>数据成功写入后，回应leader，最后由leader来做整个事务提交</li>
<li>当数据成功写入后，由最初和客户端发生连接的 server1 回应客户端数据写入成功。</li>
</ol>
<h1 id="五、Zookeeper-面试真题"><a href="#五、Zookeeper-面试真题" class="headerlink" title="五、Zookeeper 面试真题"></a>五、Zookeeper 面试真题</h1><h2 id="5-1-请简述ZooKeeper的选举机制"><a href="#5-1-请简述ZooKeeper的选举机制" class="headerlink" title="5.1 请简述ZooKeeper的选举机制"></a>5.1 请简述ZooKeeper的选举机制</h2><h2 id="5-2-ZooKeeper的监听原理是什么？"><a href="#5-2-ZooKeeper的监听原理是什么？" class="headerlink" title="5.2 ZooKeeper的监听原理是什么？"></a>5.2 ZooKeeper的监听原理是什么？</h2><h2 id="5-3-ZooKeeper的部署方式有哪几种？集群中的角色有哪些？集群最少需要几台机器？"><a href="#5-3-ZooKeeper的部署方式有哪几种？集群中的角色有哪些？集群最少需要几台机器？" class="headerlink" title="5.3 ZooKeeper的部署方式有哪几种？集群中的角色有哪些？集群最少需要几台机器？"></a>5.3 ZooKeeper的部署方式有哪几种？集群中的角色有哪些？集群最少需要几台机器？</h2><ol>
<li>部署方式单机模式、集群模式</li>
<li>角色：Leader和Follower</li>
<li>集群最少需要机器数：3</li>
</ol>
<h2 id="5-4-ZooKeeper的常用命令Keeper的常用命令"><a href="#5-4-ZooKeeper的常用命令Keeper的常用命令" class="headerlink" title="5.4 ZooKeeper的常用命令Keeper的常用命令"></a>5.4 ZooKeeper的常用命令Keeper的常用命令</h2>]]></content>
      <categories>
        <category>Zookeeper</category>
      </categories>
      <tags>
        <tag>Zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title>Yarn</title>
    <url>/2021/11/07/Yarn/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="Yarn"><a href="#Yarn" class="headerlink" title="Yarn"></a>Yarn</h1><h1 id="一、Yarn资源调度器"><a href="#一、Yarn资源调度器" class="headerlink" title="一、Yarn资源调度器"></a>一、Yarn资源调度器</h1><ul>
<li>Yarn是一个资源调度平台，负责为运算程序提供服务器运算资源，相当于一个<font color ='red' >分布式操作系统平台</font>，而MapReduce等运算程序则相当于<font color ='red' >运行于操作系统之上的应用程序</font>。<h2 id="1-1-Yarn基本架构"><a href="#1-1-Yarn基本架构" class="headerlink" title="1.1 Yarn基本架构"></a>1.1 Yarn基本架构</h2></li>
<li>YARN主要由ResourceManager、NodeManager、ApplicationMaster和Container等组件构成。<br>  <img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16344718758198.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></li>
</ul>
<h2 id="1-2-Yarn工作机制"><a href="#1-2-Yarn工作机制" class="headerlink" title="1.2 Yarn工作机制"></a>1.2 Yarn工作机制</h2><ol>
<li>MR程序在客户端通过job.submit()方法提交任务到本地或者远程hadoop集群，创建YranRunner</li>
<li>YarnRunner向ResourceManager申请一个Application</li>
<li>ResourceManager将程序运行的资源路径(资源提交路径及application_id)返回给Yarnrunner</li>
<li>程序将运行所需资源提交到hdfs上（jar包，配置文件，split信息）</li>
<li>程序资源提交之后申请运行MRAppMaster</li>
<li>ResourceManager将用户请求初始化为一个Task，该task会被放到任务队列中，等待调度器分配资源</li>
<li>NodeManager领取task任务</li>
<li>该NodeManager创建container，并启动MRAppMaster</li>
<li>container从hdfs拷贝资源到本地</li>
<li>MRAppMaster向RM申请运行MapTask资源</li>
<li>ResourceManager将MapTask任务分配给NodeManager，领取到任务的NodeManager创建容器</li>
<li>MRAppMaster向接收到任务的NodeManager发送启动程序脚本，NodeManeger启动MapTask,执行map逻辑，输出分区排序后的结果</li>
<li>MRAppMaster等待所有MapTak运行结束，或者指定数量的MapTask运行结束后，向ResourceManager申请容器，运行ReduceTask.</li>
<li>ReduceTask从MapTask输出中取对应分区的数据，执行reduce逻辑</li>
<li>程序运行结束，MRAppMaster会向ResourceManager申请注销自己<h2 id="1-3-作业提交过程"><a href="#1-3-作业提交过程" class="headerlink" title="1.3 作业提交过程"></a>1.3 作业提交过程</h2><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16344721580647.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"><br><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16344723700779.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></li>
<li>作业提交阶段<ol>
<li>client调用job.waitForCompletion()方法，提交任务到集群</li>
<li>client想ResourceManager申请一个作业id</li>
<li>ResourceManager返回改job资源的提交路径和作业id</li>
<li>client提交jar，切片信息，配置文件到指定的资源提交路径</li>
<li>client提交资源后，向ResourceManager申请运行MRAppMaster</li>
</ol>
</li>
<li>作业初始化<ol>
<li>ResourceManager收到请求后，将job添加到任务队列</li>
<li>某一个空闲的NodeManager领取到任务后，创建Container，运行MRAppMaster</li>
<li>下载client提交的资源文件到本地</li>
</ol>
</li>
<li>任务分配<ol>
<li>MrAppMaster向ResourceManager申请MapTask运行资源</li>
<li>ResourceManager将MapTask分配给其他NodeManager</li>
<li>领取到任务的NodeManager创建MapTask容器</li>
</ol>
</li>
<li>任务运行<ol>
<li>MRAppMaster向领取到任务的NodeManager发送启动程序脚本</li>
<li>NodeManeger启动MapTask,执行map逻辑，输出分区排序后的结果</li>
<li>MRAppMaster等待所有MapTask运行结束，或者指定数量的MapTask运行结束，向ResourceManager申请容器，运行ReduceTask</li>
<li>ReduceTask从MapTask的输出文件中获取响应分区数据，执行Reduce逻辑</li>
<li>程序运行结束，MRAppMaster会向ResourceManager申请注销自己</li>
</ol>
</li>
<li>更新运行状态和进度<ol>
<li>Yarn中的任务进度和状态返回给应用管理器，客户端每秒(通过mapreduce.client.progressmonitor.pollinterval设置)向应用管理器请求更新进度，展示给用户</li>
</ol>
</li>
<li>作业完成<ol>
<li>除了向应用管理器请求作业进度外, 客户端每5秒都会通过调用waitForCompletion()来检查作业是否完成。时间间隔可以通过mapreduce.client.completion.pollinterval来设置。作业完成之后, 应用管理器和Container会清理工作状态。作业的信息会被作业历史服务器存储以备之后用户核查。</li>
</ol>
</li>
</ol>
<h2 id="1-4-资源调度器"><a href="#1-4-资源调度器" class="headerlink" title="1.4 资源调度器"></a>1.4 资源调度器</h2><ul>
<li><p>目前，Hadoop作业调度器主要有三种：FIFO、Capacity Scheduler和Fair Scheduler。Hadoop3.1.3默认的资源调度器是Capacity Scheduler。</p>
</li>
<li><p>具体设置详见：yarn-default.xml文件</p>
  <figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>The class to use as the resource scheduler.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.scheduler.class<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h3 id="1-4-1-FIFO先进先出调度器"><a href="#1-4-1-FIFO先进先出调度器" class="headerlink" title="1.4.1 FIFO先进先出调度器"></a>1.4.1 FIFO先进先出调度器</h3><p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16362766014573.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
</li>
<li><p>单队列，根据提交作业的先后顺序，先来先服务。</p>
</li>
<li><p>优点：简单易懂；</p>
</li>
<li><p>缺点：不支持多队列，生产环境很少使用；</p>
</li>
</ul>
<h3 id="1-4-2-容量调度器（Capacity-Scheduler）"><a href="#1-4-2-容量调度器（Capacity-Scheduler）" class="headerlink" title="1.4.2 容量调度器（Capacity Scheduler）"></a>1.4.2 容量调度器（Capacity Scheduler）</h3><p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16362766636662.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
<ul>
<li>Capacity Scheduler Capacity Scheduler 是Yahoo开发的多用户调度器，它以队列为单位划分资源，每个队列可设定一定比例的资源最低保证和使用上限，同时，每个用户也可设定一定的资源使用上限以防止资源滥用。而当一个队列的资源有剩余时，可暂时将剩余资源共享给其他队列。</li>
<li>总之，Capacity Scheduler 主要有以下几个特点：<ol>
<li>多队列：每个队列可配置一定的资源量，每个队列采用FIFO调度策略</li>
<li>容量保证：管理员可为每个队列设置资源最低保证和资源使用上限，所有提交到该队列的应用程序共享这些资源。</li>
<li>灵活性：如果一个队列中的资源有剩余，可以暂时共享给那些需要资源的队列，而一旦该队列有新的应用程序提交，则其他队列借调的资源会归还给该队列。这种资源灵活分配的方式可明显提高资源利用率。</li>
<li>多重租赁：支持多用户共享集群和多应用程序同时运行。为防止单个应用程序、用户或者队列独占集群中的资源，管理员可为之增加多重约束（比如单个应用程序同时运行的任务数等）。</li>
<li>安全保证：每个队列有严格的ACL列表规定它的访问用户，每个用户可指定哪些用户允许查看自己应用程序的运行状态或者控制应用程序（比如杀死应用程序）。此外，管理员可指定队列管理员和集群系统管理员。</li>
<li>动态更新配置文件：管理员可根据需要动态修改各种配置参数，以实现在线集群管理。</li>
</ol>
</li>
</ul>
<h3 id="1-4-3-公平调度器（Fair-Scheduler）"><a href="#1-4-3-公平调度器（Fair-Scheduler）" class="headerlink" title="1.4.3 公平调度器（Fair Scheduler）"></a>1.4.3 公平调度器（Fair Scheduler）</h3><p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16362766917702.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
<ul>
<li>Fair Scheduler Fair Schedulere是Facebook开发的多用户调度器。</li>
<li>公平调度器的目的是让所有的作业随着时间的推移，都能平均地获取等同的共享资源。当有作业提交上来，系统会将空闲的资源分配给新的作业，每个任务大致上会获取平等数量的资源。和传统的调度策略不同的是它会让小的任务在合理的时间完成，同时不会让需要长时间运行的耗费大量资源的任务挨饿！同Capacity Scheduler类似，它以队列为单位划分资源，每个队列可设定一定比例的资源最低保证和使用上限，同时，每个用户也可设定一定的资源使用上限以防止资源滥用；当一个队列的资源有剩余时，可暂时将剩余资源共享给其他队列。当然，Fair Scheduler也存在很多与Capacity Scheduler不同之处，这主要体现在以下几个方面：<ol>
<li>资源公平共享。在每个队列中，Fair Scheduler 可选择按照FIFO、Fair或DRF策略为应用程序分配资源。其中， </li>
<li>FIFO策略: 公平调度器每个队列资源分配策略如果选择FIFO的话，就是禁用掉每个队列中的Task共享队列资源，此时公平调度器相当于上面讲过的容量调度器。</li>
<li>Fair策略: 基于最大最小公平算法实现的资源多路复用方式，默认情况下，每个队列内部采用该方式分配资源。这意味着，如果一个队列中有两个应用程序同时运行，则每个应用程序可得到1/2的资源；如果三个应用程序同时运行，则每个应用程序可得到1/3的资源。<ul>
<li>扩展：最大最小公平算法举例：<ol>
<li>不加权(关注点是job的个数)： <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"> 有一条队列总资源12个, 有4个job，对资源的需求分别是: </span><br><span class="line">job1-&gt;1,  job2-&gt;2 , job3-&gt;6,  job4-&gt;5</span><br><span class="line">     第一次算:  12 / 4 = 3 </span><br><span class="line">		job1: 分3 --&gt; 多2个 </span><br><span class="line">		job2: 分3 --&gt; 多1个</span><br><span class="line">		job3: 分3 --&gt; 差3个</span><br><span class="line">		job4: 分3 --&gt; 差2个</span><br><span class="line">	第二次算: 3 / 2  = 1.5 </span><br><span class="line">         job1: 分1</span><br><span class="line">		job2: 分2</span><br><span class="line">		job3: 分3 --&gt; 差3个 --&gt; 分1.5 --&gt; 最终: 4.5 </span><br><span class="line">		job4: 分3 --&gt; 差2个 --&gt; 分1.5 --&gt; 最终: 4.5 </span><br><span class="line">	第n次算: 一直算到没有空闲资源</span><br></pre></td></tr></table></figure></li>
<li>加权(关注点是job的权重)： <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">有一条队列总资源16，有4个job </span><br><span class="line">对资源的需求分别是: job1-&gt;4   job2-&gt;2  job3-&gt;10  job4-&gt;4 </span><br><span class="line">每个job的权重为:   job1-&gt;5   job2-&gt;8  job3-&gt;1   job4-&gt;2	</span><br><span class="line">	第一次算: 16 / (5+8+1+2) =  1</span><br><span class="line">	    job1:  分5 --&gt; 多1</span><br><span class="line">	    job2:  分8 --&gt; 多6</span><br><span class="line">	    job3:  分1 --&gt; 少9</span><br><span class="line">	    job4:  分2 --&gt; 少2            </span><br><span class="line">	第二次算: 7 / (1+2) = 7/3</span><br><span class="line">	    job1: 分4</span><br><span class="line">	    job2: 分2</span><br><span class="line">	    job3: 分1 --&gt; 分7/3（2.33） --&gt; 少 6.67</span><br><span class="line">	    job4: 分2 --&gt; 分14/3(4.66) --&gt;多2.66</span><br><span class="line">    第三次算: </span><br><span class="line">	    job1: 分4</span><br><span class="line">	    job2: 分2</span><br><span class="line">	    job3: 分1 --&gt; 分7/3 --&gt; 分2.66</span><br><span class="line">	    job4: 分4</span><br><span class="line">    第n次算: 一直算到没有空闲资源</span><br></pre></td></tr></table></figure></li>
</ol>
</li>
</ul>
<ol start="4">
<li>DRF策略: DRF(Dominant Resource Fairness)，我们之前说的资源，都是单一标准，例如只考虑内存(也是yarn默认的情况)。但是很多时候我们资源有很多种，例如内存，CPU，网络带宽等，这样我们很难衡量两个应用应该分配的资源比例。那么在YARN中，我们用DRF来决定如何调度：假设集群一共有100 CPU和10T 内存，而应用A需要(2 CPU, 300GB)，应用B需要(6 CPU, 100GB)。则两个应用分别需要A(2%CPU, 3%内存)和B(6%CPU, 1%内存)的资源，这就意味着A是内存主导的, B是CPU主导的，针对这种情况，我们可以选择DRF策略对不同应用进行不同资源（CPU和内存）的一个不同比例的限制。 <ul>
<li>支持资源抢占。当某个队列中有剩余资源时，调度器会将这些资源共享给其他队列，而当该队列中有新的应用程序提交时，调度器要为它回收资源。为了尽可能降低不必要的计算浪费，调度器采用了先等待再强制回收的策略，即如果等待一段时间后尚有未归还的资源，则会进行资源抢占：从那些超额使用资源的队列中杀死一部分任务，进而释放资源。yarn.scheduler.fair.preemption=true 通过该配置开启资源抢占。</li>
<li>提高小应用程序响应时间。由于采用了最大最小公平算法，小作业可以快速获取资源并运行完成<h2 id="1-5-Yarn常用命令"><a href="#1-5-Yarn常用命令" class="headerlink" title="1.5 Yarn常用命令"></a>1.5 Yarn常用命令</h2><h3 id="1-5-1-yarn-application查看任务"><a href="#1-5-1-yarn-application查看任务" class="headerlink" title="1.5.1 yarn application查看任务"></a>1.5.1 yarn application查看任务</h3></li>
</ul>
</li>
</ol>
</li>
</ol>
</li>
</ul>
<ol>
<li>yarn application -list 查看所有任务 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 ~]$ yarn application -list</span><br><span class="line">Total number of applications (application-types: [], states: [SUBMITTED, ACCEPTED, RUNNING] and tags: []):1</span><br><span class="line">                Application-Id	    Application-Name	    Application-Type	      User	     Queue	             State	       Final-State	       Progress	                       Tracking-URL</span><br><span class="line">application_1635042010561_0004	sgg-hadoop-mapreduce-0.0.1-SNAPSHOT.jar	           MAPREDUCE	   atguigu	      hive	           RUNNING	         UNDEFINED	             5%	             http://hadoop002:38122</span><br><span class="line">[atguigu@hadoop001 ~]$</span><br></pre></td></tr></table></figure></li>
<li>根据Application状态过滤：yarn application -list -appStates<ul>
<li>所有状态：ALL、NEW、NEW_SAVING、SUBMITTED、ACCEPTED、RUNNING、FINISHED、FAILED、KILLED<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 ~]$ yarn application -list -appStates FINISHED</span><br><span class="line">Total number of applications (application-types: [], states: [FINISHED] and tags: []):15</span><br><span class="line">                Application-Id	    Application-Name	    Application-Type	      User	     Queue	             State	       Final-State	       Progress	                       Tracking-URL</span><br><span class="line">application_1635042010561_0004	sgg-hadoop-mapreduce-0.0.1-SNAPSHOT.jar	           MAPREDUCE	   atguigu	      hive	          FINISHED	         SUCCEEDED	           100%	http://hadoop002:19888/jobhistory/job/job_1635042010561_0004</span><br><span class="line">application_1634633871057_0005	hadoop-archives-3.1.3.jar	           MAPREDUCE	   atguigu	   default	          FINISHED	         SUCCEEDED	           100%	http://hadoop001:19888/jobhistory/job/job_1634633871057_0005</span><br><span class="line">application_1634633871057_0004	hadoop-archives-3.1.3.jar	           MAPREDUCE	   atguigu	   default	          FINISHED	         SUCCEEDED	           100%	http://hadoop001:19888/jobhistory/job/job_1634633871057_0004</span><br><span class="line">application_1634633871057_0003	              distcp	           MAPREDUCE	   atguigu	   default	          FINISHED	         SUCCEEDED	           100%	http://hadoop001:19888/jobhistory/job/job_1634633871057_0003</span><br><span class="line">application_1634633871057_0002	sgg-hadoop-mapreduce-0.0.1-SNAPSHOT.jar	           MAPREDUCE	   atguigu	   default	          FINISHED	         SUCCEEDED	           100%	http://hadoop001:19888/jobhistory/job/job_1634633871057_0002</span><br><span class="line">[atguigu@hadoop001 ~]$</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>Kill掉Application：yarn application -kill Application-Id <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 ~]$ yarn application -<span class="built_in">kill</span> application_1635042010561_0006</span><br><span class="line">Killing application application_1635042010561_0006</span><br><span class="line">2021-10-24 13:25:54,458 INFO impl.YarnClientImpl: Killed application application_1635042010561_0006</span><br><span class="line">[atguigu@hadoop001 ~]$</span><br></pre></td></tr></table></figure>
 客户端 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[INFO] [2021-10-24 13:25:55][org.apache.hadoop.mapreduce.Job]Job job_1635042010561_0006 failed with state KILLED due to: Application application_1635042010561_0006 was killed by user atguigu at 192.168.2.17</span><br><span class="line">[INFO] [2021-10-24 13:25:55][org.apache.hadoop.mapreduce.Job]Counters: 0</span><br></pre></td></tr></table></figure>
<h3 id="1-5-2-yarn-logs查看日志"><a href="#1-5-2-yarn-logs查看日志" class="headerlink" title="1.5.2 yarn logs查看日志"></a>1.5.2 yarn logs查看日志</h3></li>
<li>查询Application日志：yarn logs -applicationId <ApplicationId> <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 ~]$ yarn logs -applicationId application_1635042010561_0007 &gt; application_1635042010561_0007.log</span><br><span class="line">2021-10-24 13:31:26,239 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = <span class="literal">false</span>, remoteHostTrusted = <span class="literal">false</span></span><br><span class="line">2021-10-24 13:31:26,308 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = <span class="literal">false</span>, remoteHostTrusted = <span class="literal">false</span></span><br><span class="line">[atguigu@hadoop001 ~]$ head application_1635042010561_0007.log</span><br><span class="line">Container: container_e06_1635042010561_0007_01_000003 on hadoop001_34161</span><br><span class="line">LogAggregationType: AGGREGATED</span><br><span class="line">========================================================================</span><br><span class="line">LogType:directory.info</span><br><span class="line">LogLastModifiedTime:星期日 十月 24 13:30:33 +0800 2021</span><br><span class="line">LogLength:2011</span><br><span class="line">LogContents:</span><br><span class="line">ls -l:</span><br><span class="line">total 20</span><br><span class="line">-rw-r--r--. 1 atguigu atguigu  129 Oct 24 13:30 container_tokens</span><br><span class="line">[atguigu@hadoop001 ~]$ tail application_1635042010561_0007.log</span><br><span class="line">2021-10-24 13:30:31,976 INFO [Thread-75] org.apache.hadoop.ipc.Server: Stopping server on 33721</span><br><span class="line">2021-10-24 13:30:31,977 INFO [IPC Server Responder] org.apache.hadoop.ipc.Server: Stopping IPC Server Responder</span><br><span class="line">2021-10-24 13:30:31,978 INFO [IPC Server listener on 0] org.apache.hadoop.ipc.Server: Stopping IPC Server listener on 0</span><br><span class="line">2021-10-24 13:30:31,981 INFO [Thread-75] org.eclipse.jetty.server.handler.ContextHandler: Stopped o.e.j.w.WebAppContext@3cd26422&#123;/,null,UNAVAILABLE&#125;&#123;/mapreduce&#125;</span><br><span class="line">2021-10-24 13:30:31,984 INFO [Thread-75] org.eclipse.jetty.server.AbstractConnector: Stopped ServerConnector@57e388c3&#123;HTTP/1.1,[http/1.1]&#125;&#123;0.0.0.0:0&#125;</span><br><span class="line">2021-10-24 13:30:31,984 INFO [Thread-75] org.eclipse.jetty.server.handler.ContextHandler: Stopped o.e.j.s.ServletContextHandler@234a8f27&#123;/static,jar:file:/opt/module/ha-hadoop-3.1.3/share/hadoop/yarn/hadoop-yarn-common-3.1.3.jar!/webapps/static,UNAVAILABLE&#125;</span><br><span class="line"></span><br><span class="line">End of LogType:syslog</span><br><span class="line">***********************************************************************</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop001 ~]$</span><br></pre></td></tr></table></figure></li>
<li>查询Container日志：yarn logs -applicationId <ApplicationId> -containerId <ContainerId>  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 ~]$ yarn logs -applicationId application_1635042010561_0007 -containerId container_e06_1635042010561_0007_01_000003 &gt; container_e06_1635042010561_0007_01_000003.log</span><br><span class="line">2021-10-24 13:33:52,284 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = <span class="literal">false</span>, remoteHostTrusted = <span class="literal">false</span></span><br><span class="line">2021-10-24 13:33:52,353 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = <span class="literal">false</span>, remoteHostTrusted = <span class="literal">false</span></span><br><span class="line">[atguigu@hadoop001 ~]$ head container_e06_1635042010561_0007_01_000003.log</span><br><span class="line">Container: container_e06_1635042010561_0007_01_000003 on hadoop001_34161</span><br><span class="line">LogAggregationType: AGGREGATED</span><br><span class="line">========================================================================</span><br><span class="line">LogType:directory.info</span><br><span class="line">LogLastModifiedTime:星期日 十月 24 13:30:33 +0800 2021</span><br><span class="line">LogLength:2011</span><br><span class="line">LogContents:</span><br><span class="line">ls -l:</span><br><span class="line">total 20</span><br><span class="line">-rw-r--r--. 1 atguigu atguigu  129 Oct 24 13:30 container_tokens</span><br><span class="line">[atguigu@hadoop001 ~]$ tail container_e06_1635042010561_0007_01_000003.log</span><br><span class="line">2021-10-24 13:30:25,651 INFO [main] org.apache.hadoop.mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 16734 bytes</span><br><span class="line">2021-10-24 13:30:25,661 INFO [main] org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl: Merged 1 segments, 16746 bytes to disk to satisfy reduce memory <span class="built_in">limit</span></span><br><span class="line">2021-10-24 13:30:25,661 INFO [main] org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl: Merging 1 files, 16750 bytes from disk</span><br><span class="line">2021-10-24 13:30:25,661 INFO [main] org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce</span><br><span class="line">2021-10-24 13:30:25,661 INFO [main] org.apache.hadoop.mapred.Merger: Merging 1 sorted segments</span><br><span class="line">2021-10-24 13:30:25,663 INFO [main] org.apache.hadoop.mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 16734 bytes</span><br><span class="line"></span><br><span class="line">End of LogType:syslog.shuffle</span><br><span class="line">*******************************************************************************</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop001 ~]$</span><br></pre></td></tr></table></figure>
<h3 id="1-5-3-yarn-application-attempt查看尝试运行的任务"><a href="#1-5-3-yarn-application-attempt查看尝试运行的任务" class="headerlink" title="1.5.3 yarn application attempt查看尝试运行的任务"></a>1.5.3 yarn application attempt查看尝试运行的任务</h3></li>
<li>列出所有Application尝试的列表：yarn applicationattempt -list <ApplicationId> <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 ~]$ yarn applicationattempt -list application_1635042010561_0007</span><br><span class="line">Total number of application attempts :1</span><br><span class="line">         ApplicationAttempt-Id	               State	                    AM-Container-Id	                       Tracking-URL</span><br><span class="line">appattempt_1635042010561_0007_000001	            FINISHED	container_e06_1635042010561_0007_01_000001	http://hadoop001:8088/proxy/application_1635042010561_0007/</span><br><span class="line">[atguigu@hadoop001 ~]$</span><br></pre></td></tr></table></figure></li>
<li>打印ApplicationAttemp状态：yarn applicationattempt -status <ApplicationAttemptId> <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 ~]$ yarn applicationattempt -status appattempt_1635042010561_0007_000001</span><br><span class="line">Application Attempt Report :</span><br><span class="line">	ApplicationAttempt-Id : appattempt_1635042010561_0007_000001</span><br><span class="line">	State : FINISHED</span><br><span class="line">	AMContainer : container_e06_1635042010561_0007_01_000001</span><br><span class="line">	Tracking-URL : http://hadoop001:8088/proxy/application_1635042010561_0007/</span><br><span class="line">	RPC Port : 33721</span><br><span class="line">	AM Host : hadoop002</span><br><span class="line">	Diagnostics :</span><br><span class="line">[atguigu@hadoop001 ~]$</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="1-5-4-yarn-container查看容器"><a href="#1-5-4-yarn-container查看容器" class="headerlink" title="1.5.4 yarn container查看容器"></a>1.5.4 yarn container查看容器</h3><font color ='blue' >注：只有在任务跑的途中才能看到container的状态</font></li>
<li>打印Container状态：    yarn container -status <ContainerId> <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 ~]$ yarn container -list appattempt_1635042010561_0010_000001</span><br><span class="line">Total number of containers :3</span><br><span class="line">                  Container-Id	          Start Time	         Finish Time	               State	                Host	   Node Http Address	                            LOG-URL</span><br><span class="line">container_e06_1635042010561_0010_01_000001	星期日 十月 24 13:45:46 +0800 2021	                 N/A	             RUNNING	     hadoop002:35693	http://hadoop002:8042	http://hadoop002:8042/node/containerlogs/container_e06_1635042010561_0010_01_000001/atguigu</span><br><span class="line">container_e06_1635042010561_0010_01_000003	星期日 十月 24 13:45:50 +0800 2021	                 N/A	             RUNNING	     hadoop005:37776	http://hadoop005:8042	http://hadoop005:8042/node/containerlogs/container_e06_1635042010561_0010_01_000003/atguigu</span><br><span class="line">container_e06_1635042010561_0010_01_000005	星期日 十月 24 13:46:33 +0800 2021	                 N/A	             RUNNING	     hadoop002:35693	http://hadoop002:8042	http://hadoop002:8042/node/containerlogs/container_e06_1635042010561_0010_01_000005/atguigu</span><br><span class="line">[atguigu@hadoop001 ~]$ yarn container -status container_e06_1635042010561_0010_01_000001</span><br><span class="line">Container Report :</span><br><span class="line">	Container-Id : container_e06_1635042010561_0010_01_000001</span><br><span class="line">	Start-Time : 1635054346921</span><br><span class="line">	Finish-Time : 0</span><br><span class="line">	State : RUNNING</span><br><span class="line">	Execution-Type : GUARANTEED</span><br><span class="line">	LOG-URL : http://hadoop002:8042/node/containerlogs/container_e06_1635042010561_0010_01_000001/atguigu</span><br><span class="line">	Host : hadoop002:35693</span><br><span class="line">	NodeHttpAddress : http://hadoop002:8042</span><br><span class="line">	Diagnostics : null</span><br><span class="line">[atguigu@hadoop001 ~]$</span><br></pre></td></tr></table></figure></li>
<li>列出所有Container：yarn container -list <ApplicationAttemptId> <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 ~]$ yarn applicationattempt -list application_1635042010561_0009</span><br><span class="line">Total number of application attempts :1</span><br><span class="line">         ApplicationAttempt-Id	               State	                    AM-Container-Id	                       Tracking-URL</span><br><span class="line">appattempt_1635042010561_0009_000001	             RUNNING	container_e06_1635042010561_0009_01_000001	http://hadoop001:8088/proxy/application_1635042010561_0009/</span><br><span class="line">[atguigu@hadoop001 ~]$ yarn container -list appattempt_1635042010561_0009_000001</span><br><span class="line">Total number of containers :4</span><br><span class="line">                  Container-Id	          Start Time	         Finish Time	               State	                Host	   Node Http Address	                            LOG-URL</span><br><span class="line">container_e06_1635042010561_0009_01_000003	星期日 十月 24 13:43:31 +0800 2021	                 N/A	             RUNNING	     hadoop005:37776	http://hadoop005:8042	http://hadoop005:8042/node/containerlogs/container_e06_1635042010561_0009_01_000003/atguigu</span><br><span class="line">container_e06_1635042010561_0009_01_000002	星期日 十月 24 13:43:31 +0800 2021	                 N/A	             RUNNING	     hadoop005:37776	http://hadoop005:8042	http://hadoop005:8042/node/containerlogs/container_e06_1635042010561_0009_01_000002/atguigu</span><br><span class="line">container_e06_1635042010561_0009_01_000001	星期日 十月 24 13:43:27 +0800 2021	                 N/A	             RUNNING	     hadoop005:37776	http://hadoop005:8042	http://hadoop005:8042/node/containerlogs/container_e06_1635042010561_0009_01_000001/atguigu</span><br><span class="line">container_e06_1635042010561_0009_01_000004	星期日 十月 24 13:43:31 +0800 2021	                 N/A	             RUNNING	     hadoop005:37776	http://hadoop005:8042	http://hadoop005:8042/node/containerlogs/container_e06_1635042010561_0009_01_000004/atguigu</span><br><span class="line">[atguigu@hadoop001 ~]$</span><br></pre></td></tr></table></figure>
<h3 id="1-5-5-yarn-node查看节点状态"><a href="#1-5-5-yarn-node查看节点状态" class="headerlink" title="1.5.5 yarn node查看节点状态"></a>1.5.5 yarn node查看节点状态</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 ~]$ yarn node -list -all</span><br><span class="line">Total Nodes:5</span><br><span class="line">         Node-Id	     Node-State	Node-Http-Address	Number-of-Running-Containers</span><br><span class="line"> hadoop004:38785	        RUNNING	   hadoop004:8042	                           0</span><br><span class="line"> hadoop005:37776	        RUNNING	   hadoop005:8042	                           0</span><br><span class="line"> hadoop003:39188	        RUNNING	   hadoop003:8042	                           0</span><br><span class="line"> hadoop002:35693	        RUNNING	   hadoop002:8042	                           0</span><br><span class="line"> hadoop001:34161	        RUNNING	   hadoop001:8042	                           0</span><br><span class="line">[atguigu@hadoop001 ~]$</span><br></pre></td></tr></table></figure>
<h3 id="1-5-6-yarn-rmadmin更新配置"><a href="#1-5-6-yarn-rmadmin更新配置" class="headerlink" title="1.5.6 yarn rmadmin更新配置"></a>1.5.6 yarn rmadmin更新配置</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 ~]$ yarn rmadmin -refreshQueues</span><br><span class="line">[atguigu@hadoop001 ~]$</span><br></pre></td></tr></table></figure>
<h3 id="1-5-7-yarn-queue查看队列"><a href="#1-5-7-yarn-queue查看队列" class="headerlink" title="1.5.7 yarn queue查看队列"></a>1.5.7 yarn queue查看队列</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 ~]$  yarn queue -status default</span><br><span class="line">Queue Information :</span><br><span class="line">Queue Name : default</span><br><span class="line">	State : RUNNING</span><br><span class="line">	Capacity : 40.0%</span><br><span class="line">	Current Capacity : .0%</span><br><span class="line">	Maximum Capacity : 60.0%</span><br><span class="line">	Default Node Label expression : &lt;DEFAULT_PARTITION&gt;</span><br><span class="line">	Accessible Node Labels : *</span><br><span class="line">	Preemption : disabled</span><br><span class="line">	Intra-queue Preemption : disabled</span><br><span class="line">[atguigu@hadoop001 ~]$</span><br></pre></td></tr></table></figure>
<h2 id="1-6-Yarn生产环境核心参数"><a href="#1-6-Yarn生产环境核心参数" class="headerlink" title="1.6 Yarn生产环境核心参数"></a>1.6 Yarn生产环境核心参数</h2><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16350552254539.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></li>
</ol>
<table>
<thead>
<tr>
<th>类型</th>
<th>配置</th>
<th>描述</th>
<th>备注</th>
</tr>
</thead>
<tbody><tr>
<td>ResourceManager相关</td>
<td>yarn.resourcemanager.scheduler.class</td>
<td>配置调度器，默认容量</td>
<td></td>
</tr>
<tr>
<td></td>
<td>yarn.resourcemanager.scheduler.client.thread-count</td>
<td>ResourceManager处理调度器请求的线程数量，默认50</td>
<td></td>
</tr>
<tr>
<td>NodeManager相关</td>
<td>yarn.nodemanager.resource.detect-hardware-capabilities</td>
<td>是否让yarn自己检测硬件进行配置，默认false</td>
<td></td>
</tr>
<tr>
<td></td>
<td>yarn.nodemanager.resource.count-logical-processors-as-core</td>
<td>是否将虚拟核数当作CPU核数，默认false</td>
<td></td>
</tr>
<tr>
<td></td>
<td>yarn.nodemanager.resource.pcores-vcores-multiplier</td>
<td>虚拟核数和物理核数乘数，例如：4核8线程，该参数就应设为2，默认1.0</td>
<td></td>
</tr>
<tr>
<td></td>
<td>yarn.nodemanager.resource.memory-mb</td>
<td>NodeManager使用内存，默认8G</td>
<td></td>
</tr>
<tr>
<td></td>
<td>yarn.nodemanager.resource.system-reserved-memory-mb</td>
<td>NodeManager为系统保留多少内存</td>
<td>以上二个参数配置一个即可</td>
</tr>
<tr>
<td></td>
<td>yarn.nodemanager.resource.cpu-vcores</td>
<td>NodeManager使用CPU核数，默认8个</td>
<td></td>
</tr>
<tr>
<td></td>
<td>yarn.nodemanager.pmem-check-enabled</td>
<td>是否开启物理内存检查限制container，默认打开</td>
<td></td>
</tr>
<tr>
<td></td>
<td>yarn.nodemanager.vmem-check-enabled</td>
<td>是否开启虚拟内存检查限制container，默认打开</td>
<td></td>
</tr>
<tr>
<td></td>
<td>yarn.nodemanager.vmem-pmem-ratio</td>
<td>虚拟内存物理内存比例，默认2.1</td>
<td></td>
</tr>
<tr>
<td>Container相关</td>
<td>yarn.scheduler.minimum-allocation-mb</td>
<td>容器最最小内存，默认1G</td>
<td></td>
</tr>
<tr>
<td></td>
<td>yarn.scheduler.maximum-allocation-mb</td>
<td>容器最最大内存，默认8G</td>
<td></td>
</tr>
<tr>
<td></td>
<td>yarn.scheduler.minimum-allocation-vcores</td>
<td>容器最小CPU核数，默认1个</td>
<td></td>
</tr>
<tr>
<td></td>
<td>yarn.scheduler.maximum-allocation-vcores</td>
<td>容器最大CPU核数，默认4个</td>
<td></td>
</tr>
</tbody></table>
<ol>
<li> <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">	            </span><br><span class="line">yarn.resourcemanager.scheduler.client.thread-count     ResourceManager处理调度器请求的线程数量，默认50</span><br></pre></td></tr></table></figure>
<h1 id="二、容量调度器多队列提交案例"><a href="#二、容量调度器多队列提交案例" class="headerlink" title="二、容量调度器多队列提交案例"></a>二、容量调度器多队列提交案例</h1><h3 id="4-5-1-需求"><a href="#4-5-1-需求" class="headerlink" title="4.5.1 需求"></a>4.5.1 需求</h3></li>
</ol>
<ul>
<li>Yarn默认的容量调度器是一条单队列的调度器，在实际使用中会出现单个任务阻塞整个队列的情况。同时，随着业务的增长，公司需要分业务限制集群使用率。这就需要我们按照业务种类配置多条任务队列。</li>
</ul>
<h3 id="4-5-2-配置多队列的容量调度器"><a href="#4-5-2-配置多队列的容量调度器" class="headerlink" title="4.5.2 配置多队列的容量调度器"></a>4.5.2 配置多队列的容量调度器</h3><ul>
<li>默认Yarn的配置下，容量调度器只有一条Default队列。在capacity-scheduler.xml中可以配置多条队列，并降低default队列资源占比：  <figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 指定多队列，增加hive队列 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.root.queues<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>default,hive<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span></span><br><span class="line">      The queues at the this level (root is the root queue).</span><br><span class="line">    <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 降低default队列资源额定容量为40%，默认100% --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.root.default.capacity<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>40<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 降低default队列资源最大容量为60%，默认100% --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.root.default.maximum-capacity<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>60<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li>
<li>同时为新加队列添加必要属性：  <figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 指定hive队列的资源额定容量 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.root.hive.capacity<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>60<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.root.hive.user-limit-factor<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 指定hive队列的资源最大容量 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.root.hive.maximum-capacity<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span><span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.root.hive.state<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>RUNNING<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.root.hive.acl_submit_applications<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.root.hive.acl_administer_queue<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.root.hive.acl_application_max_priority<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.root.hive.maximum-application-lifetime<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>-1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.root.hive.default-application-lifetime<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>-1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li>
<li>在配置完成后，重启Yarn或者执行yarn rmadmin -refreshQueues刷新队列，就可以看到两条队列：</li>
</ul>
<h1 id="问答"><a href="#问答" class="headerlink" title="问答"></a>问答</h1><h3 id="一、MR中的一个Job是如何提交的？"><a href="#一、MR中的一个Job是如何提交的？" class="headerlink" title="一、MR中的一个Job是如何提交的？"></a>一、MR中的一个Job是如何提交的？</h3><ol>
<li>作业提交阶段<ol>
<li>client调用job.waitForCompletion()方法，提交任务到集群</li>
<li>client想ResourceManager申请一个作业id</li>
<li>ResourceManager返回改job资源的提交路径和作业id</li>
<li>client提交jar，切片信息，配置文件到指定的资源提交路径</li>
<li>client提交资源后，向ResourceManager申请运行MRAppMaster</li>
</ol>
</li>
<li>作业初始化<ol>
<li>ResourceManager收到请求后，将job添加到任务队列</li>
<li>某一个空闲的NodeManager领取到任务后，创建Container，运行MRAppMaster</li>
<li>下载client提交的资源文件到本地</li>
</ol>
</li>
<li>任务分配<ol>
<li>MrAppMaster向ResourceManager申请MapTask运行资源</li>
<li>ResourceManager将MapTask分配给其他NodeManager</li>
<li>领取到任务的NodeManager创建MapTask容器</li>
</ol>
</li>
<li>任务运行<ol>
<li>MRAppMaster向领取到任务的NodeManager发送启动程序脚本</li>
<li>NodeManeger启动MapTask,执行map逻辑，输出分区排序后的结果</li>
<li>MRAppMaster等待所有MapTask运行结束，或者指定数量的MapTask运行结束，向ResourceManager申请容器，运行ReduceTask</li>
<li>ReduceTask从MapTask的输出文件中获取响应分区数据，执行Reduce逻辑</li>
<li>程序运行结束，MRAppMaster会向ResourceManager申请注销自己</li>
</ol>
</li>
<li>更新运行状态和进度<ol>
<li>Yarn中的任务进度和状态返回给应用管理器，客户端每秒(通过mapreduce.client.progressmonitor.pollinterval设置)向应用管理器请求更新进度，展示给用户</li>
</ol>
</li>
<li>作业完成<ol>
<li>除了向应用管理器请求作业进度外, 客户端每5秒都会通过调用waitForCompletion()来检查作业是否完成。时间间隔可以通过mapreduce.client.completion.pollinterval来设置。作业完成之后, 应用管理器和Container会清理工作状态。作业的信息会被作业历史服务器存储以备之后用户核查。</li>
</ol>
</li>
</ol>
<h3 id="二、描述一下YARN的工作机制。"><a href="#二、描述一下YARN的工作机制。" class="headerlink" title="二、描述一下YARN的工作机制。"></a>二、描述一下YARN的工作机制。</h3><ol>
<li>MR程序在客户端通过job.submit()方法提交任务到本地或者远程hadoop集群，创建YranRunner</li>
<li>YarnRunner向ResourceManager申请一个Application</li>
<li>ResourceManager将程序运行的资源路径(资源提交路径及application_id)返回给Yarnrunner</li>
<li>程序将运行所需资源提交到hdfs上（jar包，配置文件，split信息）</li>
<li>程序资源提交之后申请运行MRAppMaster</li>
<li>ResourceManager将用户请求初始化为一个Task，该task会被放到任务队列中，等待调度器分配资源</li>
<li>NodeManager领取task任务</li>
<li>该NodeManager创建container，并启动MRAppMaster</li>
<li>container从hdfs拷贝资源到本地</li>
<li>MRAppMaster向RM申请运行MapTask资源</li>
<li>ResourceManager将MapTask任务分配给NodeManager，领取到任务的NodeManager创建容器</li>
<li>MRAppMaster向接收到任务的NodeManager发送启动程序脚本，NodeManeger启动MapTask,执行map逻辑，输出分区排序后的结果</li>
<li>MRAppMaster等待所有MapTak运行结束，或者指定数量的MapTask运行结束后，向ResourceManager申请容器，运行ReduceTask.</li>
<li>ReduceTask从MapTask输出中取对应分区的数据，执行reduce逻辑</li>
<li>程序运行结束，MRAppMaster会向ResourceManager申请注销自己</li>
</ol>
<h3 id="三、YARN中有几种资源调度实现策略？分别阐述一下各自的特点"><a href="#三、YARN中有几种资源调度实现策略？分别阐述一下各自的特点" class="headerlink" title="三、YARN中有几种资源调度实现策略？分别阐述一下各自的特点"></a>三、YARN中有几种资源调度实现策略？分别阐述一下各自的特点</h3><ol>
<li>FIFO调度器：<ul>
<li>单队列，根据作业提交的顺序，先到先分配资源</li>
<li>优点：简单易懂</li>
<li>缺点：不支持多队列，无法确定优先级，不够灵活</li>
</ul>
</li>
<li>CapacityScheduler容量调度区：<ul>
<li>多队列：每个队列可以配置一定比例的资源，每个队列内部采用FIFO调度策略</li>
<li>容量保证：可以设置每个队列的资源最低保证和资源使用上限，所有提交到该队列的应用程序共享这些资源</li>
<li>灵活性：如果一个队列中资源有空闲，可以暂时共享给需要资源的队列；一旦空闲队列有新的应用程序提交，借调资源的队列会归还资源，通过这种方式提高了资源的利用率</li>
<li>多重租赁：支持多用户共享集群和多应用程序同时运行，可以对用户，程序添加多重约束，避免产生用户或者程序独占集群资源</li>
<li>安全保证：每个队列有严格的ACL列表限制访问用户，可指用户对应用程序的查看权限，控制权限</li>
<li>动态更新配置文件：管理员可以根据需要动态修改参数配置，实现在线集群管理</li>
</ul>
</li>
</ol>
<h3 id="四、简述一下如何配置一个自定的-队列？"><a href="#四、简述一下如何配置一个自定的-队列？" class="headerlink" title="四、简述一下如何配置一个自定的 队列？"></a>四、简述一下如何配置一个自定的 队列？</h3><ol>
<li>配置多队列</li>
<li>调整队列资源分配</li>
<li>配置新队列属性</li>
<li>刷新队列</li>
</ol>
]]></content>
      <categories>
        <category>Yarn</category>
      </categories>
      <tags>
        <tag>Yarn</tag>
      </tags>
  </entry>
  <entry>
    <title>Shell</title>
    <url>/2021/11/07/Shell/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="Shell编程"><a href="#Shell编程" class="headerlink" title="Shell编程"></a>Shell编程</h1><h3 id="初识Shell"><a href="#初识Shell" class="headerlink" title="初识Shell"></a>初识Shell</h3><h4 id="1、概述"><a href="#1、概述" class="headerlink" title="1、概述"></a>1、概述</h4><p>shell是一个命令行解释器，它接收应用程序/用户命令，然后调用操作系统内核。shell还是一个功能强大的编程语言，易编写、易调试、灵活性强</p>
<p>Linux提供的shell解释器：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[atguigu@centos7_base study]$ cat /etc/shells </span><br><span class="line">/bin/sh</span><br><span class="line">/bin/bash</span><br><span class="line">/sbin/nologin</span><br><span class="line">/usr/bin/sh</span><br><span class="line">/usr/bin/bash</span><br><span class="line">/usr/sbin/nologin</span><br><span class="line">/bin/tcsh</span><br><span class="line">/bin/csh</span><br></pre></td></tr></table></figure>

<p>Centos默认的解析器是bash</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[atguigu@centos7_base study]$ echo $SHELL</span><br><span class="line">/bin/bash</span><br></pre></td></tr></table></figure>

<p>bash和sh的关系</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[atguigu@centos7_base study]$ ll /bin/ | grep bash</span><br><span class="line">-rwxr-xr-x. 1 root root     964544 4月  11 2018 bash</span><br><span class="line">lrwxrwxrwx. 1 root root         10 7月  28 00:28 bashbug -&gt; bashbug-64</span><br><span class="line">-rwxr-xr-x. 1 root root       6964 4月  11 2018 bashbug-64</span><br><span class="line">lrwxrwxrwx. 1 root root          4 7月  28 00:28 sh -&gt; bash</span><br></pre></td></tr></table></figure>

<h4 id="2、入门"><a href="#2、入门" class="headerlink" title="2、入门"></a>2、入门</h4><p>注意：shell脚本后缀为.sh文件，脚本以<code>#!/bin/bash</code>开头，目的是为了指定解析器</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">echo &#x27;hello word!&#x27;</span><br></pre></td></tr></table></figure>

<p>运行脚本的方式：</p>
<ol>
<li><p>第一种：采用bash或sh+脚本的相对路径或绝对路径（不用赋予脚本+x权限）</p>
<figure class="highlight powershell"><table><tr><td class="code"><pre><span class="line">[<span class="type">atguigu</span>@<span class="type">centos7_base</span> <span class="type">study</span>]<span class="variable">$</span> sh ./helloword.sh </span><br><span class="line">hello word!</span><br><span class="line">[<span class="type">atguigu</span>@<span class="type">centos7_base</span> <span class="type">study</span>]<span class="variable">$</span> sh /home/atguigu/study/helloword.sh </span><br><span class="line">hello word!</span><br><span class="line">[<span class="type">atguigu</span>@<span class="type">centos7_base</span> <span class="type">study</span>]<span class="variable">$</span> bash ./helloword.sh </span><br><span class="line">hello word!</span><br><span class="line">[<span class="type">atguigu</span>@<span class="type">centos7_base</span> <span class="type">study</span>]<span class="variable">$</span> bash /home/atguigu/study/helloword.sh </span><br><span class="line">hello word!</span><br></pre></td></tr></table></figure></li>
<li><p>采用输入脚本的绝对路径或相对路径执行脚本（必须具有可执行权限+x）</p>
<figure class="highlight powershell"><table><tr><td class="code"><pre><span class="line">[<span class="type">atguigu</span>@<span class="type">centos7_base</span> <span class="type">study</span>]<span class="variable">$</span> ll</span><br><span class="line">总用量 <span class="number">4</span></span><br><span class="line"><span class="literal">-rw</span><span class="literal">-rw</span><span class="literal">-r</span>--. <span class="number">1</span> atguigu atguigu <span class="number">32</span> <span class="number">8</span>月   <span class="number">6</span> <span class="number">18</span>:<span class="number">29</span> helloword.sh</span><br><span class="line">[<span class="type">atguigu</span>@<span class="type">centos7_base</span> <span class="type">study</span>]<span class="variable">$</span> chmod u+x helloword.sh </span><br><span class="line">[<span class="type">atguigu</span>@<span class="type">centos7_base</span> <span class="type">study</span>]<span class="variable">$</span> ll</span><br><span class="line">总用量 <span class="number">4</span></span><br><span class="line"><span class="literal">-rwxrw</span><span class="literal">-r</span>--. <span class="number">1</span> atguigu atguigu <span class="number">32</span> <span class="number">8</span>月   <span class="number">6</span> <span class="number">18</span>:<span class="number">29</span> helloword.sh</span><br><span class="line">[<span class="type">atguigu</span>@<span class="type">centos7_base</span> <span class="type">study</span>]<span class="variable">$</span> ./helloword.sh </span><br><span class="line">hello word!</span><br><span class="line">[<span class="type">atguigu</span>@<span class="type">centos7_base</span> <span class="type">study</span>]<span class="variable">$</span> /home/atguigu/study/helloword.sh hello word!</span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="变量"><a href="#变量" class="headerlink" title="变量"></a>变量</h3><h4 id="1、系统变量"><a href="#1、系统变量" class="headerlink" title="1、系统变量"></a>1、系统变量</h4><p><code>$PATH</code>、<code>$HOME</code>、<code>$PWD</code>、<code>$SHELL</code>、<code>$USER</code></p>
<p>显示shell中所有变量：set</p>
<p>说明：在终端输入sh进入到shell交互式界面，当然也可以直接在黑屏终端下操作测试，使用exit回退到终端</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[atguigu@centos7_base study]$ echo $PATH</span><br><span class="line">/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/home/atguigu/.local/bin:/home/atguigu/bin</span><br><span class="line">[atguigu@centos7_base study]$ sh</span><br><span class="line">sh-4.2$ echo $PATH</span><br><span class="line">/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/home/atguigu/.local/bin:/home/atguigu/bin</span><br><span class="line">sh-4.2$ exit</span><br><span class="line">exit</span><br><span class="line">[atguigu@centos7_base study]$sh</span><br><span class="line">sh-4.2$ set</span><br></pre></td></tr></table></figure>

<h4 id="2、自定义变量"><a href="#2、自定义变量" class="headerlink" title="2、自定义变量"></a>2、自定义变量</h4><p>定义变量：<code>变量=值</code></p>
<p>撤销变量：<code>unset 变量</code></p>
<p>声明静态变量：<code>readonly 变量</code>，静态变量不能unset</p>
<p>定义规则：</p>
<ol>
<li>变量名称可以由字母、数字和下划线组成，但是不能以数字开头，环境变量名建议大写</li>
<li>等号两侧不能有空格</li>
<li>在bash中，变量默认类型都是字符串类型，无法直接进行数值运算</li>
<li>变量的值如果有空格，需要使用双引号或单引号括起来</li>
</ol>
<p>单引号与双引号的区别：</p>
<ol>
<li>以单引号包围变量的值时，单引号里面是什么就输出什么，即使内容中有变量和命令（命令需要反引起来）也会把它们原样输出。这种方式比较适合定义显示纯字符串的情况，即不希望解析变量、命令等的场景</li>
<li>以双引号包围变量的值时，输出时会先解析里面的变量和命令，而不是把双引号中的变量名和命令原样输出。这种方式比较适合字符串中附带有变量和命令并且想将其解析后再输出的变量定义</li>
</ol>
<p>基本使用：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[atguigu@centos7_base study]$ sh</span><br><span class="line">sh-4.2$ a = 1</span><br><span class="line">sh: a: 未找到命令</span><br><span class="line">sh-4.2$ a= 1</span><br><span class="line">sh: 1: 未找到命令</span><br><span class="line">sh-4.2$ a =1</span><br><span class="line">sh: a: 未找到命令</span><br><span class="line">sh-4.2$ a=1</span><br><span class="line">sh-4.2$ echo $a</span><br><span class="line">1</span><br><span class="line">sh-4.2$ a=11</span><br><span class="line">sh-4.2$ echo $a</span><br><span class="line">11</span><br><span class="line">sh-4.2$ unset a</span><br><span class="line">sh-4.2$ echo $a</span><br><span class="line"></span><br><span class="line">sh-4.2$ readonly b=2</span><br><span class="line">sh-4.2$ echo $b</span><br><span class="line">2</span><br><span class="line">sh-4.2$ b=22</span><br><span class="line">sh: b: 只读变量</span><br><span class="line">sh-4.2$ unset b</span><br><span class="line">sh: unset: b: 无法反设定: 只读 variable</span><br><span class="line">sh-4.2$ c=1+2</span><br><span class="line">sh-4.2$ echo $c</span><br><span class="line">1+2</span><br><span class="line">sh-4.2$ d=sunck is a good man</span><br><span class="line">sh: is: 未找到命令</span><br><span class="line">sh-4.2$ d=&quot;sunck is a good man&quot;</span><br><span class="line">sh-4.2$ echo $d</span><br><span class="line">sunck is a good man</span><br><span class="line">sh-4.2$ e=&#x27;sunck is a good man&#x27;</span><br><span class="line">sh-4.2$ echo $e</span><br><span class="line">sunck is a good man</span><br><span class="line">sh-4.2$ f=200</span><br><span class="line">sh-4.2$ echo &#x27;变量f的值为：$f&#x27;</span><br><span class="line">变量f的值为：$g</span><br><span class="line">sh-4.2$ echo &quot;变量f的值为：$f&quot;</span><br><span class="line">变量f的值为：200</span><br></pre></td></tr></table></figure>

<p>将变量提升为全局变量：export</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sh-4.2$ G=100</span><br><span class="line">sh-4.2$ echo $G</span><br><span class="line">100</span><br><span class="line">sh-4.2$ vim helloword.sh </span><br><span class="line">sh-4.2$ cat helloword.sh </span><br><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">echo &#x27;hello word!&#x27;</span><br><span class="line">echo &quot;变量G的值为：$G&quot;</span><br><span class="line">sh-4.2$ ./helloword.sh </span><br><span class="line">hello word!</span><br><span class="line">变量G的值为：</span><br><span class="line">sh-4.2$ export G</span><br><span class="line">sh-4.2$ ./helloword.sh </span><br><span class="line">hello word!</span><br><span class="line">变量G的值为：100</span><br></pre></td></tr></table></figure>

<h4 id="3、特殊变量"><a href="#3、特殊变量" class="headerlink" title="3、特殊变量"></a>3、特殊变量</h4><p><strong>$n</strong></p>
<p>功能描述：n为数字，$0代表该脚本名称，$1-$9代表第一到第九个参数，十以上的参数需要用大括号包含，如${10}</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sh-4.2$ vim parameter1.sh</span><br><span class="line">sh-4.2$ cat parameter1.sh </span><br><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">echo &quot;命令：$0&quot;</span><br><span class="line">echo &quot;第1个参数：$1&quot;</span><br><span class="line">echo &quot;第2个参数：$2&quot;</span><br><span class="line">echo &quot;第3个参数：$3&quot;</span><br><span class="line">echo &quot;第4个参数：$4&quot;</span><br><span class="line">echo &quot;第5个参数：$5&quot;</span><br><span class="line">echo &quot;第6个参数：$6&quot;</span><br><span class="line">echo &quot;第7个参数：$7&quot;</span><br><span class="line">echo &quot;第8个参数：$8&quot;</span><br><span class="line">echo &quot;第9个参数：$9&quot;</span><br><span class="line">echo &quot;第10个参数：$&#123;10&#125;&quot;</span><br><span class="line">sh-4.2$ sh ./parameter1.sh 1 2 3 4 5 6 7 8 9 a</span><br><span class="line">命令：./parameter1.sh</span><br><span class="line">第1个参数：1</span><br><span class="line">第2个参数：2</span><br><span class="line">第3个参数：3</span><br><span class="line">第4个参数：4</span><br><span class="line">第5个参数：5</span><br><span class="line">第6个参数：6</span><br><span class="line">第7个参数：7</span><br><span class="line">第8个参数：8</span><br><span class="line">第9个参数：9</span><br><span class="line">第10个参数：a</span><br></pre></td></tr></table></figure>

<p><strong>$#</strong></p>
<p>功能描述：获取所有输入参数个数，常用于循环</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sh-4.2$ vim parameter2.sh</span><br><span class="line">sh-4.2$ cat parameter2.sh </span><br><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">echo &quot;参数个数：$#&quot;</span><br><span class="line">sh-4.2$ sh ./parameter2.sh </span><br><span class="line">参数个数：0</span><br><span class="line">sh-4.2$ sh ./parameter2.sh 1</span><br><span class="line">参数个数：1</span><br><span class="line">sh-4.2$ sh ./parameter2.sh 1 2 3</span><br><span class="line">参数个数：3</span><br></pre></td></tr></table></figure>

<p>**$<em>与$@</em>*</p>
<p>$* 功能描述：这个变量代表命令行中所有的参数，$*把所有的参数看成一个整体</p>
<p>$@ 功能描述：这个变量也代表命令行中所有的参数，不过$@把每个参数区分对待</p>
<p>说明：如果想让$*和$@ 体现区别必须用双引号括起来，并使用循环变量才能看到效果</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sh-4.2$ vim parameter3.sh</span><br><span class="line">sh-4.2$ cat parameter3.sh </span><br><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">echo $*</span><br><span class="line">echo $@</span><br><span class="line">sh-4.2$ sh ./parameter3.sh 1 2 3</span><br><span class="line">1 2 3</span><br><span class="line">1 2 3</span><br></pre></td></tr></table></figure>

<p><strong>$?</strong></p>
<p>功能描述：最后一次执行的命令的返回状态。如果这个变量的值为0，证明上一个命令正确执行；如果这个变量的值为非0（具体是哪个数，由命令自己来决定），则证明上一个命令执行不正确了</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sh-4.2$ sh ./parameter3.sh 1 2 3</span><br><span class="line">1 2 3</span><br><span class="line">1 2 3</span><br><span class="line">sh-4.2$ echo $?</span><br><span class="line">0</span><br></pre></td></tr></table></figure>

<h4 id="4、运算符"><a href="#4、运算符" class="headerlink" title="4、运算符"></a>4、运算符</h4><p>基本语法：<code>$((运算式))</code>或<code>$[运算式]</code></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sh-4.2$ h=$(((2+3)*4))</span><br><span class="line">sh-4.2$ i=$[(2+3)*4]</span><br><span class="line">sh-4.2$ echo $h $i</span><br><span class="line">20 20</span><br></pre></td></tr></table></figure>

<h4 id="5、控制台输入"><a href="#5、控制台输入" class="headerlink" title="5、控制台输入"></a>5、控制台输入</h4><p>基本语法：read (选项) (参数)</p>
<table>
<thead>
<tr>
<th>选项值</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>-p</td>
<td>指定读取值时的提示符</td>
</tr>
<tr>
<td>-t</td>
<td>指定读取值时等待的时间（秒）</td>
</tr>
</tbody></table>
<p>参数：一般为变量，指定读取值的变量名</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sh-4.2$ vim read.sh</span><br><span class="line">sh-4.2$ cat read.sh </span><br><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">read -p &quot;请输入一个数字：&quot; num1</span><br><span class="line">echo &quot;你第一次输入的数字为：$num1&quot;</span><br><span class="line"></span><br><span class="line">read -t 5 -p &quot;请在5秒内输入一个数字：&quot; num2</span><br><span class="line">echo &quot;你第二次输入的数字为：$num2&quot;</span><br><span class="line">sh-4.2$ sh ./read.sh </span><br><span class="line">请输入一个数字：1</span><br><span class="line">你第一次输入的数字为：1</span><br><span class="line">请在5秒内输入一个数字：2</span><br><span class="line">你第二次输入的数字为：2</span><br><span class="line">sh-4.2$ sh ./read.sh </span><br><span class="line">请输入一个数字：1</span><br><span class="line">你第一次输入的数字为：1</span><br><span class="line">请在5秒内输入一个数字：你第二次输入的数字为：</span><br><span class="line">sh-4.2$</span><br></pre></td></tr></table></figure>

<h4 id="6、条件判断"><a href="#6、条件判断" class="headerlink" title="6、条件判断"></a>6、条件判断</h4><p><strong>基本语法</strong>：</p>
<ul>
<li><code>test condition</code></li>
<li><code>[ condition ]</code>（注意condition前后要有空格）</li>
</ul>
<p>注意：条件非空即为true，例如[ atguigu ]与[ 0 ]返回true，[] 返回false</p>
<p><strong>常用判断条件</strong>：</p>
<ul>
<li><p>字符串比较是否相等</p>
<p><code>==</code></p>
</li>
<li><p>整数比较</p>
<p><code>-lt</code>：小于（less than）</p>
<p><code>-le</code>：小于等于（less equal）</p>
<p><code>-eq</code>：等于（equal）</p>
<p><code>-ne</code>：不等于（not equal）</p>
<p><code>-gt</code>：大于（greater than）</p>
<p><code>-ge</code>：大于等于（greater equal）</p>
</li>
<li><p>按照文件权限判断</p>
<p><code>-r</code>：有读的权限（read）</p>
<p><code>-w</code>：有写的权限（write）</p>
<p><code>-x</code>：-x 有执行的权限（execute）</p>
</li>
<li><p>按照文件类型判断</p>
<p><code>-f</code>：文件存在并且是一个常规的文件（file）</p>
<p><code>-d</code>：文件存在并是一个目录（directory）</p>
<p><code>-e</code>：文件存在（existence）</p>
</li>
<li><p>与或非</p>
<p><code>-a</code>、<code>-o</code>、<code>!</code>：在中括号内使用</p>
<p><code>&amp;&amp;</code>、<code>||</code>、<code>!</code>：在中括号外使用，计算多个中括号中的条件判断式</p>
</li>
</ul>
<p>实操：</p>
<ol>
<li><p>判断两个字符相等</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sh-4.2$ [ &quot;sunck&quot; == &quot;sunck&quot; ]</span><br><span class="line">sh-4.2$ echo $?</span><br><span class="line">0</span><br></pre></td></tr></table></figure></li>
<li><p>2是否大于等于1</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sh-4.2$ [ 2 -gt 1 ]</span><br><span class="line">sh-4.2$ echo $?</span><br><span class="line">0</span><br></pre></td></tr></table></figure></li>
<li><p>helloword.sh是否具有读权限</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sh-4.2$ [ -r ./helloword.sh ]</span><br><span class="line">sh-4.2$ echo $?</span><br><span class="line">0</span><br></pre></td></tr></table></figure></li>
<li><p>/home/atguigu/study/a.txt文件是否存在</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sh-4.2$ [ -e ./a.txt ]</span><br><span class="line">sh-4.2$ echo $?</span><br><span class="line">1</span><br><span class="line">sh-4.2$ [ -e ./helloword.sh ]</span><br><span class="line">sh-4.2$ echo $?</span><br><span class="line">0</span><br></pre></td></tr></table></figure></li>
<li><p>helloword.sh是普通文件并且可读</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sh-4.2$ [ -f ./helloword.sh -a -r ./helloword.sh ]</span><br><span class="line">sh-4.2$ echo $?</span><br><span class="line">0</span><br><span class="line">sh-4.2$ [ -d ./helloword.sh -a -r ./helloword.sh ]</span><br><span class="line">sh-4.2$ echo $?</span><br><span class="line">1</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sh-4.2$ [ -d ./helloword.sh ] &amp;&amp; [ -r ./helloword.sh ]</span><br><span class="line">sh-4.2$ echo $?</span><br><span class="line">1</span><br><span class="line">sh-4.2$ [ -f ./helloword.sh ] &amp;&amp; [ -r ./helloword.sh ]</span><br><span class="line">sh-4.2$ echo $?</span><br><span class="line">0</span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="选择判断语句"><a href="#选择判断语句" class="headerlink" title="选择判断语句"></a>选择判断语句</h3><h4 id="1、if-语句"><a href="#1、if-语句" class="headerlink" title="1、if 语句"></a>1、if 语句</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">if [ 条件判断式 ]</span><br><span class="line">then</span><br><span class="line">	语句</span><br><span class="line">fi</span><br></pre></td></tr></table></figure>

<p>逻辑：当程序执行到if语句时，首先计算”条件判断式“，如果”条件判断式“成立，则执行”语句“，否则结束整个if语句继续向下执行</p>
<p>需求：定义一个变量，如果变量的值大于10则输出“sunck is a good man”</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">read -p &quot;输入一个数字：&quot; num</span><br><span class="line">if [ $num -gt 10 ]</span><br><span class="line">then</span><br><span class="line">	echo &quot;sunck is a good man&quot;</span><br><span class="line">fi</span><br></pre></td></tr></table></figure>

<h4 id="2、if-else语句"><a href="#2、if-else语句" class="headerlink" title="2、if-else语句"></a>2、if-else语句</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">if [ 条件判断式 ]</span><br><span class="line">then</span><br><span class="line">	语句1</span><br><span class="line">else</span><br><span class="line">	语句2</span><br><span class="line">fi</span><br></pre></td></tr></table></figure>

<p>逻辑：当程序执行到if语句时，首先计算”条件判断式“，如果”条件判断式“成立，则执行”语句1“，否则执行“语句2”</p>
<p>需求：定义一个变量，如果变量的值大于10则输出“sunck is a good man”，否则输出“sunck is a nice man”</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">read -p &quot;输入一个数字：&quot; num</span><br><span class="line">if [ $num -gt 10 ]</span><br><span class="line">then</span><br><span class="line">	echo &quot;sunck is a good man&quot;</span><br><span class="line">else</span><br><span class="line">	echo &quot;sunck is a nice man&quot;</span><br><span class="line">fi</span><br></pre></td></tr></table></figure>

<h4 id="3、if-elif-else语句"><a href="#3、if-elif-else语句" class="headerlink" title="3、if-elif-else语句"></a>3、if-elif-else语句</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">if [ 条件判断式1 ];then</span><br><span class="line">	语句1</span><br><span class="line">elif [ 条件判断式2 ];then</span><br><span class="line">	语句2</span><br><span class="line">elif [ 条件判断式3 ];then</span><br><span class="line">	语句3</span><br><span class="line">……</span><br><span class="line">else</span><br><span class="line">	语句e</span><br><span class="line">fi</span><br></pre></td></tr></table></figure>

<p>逻辑：当程序执行到if-elif-else语句时，首先计算“条件判断式1”，如果“条件判断式1”成立则执行“语句1”，否则计算“条件判断式2”。如果“条件判断式2”成立则执行“语句2”，否则计算“条件判断式3”，直到某个“条件判断式”成立为止。如果所有的“条件判断式”都不成立，且有else语句，则执行“语句e”，否则结束整个if-elif-else语句继续向下运行</p>
<p>需求：定义一个age变量，如果age的值小于等于0则输出“age有误”，如果age的值大于0小于等于3则输出“婴儿”，如果age的值大于3小于等于6则输出“幼儿”，如果age的值大于6小于等于12则输出“童年”，如果age的值大于12小于等于18则输出“少年”，如果age的值大于18小于等于30则输出“青年”，如果age的值大于30小于等于40则输出“壮年”，如果age的值大于40小于等于50则输出“中年”，如果age的值大于50小于等于150则输出“老年”，其余则输出“妖怪”</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">read -p &quot;输入一个数字：&quot; age</span><br><span class="line">if [ $age -le 0 ];then</span><br><span class="line">	echo &quot;age有误！&quot;</span><br><span class="line">elif [ $age -gt 0 ] &amp;&amp; [ $age -le 3 ];then</span><br><span class="line">	echo &quot;婴儿&quot;</span><br><span class="line">elif [ $age -gt 3 ] &amp;&amp; [ $age -le 6 ];then</span><br><span class="line">	echo &quot;幼儿&quot;</span><br><span class="line">elif [ $age -gt 6 ] &amp;&amp; [ $age -le 12 ];then</span><br><span class="line">	echo &quot;童年&quot;</span><br><span class="line">elif [ $age -gt 12 ] &amp;&amp; [ $age -le 18 ];then</span><br><span class="line">	echo &quot;少年&quot;</span><br><span class="line">elif [ $age -gt 18 ] &amp;&amp; [ $age -le 30 ];then</span><br><span class="line">	echo &quot;青年&quot;</span><br><span class="line">elif [ $age -gt 30 ] &amp;&amp; [ $age -le 40 ];then</span><br><span class="line">	echo &quot;壮年&quot;</span><br><span class="line">elif [ $age -gt 40 ] &amp;&amp; [ $age -le 50 ];then</span><br><span class="line">	echo &quot;中年&quot;</span><br><span class="line">elif [ $age -gt 50 ] &amp;&amp; [ $age -le 150 ];then</span><br><span class="line">	echo &quot;老年&quot;</span><br><span class="line">else</span><br><span class="line">	echo &quot;妖怪&quot;</span><br><span class="line">fi</span><br></pre></td></tr></table></figure>

<p>精髓：每一个else都是对它上面所有表达式的否定</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">read -p &quot;输入一个数字：&quot; age</span><br><span class="line">if [ $age -le 0 ];then</span><br><span class="line">	echo &quot;age有误！&quot;</span><br><span class="line">elif [ $age -le 3 ];then</span><br><span class="line">	echo &quot;婴儿&quot;</span><br><span class="line">elif [ $age -le 6 ];then</span><br><span class="line">	echo &quot;幼儿&quot;</span><br><span class="line">elif [ $age -le 12 ];then</span><br><span class="line">	echo &quot;童年&quot;</span><br><span class="line">elif [ $age -le 18 ];then</span><br><span class="line">	echo &quot;少年&quot;</span><br><span class="line">elif [ $age -le 30 ];then</span><br><span class="line">	echo &quot;青年&quot;</span><br><span class="line">elif [ $age -le 40 ];then</span><br><span class="line">	echo &quot;壮年&quot;</span><br><span class="line">elif [ $age -le 50 ];then</span><br><span class="line">	echo &quot;中年&quot;</span><br><span class="line">elif [ $age -le 150 ];then</span><br><span class="line">	echo &quot;老年&quot;</span><br><span class="line">else</span><br><span class="line">	echo &quot;妖怪&quot;</span><br><span class="line">fi</span><br></pre></td></tr></table></figure>

<h4 id="4、case语句"><a href="#4、case语句" class="headerlink" title="4、case语句"></a>4、case语句</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">case $变量名 in</span><br><span class="line">&quot;值1&quot;)</span><br><span class="line">	语句1</span><br><span class="line">	;;</span><br><span class="line">&quot;值2&quot;)</span><br><span class="line">	语句2</span><br><span class="line">	;;</span><br><span class="line">……</span><br><span class="line">*)</span><br><span class="line">	语句*</span><br><span class="line">	;;</span><br><span class="line">esac</span><br></pre></td></tr></table></figure>

<p>逻辑：当程序执行到case语句时，匹配“变量”的值，匹配上哪个“值”就执行对应的“语句”，执行完“语句”后结束整个case变量。如果没有匹配的“值”则执行“语句*”</p>
<ol>
<li>case行尾必须为单词“in”，每一个模式匹配必须以右括号“）”结束</li>
<li>双分号“**;;**”表示命令序列结束，相当于java中的break</li>
<li>最后的“*）”表示默认模式，相当于java中的default</li>
</ol>
<p>需求：执行脚本时输入一个数字，输入数字1则打印“星期1”，输入数字2则打印“星期2”，输入数字3则打印“星期3”，输入数字4则打印“星期4”，输入数字5则打印“星期5”，输入数字6则打印“星期6”，输入数字7则打印“星期7”，其他输出“输入有误”</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">read -p &quot;请输入一个数字：&quot; num</span><br><span class="line"></span><br><span class="line">case $num in</span><br><span class="line">&quot;1&quot;)</span><br><span class="line">	echo &quot;星期1&quot;</span><br><span class="line">	;;</span><br><span class="line">&quot;2&quot;)</span><br><span class="line">	echo &quot;星期2&quot;</span><br><span class="line">	;;</span><br><span class="line">&quot;3&quot;)</span><br><span class="line">	echo &quot;星期3&quot;</span><br><span class="line">	;;</span><br><span class="line">&quot;4&quot;)</span><br><span class="line">	echo &quot;星期4&quot;</span><br><span class="line">	;;</span><br><span class="line">&quot;5&quot;)</span><br><span class="line">	echo &quot;星期5&quot;</span><br><span class="line">	;;</span><br><span class="line">&quot;6&quot;)</span><br><span class="line">	echo &quot;星期6&quot;</span><br><span class="line">	;;</span><br><span class="line">&quot;7&quot;)</span><br><span class="line">	echo &quot;星期7&quot;</span><br><span class="line">	;;</span><br><span class="line">*)</span><br><span class="line">	echo &quot;输入有误&quot;</span><br><span class="line">	;;</span><br><span class="line">esac</span><br></pre></td></tr></table></figure>

<h3 id="循环语句"><a href="#循环语句" class="headerlink" title="循环语句"></a>循环语句</h3><h4 id="1、while循环"><a href="#1、while循环" class="headerlink" title="1、while循环"></a>1、while循环</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">while [ 条件判断式 ]</span><br><span class="line">do</span><br><span class="line">	语句</span><br><span class="line">done</span><br></pre></td></tr></table></figure>

<p>逻辑：当程序执行到while语句时，首先计算“条件判断式”的值，如果“条件判断式”不成立则结束整个while语句继续向下执行，如果“条件判断式”成立则执行“语句”，执行完“语句”再去计算“条件判断式”的值。如果“条件判断式”不成立则结束整个while语句继续向下执行，如果“条件判断式”成立则执行“语句”，执行完“语句”再去计算“条件判断式”的值。如此循环往复，直到“条件判断式”不成立才终止while语句。</p>
<p>需求：计算1加到100的和</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">ret=0</span><br><span class="line">index=1</span><br><span class="line"></span><br><span class="line">while [ $index -le 100 ]</span><br><span class="line">do</span><br><span class="line">	ret=$[$ret+$index]</span><br><span class="line">	index=$[$index+1]</span><br><span class="line">done</span><br><span class="line"></span><br><span class="line">echo &quot;1加到100的和为：$ret&quot;</span><br></pre></td></tr></table></figure>

<h4 id="2、for语句"><a href="#2、for语句" class="headerlink" title="2、for语句"></a>2、for语句</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">for ((变量初始化;循环控制条件;变量变化))</span><br><span class="line">do</span><br><span class="line">	语句</span><br><span class="line">done</span><br></pre></td></tr></table></figure>

<p>逻辑：当程序执行到for语句时，首先初始化一个“变量”的值，再去计算”循环控制条件”，如果“循环控制条件”不成立则结束整个for语句继续向下运行，如果“循环控制条件”成立则执行“语句”，执行完“语句”在执行“变量变化”修改“变量的值”，再去计算“循环控制条件”。如此循环往复，直到“循环控制条件”不成立才停止整个for语句。</p>
<p>需求：计算1加到100的和</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">ret=0</span><br><span class="line">for ((i=0;i&lt;=100;i++))</span><br><span class="line">do</span><br><span class="line">	ret=$[$ret+$i]</span><br><span class="line">done</span><br><span class="line">echo &quot;1加到100的和为：$ret&quot;</span><br></pre></td></tr></table></figure>

<p>注意：只有在for里可以这么写</p>
<h4 id="3、for-in语句"><a href="#3、for-in语句" class="headerlink" title="3、for-in语句"></a>3、for-in语句</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">for 变量 in 计算值1 计算值2 计算值3 ……</span><br><span class="line">do</span><br><span class="line">	语句</span><br><span class="line">done</span><br></pre></td></tr></table></figure>

<p>逻辑：按顺序便利in后面的所有“计算值”</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">a=1</span><br><span class="line">b=2</span><br><span class="line"></span><br><span class="line">for i in &#x27;good&#x27; &#x27;cool&#x27; $a $b $[1+2]</span><br><span class="line">do</span><br><span class="line">	echo &quot;i的值为：$i&quot;</span><br><span class="line">done</span><br></pre></td></tr></table></figure>

<h4 id="4、-与"><a href="#4、-与" class="headerlink" title="4、$*与$@"></a>4、$*与$@</h4><p>$*和$@都表示传递给函数或脚本的所有参数，不被双引号“”包含时，都以$1 $2 …$n的形式输出所有参数。当它们被双引号“”包含时，“$*”会将所有的参数作为一个整体，以“$1 $2 …$n”的形式输出所有参数；“$@”会将各个参数分开，以“$1” “$2”…”$n”的形式输出所有参数</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">echo &#x27;------------$*--------------&#x27;</span><br><span class="line">for i in $*</span><br><span class="line">do</span><br><span class="line">	echo &quot;i的值为：$i&quot;</span><br><span class="line">done</span><br><span class="line"></span><br><span class="line">echo &#x27;------------$@--------------&#x27;</span><br><span class="line">for j in $@</span><br><span class="line">do</span><br><span class="line">	echo &quot;j的值为：$i&quot;</span><br><span class="line">done</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sh-4.2$ sh ./test.sh 1 2 3</span><br><span class="line"><span class="meta">------------$</span><span class="bash">*--------------</span></span><br><span class="line">i的值为：1</span><br><span class="line">i的值为：2</span><br><span class="line">i的值为：3</span><br><span class="line"><span class="meta">------------$</span><span class="bash">@--------------</span></span><br><span class="line">j的值为：3</span><br><span class="line">j的值为：3</span><br><span class="line">j的值为：3</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">echo &#x27;------------$*--------------&#x27;</span><br><span class="line">for i in &quot;$*&quot;</span><br><span class="line">do</span><br><span class="line">	echo &quot;i的值为：$i&quot;</span><br><span class="line">done</span><br><span class="line"></span><br><span class="line">echo &#x27;------------$@--------------&#x27;</span><br><span class="line">for j in &quot;$@&quot;</span><br><span class="line">do</span><br><span class="line">	echo &quot;j的值为：$i&quot;</span><br><span class="line">done</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sh-4.2$ sh ./test.sh 1 2 3</span><br><span class="line"><span class="meta">------------$</span><span class="bash">*--------------</span></span><br><span class="line">i的值为：1 2 3</span><br><span class="line"><span class="meta">------------$</span><span class="bash">@--------------</span></span><br><span class="line">j的值为：1 2 3</span><br><span class="line">j的值为：1 2 3</span><br><span class="line">j的值为：1 2 3</span><br></pre></td></tr></table></figure>

<h4 id="5、break与continue"><a href="#5、break与continue" class="headerlink" title="5、break与continue"></a>5、break与continue</h4><p>break：调出距离最近的那一层循环</p>
<p>continue：跳过距离最近的那一层循环，继续下一次循环继</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">echo &quot;----------------break-------------------&quot;</span><br><span class="line">for ((i=1;i&lt;=10;i++))</span><br><span class="line">do</span><br><span class="line">	if [ $i -eq 5 ]</span><br><span class="line">	then</span><br><span class="line">		break</span><br><span class="line">	fi</span><br><span class="line">	echo &quot;i的值为：$i&quot;</span><br><span class="line">done</span><br><span class="line">echo &quot;----------------continue-------------------&quot;</span><br><span class="line">for ((j=1;j&lt;=10;j++))</span><br><span class="line">do</span><br><span class="line">	if [ $j -eq 5 ]</span><br><span class="line">	then</span><br><span class="line">		continue</span><br><span class="line">	fi</span><br><span class="line">	echo &quot;j的值为：$j&quot;</span><br><span class="line">done</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sh-4.2$ sh ./test.sh </span><br><span class="line">----------------break-------------------</span><br><span class="line">i的值为：1</span><br><span class="line">i的值为：2</span><br><span class="line">i的值为：3</span><br><span class="line">i的值为：4</span><br><span class="line">----------------continue-------------------</span><br><span class="line">j的值为：1</span><br><span class="line">j的值为：2</span><br><span class="line">j的值为：3</span><br><span class="line">j的值为：4</span><br><span class="line">j的值为：6</span><br><span class="line">j的值为：7</span><br><span class="line">j的值为：8</span><br><span class="line">j的值为：9</span><br><span class="line">j的值为：10</span><br></pre></td></tr></table></figure>

<h3 id="系统函数"><a href="#系统函数" class="headerlink" title="系统函数"></a>系统函数</h3><h4 id="1、basename"><a href="#1、basename" class="headerlink" title="1、basename"></a>1、basename</h4><p>基本语法：<code>basename [string/pathname] [suffix]</code></p>
<p>功能描述：basename命令会删掉所有的前缀包括最后一个（‘/’）字符，然后将字符串显示出来</p>
<p>选项suffix：如果suffix被指定了，basename会将pathname或string中的suffix去掉</p>
<figure class="highlight powershell"><table><tr><td class="code"><pre><span class="line">[<span class="type">atguigu</span>@<span class="type">centos7_base</span> <span class="type">study</span>]<span class="variable">$</span> basename /a/b/c/d/e.txt</span><br><span class="line">e.txt</span><br><span class="line">[<span class="type">atguigu</span>@<span class="type">centos7_base</span> <span class="type">study</span>]<span class="variable">$</span> basename /a/b/c/d/e.txt txt</span><br><span class="line">e.</span><br><span class="line">[<span class="type">atguigu</span>@<span class="type">centos7_base</span> <span class="type">study</span>]<span class="variable">$</span> basename /a/b/c/d/e.txt .txt</span><br><span class="line">e</span><br></pre></td></tr></table></figure>

<h4 id="2、dirname"><a href="#2、dirname" class="headerlink" title="2、dirname"></a>2、dirname</h4><p>基本语法：<code>dirname 文件绝对路径</code></p>
<p>功能描述：从给定的包含绝对路径的文件名中去除文件名（非目录的部分），然后返回剩下的路径（目录的部分）</p>
<figure class="highlight powershell"><table><tr><td class="code"><pre><span class="line">[<span class="type">atguigu</span>@<span class="type">centos7_base</span> <span class="type">study</span>]<span class="variable">$</span> dirname /a/b/c/d/e.txt</span><br><span class="line">/a/b/c/d</span><br></pre></td></tr></table></figure>

<h3 id="自定义函数"><a href="#自定义函数" class="headerlink" title="自定义函数"></a>自定义函数</h3><h4 id="1、定义"><a href="#1、定义" class="headerlink" title="1、定义"></a>1、定义</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[function] funname [()]</span><br><span class="line">&#123;</span><br><span class="line">    action;</span><br><span class="line"></span><br><span class="line">    [return int;]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<table>
<thead>
<tr>
<th>结构</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>function</td>
<td>说明定义函数，可以省略</td>
</tr>
<tr>
<td>funname</td>
<td>自定义函数名，遵循标识符规则</td>
</tr>
<tr>
<td>()</td>
<td>可以省略，和function不能同时省略</td>
</tr>
<tr>
<td>action</td>
<td>封装的功能</td>
</tr>
<tr>
<td>return int</td>
<td>函数返回值，只能通过$?系统变量来获取，可以显示添加。如果不添加，将以最后一条命令运行的结果作为返回值。注意return后的数值在[0~255]之间，如果超过255，将返回该值与256的余数。</td>
</tr>
<tr>
<td>;</td>
<td>语句结束标志，可以不写</td>
</tr>
</tbody></table>
<h4 id="2、调用"><a href="#2、调用" class="headerlink" title="2、调用"></a>2、调用</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">funname [……]</span><br><span class="line">echo &quot;函数的返回值为：$?&quot;</span><br></pre></td></tr></table></figure>

<table>
<thead>
<tr>
<th>结果</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>funname</td>
<td>要调用的函数名</td>
</tr>
<tr>
<td>……</td>
<td>传递参数，如果无需传参即省略</td>
</tr>
<tr>
<td>$?</td>
<td>获取函数的返回值</td>
</tr>
<tr>
<td>“”</td>
<td>需要使用双引号，可以使用特殊变量</td>
</tr>
</tbody></table>
<p>注意：要在定义函数后再调用函数</p>
<h4 id="3、无参无返回值的函数"><a href="#3、无参无返回值的函数" class="headerlink" title="3、无参无返回值的函数"></a>3、无参无返回值的函数</h4><p>需求：打印“hello word”</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 定义函数</span></span><br><span class="line">function func()</span><br><span class="line">&#123;</span><br><span class="line">	echo &quot;hello word&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">echo &quot;-----函数开始执行------&quot;</span><br><span class="line"><span class="meta">#</span><span class="bash"> 调用函数</span></span><br><span class="line">func</span><br><span class="line"><span class="meta">#</span><span class="bash"> 获取函数的返回值的值</span></span><br><span class="line">echo &quot;返回值为：$?&quot;</span><br><span class="line">echo &quot;-----函数执行完毕------&quot;</span><br></pre></td></tr></table></figure>

<h4 id="4、使用参数"><a href="#4、使用参数" class="headerlink" title="4、使用参数"></a>4、使用参数</h4><p>需求：计算两个数据的和</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">function func()</span><br><span class="line">&#123;</span><br><span class="line"><span class="meta">	#</span><span class="bash"> 使用传递过来的参数值</span></span><br><span class="line">	ret=$[$1+$2]</span><br><span class="line">	echo &quot;$1+$2的结果为：$ret&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">echo &quot;-----函数开始执行------&quot;</span><br><span class="line"><span class="meta">#</span><span class="bash"> 调用函数，传递参数</span></span><br><span class="line">func 3 5</span><br><span class="line">echo &quot;返回值为：$?&quot;</span><br><span class="line">echo &quot;-----函数执行完毕------&quot;</span><br></pre></td></tr></table></figure>

<h4 id="5、返回值"><a href="#5、返回值" class="headerlink" title="5、返回值"></a>5、返回值</h4><p>说明：return后的数值在[0~255]之间，如果超过255，将返回该值与256的余数</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">function func()</span><br><span class="line">&#123;</span><br><span class="line">	ret=$[$1+$2]</span><br><span class="line">	return $ret</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">echo &quot;-----函数开始执行------&quot;</span><br><span class="line">func 3 5</span><br><span class="line">echo &quot;返回值为：$?&quot;</span><br><span class="line">echo &quot;-----函数执行完毕------&quot;</span><br></pre></td></tr></table></figure>

<ul>
<li><p>需求：在执行脚本时传递两个数字求和</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">function func()</span><br><span class="line">&#123;</span><br><span class="line">	ret=$[$1+$2]</span><br><span class="line">	return $ret</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">echo &quot;-----函数开始执行------&quot;</span><br><span class="line">func $1 $2</span><br><span class="line">echo &quot;返回值为：$?&quot;</span><br><span class="line">echo &quot;-----函数执行完毕------&quot;</span><br></pre></td></tr></table></figure></li>
<li><p>需求：在执行脚本后从控制台输入两个数字求和</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">function func()</span><br><span class="line">&#123;</span><br><span class="line">	ret=$[$1+$2]</span><br><span class="line">	return $ret</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">echo &quot;-----函数开始执行------&quot;</span><br><span class="line">read -p &#x27;请输入第一个数字：&#x27; num1</span><br><span class="line">read -p &#x27;请输入第二个数字：&#x27; num2</span><br><span class="line">func $num1 $num2</span><br><span class="line">echo &quot;返回值为：$?&quot;</span><br><span class="line">echo &quot;-----函数执行完毕------&quot;</span><br></pre></td></tr></table></figure></li>
<li><p>返回值超过255，但是就想使用真实结果</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">function func()</span><br><span class="line">&#123;</span><br><span class="line">	ret=$[$1+$2]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">echo &quot;-----函数开始执行------&quot;</span><br><span class="line">read -p &#x27;请输入第一个数字：&#x27; num1</span><br><span class="line">read -p &#x27;请输入第二个数字：&#x27; num2</span><br><span class="line">func $num1 $num2</span><br><span class="line">echo &quot;结果为：$ret&quot;</span><br><span class="line">echo &quot;-----函数执行完毕------&quot;</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="正则表达式"><a href="#正则表达式" class="headerlink" title="正则表达式"></a>正则表达式</h3><h4 id="1、常规匹配"><a href="#1、常规匹配" class="headerlink" title="1、常规匹配"></a>1、常规匹配</h4><p>一串不包含特殊字符的正则表达式匹配它自己</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[atguigu@centos7_base study]$ cat test.txt </span><br><span class="line">sunck is a good man</span><br><span class="line">kaige is a good man</span><br><span class="line">sunyz is a niee man</span><br><span class="line"></span><br><span class="line">sunck is a nice man!</span><br><span class="line"></span><br><span class="line">kaishen is a cool man!</span><br><span class="line">sunds is a handsome man</span><br><span class="line">[atguigu@centos7_base study]$ cat test.txt | grep sunck</span><br><span class="line">sunck is a good man</span><br><span class="line">sunck is a nice man!</span><br></pre></td></tr></table></figure>

<p>查看test.txt文件中包含”sunck”的所有的行，匹配规则就是”sunck”</p>
<p>问题：匹配规则单一</p>
<h4 id="2、匹配单个字符与数字"><a href="#2、匹配单个字符与数字" class="headerlink" title="2、匹配单个字符与数字"></a>2、匹配单个字符与数字</h4><table>
<thead>
<tr>
<th>匹配</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>.</td>
<td>匹配任意字符</td>
</tr>
<tr>
<td>[]</td>
<td>里面是字符集合，匹配[]里任意一个字符</td>
</tr>
<tr>
<td>[0123456789]</td>
<td>匹配任意一个数字字符</td>
</tr>
<tr>
<td>[0-9]</td>
<td>匹配任意一个数字字符</td>
</tr>
<tr>
<td>[a-z]</td>
<td>匹配任意一个小写英文字母字符</td>
</tr>
<tr>
<td>[A-Z]</td>
<td>匹配任意一个大写英文字母字符</td>
</tr>
<tr>
<td>[A-Za-z]</td>
<td>匹配任意一个英文字母字符</td>
</tr>
<tr>
<td>[A-Za-z0-9]</td>
<td>匹配任意一个数字或英文字母字符</td>
</tr>
<tr>
<td>[^sunck]</td>
<td>[]里的^称为脱字符，表示非，匹配不在[]内的任意一个字符</td>
</tr>
<tr>
<td>\d</td>
<td>匹配任意一个数字字符，相当于[0-9]</td>
</tr>
<tr>
<td>\D</td>
<td>匹配任意一个非数字字符，相当于<code>[^0-9]</code></td>
</tr>
<tr>
<td>\w</td>
<td>匹配字母、下划线、数字中的任意一个字符，相当于[0-9A-Za-z_]</td>
</tr>
<tr>
<td>\W</td>
<td>匹配非字母、下划线、数字中的任意一个字符，相当于<code>[^0-9A-Za-z_]</code></td>
</tr>
<tr>
<td>\s</td>
<td>匹配空白符(空格、换页、换行、回车、制表)，相当于[ \f\n\r\t]</td>
</tr>
<tr>
<td>\S</td>
<td>匹配非空白符(空格、换页、换行、回车、制表)，相当于<code>[^ \f\n\r\t]</code></td>
</tr>
</tbody></table>
<p>注意：在使用\w、\W、\s、\S时需要使用引号包裹，最好使用单引号</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">echo &quot;sunck.www.wang.wwwqqww1fww\nffwweeeww3ssssww gege&quot; | grep ww.</span><br><span class="line"></span><br><span class="line">echo &quot;sunck.www.wang.wwwqqww1fww\nffwweeeww3ssssww gege&quot; | grep ww[^0-9]</span><br><span class="line"></span><br><span class="line">echo &quot;sunck.www.wang.wwwqqww1fww\nffwweeeww3ssssww gege&quot; | grep &#x27;ww\w&#x27;</span><br><span class="line"></span><br><span class="line">echo &quot;sunck.www.wang.wwwqqww1fww\nffwweeeww3ssssww gege&quot; | grep &#x27;ww\s&#x27;</span><br></pre></td></tr></table></figure>

<h4 id="3、匹配锚字符"><a href="#3、匹配锚字符" class="headerlink" title="3、匹配锚字符"></a>3、匹配锚字符</h4><table>
<thead>
<tr>
<th>匹配</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>^</td>
<td>行首匹配，和[]里的^不是一个意思</td>
</tr>
<tr>
<td>$</td>
<td>行尾匹配</td>
</tr>
</tbody></table>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[atguigu@centos7_base study]$ cat test.txt </span><br><span class="line">sunck is a good man</span><br><span class="line">kaige is a good man</span><br><span class="line">sunyz is a niee man</span><br><span class="line"></span><br><span class="line">sunck is a nice man!</span><br><span class="line"></span><br><span class="line">kaishen is a cool man!</span><br><span class="line">sunds is a handsome man</span><br><span class="line">[atguigu@centos7_base study]$ cat test.txt | grep ^sun</span><br><span class="line">sunck is a good man</span><br><span class="line">sunyz is a niee man</span><br><span class="line">sunck is a nice man!</span><br><span class="line">sunds is a handsome man</span><br><span class="line">[atguigu@centos7_base study]$ cat test.txt | grep ^good</span><br><span class="line">[atguigu@centos7_base study]$ cat test.txt | grep man$</span><br><span class="line">sunck is a good man</span><br><span class="line">kaige is a good man</span><br><span class="line">sunyz is a niee man</span><br><span class="line">sunds is a handsome man</span><br><span class="line">[atguigu@centos7_base study]$ cat test.txt | grep ^$</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[atguigu@centos7_base study]$</span><br></pre></td></tr></table></figure>

<h4 id="4、匹配边界字符"><a href="#4、匹配边界字符" class="headerlink" title="4、匹配边界字符"></a>4、匹配边界字符</h4><table>
<thead>
<tr>
<th>匹配</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>\b</td>
<td>匹配一个单词的边界，指单词和空格的位置</td>
</tr>
<tr>
<td>\B</td>
<td>匹配非单词边界</td>
</tr>
</tbody></table>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[atguigu@centos7_base study]$ cat test.txt </span><br><span class="line">sunck is a good man!</span><br><span class="line">ack is a nice ma</span><br><span class="line">sfeck,nckab</span><br><span class="line">aaackge</span><br><span class="line">bbbck geg</span><br><span class="line">[atguigu@centos7_base study]$ cat test.txt | grep &#x27;ck\b&#x27;</span><br><span class="line">sunck is a good man!</span><br><span class="line">ack is a nice ma</span><br><span class="line">sfeck,nckab</span><br><span class="line">bbbck geg</span><br><span class="line">[atguigu@centos7_base study]$ cat test.txt | grep &#x27;ck\B&#x27;</span><br><span class="line">sfeck,nckab</span><br><span class="line">aaackge</span><br></pre></td></tr></table></figure>

<h4 id="5、匹配多个字符"><a href="#5、匹配多个字符" class="headerlink" title="5、匹配多个字符"></a>5、匹配多个字符</h4><table>
<thead>
<tr>
<th>匹配</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>(xyz)</td>
<td>匹配括号内的xyz，作为一个整体去匹配</td>
</tr>
<tr>
<td>x?</td>
<td>匹配0个或者1个x，非贪婪匹配</td>
</tr>
<tr>
<td>x*</td>
<td>匹配0个或任意多个x</td>
</tr>
<tr>
<td>x+</td>
<td>匹配至少一个x</td>
</tr>
<tr>
<td>x{n}</td>
<td>确定匹配n个x，n是非负数</td>
</tr>
<tr>
<td>x{n,}</td>
<td>至少匹配n个x</td>
</tr>
<tr>
<td>x{n,m}</td>
<td>匹配至少n个最多m个x</td>
</tr>
<tr>
<td>x|y</td>
<td>|表示或的意思，匹配x或y</td>
</tr>
</tbody></table>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">echo &quot;11111a2222aa3333aaa44444aaaa55555&quot; | grep &#x27;a*&#x27;</span><br><span class="line"></span><br><span class="line">echo &quot;12340567085465046567&quot; | grep &#x27;[0-9]*0&#x27;</span><br></pre></td></tr></table></figure>

<h4 id="6、转义字符"><a href="#6、转义字符" class="headerlink" title="6、转义字符"></a>6、转义字符</h4><p><code>\ </code>表示转义，并不会单独使用。由于所有特殊字符都有其特定匹配模式，当我们想匹配某一特殊字符本身时（例如，我想找出所有包含 ‘$’ 的行），就会碰到困难。此时我们就要将转义字符和特殊字符连用，来表示特殊字符本身</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[atguigu@centos7_base study]$ cat test.txt </span><br><span class="line">sunck is a go$od man</span><br><span class="line">kaige is a goaod man</span><br><span class="line">sunyz is a nice mano</span><br><span class="line">sunck is a nice man!</span><br><span class="line">kaishen is a co$ol man!</span><br><span class="line">sunds is a handsome man</span><br><span class="line">[atguigu@centos7_base study]$ cat test.txt | grep o$</span><br><span class="line">sunyz is a nice mano</span><br><span class="line">[atguigu@centos7_base study]$ cat test.txt | grep o\$</span><br><span class="line">sunyz is a nice mano</span><br><span class="line">[atguigu@centos7_base study]$ cat test.txt | grep &quot;o\$&quot;</span><br><span class="line">sunyz is a nice mano</span><br><span class="line">[atguigu@centos7_base study]$ cat test.txt | grep &#x27;o\$&#x27;</span><br><span class="line">sunck is a go$od man</span><br><span class="line">kaishen is a co$ol man!</span><br><span class="line">[atguigu@centos7_base study]$ cat test.txt | grep &#x27;o$&#x27;</span><br><span class="line">sunyz is a nice mano</span><br><span class="line">[atguigu@centos7_base study]$</span><br></pre></td></tr></table></figure>

<h4 id="7、常用正则表达式"><a href="#7、常用正则表达式" class="headerlink" title="7、常用正则表达式"></a>7、常用正则表达式</h4><p>详见《正则匹配示例.docx》</p>
<h3 id="shell工具"><a href="#shell工具" class="headerlink" title="shell工具"></a>shell工具</h3><p>ETL工程师做的一些数据清洗的工作，我们可以使用cut、awk、sort来对一些基本数据进行处理</p>
<h4 id="1、cut"><a href="#1、cut" class="headerlink" title="1、cut"></a>1、cut</h4><p>cut的工作就是“剪”，具体的说就是在文件中负责剪切数据用的。cut 命令从文件的每一行剪切字节、字符和字段并将这些字节、字符和字段输出</p>
<p>格式：cut [选项参数]  filename</p>
<p>说明：默认分隔符是制表符</p>
<table>
<thead>
<tr>
<th>选项参数</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>-f</td>
<td>列号，提取第几列</td>
</tr>
<tr>
<td>-d</td>
<td>分隔符，按照指定分隔符分割列</td>
</tr>
<tr>
<td>-c</td>
<td>指定具体的字符</td>
</tr>
</tbody></table>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">de hua good man</span><br><span class="line">hui mei nice women</span><br><span class="line">xue you cool man</span><br><span class="line">ruo tong good women</span><br><span class="line">dao lang cool man</span><br></pre></td></tr></table></figure>

<ul>
<li><p>切割cut.txt文件第一列</p>
<p><code>cut -d &quot; &quot; -f 1 cut.txt</code></p>
</li>
<li><p>切割cut.txt文件第二、三列</p>
<p><code>cut -d &quot; &quot; -f 2,3 cut.txt</code></p>
</li>
<li><p>切割cut.txt文件第二到四列</p>
<p><code>cut -d &quot; &quot; -f 2,3,4 cut.txt</code></p>
<p><code>cut -d &quot; &quot; -f 2-4 cut.txt</code></p>
</li>
<li><p>切割cut.txt文件第二到最后列</p>
<p><code>cut -d &quot; &quot; -f 2- cut.txt</code></p>
</li>
<li><p>在cut.txt文件中切割出hua</p>
<p><code>cat cut.txt | grep hua | cut -d &quot; &quot; -f 2</code></p>
</li>
<li><p>选取系统PATH变量值，第二个”:”开始后的所有路径</p>
<p><code>echo $PATH | cut -d : -f 3-</code></p>
</li>
<li><p>切割ifconfig后打印的IP地址</p>
<p><code>ifconfig | grep &quot;netmask&quot; | cut -d i -f 2- | cut -d &quot; &quot; -f 2</code></p>
</li>
</ul>
<h4 id="2、awk"><a href="#2、awk" class="headerlink" title="2、awk"></a>2、awk</h4><p>作用：一个强大的文本分析工具，把文件逐行的读入，以空格为默认分隔符将每行切片，切开的部分再进行分析处理</p>
<p>语法：awk [选项参数] ‘pattern1{action1} pattern2{action2}…’ filename</p>
<table>
<thead>
<tr>
<th>结构</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>pattern</td>
<td>表示AWK在数据中查找的内容，就是匹配模式</td>
</tr>
<tr>
<td>action</td>
<td>在找到匹配内容时所执行的一系列命令</td>
</tr>
<tr>
<td>filename</td>
<td>文件路径</td>
</tr>
</tbody></table>
<table>
<thead>
<tr>
<th>选项参数</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>-F</td>
<td>指定输入文件折分隔符</td>
</tr>
<tr>
<td>-v</td>
<td>赋值一个用户定义变量</td>
</tr>
</tbody></table>
<ul>
<li><p>搜索passwd文件以root关键字开头的所有行，并输出该行的第7列<br><code>awk -F : &#39;/^root/&#123;print $7&#125;&#39; passwd.txt</code></p>
</li>
<li><p>搜索passwd文件以root关键字开头的所有行，并输出该行的第1列和第7列，中间以“，”号分割</p>
<p><code>awk -F : &#39;/^root/&#123;print $1 &quot;,&quot; $7&#125;&#39; passwd.txt</code></p>
</li>
<li><p>只显示/etc/passwd的第一列和第七列，以逗号分割，且在所有行前面添加列名user，shell在最后一行添加”dahaige，/bin/zuishuai”</p>
<p><code>awk -F : &#39;BEGIN&#123;print &quot;user,shell&quot;&#125;&#123;print $1 &quot;,&quot; $7&#125;END&#123;print &quot;sunck,/bin/bash&quot;&#125;&#39; passwd.txt</code></p>
</li>
<li><p>将passwd文件中的用户id增加数值1并输出</p>
<p><code>awk -v i=1 -F : &#39;&#123;print $3+i &quot;,&quot; $4&#125;&#39; passwd.txt</code></p>
</li>
</ul>
<table>
<thead>
<tr>
<th>awk内置变量</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>FILENAME</td>
<td>文件名</td>
</tr>
<tr>
<td>NR</td>
<td>已读的记录数（行数）</td>
</tr>
<tr>
<td>NF</td>
<td>浏览记录的域的个数（切割后，列的个数）</td>
</tr>
</tbody></table>
<ul>
<li><p>统计passwd文件名，每行的行号，每行的列数</p>
<p><code>awk -F : &#39;&#123;print &quot;filename:&quot; FILENAME &quot;,linenumber:&quot; NR &quot;,columns:&quot; NF&#125;&#39; passwd.txt</code></p>
</li>
<li><p>切割IP</p>
<p><code>ifconfig | grep netmask | awk -F &quot;inet &quot; &#39;&#123;print $2&#125;&#39; | awk -F &quot; &quot; &#39;&#123;print $1&#125;&#39;</code></p>
</li>
<li><p>查询cut.txt中空行所在的行号</p>
<p><code>awk &#39;/^$/&#123;print NR&#125;&#39; cut.txt</code></p>
</li>
</ul>
<h4 id="3、sort"><a href="#3、sort" class="headerlink" title="3、sort"></a>3、sort</h4><p>作用：sort命令是在Linux里非常有用，它将文件进行排序，并将排序结果标准输出</p>
<p>格式：sort (选项) (参数)</p>
<table>
<thead>
<tr>
<th>选项</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>-n</td>
<td>依照数值的大小排序</td>
</tr>
<tr>
<td>-r</td>
<td>以相反的顺序来排序</td>
</tr>
<tr>
<td>-t</td>
<td>设置排序时所用的分隔字符</td>
</tr>
<tr>
<td>-k</td>
<td>指定需要排序的列</td>
</tr>
</tbody></table>
<p>参数：指定待排序的文件列表</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[atguigu@centos7_base study]$ cat sort.txt </span><br><span class="line">bb:40:5.4</span><br><span class="line">bd:20:4.2</span><br><span class="line">xz:50:2.3</span><br><span class="line">cls:10:3.5</span><br><span class="line">ss:30:1.6</span><br></pre></td></tr></table></figure>

<p>需求：按照“：”分割后的第三列倒序排序</p>
<p><code>sort -t : -nrk 3 sort.txt</code></p>
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title>Mysql安装</title>
    <url>/2021/11/07/MySQL/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="Mysql安装"><a href="#Mysql安装" class="headerlink" title="Mysql安装"></a>Mysql安装</h1><h2 id="一、卸载Mysql"><a href="#一、卸载Mysql" class="headerlink" title="一、卸载Mysql"></a>一、卸载Mysql</h2><h4 id="1-查询Linux上是否安装Mariadb"><a href="#1-查询Linux上是否安装Mariadb" class="headerlink" title="1. 查询Linux上是否安装Mariadb"></a>1. 查询Linux上是否安装Mariadb</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@mysql ~]# rpm -qa | grep maria</span><br><span class="line">mariadb-libs-5.5.68-1.el7.x86_64</span><br><span class="line">[root@mysql ~]# rpm -e --nodeps mariadb-libs-5.5.68-1.el7.x86_64</span><br><span class="line">[root@mysql ~]# rpm -qa | grep maria</span><br><span class="line">[root@mysql ~]#</span><br></pre></td></tr></table></figure>

<h5 id="2-如果安装过需要卸载Mysql-一步到位"><a href="#2-如果安装过需要卸载Mysql-一步到位" class="headerlink" title="2. 如果安装过需要卸载Mysql 一步到位"></a>2. 如果安装过需要卸载Mysql 一步到位</h5><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo rpm -qa | grep mysql | xargs -n2 sudo rpm -e --nodeps</span><br></pre></td></tr></table></figure>

<h5 id="3-查看-etc-my-cnf-文件，其中-datadir-var-lib-mysql-指向的目录下内容全部删除"><a href="#3-查看-etc-my-cnf-文件，其中-datadir-var-lib-mysql-指向的目录下内容全部删除" class="headerlink" title="3.查看 /etc/my.cnf 文件，其中 datadir=/var/lib/mysql 指向的目录下内容全部删除"></a>3.查看 /etc/my.cnf 文件，其中 datadir=/var/lib/mysql 指向的目录下内容全部删除</h5><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo rm -rf /var/lib/mysql/*</span><br></pre></td></tr></table></figure>

<p>注意：如果首次安装MySQL，此目录下没有内容！而且如果有内容有可能普通用户即便加上sudo 也无法删除，那就切换root用户删除！！！</p>
<h2 id="二、安装MySql"><a href="#二、安装MySql" class="headerlink" title="二、安装MySql"></a>二、安装MySql</h2><h5 id="1-把Mysql安装包上传到Linux指定目录"><a href="#1-把Mysql安装包上传到Linux指定目录" class="headerlink" title="1.把Mysql安装包上传到Linux指定目录"></a>1.把Mysql安装包上传到Linux指定目录</h5><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@mysql software]<span class="comment"># pwd</span></span><br><span class="line">/opt/software</span><br><span class="line">[root@mysql software]<span class="comment"># ls</span></span><br><span class="line">jdk-8u212-linux-x64.tar.gz  mysql-5.7.28-1.el7.x86_64.rpm-bundle.tar</span><br><span class="line">[root@mysql software]<span class="comment">#</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h5 id="2-将Mysql安装包进行解包操作"><a href="#2-将Mysql安装包进行解包操作" class="headerlink" title="2. 将Mysql安装包进行解包操作"></a>2. 将Mysql安装包进行解包操作</h5><p>​注意：由于Mysql解包后会有多个文件，为了方便管理最好新建一个文件夹放入</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@mysql software]# mkdir mysql</span><br><span class="line">[root@mysql software]# tar -xvf mysql-5.7.28-1.el7.x86_64.rpm-bundle.tar -C mysql</span><br><span class="line">mysql-community-embedded-5.7.28-1.el7.x86_64.rpm</span><br><span class="line">mysql-community-libs-compat-5.7.28-1.el7.x86_64.rpm</span><br><span class="line">mysql-community-devel-5.7.28-1.el7.x86_64.rpm</span><br><span class="line">mysql-community-embedded-compat-5.7.28-1.el7.x86_64.rpm</span><br><span class="line">mysql-community-libs-5.7.28-1.el7.x86_64.rpm</span><br><span class="line">mysql-community-test-5.7.28-1.el7.x86_64.rpm</span><br><span class="line">mysql-community-common-5.7.28-1.el7.x86_64.rpm</span><br><span class="line">mysql-community-embedded-devel-5.7.28-1.el7.x86_64.rpm</span><br><span class="line">mysql-community-client-5.7.28-1.el7.x86_64.rpm</span><br><span class="line">mysql-community-server-5.7.28-1.el7.x86_64.rpm</span><br><span class="line">[root@mysql software]# ll mysql/</span><br><span class="line">total 595272</span><br><span class="line">-rw-r--r--. 1 7155 31415  45109364 Sep 30  2019 mysql-community-client-5.7.28-1.el7.x86_64.rpm</span><br><span class="line">-rw-r--r--. 1 7155 31415    318768 Sep 30  2019 mysql-community-common-5.7.28-1.el7.x86_64.rpm</span><br><span class="line">-rw-r--r--. 1 7155 31415   7037096 Sep 30  2019 mysql-community-devel-5.7.28-1.el7.x86_64.rpm</span><br><span class="line">-rw-r--r--. 1 7155 31415  49329100 Sep 30  2019 mysql-community-embedded-5.7.28-1.el7.x86_64.rpm</span><br><span class="line">-rw-r--r--. 1 7155 31415  23354908 Sep 30  2019 mysql-community-embedded-compat-5.7.28-1.el7.x86_64.rpm</span><br><span class="line">-rw-r--r--. 1 7155 31415 136837816 Sep 30  2019 mysql-community-embedded-devel-5.7.28-1.el7.x86_64.rpm</span><br><span class="line">-rw-r--r--. 1 7155 31415   4374364 Sep 30  2019 mysql-community-libs-5.7.28-1.el7.x86_64.rpm</span><br><span class="line">-rw-r--r--. 1 7155 31415   1353312 Sep 30  2019 mysql-community-libs-compat-5.7.28-1.el7.x86_64.rpm</span><br><span class="line">-rw-r--r--. 1 7155 31415 208694824 Sep 30  2019 mysql-community-server-5.7.28-1.el7.x86_64.rpm</span><br><span class="line">-rw-r--r--. 1 7155 31415 133129992 Sep 30  2019 mysql-community-test-5.7.28-1.el7.x86_64.rpm</span><br><span class="line">[root@mysql software]#</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>注意：mysql解包后的所有rpm并非都要安装 我们可以把需要安装的通过改名的方式编号，方便后续的顺序安装，严格安装编号的顺序安装</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@mysql mysql]<span class="comment"># ll</span></span><br><span class="line">total 595272</span><br><span class="line">-rw-r--r--. 1 7155 31415    318768 Sep 30  2019 01-mysql-community-common-5.7.28-1.el7.x86_64.rpm</span><br><span class="line">-rw-r--r--. 1 7155 31415   4374364 Sep 30  2019 02-mysql-community-libs-5.7.28-1.el7.x86_64.rpm</span><br><span class="line">-rw-r--r--. 1 7155 31415   1353312 Sep 30  2019 03-mysql-community-libs-compat-5.7.28-1.el7.x86_64.rpm</span><br><span class="line">-rw-r--r--. 1 7155 31415  45109364 Sep 30  2019 04-mysql-community-client-5.7.28-1.el7.x86_64.rpm</span><br><span class="line">-rw-r--r--. 1 7155 31415 208694824 Sep 30  2019 05-mysql-community-server-5.7.28-1.el7.x86_64.rpm</span><br><span class="line">-rw-r--r--. 1 7155 31415   7037096 Sep 30  2019 mysql-community-devel-5.7.28-1.el7.x86_64.rpm</span><br><span class="line">-rw-r--r--. 1 7155 31415  49329100 Sep 30  2019 mysql-community-embedded-5.7.28-1.el7.x86_64.rpm</span><br><span class="line">-rw-r--r--. 1 7155 31415  23354908 Sep 30  2019 mysql-community-embedded-compat-5.7.28-1.el7.x86_64.rpm</span><br><span class="line">-rw-r--r--. 1 7155 31415 136837816 Sep 30  2019 mysql-community-embedded-devel-5.7.28-1.el7.x86_64.rpm</span><br><span class="line">-rw-r--r--. 1 7155 31415 133129992 Sep 30  2019 mysql-community-test-5.7.28-1.el7.x86_64.rpm</span><br><span class="line">[root@mysql mysql]<span class="comment">#</span></span><br></pre></td></tr></table></figure>

<h5 id="3-按照编号顺序安装"><a href="#3-按照编号顺序安装" class="headerlink" title="3. 按照编号顺序安装"></a>3. 按照编号顺序安装</h5><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@mysql mysql]<span class="comment"># rpm -ivh 0*.rpm</span></span><br><span class="line">warning: 01-mysql-community-common-5.7.28-1.el7.x86_64.rpm: Header V3 DSA/SHA1 Signature, key ID 5072e1f5: NOKEY</span><br><span class="line">Preparing...                          <span class="comment">################################# [100%]</span></span><br><span class="line">Updating / installing...</span><br><span class="line">   1:mysql-community-common-5.7.28-1.e<span class="comment">################################# [ 20%]</span></span><br><span class="line">   2:mysql-community-libs-5.7.28-1.el7<span class="comment">################################# [ 40%]</span></span><br><span class="line">   3:mysql-community-client-5.7.28-1.e<span class="comment">################################# [ 60%]</span></span><br><span class="line">   4:mysql-community-server-5.7.28-1.e<span class="comment">################################# [ 80%]</span></span><br><span class="line">   5:mysql-community-libs-compat-5.7.2<span class="comment">################################# [100%]</span></span><br><span class="line">[root@mysql mysql]<span class="comment">#</span></span><br></pre></td></tr></table></figure>
<h4 id="4-异常情况"><a href="#4-异常情况" class="headerlink" title="4. 异常情况"></a>4. 异常情况</h4><p>注意：安装第5个包时，如果首次安装会报错，原因是缺少依赖，通过yum安装的方式把依赖安装即可，然后再重新安装第5个包！！！<br><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/image20210624154408190.png?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10" alt="image-20210624154408190"></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo yum install -y libaio</span><br></pre></td></tr></table></figure>

<h4 id="5-到此Mysql安装就完成了，可以通过一下命令检测是否安装完成"><a href="#5-到此Mysql安装就完成了，可以通过一下命令检测是否安装完成" class="headerlink" title="5. 到此Mysql安装就完成了，可以通过一下命令检测是否安装完成"></a>5. 到此Mysql安装就完成了，可以通过一下命令检测是否安装完成</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@mysql mysql]<span class="comment"># rpm -qa | grep mysql</span></span><br><span class="line">mysql-community-libs-5.7.28-1.el7.x86_64</span><br><span class="line">mysql-community-common-5.7.28-1.el7.x86_64</span><br><span class="line">mysql-community-client-5.7.28-1.el7.x86_64</span><br><span class="line">mysql-community-libs-compat-5.7.28-1.el7.x86_64</span><br><span class="line">mysql-community-server-5.7.28-1.el7.x86_64</span><br><span class="line">[root@mysql mysql]<span class="comment">#</span></span><br></pre></td></tr></table></figure>


<h2 id="三、配置MySql"><a href="#三、配置MySql" class="headerlink" title="三、配置MySql"></a>三、配置MySql</h2><h5 id="1-初始化数据库"><a href="#1-初始化数据库" class="headerlink" title="1. 初始化数据库"></a>1. 初始化数据库</h5><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@mysql mysql]# mysqld --initialize --user=mysql</span><br><span class="line">[root@mysql mysql]#</span><br></pre></td></tr></table></figure>

<h5 id="2-启动Mysql服务"><a href="#2-启动Mysql服务" class="headerlink" title="2.启动Mysql服务"></a>2.启动Mysql服务</h5><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@mysql mysql]# mysqld --initialize --user=mysql</span><br><span class="line">[root@mysql mysql]#</span><br><span class="line">[root@mysql mysql]# systemctl status mysqld</span><br><span class="line">● mysqld.service - MySQL Server</span><br><span class="line">   Loaded: loaded (/usr/lib/systemd/system/mysqld.service; enabled; vendor preset: disabled)</span><br><span class="line">   Active: inactive (dead)</span><br><span class="line">     Docs: man:mysqld(8)</span><br><span class="line">           http://dev.mysql.com/doc/refman/en/using-systemd.html</span><br><span class="line">[root@mysql mysql]# systemctl is-enabled mysqld</span><br><span class="line">enabled</span><br><span class="line">[root@mysql mysql]# systemctl start mysqld</span><br><span class="line">[root@mysql mysql]#</span><br></pre></td></tr></table></figure>


<h5 id="3-查看Mysql的临时密码"><a href="#3-查看Mysql的临时密码" class="headerlink" title="3.查看Mysql的临时密码"></a>3.查看Mysql的临时密码</h5><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@mysql mysql]<span class="comment"># cat /var/log/mysqld.log | grep password</span></span><br><span class="line">2021-10-20T04:26:51.462143Z 1 [Note] A temporary password is generated <span class="keyword">for</span> root@localhost: -NGjuzi5&amp;qlt</span><br><span class="line">[root@mysql mysql]<span class="comment">#</span></span><br></pre></td></tr></table></figure>


<h5 id="4-利用临时密码登录Mysql"><a href="#4-利用临时密码登录Mysql" class="headerlink" title="4. 利用临时密码登录Mysql"></a>4. 利用临时密码登录Mysql</h5><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@mysql mysql]# mysql -uroot -p</span><br><span class="line">Enter password:</span><br><span class="line"><span class="meta">#</span><span class="bash">输入密码-NGjuzi5&amp;qlt</span></span><br><span class="line">Welcome to the MySQL monitor.  Commands end with ; or \g.</span><br><span class="line">Your MySQL connection id is 2</span><br><span class="line">Server version: 5.7.28</span><br><span class="line"></span><br><span class="line">Copyright (c) 2000, 2019, Oracle and/or its affiliates. All rights reserved.</span><br><span class="line"></span><br><span class="line">Oracle is a registered trademark of Oracle Corporation and/or its</span><br><span class="line">affiliates. Other names may be trademarks of their respective</span><br><span class="line">owners.</span><br><span class="line"></span><br><span class="line">Type &#x27;help;&#x27; or &#x27;\h&#x27; for help. Type &#x27;\c&#x27; to clear the current input statement.</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">mysql&gt;</span></span><br></pre></td></tr></table></figure>


<h5 id="5-修改密码"><a href="#5-修改密码" class="headerlink" title="5.修改密码"></a>5.修改密码</h5><p>注意：首次登录进来以后，必须要先修改指定自定义的密码，不然后续操作都会报错！</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">mysql&gt;</span><span class="bash"> show databases;</span></span><br><span class="line">ERROR 1820 (HY000): You must reset your password using ALTER USER statement before executing this statement.</span><br><span class="line"><span class="meta">mysql&gt;</span><span class="bash"> <span class="built_in">set</span> password = password(<span class="string">&quot;123456&quot;</span>);</span></span><br><span class="line">Query OK, 0 rows affected, 1 warning (0.00 sec)</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">mysql&gt;</span><span class="bash"> show databases;</span></span><br><span class="line">+--------------------+</span><br><span class="line">| Database           |</span><br><span class="line">+--------------------+</span><br><span class="line">| information_schema |</span><br><span class="line">| mysql              |</span><br><span class="line">| performance_schema |</span><br><span class="line">| sys                |</span><br><span class="line">+--------------------+</span><br><span class="line">4 rows in set (0.00 sec)</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">mysql&gt;</span></span><br></pre></td></tr></table></figure>

<h6 id="6-修改允许root远程登录"><a href="#6-修改允许root远程登录" class="headerlink" title="6. 修改允许root远程登录"></a>6. 修改允许root远程登录</h6><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mysql&gt; use mysql;</span><br><span class="line">Reading table information <span class="keyword">for</span> completion of table and column names</span><br><span class="line">You can turn off this feature to get a quicker startup with -A</span><br><span class="line"></span><br><span class="line">Database changed</span><br><span class="line">mysql&gt; update mysql.user <span class="built_in">set</span> host=<span class="string">&#x27;%&#x27;</span> <span class="built_in">where</span> user=<span class="string">&#x27;root&#x27;</span>;</span><br><span class="line">Query OK, 1 row affected (0.00 sec)</span><br><span class="line">Rows matched: 1  Changed: 1  Warnings: 0</span><br><span class="line"></span><br><span class="line">mysql&gt; flush privileges;</span><br><span class="line">Query OK, 0 rows affected (0.00 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title>MySQL面试题</title>
    <url>/2021/11/07/MySQL%E9%9D%A2%E8%AF%95%E9%A2%98/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="MySQL"><a href="#MySQL" class="headerlink" title="MySQL"></a>MySQL</h1><ol>
<li><p>为什么互联网公司一般选择 MySQL 而不是 Oracle?</p>
<ul>
<li>免费、流行、够用。</li>
</ul>
</li>
<li><p>数据库的三范式是什么？什么是反模式？</p>
<ul>
<li>第一范式，强调属性的原子性约束，要求属性具有原子性，不可再分解。</li>
<li>第二范式，强调记录的唯一性约束，表必须有一个主键，并且没有包含在主键中的列必须完全依赖于主键，而不能只依赖于主键的一部分。</li>
<li>第三范式，强调属性冗余性的约束，即非主键列必须直接依赖于主键。</li>
<li>反1空间换取时间，采取数据冗余的方式避免表之间的关联查询</li>
<li>使存储数据尽可能达到用户一致，保证系统经过一段较短的时间的自我恢复和修正，数据最终达到一致</li>
</ul>
</li>
<li><p>MySQL 有哪些数据类型？</p>
<ul>
<li>数值、日期/时间和字符串(字符)类型</li>
</ul>
</li>
<li><p>MySQL 中 varchar 与 char 的区别？varchar(50) 中的 50 代表的涵义？</p>
<ul>
<li>1、varchar 与 char 的区别，char 是一种固定长度的类型，varchar 则是一种可变长度的类型。</li>
<li>varchar(50) 中 50 的涵义最多存放 50 个字符。varchar(50) 和 (200) 存储 hello 所占空间一样，但后者在排序时会消耗更多内存，因为 ORDER BY col 采用 fixed_length 计算 col 长度(memory引擎也一样)。</li>
</ul>
</li>
<li><p>int(11) 中的 11 代表什么涵义？</p>
<ul>
<li>int(11) 中的 11 ，不影响字段存储的范围，只影响展示效果</li>
</ul>
</li>
<li><p>金额(金钱)相关的数据，选择什么数据类型？</p>
<ul>
<li>方式一，使用 int 或者 bigint 类型。如果需要存储到分的维度，需要 *100 进行放大。</li>
<li>方式二，使用 decimal 类型，避免精度丢失。如果使用 Java 语言时，需要使用 BigDecimal 进行对应。</li>
</ul>
</li>
<li><p>一张表，里面有 ID 自增主键，当 insert 了 17 条记录之后，删除了第 15,16,17 条记录，再把 MySQL 重启，再 insert 一条记录，这条记录的 ID 是 18 还是 15？</p>
<ul>
<li>一般情况下，我们创建的表的类型是 InnoDB ，如果新增一条记录（不重启 MySQL 的情况下），这条记录的 ID 是18 ；但是如果重启 MySQL 的话，这条记录的 ID 是 15 。因为 InnoDB 表只把自增主键的最大 ID 记录到内存中，所以重启数据库或者对表 OPTIMIZE 操作，都会使最大 ID 丢失。</li>
<li>但是，如果我们使用表的类型是 MyISAM ，那么这条记录的 ID 就是 18 。因为 MyISAM 表会把自增主键的最大 ID 记录到数据文件里面，重启 MYSQL 后，自增主键的最大 ID 也不会丢失。</li>
</ul>
</li>
<li><p>表中有大字段 X(例如：text 类型)，且字段 X 不会经常更新，以读为为主，请问您是选择拆成子表，还是继续放一起?写出您这样选择的理由</p>
<ul>
<li>拆带来的问题：连接消耗 + 存储拆分空间。<ul>
<li>如果能容忍拆分带来的空间问题，拆的话最好和经常要查询的表的主键在物理结构上放置在一起(分区) 顺序 IO ，减少连接消耗，最后这是一个文本列再加上一个全文索引来尽量抵消连接消耗。</li>
</ul>
</li>
<li>不拆可能带来的问题：查询性能。<ul>
<li>如果能容忍不拆分带来的查询性能损失的话，上面的方案在某个极致条件下肯定会出现问题，那么不拆就是最好的选择。</li>
</ul>
</li>
</ul>
</li>
<li><p>MySQL 有哪些存储引擎？</p>
<ul>
<li>InnoDB<ul>
<li>InnoDB是一个健壮的<strong>事务型</strong>存储引擎</li>
<li>更新密集的表。InnoDB存储引擎特别适合处理多重并发的更新请求。</li>
<li>事务。InnoDB存储引擎是支持事务的标准MySQL存储引擎。</li>
<li>自动灾难恢复。与其它存储引擎不同，InnoDB表能够自动从灾难中恢复。</li>
<li>外键约束。MySQL支持外键的存储引擎只有InnoDB。</li>
<li>支持自动增加列AUTO_INCREMENT属性。</li>
<li>从5.7开始innodb存储引擎成为默认的存储引擎。</li>
</ul>
</li>
<li>MyISAM<ul>
<li>MyISAM表无法处理事务，这就意味着有事务处理需求的表，不能使用MyISAM存储引擎。MyISAM存储引擎特别适合在以下几种情况下使用：</li>
<li>选择密集型的表。MyISAM存储引擎在筛选大量数据时非常迅速，这是它最突出的优点。</li>
<li>插入密集型的表。MyISAM的并发插入特性允许同时选择和插入数据。例如：MyISAM存储引擎很适合管理邮件或Web服务器日志数据。</li>
</ul>
</li>
</ul>
</li>
<li><p>如何选择合适的存储引擎？</p>
<ul>
<li>是否需要支持事务。</li>
<li>对索引和缓存的支持。</li>
<li>是否需要使用热备。</li>
<li>崩溃恢复，能否接受崩溃。</li>
<li>存储的限制。</li>
<li>MySQL 默认的存储引擎是 InnoDB ，并且也是最主流的选择。主要原因如下:<ul>
<li>【最重要】支持事务。</li>
<li>支持行级锁和表级锁，能支持更多的并发量。</li>
<li>查询不加锁，完全不影响查询。</li>
<li>支持崩溃后恢复。</li>
</ul>
</li>
<li>在 MySQL5.1 以及之前的版本，默认的存储引擎是 MyISAM ，但是目前已经不再更新，且它有几个比较关键的缺点：</li>
<li>不支持事务。</li>
<li>使用表级锁，如果数据量大，一个插入操作锁定表后，其他请求都将阻塞。</li>
</ul>
</li>
<li><p>请说明 InnoDB 和 MyISAM 的区别</p>
<table>
<thead>
<tr>
<th>对比项</th>
<th>InnoDB</th>
<th>MyISAM</th>
</tr>
</thead>
<tbody><tr>
<td>事务</td>
<td>支持</td>
<td>不支持</td>
</tr>
<tr>
<td>存储限制</td>
<td>64TB</td>
<td>无</td>
</tr>
<tr>
<td>锁粒度</td>
<td>行锁</td>
<td>表锁</td>
</tr>
<tr>
<td>崩溃后的恢复</td>
<td>支持</td>
<td>不支持</td>
</tr>
<tr>
<td>外键</td>
<td>支持</td>
<td>不支持</td>
</tr>
<tr>
<td>全文检索</td>
<td>5.7 版本后支持</td>
<td>支持</td>
</tr>
</tbody></table>
</li>
<li><p>为什么 <code>SQL SELECT COUNT(*) FROM table </code> 在 InnoDB 比 MyISAM 慢？</p>
<ul>
<li>对于 SELECT COUNT(*) FROM table 语句，在没有 WHERE 条件的情况下，InnoDB 比 MyISAM 可能会慢很多，尤其在大表的情况下。因为，InnoDB 是去实时统计结果，会全表扫描；而 MyISAM 内部维持了一个计数器，预存了结果，所以直接返回即可。</li>
</ul>
</li>
</ol>
<h3 id="索引"><a href="#索引" class="headerlink" title="索引"></a>索引</h3><ol start="13">
<li><p>什么是索引？</p>
<ul>
<li>索引，类似于书籍的目录，想找到一本书的某个特定的主题，需要先找到书的目录，定位对应的页码。</li>
<li>MySQL 中存储引擎使用类似的方式进行查询，先去索引中查找对应的值，然后根据匹配的索引找到对应的数据行。</li>
</ul>
</li>
<li><p>索引有什么好处？</p>
<ul>
<li>提高数据的检索速度，降低数据库IO成本：使用索引的意义就是通过缩小表中需要查询的记录的数目从而加快搜索的速度。</li>
<li>降低数据排序的成本，降低CPU消耗：索引之所以查的快，是因为先将数据排好序，若该字段正好需要排序，则正好降低了排序的成本。</li>
</ul>
</li>
<li><p>索引有什么坏处？</p>
<ul>
<li>占用存储空间：索引实际上也是一张表，记录了主键与索引字段，一般以索引文件的形式存储在磁盘上。</li>
<li>降低更新表的速度：表的数据发生了变化，对应的索引也需要一起变更，从而减低的更新速度。否则索引指向的物理数据可能不对，这也是索引失效的原因之一。 </li>
</ul>
</li>
<li><p>索引的使用场景？</p>
<ul>
<li>对非常小的表，大部分情况下全表扫描效率更高。</li>
<li>对中大型表，索引非常有效。</li>
<li>特大型的表，建立和使用索引的代价随着增长，可以使用分区技术来解决。 </li>
</ul>
</li>
<li><p>索引的类型？</p>
<ul>
<li>索引，都是实现在存储引擎层的。主要有六种类型：<ul>
<li>普通索引：最基本的索引，没有任何约束。</li>
<li>唯一索引：与普通索引类似，但具有唯一性约束。</li>
<li>主键索引：特殊的唯一索引，不允许有空值。</li>
<li>复合索引：将多个列组合在一起创建索引，可以覆盖多个列。</li>
<li>外键索引：只有InnoDB类型的表才可以使用外键索引，保证数据的一致性、完整性和实现级联操作。</li>
<li>全文索引：MySQL 自带的全文索引只能用于 InnoDB、MyISAM ，并且只能对英文进行全文检索，一般使用全文索引引擎。</li>
</ul>
</li>
</ul>
</li>
<li><p>MySQL 索引的“创建”原则？</p>
<ul>
<li>最适合索引的列是出现在 WHERE 子句中的列，或连接子句中的列，而不是出现在 SELECT 关键字后的列。</li>
<li>索引列的基数越大，索引效果越好。</li>
<li>因为复合索引的基数会更大，根据情况创建复合索引可以提高查询效率。</li>
<li>避免创建过多的索引，索引会额外占用磁盘空间，降低写操作效率。</li>
<li>主键尽可能选择较短的数据类型，可以有效减少索引的磁盘占用提高查询效率。</li>
<li>对字符串进行索引，应该定制一个前缀长度，可以节省大量的索引空间。</li>
</ul>
</li>
<li><p>MySQL 索引的“使用”注意事项？</p>
<ul>
<li>应尽量避免在 WHERE 子句中使用 != 或 &lt;&gt; 操作符，否则将引擎放弃使用索引而进行全表扫描。优化器将无法通过索引来确定将要命中的行数,因此需要搜索该表的所有行。</li>
<li>应尽量避免在 WHERE 子句中使用 OR 来连接条件，否则将导致引擎放弃使用索引而进行全表扫描，如：SELECT id FROM t WHERE num = 10 OR num = 20 。</li>
<li>应尽量避免在 WHERE 子句中对字段进行表达式操作，这将导致引擎放弃使用索引而进行全表扫描。</li>
<li>应尽量避免在 WHERE 子句中对字段进行函数操作，这将导致引擎放弃使用索引而进行全表扫描</li>
<li>不要在 WHERE 子句中的 = 左边进行函数、算术运算或其他表达式运算，否则系统将可能无法正确使用索引。</li>
<li>复合索引遵循前缀原则。</li>
<li>如果 MySQL 评估使用索引比全表扫描更慢，会放弃使用索引。如果此时想要索引，可以在语句中添加强制索引。</li>
<li>列类型是字符串类型，查询时一定要给值加引号，否则索引失效。</li>
<li>LIKE 查询，% 不能在前，因为无法使用索引。如果需要模糊匹配，可以使用全文索引。</li>
</ul>
</li>
<li><p>以下三条 SQL 如何建索引，只建一条怎么建？</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">WHERE</span> a <span class="operator">=</span> <span class="number">1</span> <span class="keyword">AND</span> b <span class="operator">=</span> <span class="number">1</span></span><br><span class="line"><span class="keyword">WHERE</span> b <span class="operator">=</span> <span class="number">1</span></span><br><span class="line"><span class="keyword">WHERE</span> b <span class="operator">=</span> <span class="number">1</span> <span class="keyword">ORDER</span> <span class="keyword">BY</span> <span class="type">time</span> <span class="keyword">DESC</span></span><br></pre></td></tr></table></figure>
<ul>
<li>以顺序 b , a, time 建立复合索引，CREATE INDEX table1_b_a_time ON index_test01(b, a, time)。</li>
<li>对于第一条 SQL ，因为最新 MySQL 版本会优化 WHERE 子句后面的列顺序，以匹配复合索引顺序。</li>
</ul>
</li>
<li><p>想知道一个查询用到了哪个索引，如何查看?</p>
<ul>
<li>EXPLAIN 显示了 MYSQL 如何使用索引来处理 SELECT 语句以及连接表,可以帮助选择更好的索引和写出更优化的查询语句。</li>
<li><code>explain select * from table;</code><ul>
<li>id: id代表执行select子句或操作表的顺序</li>
<li>select_type: 查询的类型,用于区别普通查询,联合查询,子查询等复杂查询<ul>
<li>simple: 简单的select查询,查询中不包含子查询或union查询</li>
<li>primary: 查询中若包含任何复杂的子部分,最外层查询则被标记为primary</li>
<li>subquery 在select 或where 列表中包含了子查询</li>
<li>derived 在from列表中包含的子查询被标记为derived,mysql会递归这些子查询,把结果放在临时表里</li>
<li>union 做第二个select出现在union之后,则被标记为union,若union包含在from子句的子查询中,外层select将被标记为derived</li>
<li>union result 从union表获取结果的select</li>
</ul>
</li>
<li>table: 显示一行的数据时关于哪张表的</li>
<li>type: 查询类型从最好到最差依次是:system&gt;const&gt;eq_ref&gt;ref&gt;range&gt;index&gt;All,一般情况下,得至少保证达到range级别,最好能达到ref<ul>
<li>system:表只有一行记录,这是const类型的特例,平时不会出现</li>
<li>const:表示通过索引一次就找到了,const即常量,它用于比较primary key或unique索引,因为只匹配一行数据,所以效率很快,如将主键置于where条件中,mysql就能将该查询转换为一个常量 </li>
<li>eq_ref:唯一性索引扫描,对于每个索引键,表中只有一条记录与之匹配,常见于主键或唯一索引扫描</li>
<li>ref:非唯一性索引扫描,返回匹配某个单独值的行,它可能会找到多个符合条件的行,所以他应该属于查找和扫描的混合体</li>
<li>range:只检索给定范围的行,使用一个索引来选择行,如where语句中出现了between,&lt;,&gt;,in等查询,这种范围扫描索引比全表扫描要好，因为它只需要开始于索引的某一点，而结束于另一点，不用扫描全部索引。</li>
<li>index:index类型只遍历索引树,这通常比All快,因为索引文件通常比数据文件小,index是从索引中读取,all从硬盘中读取</li>
<li>all:全表扫描,是最差的一种查询类型</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>MySQL 有哪些索引方法？</p>
<ul>
<li>B-Tree 索引。</li>
<li>Hash 索引。</li>
</ul>
</li>
<li><p>什么是 B-Tree 索引？</p>
<ul>
<li>B-Tree 是为磁盘等外存储设备设计的一种平衡查找树。</li>
</ul>
</li>
<li><p>为什么索引结构默认使用B-Tree，而不是hash，二叉树，红黑树？</p>
<ul>
<li>hash：虽然可以快速定位，但是没有顺序，IO复杂度高。</li>
<li>二叉树：树的高度不均匀，不能自平衡，查找效率跟数据有关（树的高度），并且IO代价高。</li>
<li>红黑树：树的高度随着数据量增加而增加，IO代价高。</li>
</ul>
</li>
<li><p>为什么官方建议使用自增长主键作为索引。</p>
<ul>
<li><p>结合B+Tree的特点，自增主键是连续的，在插入过程中尽量减少页分裂，即使要进行页分裂，也只会分裂很少一部分。并且能减少数据的移动，每次插入都是插入到最后。总之就是减少分裂和移动的频率。</p>
</li>
<li><p>插入连续的数据：</p>
<ul>
<li><ul>
<li><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/15848449061730.gif?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></li>
</ul>
</li>
</ul>
</li>
<li><p>插入非连续的数据</p>
<ul>
<li><ul>
<li><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/15848449240023.gif?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>简述索引定位数据的过程</p>
<ul>
<li><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/15852273948834.png?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></li>
<li>根节点常驻内存</li>
<li>根据值确定节点位置，二分查找，最终确定叶子节点</li>
</ul>
</li>
<li><p>B-Tree 有哪些索引类型？</p>
<ul>
<li>主键索引的叶子节点存的数据是整行数据( 即具体数据 )。在 InnoDB 里，主键索引也被称为聚集索引（clustered index）。</li>
<li>非主键索引的叶子节点存的数据是整行数据的主键，键值是索引。在 InnoDB 里，非主键索引也被称为辅助索引（secondary index）。</li>
</ul>
</li>
<li><p>聚簇索引的注意点有哪些？</p>
<ul>
<li>聚簇索引表最大限度地提高了 I/O 密集型应用的性能，但它也有以下几个限制：<ul>
<li>插入速度严重依赖于插入顺序，按照主键的顺序插入是最快的方式，否则将会出现页分裂，严重影响性能。因此，对于 InnoDB 表，我们一般都会定义一个自增的 ID 列为主键。</li>
<li>更新主键的代价很高，因为将会导致被更新的行移动。因此，对于InnoDB 表，我们一般定义主键为不可更新。</li>
<li>二级索引访问需要两次索引查找，第一次找到主键值，第二次根据主键值找到行数据。</li>
<li>主键 ID 建议使用整型。因为，每个主键索引的 B+Tree 节点的键值可以存储更多主键 ID ，每个非主键索引的 B+Tree 节点的数据可以存储更多主键 ID 。</li>
</ul>
</li>
</ul>
</li>
<li><p>什么是索引的最左匹配特性？</p>
<ul>
<li>当 B+Tree 的数据项是复合的数据结构，比如索引 (name, age, sex) 的时候，B+Tree 是按照从左到右的顺序来建立搜索树的。<ul>
<li>比如当 (张三, 20, F) 这样的数据来检索的时候，B+Tree 会优先比较 name 来确定下一步的所搜方向，如果 name 相同再依次比较 age 和 sex ，最后得到检索的数据。</li>
<li>但当 (20, F) 这样的没有 name 的数据来的时候，B+Tree 就不知道下一步该查哪个节点，因为建立搜索树的时候 name 就是第一个比较因子，必须要先根据 name 来搜索才能知道下一步去哪里查询。</li>
<li>比如当 (张三, F) 这样的数据来检索时，B+Tree 可以用 name 来指定搜索方向，但下一个字段 age 的缺失，所以只能把名字等于张三的数据都找到，然后再匹配性别是 F 的数据了。</li>
</ul>
</li>
</ul>
</li>
</ol>
<h3 id="事务"><a href="#事务" class="headerlink" title="事务"></a>事务</h3><ol>
<li>事务的特性指的是？<ul>
<li>事务就是对一系列的数据库操作（比如插入多条数据）进行统一的提交或回滚操作，如果插入成功，那么一起成功，如果中间有一条出现异常，那么回滚之前的所有操作。</li>
<li>ACID ，如下图所示：<ul>
<li><ul>
<li><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/15848457672689.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></li>
</ul>
</li>
<li>原子性 Atomicity ：一个事务（transaction）中的所有操作，或者全部完成，或者全部不完成，不会结束在中间某个环节。事务在执行过程中发生错误，会被恢复（Rollback）到事务开始前的状态，就像这个事务从来没有执行过一样。即，事务不可分割、不可约简。</li>
<li>一致性 Consistency ：在事务开始之前和事务结束以后，数据库的完整性没有被破坏。这表示写入的资料必须完全符合所有的预设约束、触发器)、级联回滚等。</li>
<li>隔离性 Isolation ：数据库允许多个并发事务同时对其数据进行读写和修改的能力，隔离性可以防止多个事务并发执行时由于交叉执行而导致数据的不一致。事务隔离分为不同级别，包括读未提交（Read uncommitted）、读提交（read committed）、可重复读（repeatable read）和串行化（Serializable）。</li>
<li>持久性 Durability ：事务处理结束后，对数据的修改就是永久的，即便系统故障也不会丢失。</li>
</ul>
</li>
</ul>
</li>
<li>事务的并发问题？<ul>
<li>脏读：事务 A 读取了事务 B 更新的数据，然后 B 回滚操作，那么 A 读取到的数据是脏数据。</li>
<li>不可重复读：事务 A 多次读取同一数据，事务 B 在事务 A 多次读取的过程中，对数据作了更新并提交，导致事务 A 多次读取同一数据时，结果不一致。</li>
<li>幻读：系统管理员 A 将数据库中所有学生的成绩从具体分数改为 ABCDE 等级，但是系统管理员 B 就在这个时候插入了一条具体分数的记录，当系统管理员 A 改结束后发现还有一条记录没有改过来，就好像发生了幻觉一样，这就叫幻读。</li>
</ul>
</li>
<li>MySQL 事务隔离级别会产生的并发问题？<ul>
<li>不同的隔离级别有不同的现象，并有不同的锁定/并发机制，隔离级别越高，数据库的并发性就越差。</li>
<li></li>
</ul>
</li>
</ol>
]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>面试宝典</tag>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title>MapReduce</title>
    <url>/2021/11/07/MapReduce/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="MapReduce"><a href="#MapReduce" class="headerlink" title="MapReduce"></a>MapReduce</h1><h1 id="一、MapReduce概述"><a href="#一、MapReduce概述" class="headerlink" title="一、MapReduce概述"></a>一、MapReduce概述</h1><h2 id="1-1-MapReduce定义"><a href="#1-1-MapReduce定义" class="headerlink" title="1.1 MapReduce定义"></a>1.1 MapReduce定义</h2><ul>
<li>MapReduce是一个<font color ='red' >分布式运算程序的编程框架</font>，是开发“基于Hadoop的数据分析应用”的核心框架。</li>
<li>MapReduce核心功能是将<font color ='red' >用户编写的业务逻辑代码和自带默认组件</font>整合成一个完整的<font color ='red' >分布式运算程序</font>，并<font color ='red' >运行在一个Hadoop集群上</font>。</li>
</ul>
<hr>
<h2 id="1-2-MapReduce优缺点"><a href="#1-2-MapReduce优缺点" class="headerlink" title="1.2 MapReduce优缺点"></a>1.2 MapReduce优缺点</h2><h3 id="1-2-1-优点"><a href="#1-2-1-优点" class="headerlink" title="1.2.1 优点"></a>1.2.1 优点</h3><ol>
<li>MapReduce 易于编程<br> 它简单的<font color ='red' >实现一些接口，就可以完成一个分布式程序</font>，这个分布式程序可以分布到大量廉价的PC机器上运行。也就是说你写一个分布式程序，跟写一个简单的串行程序是一模一样的。就是因为这个特点使得MapReduce编程变得非常流行。</li>
<li>良好的扩展性<br> 当你的计算资源不能得到满足的时候，你可以<font color ='red' >通过简单的增加机器来扩展它的计算能力</font>。</li>
<li>高容错性<br> MapReduce设计的初衷就是使程序能够部署在廉价的PC机器上，这就要求它具有很高的容错性。比如<font color ='red' >其中一台机器挂了，它可以把上面的计算任务转移到另外一个节点上运行，不至于这个任务运行失败</font>，而且这个过程不需要人工参与，而完全是由Hadoop内部完成的。</li>
<li>适合PB级以上海量数据的离线处理<br> 可以实现上千台服务器集群并发工作，提供数据处理能力。</li>
</ol>
<h3 id="1-2-2-缺点"><a href="#1-2-2-缺点" class="headerlink" title="1.2.2 缺点"></a>1.2.2 缺点</h3><ol>
<li>不擅长实时计算<br> MapReduce无法像MySQL一样，在毫秒或者秒级内返回结果。</li>
<li>不擅长流式计算<br> 流式计算的输入数据是动态的，而MapReduce的输入数据集是静态的，不能动态变化。这是因为MapReduce自身的设计特点决定了数据源必须是静态的。</li>
<li>不擅长DAG（有向无环图）计算<br> 多个应用程序存在依赖关系，后一个应用程序的输入为前一个的输出。在这种情况下，MapReduce并不是不能做，而是使用后，每个MapReduce作业的输出结果都会写入到磁盘，会造成大量的磁盘IO，导致性能非常的低下。</li>
</ol>
<hr>
<h2 id="1-3-MapReduce核心思想"><a href="#1-3-MapReduce核心思想" class="headerlink" title="1.3 MapReduce核心思想"></a>1.3 MapReduce核心思想</h2><p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16340999252814.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
<ul>
<li>数据输入：分片， Map阶段根据逻辑分片的理念对要计算的文件进行分片读取（128M）</li>
<li>Map阶段： 将文件进行逻辑划分后，进行分割处理 </li>
<li>Reduce阶段：将Map阶段处理好的数据进行汇总</li>
<li>数据输出：将Reduce阶段的输出结果保存至结果文件</li>
</ul>
<hr>
<h2 id="1-4-MapReduce进程"><a href="#1-4-MapReduce进程" class="headerlink" title="1.4 MapReduce进程"></a>1.4 MapReduce进程</h2><p>一个完整的MapReduce程序在分布式运行时有三类实例进程：</p>
<ol>
<li>MrAppMaster：负责整个程序的过程调度及状态协调。</li>
<li>MapTask：负责Map阶段的整个数据处理流程。</li>
<li>ReduceTask：负责Reduce阶段的整个数据处理流程。</li>
</ol>
<hr>
<h2 id="1-5-MR程序的编程规范"><a href="#1-5-MR程序的编程规范" class="headerlink" title="1.5 MR程序的编程规范"></a>1.5 MR程序的编程规范</h2><ul>
<li>驱动类（负责程序的job提交）</li>
<li>自定义 Mapper类型，并且继承Hadoop提供的Mapper<ul>
<li>重写map方法，在该方法中实现业务逻辑的编写</li>
</ul>
</li>
<li>自定义 Reducer类型，并且继承Hadoop提供的Reducer<ul>
<li>重写reduce方法，在该方法中实现业务逻辑的编写</li>
</ul>
</li>
</ul>
<h1 id="二、Hadoop序列化"><a href="#二、Hadoop序列化" class="headerlink" title="二、Hadoop序列化"></a>二、Hadoop序列化</h1><h2 id="2-1-序列化概述"><a href="#2-1-序列化概述" class="headerlink" title="2.1 序列化概述"></a>2.1 序列化概述</h2><h3 id="2-1-1-什么是序列化"><a href="#2-1-1-什么是序列化" class="headerlink" title="2.1.1 什么是序列化"></a>2.1.1 什么是序列化</h3><ul>
<li>序列化就是把内存中的对象，转换成字节序列（或其他数据传输协议）以便于存储到磁盘（持久化）和网络传输。 </li>
<li>反序列化就是将收到字节序列（或其他数据传输协议）或者是磁盘的持久化数据，转换成内存中的对象。</li>
</ul>
<h3 id="2-1-2-为什么要序列化"><a href="#2-1-2-为什么要序列化" class="headerlink" title="2.1.2 为什么要序列化"></a>2.1.2 为什么要序列化</h3><ul>
<li>一般来说，“活的”对象只生存在内存里，关机断电就没有了。而且“活的”对象只能由本地的进程使用，不能被发送到网络上的另外一台计算机。 然而序列化可以存储“活的”对象，可以将“活的”对象发送到远程计算机。</li>
</ul>
<h3 id="2-1-3-为什么不用Java的序列化"><a href="#2-1-3-为什么不用Java的序列化" class="headerlink" title="2.1.3 为什么不用Java的序列化"></a>2.1.3 为什么不用Java的序列化</h3><ul>
<li>Java的序列化是一个重量级序列化框架（Serializable），一个对象被序列化后，会附带很多额外的信息（各种校验信息，Header，继承体系等），不便于在网络中高效传输。所以，Hadoop自己开发了一套序列化机制（Writable）。</li>
<li>Hadoop序列化特点：<ul>
<li>紧凑 ：高效使用存储空间。</li>
<li>快速：读写数据的额外开销小。</li>
<li>可扩展：随着通信协议的升级而可升级</li>
<li>互操作：支持多语言的交互</li>
</ul>
</li>
</ul>
<hr>
<h2 id="2-2-自定义bean对象实现序列化接口（Writable）"><a href="#2-2-自定义bean对象实现序列化接口（Writable）" class="headerlink" title="2.2 自定义bean对象实现序列化接口（Writable）"></a>2.2 自定义bean对象实现序列化接口（Writable）</h2><ul>
<li>在企业开发中往往常用的基本序列化类型不能满足所有需求，比如在Hadoop框架内部传递一个bean对象，那么该对象就需要实现序列化接口。</li>
<li>具体实现bean对象序列化步骤如下7步。</li>
</ul>
<ol>
<li>必须实现Writable接口</li>
<li>反序列化时，需要反射调用空参构造函数，所以必须有空参构造 <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">FlowBean</span><span class="params">()</span> </span>&#123;</span><br><span class="line">	<span class="keyword">super</span>();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>重写序列化方法 <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(DataOutput out)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">	out.writeLong(upFlow);</span><br><span class="line">	out.writeLong(downFlow);</span><br><span class="line">	out.writeLong(sumFlow);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>重写反序列化方法 <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">readFields</span><span class="params">(DataInput in)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">	upFlow = in.readLong();</span><br><span class="line">	downFlow = in.readLong();</span><br><span class="line">	sumFlow = in.readLong();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>注意反序列化的顺序和序列化的顺序完全一致</li>
<li>要想把结果显示在文件中，需要重写toString()，可用”\t”分开，方便后续用。</li>
<li>如果需要将自定义的bean放在key中传输，则还需要实现Comparable接口，因为MapReduce框中的Shuffle过程要求对key必须能排序。详见后面排序案例。 <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compareTo</span><span class="params">(FlowBean o)</span> </span>&#123;</span><br><span class="line">	<span class="comment">// 倒序排列，从大到小</span></span><br><span class="line">	<span class="keyword">return</span> <span class="keyword">this</span>.sumFlow &gt; o.getSumFlow() ? -<span class="number">1</span> : <span class="number">1</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
</ol>
<hr>
<h1 id="三、MapReduce框架原理"><a href="#三、MapReduce框架原理" class="headerlink" title="三、MapReduce框架原理"></a>三、MapReduce框架原理</h1><h2 id="3-1-MapReduce的工作流程"><a href="#3-1-MapReduce的工作流程" class="headerlink" title="3.1 MapReduce的工作流程"></a>3.1 MapReduce的工作流程</h2><h3 id="3-1-1-MapReduce的数据流"><a href="#3-1-1-MapReduce的数据流" class="headerlink" title="3.1.1 MapReduce的数据流"></a>3.1.1 MapReduce的数据流</h3><p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16341977072186.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
<hr>
<h3 id="3-1-2-MapReduce的执行流程"><a href="#3-1-2-MapReduce的执行流程" class="headerlink" title="3.1.2 MapReduce的执行流程"></a>3.1.2 MapReduce的执行流程</h3><ul>
<li>简易版：InputFormat –&gt; Mapper –&gt; Reduce –&gt; OutputFormat</li>
<li>详细版：InputFormat –&gt; map sort –&gt; copy sort group reduce –&gt; OutputFormat</li>
<li>MR执行的整体大致分为两个阶段<ul>
<li>提交Job<ul>
<li>对当前MR进行初始化工作以及对MT和RT进行规划</li>
</ul>
</li>
<li>执行Job<ul>
<li>执行的就是每一个MT和RT</li>
</ul>
</li>
</ul>
</li>
</ul>
<hr>
<h2 id="3-2-InputFormat数据输入"><a href="#3-2-InputFormat数据输入" class="headerlink" title="3.2 InputFormat数据输入"></a>3.2 InputFormat数据输入</h2><h3 id="3-3-1-数据切片与MapTask并行度决定机制"><a href="#3-3-1-数据切片与MapTask并行度决定机制" class="headerlink" title="3.3.1 数据切片与MapTask并行度决定机制"></a>3.3.1 数据切片与MapTask并行度决定机制</h3><p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16341008360509.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
<ol>
<li>切片的概念：切片是计算数据是从逻辑上文件的进行划分，切块存储数据时从物理上将文件进行切分</li>
<li>一个切片对应一个MapTask来处理</li>
<li>切片大小默认情况等于切块大小128M（这样做的目的是为了计算时读取读取数据效率更高，避免了跨机器读取）</li>
<li>切片的时候不考虑数据整体集，默认情况下对单个文件的进行切片</li>
</ol>
<h3 id="3-3-2-InputFormat类的体系结构"><a href="#3-3-2-InputFormat类的体系结构" class="headerlink" title="3.3.2 InputFormat类的体系结构"></a>3.3.2 InputFormat类的体系结构</h3><p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16341011423965.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
<ul>
<li>InputFormat 抽象类的子实现类是 FileInputFormat </li>
<li>FileInputFormat 类的子实现类是 TextInputFormat </li>
<li>InputFormat抽象类中有两个抽象方法 <ul>
<li>getSplits() –&gt; 在 FileInputFormat 做了具体实现<br> 具体的切片的逻辑！！！</li>
<li>createRecordReader() –&gt; TextInputFormat 做了具体实现<br>  创建RecordReader对象， RecordReader最终帮助我们读取待处理的文件的数据，<br>  读取规则就是按行读取由LineRecordReader实现！！！</li>
</ul>
</li>
<li>遇到小文件计算的场景：<br> CombineTextInputFormat –&gt; FileInputFormat的子实现类（用于处理小文件场景）<h4 id="3-3-2-1-分析MapReduce中InputFormat的默认实现"><a href="#3-3-2-1-分析MapReduce中InputFormat的默认实现" class="headerlink" title="3.3.2.1 分析MapReduce中InputFormat的默认实现"></a>3.3.2.1 分析MapReduce中InputFormat的默认实现</h4>InputFormat-&gt;FileInputFormat-&gt;TextInputFormat</li>
<li>定位 驱动类的中的<code>job.waitForCompletion(true);</code></li>
<li>跟进 <code>org.apache.hadoop.mapreduce.Job#waitForCompletion</code></li>
<li>跟进 <code>org.apache.hadoop.mapreduce.Job#submit</code></li>
<li>定位 <code>return submitter.submitJobInternal(Job.this, cluster);</code></li>
<li>跟进 <code>org.apache.hadoop.mapreduce.JobSubmitter#submitJobInternal</code></li>
<li>关注 <code>JobSubmitter.java:200 int maps = writeSplits(job, submitJobDir);</code></li>
<li>跟进 <code>org.apache.hadoop.mapreduce.JobSubmitter#writeSplits</code></li>
<li>跟进 <code>org.apache.hadoop.mapreduce.JobSubmitter#writeNewSplits</code></li>
<li>关注 根据Job中设置的InputFormatClass，然后通过反射的手段获取 InputFormat 的实例  <figure class="highlight java"><table><tr><td class="code"><pre><span class="line">InputFormat&lt;?, ?&gt; input = ReflectionUtils.newInstance(job.getInputFormatClass(), conf);</span><br></pre></td></tr></table></figure></li>
<li>跟进 <code>job.getInputFormatClass()</code>  <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> Class&lt;? extends InputFormat&lt;?,?&gt;&gt; getInputFormatClass() </span><br><span class="line">    <span class="keyword">throws</span> ClassNotFoundException;</span><br></pre></td></tr></table></figure></li>
<li>定位到 JobContextImpl 实现类  <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> Class&lt;? extends InputFormat&lt;?,?&gt;&gt; getInputFormatClass() <span class="keyword">throws</span> ClassNotFoundException &#123;</span><br><span class="line">    <span class="comment">// 先根据 Job中配置信息中的 mapreduce.job.inputformat.class 获取配置，如果获取不到</span></span><br><span class="line">    <span class="comment">// 走默认的 TextInputFormat.class</span></span><br><span class="line">    <span class="keyword">return</span> (Class&lt;? extends InputFormat&lt;?,?&gt;&gt;) </span><br><span class="line">    conf.getClass(INPUT_FORMAT_CLASS_ATTR, TextInputFormat.class);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>核对无默认配置<code>mapreduce.job.inputformat.class</code><a href="media/16340187305977/core-default.xml">hadoop-common-3.1.3.jar!/core-default.xml</a></li>
</ul>
<h4 id="3-3-2-2-分析Hadoop默认的切片规则"><a href="#3-3-2-2-分析Hadoop默认的切片规则" class="headerlink" title="3.3.2.2 分析Hadoop默认的切片规则"></a>3.3.2.2 分析Hadoop默认的切片规则</h4><ul>
<li>定位 <code>org.apache.hadoop.mapreduce.InputFormat#getSplits</code>抽象方法</li>
<li>定位 <code>org.apache.hadoop.mapreduce.lib.input.FileInputFormat#getSplits</code>实现  <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Generate the list of files and make them into FileSplits.</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> job the job context</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@throws</span> IOException</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> List&lt;InputSplit&gt; <span class="title">getSplits</span><span class="params">(JobContext job)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">  <span class="comment">// 计时器，负责记录当前切片的所花费的时间 最后记录日志中</span></span><br><span class="line">  StopWatch sw = <span class="keyword">new</span> StopWatch().start();</span><br><span class="line">  <span class="comment">// minSize默认情况为1  但是可以通过配置 mapreduce.input.fileinputformat.split.minsize 改变它的值</span></span><br><span class="line">  <span class="keyword">long</span> minSize = Math.max(getFormatMinSplitSize(), getMinSplitSize(job));</span><br><span class="line">  <span class="comment">// maxSize默认情况为Long.MAX_VALUE   但是可以通过配置 mapreduce.input.fileinputformat.split.maxsize 改变它的值</span></span><br><span class="line">  <span class="keyword">long</span> maxSize = getMaxSplitSize(job);</span><br><span class="line">    </span><br><span class="line">  <span class="comment">// generate splits</span></span><br><span class="line">  List&lt;InputSplit&gt; splits = <span class="keyword">new</span> ArrayList&lt;InputSplit&gt;();</span><br><span class="line">  List&lt;FileStatus&gt; files = listStatus(job);</span><br><span class="line">    </span><br><span class="line">  <span class="comment">// ignoreDirs 默认是false</span></span><br><span class="line">  <span class="keyword">boolean</span> ignoreDirs = !getInputDirRecursive(job)</span><br><span class="line">          &amp;&amp; job.getConfiguration().getBoolean(INPUT_DIR_NONRECURSIVE_IGNORE_SUBDIRS, <span class="keyword">false</span>);</span><br><span class="line">  <span class="keyword">for</span> (FileStatus file : files) &#123;</span><br><span class="line">    <span class="comment">// 默认情况下，对Job中设置的输入路径中的文件以及子目录中的文件全都处理</span></span><br><span class="line">    <span class="comment">// 如果考虑只处理当前设置的路径的子文件，而不管子目录中的文件需要自行定义</span></span><br><span class="line">    <span class="comment">// INPUT_DIR_NONRECURSIVE_IGNORE_SUBDIRS 为 true</span></span><br><span class="line">    <span class="keyword">if</span> (ignoreDirs &amp;&amp; file.isDirectory()) &#123;</span><br><span class="line">      <span class="keyword">continue</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 获取当前文件的大小</span></span><br><span class="line">    Path path = file.getPath();</span><br><span class="line">    <span class="comment">// 对当前文件进行非空判断</span></span><br><span class="line">    <span class="keyword">long</span> length = file.getLen();</span><br><span class="line">    <span class="keyword">if</span> (length != <span class="number">0</span>) &#123;</span><br><span class="line">      <span class="comment">// 获取当前文件的对应数据块</span></span><br><span class="line">      BlockLocation[] blkLocations;</span><br><span class="line">      <span class="keyword">if</span> (file <span class="keyword">instanceof</span> LocatedFileStatus) &#123;</span><br><span class="line">        blkLocations = ((LocatedFileStatus) file).getBlockLocations();</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        FileSystem fs = path.getFileSystem(job.getConfiguration());</span><br><span class="line">        blkLocations = fs.getFileBlockLocations(file, <span class="number">0</span>, length);</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="comment">// 判断当前文件是否可以进行切片</span></span><br><span class="line">      <span class="comment">// 主要考虑的是压缩文件这种情况</span></span><br><span class="line">      <span class="keyword">if</span> (isSplitable(job, path)) &#123;</span><br><span class="line">        <span class="comment">// 获取当前文件的块大小</span></span><br><span class="line">        <span class="keyword">long</span> blockSize = file.getBlockSize();</span><br><span class="line">        <span class="comment">// 计算切片大小计算切片逻辑如下</span></span><br><span class="line">        <span class="comment">// 切片大小是否可变？ 可变</span></span><br><span class="line">        <span class="comment">// 如果想调大：改变minSize</span></span><br><span class="line">        <span class="comment">// 如果想调小：改变maxSize</span></span><br><span class="line">        <span class="keyword">long</span> splitSize = computeSplitSize(blockSize, minSize, maxSize);</span><br><span class="line">        <span class="comment">// 当前文件的剩余大小</span></span><br><span class="line">        <span class="keyword">long</span> bytesRemaining = length;</span><br><span class="line">        <span class="comment">// 判断剩余文件是否要继续切分  剩余大小 / 切片大小 是否大于 1.1</span></span><br><span class="line">        <span class="comment">// 目的：就是为了更合理的使用资源计算数据</span></span><br><span class="line">        <span class="keyword">while</span> (((<span class="keyword">double</span>) bytesRemaining) / splitSize &gt; SPLIT_SLOP) &#123;</span><br><span class="line">          <span class="keyword">int</span> blkIndex = getBlockIndex(blkLocations, length - bytesRemaining);</span><br><span class="line">          splits.add(makeSplit(path, length - bytesRemaining, splitSize,</span><br><span class="line">                  blkLocations[blkIndex].getHosts(),</span><br><span class="line">                  blkLocations[blkIndex].getCachedHosts()));</span><br><span class="line">          bytesRemaining -= splitSize;</span><br><span class="line">        &#125;</span><br><span class="line">    </span><br><span class="line">        <span class="keyword">if</span> (bytesRemaining != <span class="number">0</span>) &#123;</span><br><span class="line">          <span class="keyword">int</span> blkIndex = getBlockIndex(blkLocations, length - bytesRemaining);</span><br><span class="line">          splits.add(makeSplit(path, length - bytesRemaining, bytesRemaining,</span><br><span class="line">                  blkLocations[blkIndex].getHosts(),</span><br><span class="line">                  blkLocations[blkIndex].getCachedHosts()));</span><br><span class="line">        &#125;</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123; <span class="comment">// not splitable</span></span><br><span class="line">        <span class="keyword">if</span> (LOG.isDebugEnabled()) &#123;</span><br><span class="line">          <span class="comment">// Log only if the file is big enough to be splitted</span></span><br><span class="line">          <span class="keyword">if</span> (length &gt; Math.min(file.getBlockSize(), minSize)) &#123;</span><br><span class="line">            LOG.debug(<span class="string">&quot;File is not splittable so no parallelization &quot;</span></span><br><span class="line">                    + <span class="string">&quot;is possible: &quot;</span> + file.getPath());</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        splits.add(makeSplit(path, <span class="number">0</span>, length, blkLocations[<span class="number">0</span>].getHosts(),</span><br><span class="line">                blkLocations[<span class="number">0</span>].getCachedHosts()));</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="comment">//Create empty hosts array for zero length files</span></span><br><span class="line">      splits.add(makeSplit(path, <span class="number">0</span>, length, <span class="keyword">new</span> String[<span class="number">0</span>]));</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// Save the number of input files for metrics/loadgen</span></span><br><span class="line">  job.getConfiguration().setLong(NUM_INPUT_FILES, files.size());</span><br><span class="line">  sw.stop();</span><br><span class="line">  <span class="keyword">if</span> (LOG.isDebugEnabled()) &#123;</span><br><span class="line">    LOG.debug(<span class="string">&quot;Total # of splits generated by getSplits: &quot;</span> + splits.size()</span><br><span class="line">            + <span class="string">&quot;, TimeTaken: &quot;</span> + sw.now(TimeUnit.MILLISECONDS));</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> splits;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>总结</li>
</ul>
<h4 id="3-3-2-3-CombineTextInputFormat切片机制"><a href="#3-3-2-3-CombineTextInputFormat切片机制" class="headerlink" title="3.3.2.3 CombineTextInputFormat切片机制"></a>3.3.2.3 CombineTextInputFormat切片机制</h4><ol>
<li>框架默认的TextInputFormat切片机制是对任务按文件规划切片，不管文件多小，都会是一个单独的切片，都会交给一个MapTask，这样如果有大量小文件，就会产生大量的MapTask，处理效率极其低下。</li>
<li>应用场景：CombineTextInputFormat用于小文件过多的场景，它可以将多个小文件从逻辑上规划到一个切片中，这样，多个小文件就可以交给一个MapTask处理。</li>
<li>虚拟存储切片最大值设置CombineTextInputFormat.setMaxInputSplitSize(job, 4194304);// 4m<br> 注意：虚拟存储切片最大值设置最好根据实际的小文件大小情况来设置具体的值。</li>
<li>CombineTextInputFormat切片机制<ol>
<li>用户设置切片大小</li>
<li>虚拟过程：根据和切片大小进行比较 <ul>
<li>如果当前文件 &gt; 设置的大小 且 小于2倍的设置的大小就一分为2</li>
<li>如果当前文件大于2倍的设置的大小就先切分出设置大小的块，然后重复步骤2虚拟切分</li>
</ul>
</li>
<li>切片过程：根据虚拟后的结果 把每个虚拟文件和 设置的大小比较<ul>
<li>如果大于等于设置的大小就单独形成一个切片</li>
<li>如果小于设置大小就和下一个虚拟文件进行合并，重复执行</li>
<li>如果合并后大于设置大小就单独形成一个切片</li>
</ul>
</li>
</ol>
</li>
</ol>
<hr>
<h2 id="3-3-Shuffle机制"><a href="#3-3-Shuffle机制" class="headerlink" title="3.3 Shuffle机制"></a>3.3 Shuffle机制</h2><p>Shuffle：Map方法之后，Reduce方法之前的数据处理过程<br><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16341965884201.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
<h3 id="3-3-1-Shuffle机制"><a href="#3-3-1-Shuffle机制" class="headerlink" title="3.3.1 Shuffle机制"></a>3.3.1 Shuffle机制</h3><h4 id="3-3-1-1-Shuffle过程实现的作用"><a href="#3-3-1-1-Shuffle过程实现的作用" class="headerlink" title="3.3.1.1 Shuffle过程实现的作用"></a>3.3.1.1 Shuffle过程实现的作用</h4><ol>
<li>分区：由业务决定<ol>
<li>决定当前的Key交给那个Reduce进行处理</li>
<li>相同的key必须由同一个Reduce进行处理</li>
<li>默认根据Key的Hash值，对Reduce的个数取模</li>
</ol>
</li>
<li>分组：<ol>
<li>将相同Key的value进行合并</li>
<li>相同Key的value分到同一个组</li>
</ol>
</li>
<li>排序<ol>
<li>对key的index进行排序</li>
<li>排序算法为快排，顺序为字典顺序</li>
</ol>
</li>
<li>合并<ol>
<li>相同key溢写的文件合并成一个文件</li>
<li>保证1个MapTask结果输出一个文件</li>
</ol>
</li>
</ol>
<h4 id="3-3-1-2-map端Shuffle"><a href="#3-3-1-2-map端Shuffle" class="headerlink" title="3.3.1.2 map端Shuffle"></a>3.3.1.2 map端Shuffle</h4><p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16341966144406.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
<ol>
<li>Partition：将map的结果发送到相应的reduce端，总的partition的数目等于reducer的数量。<ol>
<li>map输出的key-value结果由Partitioner#getPartition方法决定交由那个reducer处理</li>
<li>默认由HashPartitioner实现，对key取hash值后进行取模运算 <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HashPartitioner</span>&lt;<span class="title">K</span>, <span class="title">V</span>&gt; <span class="keyword">extends</span> <span class="title">Partitioner</span>&lt;<span class="title">K</span>, <span class="title">V</span>&gt; </span>&#123;</span><br><span class="line">  <span class="comment">/** Use &#123;<span class="doctag">@link</span> Object#hashCode()&#125; to partition. */</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getPartition</span><span class="params">(K key, V value,<span class="keyword">int</span> numReduceTasks)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> (key.hashCode() &amp; Integer.MAX_VALUE) % numReduceTasks;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>将数据写入到内存缓冲区，缓冲区的作用是批量收集map结果，减少磁盘IO的影响。key-value对以及Partition的结果都会被写入缓冲区。在写入之前，key与value值都会被序列化成字节数组</li>
</ol>
</li>
<li>spill: 溢写，sort &amp; combiner<ol>
<li>把内存缓冲区中的数据写入到本地磁盘，在写入本地磁盘时先按照partition、再按照key进行排序（quick sort）</li>
<li>spill是由另外单独的线程来完成，不影响往缓冲区写map结果的线程；</li>
<li>内存缓冲区默认大小限制为100MB，它有个溢写比例（spill.percent），默认为0.8，当缓冲区的数据达到阈值时，溢写线程就会启动，先锁定这80MB的内存，执行溢写过程，maptask的输出结果还可以往剩下的20MB内存中写，互不影响。然后再重新利用这块缓冲区，因此Map的内存缓冲区又叫做环形缓冲区</li>
<li>sort：在将数据写入磁盘之前，先要对要写入磁盘的数据进行一次排序操作，先按&lt;key,value,partition&gt;中的partition分区号排序，然后再按key排序，最后溢出的小文件是分区的，且同一个分区内是保证key有序的；</li>
<li>combine：执行combine操作要求程序中通过job.setCombinerClass(myCombine.class)自定义combine操作<ul>
<li>程序中有两个阶段可能会执行combine操作：<ol>
<li>map输出数据根据分区排序完成后，在写入文件之前会执行一次combine操作（前提是作业中设置了这个操作）；</li>
<li>如果map输出比较大，溢出文件个数大于3（此值可以通过属性min.num.spills.for.combine配置）时，在merge的过程（多个spill文件合并为一个大文件）中还会执行combine操作；</li>
</ol>
</li>
<li>combine主要是把形如&lt;aa,1&gt;,&lt;aa,2&gt;这样的key值相同的数据进行计算，计算规则与reduce一致，比如：当前计算是求key对应的值求和，则combine操作后得到&lt;aa,3&gt;这样的结果。</li>
<li>注意事项：不是每种作业都可以做combine操作的，只有满足以下条件才可以：<ul>
<li>reduce的输入输出类型都一样，因为combine本质上就是reduce操作；</li>
<li>计算逻辑上，combine操作后不会影响计算结果，像求和就不会影响；</li>
</ul>
</li>
</ul>
</li>
</ol>
</li>
<li>merge: 当map很大时，每次溢写会产生一个spill_file，这样会有多个spill_file，而最终的一个map task输出只有一个文件，因此，最终的结果输出之前会对多个中间过程进行多次溢写文件（spill_file）的合并，此过程就是merge过程。也即是，待Map Task任务的所有数据都处理完后，会对任务产生的所有中间数据文件做一次合并操作，以确保一个Map Task最终只生成一个中间数据文件。<ol>
<li>如果生成的文件太多，可能会执行多次合并，每次最多能合并的文件数默认为10，可以通过属性min.num.spills.for.combine配置；</li>
<li>多个溢写文件合并时，会进行一次排序，排序算法是<font color ='red' >多路归并排序</font>；</li>
<li>是否还需要做combine操作，一是看是否设置了combine，二是看溢出的文件数是否大于等于3；</li>
<li>最终生成的文件格式与单个溢出文件一致，也是按分区顺序存储，并且输出文件会有一个对应的索引文件，记录每个分区数据的起始位置，长度以及压缩长度，这个索引文件名叫做file.out.index。</li>
</ol>
</li>
<li>内存缓冲区<ol>
<li>在MapTask任务的业务处理方法map()中，最后一步通过<code>OutputCollector.collect(key,value)</code>或<code>context.write(key,value)</code>输出Map Task的中间处理结果，在<code>collect(key,value)</code>方法中，会调用<code>Partitioner.getPartition(K2 key, V2 value, int numPartitions)</code>方法获得输出的key-value对应的分区号(分区号可以认为对应着一个要执行Reduce Task的节点)，然后将&lt;key,value,partition&gt;暂时保存在内存中的MapOutputBuffe内部的环形数据缓冲区，该缓冲区的默认大小是100MB，可以通过参数io.sort.mb来调整其大小</li>
<li>当缓冲区中的数据使用率达到一定阀值后，触发一次Spill操作，将环形缓冲区中的部分数据写到磁盘上，生成一个临时的本地数据的spill文件；然后在缓冲区的使用率再次达到阀值后，再次生成一个spill文件。直到数据处理完毕，在磁盘上会生成很多的临时文件</li>
<li>触发spill操作时，map输出还会接着往剩下的20%的内存空间写入，但是写满的80%的内存空间会被锁定，数据溢出写入磁盘。当这部分溢出的数据写完后，空出的内存空间可以接着被使用，形成像环一样的被循环使用的效果，所以又叫做环形内存缓冲区</li>
<li>MapOutputBuffe内部存数的数据采用了两个索引结构，涉及三个环形内存缓冲区。两级索引结构如下：<br> <img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16341959070345.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"><ol>
<li>写入到缓冲区的数据会进行压缩，由CompressionCodec提供实现</li>
<li>kvoffsets缓冲区：也叫偏移量索引数组，用于保存key-value信息在位置索引 kvindices 中的偏移量。当 kvoffsets 的使用率超过io.sort.spill.percent (默认为80%)后，便会触发一次 SpillThread 线程的“溢写”操作，也就是开始一次 Spill 阶段的操作。</li>
<li>kvindices缓冲区：也叫位置索引数组，用于保存 key-value 在数据缓冲区 kvbuffer 中的起始位置。</li>
<li>kvbuffer数据缓冲区：用于保存实际的 key-value 的值。默认情况下该缓冲区最多可以使用io.sort.mb的95%，当kvbuffer使用率超过io.sort.spill.percent(默认为80%)后，便会出发一次 SpillThread 线程的“溢写”操作，也就是开始一次 Spill 阶段的操作。</li>
<li>写入到本地磁盘时，对数据进行排序，实际上是对kvoffsets这个偏移量索引数组进行排序。</li>
</ol>
</li>
</ol>
</li>
<li>MapTask结束，通知appmaster,appmaster通过Reduce拉取数据</li>
</ol>
<h4 id="3-3-1-3-reduce端Shuffle"><a href="#3-3-1-3-reduce端Shuffle" class="headerlink" title="3.3.1.3 reduce端Shuffle"></a>3.3.1.3 reduce端Shuffle</h4><p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16341965290492.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
<ol>
<li>copy过程<ul>
<li>作用：拉取MapTask处理完成的数据</li>
<li>过程：Reduce进程启动一些数据copy线程(Fetcher)，通过HTTP方式请求MapTask所在的TaskTracker获取MapTask的输出文件。因为这时MapTask已经结束，这些文件就由TaskTracker管理在本地磁盘中。</li>
<li>默认情况下，当整个MapReduce作业的所有已执行完成的MapTask任务数超过MapTask总数的5%后，JobTracker便会开始调度执行ReduceTask任务。然后ReduceTask任务默认启动mapred.reduce.parallel.copies(默认为5）个MapOutputCopier线程到已完成的MapTask任务节点上分别copy一份属于自己的数据。 这些copy的数据会首先保存的内存缓冲区中，当内冲缓冲区的使用率达到一定阀值后，触发溢写写到磁盘上。</li>
<li>内存缓冲区<ol>
<li>内存缓冲区大小通过mapred.job.shuffle.input.buffer.percent（default 0.7）参数来设置，控制shuffle在reduce内存中的数据最多使用内存量为：0.7 × maxHeap of ReduceTask。</li>
<li>ReduceTask使用最大heap的一定比例用来缓存数据（最大heap通常通过mapred.child.java.opts来设置，比如设置为-Xmx1024m）。默认情况下，reduce会使用其heapsize的70%来在内存中缓存数据。如果reduce的heap由于业务原因调整的比较大，相应的缓存大小也会变大，这也是为什么reduce用来做缓存的参数是一个百分比，而不是一个固定的值了。</li>
</ol>
</li>
</ul>
</li>
<li>merge过程<ul>
<li>merge 有三种形式：1)内存到内存 2)内存到磁盘 3)磁盘到磁盘。默认情况下第一种形式是不启用的。</li>
<li>当内存中的数据量到达一定阈值，就启动内存到磁盘的 merge（图中的第一个merge，之所以进行merge是因为reduce端在从多个map端copy数据的时候，并没有进行sort，只是把它们加载到内存，当达到阈值写入磁盘时，需要进行merge） 。这和map端的很类似，这实际上就是溢写的过程，在这个过程中如果你设置有Combiner，它也是会启用的，然后在磁盘中生成了众多的溢写文件，这种merge方式一直在运行，直到没有 map 端的数据时才结束，然后才会启动第三种磁盘到磁盘的 merge （图中的第二个merge）方式生成最终的那个文件。</li>
<li>在远程copy数据的同时，ReduceTask在后台启动了两个后台线程对内存和磁盘上的数据文件做合并操作，以防止内存使用过多或磁盘生的文件过多。</li>
</ul>
</li>
<li>reducer的输入文件<ul>
<li>merge的最后会生成一个文件，大多数情况下存在于磁盘中，但是需要将其放入内存中。当reducer 输入文件已定，整个 Shuffle 阶段才算结束。然后就是 Reducer 执行，把结果放到 HDFS 上。</li>
</ul>
</li>
</ol>
<hr>
<h3 id="3-3-2-Partition分区"><a href="#3-3-2-Partition分区" class="headerlink" title="3.3.2 Partition分区"></a>3.3.2 Partition分区</h3><h4 id="3-3-2-1-分区使用场景"><a href="#3-3-2-1-分区使用场景" class="headerlink" title="3.3.2.1 分区使用场景"></a>3.3.2.1 分区使用场景</h4><ul>
<li>要求将统计结果按照条件输出到不同文件中（分区）。比如：将统计结果按照手机归属地不同省份输出到不同文件中（分区）</li>
<li>分区是由需求决定的，而分区编号的产生是由ReducerTask的数量控制的。<h4 id="3-3-2-2-Hadoop默认的分区规则"><a href="#3-3-2-2-Hadoop默认的分区规则" class="headerlink" title="3.3.2.2 Hadoop默认的分区规则"></a>3.3.2.2 Hadoop默认的分区规则</h4></li>
<li>根据key的hashCode对ReduceTasks个数取模得到的。无法控制哪个key存储到哪个分区。</li>
<li>默认分区规则源码分析<ul>
<li>定位Mapper逻辑中的 context.write(outk, outv);</li>
<li>跟进 write(outk, outv);  <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(KEYOUT key, VALUEOUT value)</span> <span class="keyword">throws</span> IOException, InterruptedException</span>;</span><br></pre></td></tr></table></figure></li>
<li>具体实现 TaskInputOutputContextImpl#write  <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(KEYOUT key, VALUEOUT value</span></span></span><br><span class="line"><span class="params"><span class="function">                )</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">    output.write(key, value);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>跟进 output.write(key, value)  <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(K key, V value)</span> <span class="keyword">throws</span> IOException, </span></span><br><span class="line"><span class="function">    InterruptedException</span>;</span><br></pre></td></tr></table></figure></li>
<li>具体实现实现类RecordWriterWithCounter#write  <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(Object key, Object value)</span> </span></span><br><span class="line"><span class="function">        <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">    context.getCounter(COUNTERS_GROUP, counterName).increment(<span class="number">1</span>);</span><br><span class="line">    writer.write(key, value);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>跟进 writer.write(key, value);具体实现 NewOutputCollector#write<ul>
<li>collector-&gt;MapOutputCollector ： 此对象就是环形缓冲区对象<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(K key, V value)</span> <span class="keyword">throws</span> IOException,InterruptedException </span>&#123;</span><br><span class="line">    collector.collect(key, value,</span><br><span class="line">        partitioner.getPartition(key, value, partitions));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>跟进 partitioner.getPartition(key, value, partitions)<br>  注意：跟进后发现来到了一个 叫做 Partitioner的抽象类中，如果想知道<br>  Hadoop默认的分区规则，必须得知道 当前Partitioner的默认实现类！<ul>
<li>查找Partitioner的默认实现类<ul>
<li>关注：partitioner赋值发生在NewOutputCollector构造方法中   <figure class="highlight java"><table><tr><td class="code"><pre><span class="line">NewOutputCollector(org.apache.hadoop.mapreduce.JobContext jobContext,</span><br><span class="line">                JobConf job,</span><br><span class="line">                TaskUmbilicalProtocol umbilical,</span><br><span class="line">                TaskReporter reporter</span><br><span class="line">                ) <span class="keyword">throws</span> IOException, ClassNotFoundException &#123;</span><br><span class="line">    <span class="comment">//获取环形缓冲区对象</span></span><br><span class="line">    collector = createSortingCollector(job, reporter);</span><br><span class="line">    <span class="comment">//获取ReduceTask数量作为分区数量</span></span><br><span class="line">    partitions = jobContext.getNumReduceTasks();</span><br><span class="line">    <span class="comment">//分区大于1个</span></span><br><span class="line">    <span class="keyword">if</span> (partitions &gt; <span class="number">1</span>) &#123;</span><br><span class="line">        <span class="comment">//根据 jobContext.getPartitionerClass() 获取Partitioner实现类</span></span><br><span class="line">        partitioner = (org.apache.hadoop.mapreduce.Partitioner&lt;K,V&gt;)</span><br><span class="line">            ReflectionUtils.newInstance(jobContext.getPartitionerClass(), job);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        partitioner = <span class="keyword">new</span> org.apache.hadoop.mapreduce.Partitioner&lt;K,V&gt;() &#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getPartition</span><span class="params">(K key, V value, <span class="keyword">int</span> numPartitions)</span> </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> partitions - <span class="number">1</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>跟进 JobContext#getPartitionerClass 实现类 JobContextImpl#getPartitionerClass  <figure class="highlight java"><table><tr><td class="code"><pre><span class="line">            <span class="comment">// 根据PARTITIONER_CLASS_ATTR枚举值对一个mapreduce.job.partitioner.class配置项</span></span><br><span class="line"><span class="comment">// 获取 Partitioner 的实现类，发现默认没有配置，那就使用后面默认的HashPartitioner.class</span></span><br><span class="line">            <span class="keyword">public</span> Class&lt;? extends Partitioner&lt;?,?&gt;&gt; getPartitionerClass() </span><br><span class="line">                    <span class="keyword">throws</span> ClassNotFoundException &#123;</span><br><span class="line">                <span class="keyword">return</span> (Class&lt;? extends Partitioner&lt;?,?&gt;&gt;) </span><br><span class="line">                    conf.getClass(PARTITIONER_CLASS_ATTR, HashPartitioner.class);</span><br><span class="line">            &#125;</span><br></pre></td></tr></table></figure></li>
<li>跟进 HashPartitioner<ul>
<li>根据以上分析 Partitioner 的实现类是 HashPartitioner.class，以下就是Hadoop的默认分区规则  <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HashPartitioner</span>&lt;<span class="title">K</span>, <span class="title">V</span>&gt; <span class="keyword">extends</span> <span class="title">Partitioner</span>&lt;<span class="title">K</span>, <span class="title">V</span>&gt; </span>&#123;</span><br><span class="line">    <span class="comment">/** Use &#123;<span class="doctag">@link</span> Object#hashCode()&#125; to partition. */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getPartition</span><span class="params">(K key, V value,</span></span></span><br><span class="line"><span class="params"><span class="function">                            <span class="keyword">int</span> numReduceTasks)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> (key.hashCode() &amp; Integer.MAX_VALUE) % numReduceTasks;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>结束：根据当前的key的hashcode值和 ReduceTask的数量取模操作得到当前key的所属分区编号</li>
<li>在MR中使用分区，通常要结合业务去做自定义分区规则！</li>
</ul>
</li>
</ul>
<h4 id="3-3-2-3-自定义Partitioner步骤"><a href="#3-3-2-3-自定义Partitioner步骤" class="headerlink" title="3.3.2.3 自定义Partitioner步骤"></a>3.3.2.3 自定义Partitioner步骤</h4><ol>
<li>自定义类继承Partitioner，重写getPartition()方法 <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Slf4j</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MobileModPartitioner</span> <span class="keyword">extends</span> <span class="title">Partitioner</span>&lt;<span class="title">Text</span>, <span class="title">FlowDTO</span>&gt;</span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getPartition</span><span class="params">(Text text, FlowDTO flowDTO, <span class="keyword">int</span> numPartitions)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">long</span> mobileNum = Long.parseLong(flowDTO.getMobile());</span><br><span class="line">        <span class="keyword">int</span> partitionNum = (<span class="keyword">int</span>) (mobileNum % numPartitions);</span><br><span class="line">        log.info(<span class="string">&quot;执行分区操作 key:&#123;&#125; partition;&#123;&#125;&quot;</span>, mobileNum, partitionNum);</span><br><span class="line">        <span class="keyword">return</span> partitionNum;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>在Job驱动中，设置自定义Partitioner <figure class="highlight java"><table><tr><td class="code"><pre><span class="line">job.setPartitionerClass(CustomPartitioner.class);</span><br></pre></td></tr></table></figure></li>
<li>自定义Partition后，要根据自定义Partitioner的逻辑设置相应数量的ReduceTask <figure class="highlight java"><table><tr><td class="code"><pre><span class="line">job.setNumReduceTasks(<span class="number">5</span>);</span><br></pre></td></tr></table></figure></li>
<li>分区器使用时注意事项<ul>
<li>当ReduceTask的数量设置 &gt; 实际用到的分区数 此时会生成空的分区文件</li>
<li>当ReduceTask的数量设置 &lt; 实际用到的分区数 此时会报错</li>
<li>当ReduceTask的数量设置 = 1 结果文件会输出到一个文件中，由以下源码可以论证：<ul>
<li>位置 NewOutputCollector#NewOutputCollector <figure class="highlight java"><table><tr><td class="code"><pre><span class="line">NewOutputCollector(org.apache.hadoop.mapreduce.JobContext jobContext,</span><br><span class="line">                    JobConf job,</span><br><span class="line">                    TaskUmbilicalProtocol umbilical,</span><br><span class="line">                    TaskReporter reporter</span><br><span class="line">                    ) <span class="keyword">throws</span> IOException, ClassNotFoundException &#123;</span><br><span class="line">    collector = createSortingCollector(job, reporter);</span><br><span class="line">    <span class="comment">// 获取当前ReduceTask的数量</span></span><br><span class="line">    partitions = jobContext.getNumReduceTasks();</span><br><span class="line">    <span class="comment">// 判断ReduceTask的数量 是否大于1，找指定分区器对象</span></span><br><span class="line">    <span class="keyword">if</span> (partitions &gt; <span class="number">1</span>) &#123;</span><br><span class="line">    partitioner = (org.apache.hadoop.mapreduce.Partitioner&lt;K,V&gt;)</span><br><span class="line">        ReflectionUtils.newInstance(jobContext.getPartitionerClass(), job);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="comment">// 执行默认的分区规则，最终返回一个唯一的0号分区</span></span><br><span class="line">        partitioner = <span class="keyword">new</span> org.apache.hadoop.mapreduce.Partitioner&lt;K,V&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getPartition</span><span class="params">(K key, V value, <span class="keyword">int</span> numPartitions)</span> </span>&#123;</span><br><span class="line">            <span class="keyword">return</span> partitions - <span class="number">1</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>分区编号生成的规则：根据指定的ReduceTask的数量 从0开始，依次累加。</li>
</ul>
</li>
</ol>
<h3 id="3-3-3-WritableComparable排序"><a href="#3-3-3-WritableComparable排序" class="headerlink" title="3.3.3 WritableComparable排序"></a>3.3.3 WritableComparable排序</h3><h4 id="3-3-3-1-排序概述"><a href="#3-3-3-1-排序概述" class="headerlink" title="3.3.3.1 排序概述"></a>3.3.3.1 排序概述</h4><ul>
<li>MapTask和ReduceTask均会对数据<font color ='red' >按照key进行排序</font>。该操作属于Hadoop的默认行为。任何应用程序中的数据均会被排序，而不管逻辑上是否需要。</li>
<li><font color ='red' >默认排序是按照字典顺序升序排序，且实现该排序的方法是快速排序</font>。</li>
<li>对于MapTask，它会将处理的结果暂时放到环形缓冲区中，当环形缓冲区使用率达到一定阈值后，再对缓冲区中的数据进行一次快速排序，并将这些有序数据溢写到磁盘上，而当数据处理完毕后，它会对磁盘上所有文件进行归并排序</li>
<li>对于ReduceTask，它从每个MapTask上远程拷贝相应的数据文件，如果文件大小超过一定阈值，则溢写磁盘上，否则存储在内存中。如果磁盘上文件数目达到一定阈值，则进行一次归并排序以生成一个更大文件；如果内存中文件大小或者数目超过一定阈值，则进行一次合并后将数据溢写到磁盘上。当所有数据拷贝完毕后，ReduceTask统一对内存和磁盘上的所有数据进行一次归并排序</li>
</ul>
<h4 id="3-3-3-2-排序分类"><a href="#3-3-3-2-排序分类" class="headerlink" title="3.3.3.2 排序分类"></a>3.3.3.2 排序分类</h4><ol>
<li>部分排序：分区内排序<ul>
<li>MapReduce根据输入记录的键对数据集排序。保证输出的每个文件内部有序</li>
</ul>
</li>
<li>全排序<ul>
<li>最终输出结果只有一个文件，且文件内部有序。实现方式是只设置一个ReduceTask。但该方法在处理大型文件时效率极低，因为一台机器处理所有文件，完全丧失了MapReduce所提供的并行架构</li>
</ul>
</li>
<li>辅助排序：GroupingComparator分组<ul>
<li>在Reduce端对key进行分组。应用于：在接收的key为bean对象时，想让一个或几个字段相同（全部字段比较不相同）的key进入到同一个reduce方法时，可以采用分组排序。</li>
</ul>
</li>
<li>二次排序<ul>
<li>在自定义排序过程中，如果compareTo中的判断条件为两个即为二次排序</li>
</ul>
</li>
</ol>
<h4 id="3-3-3-3-实现排序比较的方式"><a href="#3-3-3-3-实现排序比较的方式" class="headerlink" title="3.3.3.3 实现排序比较的方式"></a>3.3.3.3 实现排序比较的方式</h4><ol>
<li>直接让参与比较的对象实现WritableComparable 接口，并在该类中实现compareTo，在compareTo中定义自己的比较规则。这种情况 当运行的的时候Hadoop会自动生成比较器对象WritableComparator <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Data</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CompareFlowDTO</span> <span class="keyword">implements</span> <span class="title">WritableComparable</span>&lt;<span class="title">CompareFlowDTO</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> String mobile;</span><br><span class="line">    <span class="keyword">private</span> Long upFlow;</span><br><span class="line">    <span class="keyword">private</span> Long downFlow;</span><br><span class="line">    <span class="keyword">private</span> Long sumFlow;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(DataOutput out)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        out.writeUTF(mobile);</span><br><span class="line">        out.writeLong(upFlow);</span><br><span class="line">        out.writeLong(downFlow);</span><br><span class="line">        out.writeLong(sumFlow);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">readFields</span><span class="params">(DataInput in)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        mobile = in.readUTF();</span><br><span class="line">        upFlow = in.readLong();</span><br><span class="line">        downFlow = in.readLong();</span><br><span class="line">        sumFlow = in.readLong();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compareTo</span><span class="params">(CompareFlowDTO o)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> o.getSumFlow().compareTo(getSumFlow());</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> upFlow + <span class="string">&quot;\t&quot;</span> + downFlow + <span class="string">&quot;\t&quot;</span> + sumFlow;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>继承Hadoop提供的WritableComparator类，重写该类compare() 方法，在该方法中定义比较规则，注意在自定义的比较器对象中通过调用父类的super方法将自定义的比较器对象和要参与比较的对象进行关联。最后再Driver类中指定自定义的比较器对象。 <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CompareFlowDTOComparator</span> <span class="keyword">extends</span> <span class="title">WritableComparator</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">CompareFlowDTOComparator</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">super</span>(CompareFlowDTO.class, <span class="keyword">true</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compare</span><span class="params">(WritableComparable a, WritableComparable b)</span> </span>&#123;</span><br><span class="line">        CompareFlowDTO aCompareFlowDTO = (CompareFlowDTO) a;</span><br><span class="line">        CompareFlowDTO bCompareFlowDTO = (CompareFlowDTO) b;</span><br><span class="line">        <span class="keyword">return</span> aCompareFlowDTO.getSumFlow().compareTo(bCompareFlowDTO.getSumFlow());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>继承Hadoop提供的WritableComparator类，重写该类compare() 方法，在该方法中定义比较规则，注意在自定义的比较器对象中通过调用父类的super方法将自定义的比较器对象和要参与比较的对象进行关联。在比较对象的类定义中添加静态代码块，主动注册比较器 <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//主动注册</span></span><br><span class="line"><span class="keyword">static</span> &#123;</span><br><span class="line">    WritableComparator.define(CompareFlowDTO.class, <span class="keyword">new</span> CompareFlowDTOComparator());</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
</ol>
<h4 id="3-3-3-4-Hadoop中获取比较器对象的规则源码分析"><a href="#3-3-3-4-Hadoop中获取比较器对象的规则源码分析" class="headerlink" title="3.3.3.4 Hadoop中获取比较器对象的规则源码分析"></a>3.3.3.4 Hadoop中获取比较器对象的规则源码分析</h4><ul>
<li>入口 <code>org.apache.hadoop.mapred.MapTask.MapOutputBuffer#init</code></li>
<li>定位MapTask.java:1018  <figure class="highlight java"><table><tr><td class="code"><pre><span class="line">comparator = job.getOutputKeyComparator();</span><br></pre></td></tr></table></figure></li>
<li>跟进 org.apache.hadoop.mapred.JobConf#getOutputKeyComparator  <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> RawComparator <span class="title">getOutputKeyComparator</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="comment">// 配置文件查找对应配置JobContext.KEY_COMPARATOR-&gt;mapreduce.job.output.key.comparator.class </span></span><br><span class="line">  <span class="comment">// 如果配置存在且实现了Comparator接口，返回配置的比较器</span></span><br><span class="line">  <span class="comment">// 配置存在但是没有实现Comparator接口，抛出异常</span></span><br><span class="line">  <span class="comment">// 配置不存在，取默认值null</span></span><br><span class="line">  Class&lt;? extends RawComparator&gt; theClass = getClass(</span><br><span class="line">    JobContext.KEY_COMPARATOR, <span class="keyword">null</span>, RawComparator.class);</span><br><span class="line">  <span class="keyword">if</span> (theClass != <span class="keyword">null</span>)</span><br><span class="line">      <span class="comment">// 如果通过JobContext.KEY_COMPARATOR 获取到了 直接通过反射的形式实例化对象</span></span><br><span class="line">      <span class="keyword">return</span> ReflectionUtils.newInstance(theClass, <span class="keyword">this</span>);</span><br><span class="line">  <span class="comment">// 如果 JobContext.KEY_COMPARATOR 没获取到，就走一下流程获取参与排序的对象的比较器对象</span></span><br><span class="line">  <span class="comment">// 首先会检验 当前Map端输出的key是否实现WritableComparable接口</span></span><br><span class="line">  <span class="keyword">return</span> WritableComparator.get(getMapOutputKeyClass().asSubclass(WritableComparable.class), <span class="keyword">this</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>跟进WritableComparator#get  <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> WritableComparator <span class="title">get</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">      Class&lt;? extends WritableComparable&gt; c, Configuration conf)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 通过当前比较的对象的class类对象到comparators这个Map容器中获取比较器对象</span></span><br><span class="line">    <span class="comment">// 凡是在comparators能获取到的比较器对象，那当前参与比较的对象一定Hadoop自身的数据类型。</span></span><br><span class="line">    WritableComparator comparator = comparators.get(c);</span><br><span class="line">    <span class="keyword">if</span> (comparator == <span class="keyword">null</span>) &#123;</span><br><span class="line">      <span class="comment">// 考虑到加载的类可能遇到内存的一些错误，导致GC,所以再强制加载一次 已过时</span></span><br><span class="line">      forceInit(c);</span><br><span class="line">      <span class="comment">// 强制加载后再获取</span></span><br><span class="line">      comparator = comparators.get(c);</span><br><span class="line">      <span class="comment">// 如果还没有获取到，那当前参与比较的对象就不是Hadoop自身的数据类型</span></span><br><span class="line">      <span class="keyword">if</span> (comparator == <span class="keyword">null</span>) &#123;</span><br><span class="line">        <span class="comment">//如果到这还没获取到，那就是我们自定义的数据类型，此时Hadoop创建一个比较器</span></span><br><span class="line">        comparator = <span class="keyword">new</span> WritableComparator(c, conf, <span class="keyword">true</span>);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// Newly passed Configuration objects should be used.</span></span><br><span class="line">    ReflectionUtils.setConf(comparator, conf);</span><br><span class="line">    <span class="keyword">return</span> comparator;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<h4 id="3-3-3-5-Hadoop中获取自身数据类型的比较器对象源码分析"><a href="#3-3-3-5-Hadoop中获取自身数据类型的比较器对象源码分析" class="headerlink" title="3.3.3.5 Hadoop中获取自身数据类型的比较器对象源码分析"></a>3.3.3.5 Hadoop中获取自身数据类型的比较器对象源码分析</h4></li>
<li>以org.apache.hadoop.io.Text为例</li>
<li>实现了org.apache.hadoop.io.WritableComparable接口</li>
<li>在Text本类中已经声明了比较器对象 并且做了关联  <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">Comparator</span> <span class="keyword">extends</span> <span class="title">WritableComparator</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="title">Comparator</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    #关联比较器和比较对象</span><br><span class="line">    <span class="keyword">super</span>(Text.class);</span><br><span class="line">  &#125;</span><br><span class="line">  #具体比较实现</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compare</span><span class="params">(<span class="keyword">byte</span>[] b1, <span class="keyword">int</span> s1, <span class="keyword">int</span> l1,</span></span></span><br><span class="line"><span class="params"><span class="function">                     <span class="keyword">byte</span>[] b2, <span class="keyword">int</span> s2, <span class="keyword">int</span> l2)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> n1 = WritableUtils.decodeVIntSize(b1[s1]);</span><br><span class="line">    <span class="keyword">int</span> n2 = WritableUtils.decodeVIntSize(b2[s2]);</span><br><span class="line">    <span class="keyword">return</span> compareBytes(b1, s1+n1, l1-n1, b2, s2+n2, l2-n2);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>注册Comparator  <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">static</span> &#123;</span><br><span class="line">  <span class="comment">// register this comparator</span></span><br><span class="line">  WritableComparator.define(Text.class, <span class="keyword">new</span> Comparator());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>跟进 define()方法   <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">define</span><span class="params">(Class c, WritableComparator comparator)</span> </span>&#123;</span><br><span class="line">  comparators.put(c, comparator);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>破案：当Text类加载的时候，会将当前Text.class 做为key  它的比较器对象作为value会放入comparators Map容器中。</li>
</ul>
<hr>
<h3 id="3-3-4-Combiner流程"><a href="#3-3-4-Combiner流程" class="headerlink" title="3.3.4 Combiner流程"></a>3.3.4 Combiner流程</h3><ol>
<li>Combiner组件的父类就是Reducer。</li>
<li>Combiner和Reducer的区别在于运行的位置<ol>
<li>Combiner是在每一个MapTask所在的节点运行</li>
<li>Reducer是接收全局所有Mapper的输出结果</li>
</ol>
</li>
<li>Combiner的使用场景：总的来说，为了提升MR程序的运行效率，为了减轻ReduceTask的压力，另外减少IO的开销。</li>
<li>使用Combiner<ol>
<li>自定一个Combiner类 继承Hadoop提供的Reducer</li>
<li>在Job中指定自定义的Combiner类</li>
<li>Combiner的输出kv应该跟Reducer的输入kv类型要对应起来 </li>
</ol>
</li>
<li>Combiner能够应用的前提是不能影响最终的业务逻辑</li>
<li>Combiner不适用的场景：Reduce端处理的数据考虑到多个MapTask的数据的整体集时就不能提前合并了。</li>
<li>示例 <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCountCombiner</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">IntWritable</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> IntWritable valueOut = <span class="keyword">new</span> IntWritable();</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values, Reducer&lt;Text, IntWritable, Text, IntWritable&gt;.Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        String word = key.toString();</span><br><span class="line">        <span class="keyword">int</span> sum = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">while</span> (values.iterator().hasNext()) &#123;</span><br><span class="line">            sum += values.iterator().next().get();</span><br><span class="line">        &#125;</span><br><span class="line">        valueOut.set(sum);</span><br><span class="line">        log.info(<span class="string">&quot;combine-word: &#123;&#125; 累计出现次数:&#123;&#125;&quot;</span>, word, sum);</span><br><span class="line">        context.write(key, valueOut);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
</ol>
<hr>
<h2 id="3-5-OutputFormat"><a href="#3-5-OutputFormat" class="headerlink" title="3.5 OutputFormat"></a>3.5 OutputFormat</h2><p>OutputFormat主要负责最终数据的写出</p>
<h3 id="3-5-1-OutputFormat实现类"><a href="#3-5-1-OutputFormat实现类" class="headerlink" title="3.5.1 OutputFormat实现类"></a>3.5.1 OutputFormat实现类</h3><ol>
<li>探索OutputFormat的默认实现<ul>
<li>OutputFormat的实现的功能中有一个检验输出路径的方法org.apache.hadoop.mapreduce.OutputFormat#checkOutputSpecs</li>
<li>考虑到检验输出路径应该在Job提交流程中完成(不设置会报错)  <figure class="highlight java"><table><tr><td class="code"><pre><span class="line">   org.apache.hadoop.mapred.InvalidJobConfException: Output directory not set.</span><br><span class="line">at org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:<span class="number">156</span>) ~[hadoop-mapreduce-client-core-<span class="number">3.1</span><span class="number">.3</span>.jar:?]</span><br><span class="line">at org.apache.hadoop.mapreduce.JobSubmitter.checkSpecs(JobSubmitter.java:<span class="number">277</span>) ~[hadoop-mapreduce-client-core-<span class="number">3.1</span><span class="number">.3</span>.jar:?]</span><br><span class="line">at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:<span class="number">143</span>) ~[hadoop-mapreduce-client-core-<span class="number">3.1</span><span class="number">.3</span>.jar:?]</span><br><span class="line">at org.apache.hadoop.mapreduce.Job$<span class="number">11.</span>run(Job.java:<span class="number">1570</span>) ~[hadoop-mapreduce-client-core-<span class="number">3.1</span><span class="number">.3</span>.jar:?]</span><br><span class="line">at org.apache.hadoop.mapreduce.Job$<span class="number">11.</span>run(Job.java:<span class="number">1567</span>) ~[hadoop-mapreduce-client-core-<span class="number">3.1</span><span class="number">.3</span>.jar:?]</span><br><span class="line">at java.security.AccessController.doPrivileged(Native Method) ~[?:<span class="number">1.8</span><span class="number">.0_291</span>]</span><br><span class="line">at javax.security.auth.Subject.doAs(Subject.java:<span class="number">422</span>) ~[?:<span class="number">1.8</span><span class="number">.0_291</span>]</span><br><span class="line">at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:<span class="number">1729</span>) ~[hadoop-common-<span class="number">3.1</span><span class="number">.3</span>.jar:?]</span><br><span class="line">at org.apache.hadoop.mapreduce.Job.submit(Job.java:<span class="number">1567</span>) ~[hadoop-mapreduce-client-core-<span class="number">3.1</span><span class="number">.3</span>.jar:?]</span><br><span class="line">at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:<span class="number">1588</span>) ~[hadoop-mapreduce-client-core-<span class="number">3.1</span><span class="number">.3</span>.jar:?]</span><br><span class="line">at sgg.hadoop.mapreduce.combiner.WordCountCombinerDriver.main(WordCountCombinerDriver.java:<span class="number">53</span>) [classes/:?]</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
<li>跟进Job提交流程org.apache.hadoop.mapreduce.Job#waitForCompletion</li>
<li>跟进org.apache.hadoop.mapreduce.Job#submit</li>
<li>跟进org.apache.hadoop.mapreduce.JobSubmitter#submitJobInternal</li>
<li>跟进org.apache.hadoop.mapreduce.JobSubmitter#checkSpecs  <figure class="highlight java"><table><tr><td class="code"><pre><span class="line">  <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">checkSpecs</span><span class="params">(Job job)</span> <span class="keyword">throws</span> ClassNotFoundException, </span></span><br><span class="line"><span class="function">    InterruptedException, IOException </span>&#123;</span><br><span class="line">      JobConf jConf = (JobConf)job.getConfiguration();</span><br><span class="line">      <span class="comment">// Check the output specification</span></span><br><span class="line">      <span class="keyword">if</span> (jConf.getNumReduceTasks() == <span class="number">0</span> ? </span><br><span class="line">          jConf.getUseNewMapper() : jConf.getUseNewReducer()) &#123;</span><br><span class="line">          <span class="comment">//获取OutputFormat</span></span><br><span class="line">        org.apache.hadoop.mapreduce.OutputFormat&lt;?, ?&gt; output =</span><br><span class="line">          ReflectionUtils.newInstance(job.getOutputFormatClass(),</span><br><span class="line">            job.getConfiguration());</span><br><span class="line">        output.checkOutputSpecs(job);</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        jConf.getOutputFormat().checkOutputSpecs(jtFs, jConf);</span><br><span class="line">      &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>跟进org.apache.hadoop.mapreduce.task.JobContextImpl#getOutputFormatClass  <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> Class&lt;? extends OutputFormat&lt;?,?&gt;&gt; getOutputFormatClass() </span><br><span class="line">     <span class="keyword">throws</span> ClassNotFoundException &#123;</span><br><span class="line">    <span class="keyword">return</span> (Class&lt;? extends OutputFormat&lt;?,?&gt;&gt;) </span><br><span class="line">      conf.getClass(OUTPUT_FORMAT_CLASS_ATTR, TextOutputFormat.class);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>破案：OutputFormat默认实现就是TextOutputFormat</li>
</ul>
</li>
<li>OutputFormat 类的体系结构<br> <img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16342996424249.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"><ul>
<li>FileOutputFormat 是 OutputFormat的子类（实现类）<ul>
<li>对 checkOutputSpecs() 做了具体的实现</li>
</ul>
</li>
<li>TextOutputFormat 是 FileOutputFormat的子类<ul>
<li>对 getRecordWriter 做了具体实现</li>
</ul>
</li>
</ul>
</li>
</ol>
<h3 id="3-5-2-OutputFormat的使用场景"><a href="#3-5-2-OutputFormat的使用场景" class="headerlink" title="3.5.2 OutputFormat的使用场景"></a>3.5.2 OutputFormat的使用场景</h3><ul>
<li>当我们对MR最终的结果有个性化制定的需求，就可以通过自定义OutputFormat来实现</li>
</ul>
<h3 id="3-5-3-自定义OutputFormat"><a href="#3-5-3-自定义OutputFormat" class="headerlink" title="3.5.3 自定义OutputFormat"></a>3.5.3 自定义OutputFormat</h3><ul>
<li>自定一个 OutputFormat 类，继承Hadoop提供的OutputFormat，在该类中实现getRecordWriter() ,返回一个RecordWriter</li>
<li>自定义一个 RecordWriter 并且继承Hadoop提供的RecordWriter类，在该类中重写 write()  和 close()  在这些方法中完成自定义输出。</li>
<li>示例  <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCountOutputFormat</span> <span class="keyword">extends</span> <span class="title">FileOutputFormat</span>&lt;<span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> RecordWriter&lt;Text, IntWritable&gt; <span class="title">getRecordWriter</span><span class="params">(TaskAttemptContext job)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> WordCountRecordWriter(job);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCountRecordWriter</span> <span class="keyword">extends</span> <span class="title">RecordWriter</span>&lt;<span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> FileSystem fileSystem;</span><br><span class="line">    HashMap&lt;Integer, FSDataOutputStream&gt; fsDataOutputStreamHashMap = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">WordCountRecordWriter</span><span class="params">(TaskAttemptContext job)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            fileSystem = FileSystem.get(job.getConfiguration());</span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">            log.error(<span class="string">&quot;WordCountRecordWriter创建失败&quot;</span>, e);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(Text key, IntWritable value)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        String word = key.toString();</span><br><span class="line">        Integer first = word.length();</span><br><span class="line">        FSDataOutputStream fsDataOutputStream = fsDataOutputStreamHashMap.get(first);</span><br><span class="line">        <span class="keyword">if</span> (fsDataOutputStream == <span class="keyword">null</span>) &#123;</span><br><span class="line">            fsDataOutputStream = fileSystem.create(<span class="keyword">new</span> Path(<span class="string">&quot;/Users/zhenan/atguigu/project/sgg-big-data/sgg-hadoop/sgg-hadoop-mapreduce/src/main/resources/outputformat/&quot;</span> + first + <span class="string">&quot;.txt&quot;</span>));</span><br><span class="line">            fsDataOutputStreamHashMap.put(first, fsDataOutputStream);</span><br><span class="line">        &#125;</span><br><span class="line">        fsDataOutputStream.write((word + <span class="string">&quot;\t&quot;</span> + value.get()+<span class="string">&quot;\n&quot;</span>).getBytes());</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">(TaskAttemptContext context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        IOUtils.closeStream(fileSystem);</span><br><span class="line">        <span class="keyword">for</span> (Map.Entry&lt;Integer, FSDataOutputStream&gt; entry : fsDataOutputStreamHashMap.entrySet()) &#123;</span><br><span class="line">            IOUtils.closeStream(entry.getValue());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul>
<hr>
<h2 id="3-6-Join多种应用"><a href="#3-6-Join多种应用" class="headerlink" title="3.6 Join多种应用"></a>3.6 Join多种应用</h2><h3 id="3-6-1-Reduce-Join"><a href="#3-6-1-Reduce-Join" class="headerlink" title="3.6.1 Reduce Join"></a>3.6.1 Reduce Join</h3><ol>
<li>概念：在MR程序中计算数据的时候，出现输入文件是多个且文件之间存在关联性，需要在计算过程中通过两个文件之间相互关联才能获取最终的计算结果。</li>
<li>ReduceJoin的思想：<ol>
<li>分析文件之间的关系，然后定位关联字段</li>
<li>在Map阶段对多个文件进行数据整合，并且让关联字段作为输出数据的key </li>
<li>当一组相同key的values进入Reduce阶段的reduce方法中第一步：先把两个文件数据分离出来，分别放到各自的对象中维护。</li>
<li>把当前一组维护好的数据进行关联操作，得到想要的数据结果。</li>
</ol>
</li>
</ol>
<h3 id="3-6-2-Map-Join"><a href="#3-6-2-Map-Join" class="headerlink" title="3.6.2 Map Join"></a>3.6.2 Map Join</h3><ol>
<li>概念：考虑MR整体的执行效率，且业务场景是一个大文件和一个小文件进行关联操作，可以使用MapJoin来实现。另外MapJoin也是解决ReduceJoin数据倾斜问题很有效的办法。</li>
<li>MapJoin的思想：<ol>
<li>分析文件之间的关系，然后定位关联字段</li>
<li>将小文件的数据映射到内存中的一个容器维护起来。 </li>
<li>当MapTask处理大文件的数据时，每读取一行数据，就根据当前行中的关联字段到内存的容器里获取对象的信息。</li>
<li>封装结果将其输出</li>
</ol>
</li>
<li>具体办法：采用DistributedCache<ol>
<li>在Mapper的setup阶段，将文件读取到缓存集合中。</li>
<li>在Driver驱动类中加载缓存 <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//缓存普通文件到Task运行节点。</span></span><br><span class="line">job.addCacheFile(<span class="keyword">new</span> URI(<span class="string">&quot;file:///e:/cache/pd.txt&quot;</span>));</span><br><span class="line"><span class="comment">//如果是集群运行,需要设置HDFS路径</span></span><br><span class="line">job.addCacheFile(<span class="keyword">new</span> URI(<span class="string">&quot;hdfs://hadoop102:9820/cache/pd.txt&quot;</span>));</span><br></pre></td></tr></table></figure></li>
</ol>
</li>
</ol>
<hr>
<h2 id="3-7-计数器"><a href="#3-7-计数器" class="headerlink" title="3.7 计数器"></a>3.7 计数器</h2><ul>
<li>Hadoop为每个作业维护若干内置计数器，以描述多项指标。例如，某些计数器记录已处理的字节数和记录数，使用户可监控已处理的输入数据量和已产生的输出数据量</li>
<li>计数器API<ol>
<li>采用枚举的方式统计计数</li>
<li>采用计数器组、计数器名称的方式统计</li>
<li>计数结果在程序运行后的控制台上查看</li>
</ol>
</li>
</ul>
<hr>
<h2 id="3-8-数据清洗（ETL）"><a href="#3-8-数据清洗（ETL）" class="headerlink" title="3.8 数据清洗（ETL）"></a>3.8 数据清洗（ETL）</h2><ul>
<li>在运行核心业务MapReduce程序之前，往往要先对数据进行清洗，清理掉不符合用户要求的数据。清理的过程往往只需要运行Mapper程序，不需要运行Reduce程序。</li>
<li>Mapper程序不符合规则直接return</li>
</ul>
<hr>
<h2 id="3-9-MapReduce工作流程梳理"><a href="#3-9-MapReduce工作流程梳理" class="headerlink" title="3.9 MapReduce工作流程梳理"></a>3.9 MapReduce工作流程梳理</h2><p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16344606281489.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"><br><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16344606667460.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
<ol>
<li>MapTask收集map()方法输出的kv对，放到内存缓冲区中</li>
<li>从内存缓冲区不断溢写本地磁盘文件，可能会溢写多个文件</li>
<li>多个溢出文件会被合并成大的溢写文件</li>
<li>在溢写过程及合并的过程中，都要调用Partitioner进行分区和针对key进行排序</li>
<li>ReduceTask根据自己的分区号，去各个MapTask机器上取相应的结果分区数据</li>
<li>ReduceTask会抓取到同一个分区的来自不同MapTask的结果文件，ReduceTask会将这些文件再进行合并（归并排序）</li>
<li>合并成大文件后，Shuffle的过程也就结束了，后面进入ReduceTask的逻辑运算过程（从文件中取出一个一个的键值对Group，调用用户自定义的reduce()方法）</li>
<li>注意：<ol>
<li>Shuffle中的缓冲区大小会影响到MapReduce程序的执行效率，原则上说，缓冲区越大，磁盘io的次数越少，执行速度就越快。</li>
<li>缓冲区的大小可以通过参数调整，参数：mapreduce.task.io.sort.mb默认100M。</li>
</ol>
</li>
<li>MapTask源码分析<ul>
<li>定位 map方法输出结果的位置，TaskInputOutputContext#write<ul>
<li>实现类TaskInputOutputContextImpl#write</li>
</ul>
</li>
<li>跟进org.apache.hadoop.mapred.MapTask.NewOutputCollector#write<ul>
<li>获取分区编号org.apache.hadoop.mapreduce.Partitioner#getPartition</li>
<li>k-v放入环形缓冲区org.apache.hadoop.mapred.MapOutputCollector#collect</li>
<li>map端所有的kv全部写出后会执行org.apache.hadoop.mapred.MapTask.NewOutputCollector#close<ul>
<li>执行溢写org.apache.hadoop.mapred.MapTask.MapOutputBuffer#flush </li>
<li>排序溢写org.apache.hadoop.mapred.MapTask.MapOutputBuffer#sortAndSpill<ul>
<li>执行combiner org.apache.hadoop.mapred.Task.CombinerRunner#combine</li>
<li>初始化combiner org.apache.hadoop.mapred.Task.CombinerRunner#create </li>
</ul>
</li>
<li>合并文件org.apache.hadoop.mapred.MapTask.MapOutputBuffer#mergeParts</li>
<li>结束</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>ReduceTask源码分析<ul>
<li>ReduceTask执行入口org.apache.hadoop.mapred.ReduceTask#run<ul>
<li>初始化ReduceTask org.apache.hadoop.mapred.Task#initialize<ul>
<li>获取OutputFormat org.apache.hadoop.mapred.Task:605</li>
<li>获取Shuffer Consumer  <figure class="highlight java"><table><tr><td class="code"><pre><span class="line">  Class&lt;? extends ShuffleConsumerPlugin&gt; clazz =</span><br><span class="line">job.getClass(MRConfig.SHUFFLE_CONSUMER_PLUGIN, Shuffle.class, ShuffleConsumerPlugin.class);</span><br></pre></td></tr></table></figure></li>
<li>初始化shuffle consumer org.apache.hadoop.mapred.ShuffleConsumerPlugin#init<ul>
<li>创建shuffle实现类org.apache.hadoop.mapreduce.task.reduce.ShuffleSchedulerImpl<ul>
<li>获取MapTask数量 <code>totalMaps = job.getNumMapTasks();</code></li>
</ul>
</li>
<li>创建合并管理器org.apache.hadoop.mapreduce.task.reduce.Shuffle#createMergeManager<ul>
<li>实现类 org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl#MergeManagerImpl<ul>
<li>ReduceTask内存最大值  <figure class="highlight java"><table><tr><td class="code"><pre><span class="line">    <span class="comment">// Allow unit tests to fix Runtime memory</span></span><br><span class="line"><span class="keyword">this</span>.memoryLimit = (<span class="keyword">long</span>)(jobConf.getLong(</span><br><span class="line">    MRJobConfig.REDUCE_MEMORY_TOTAL_BYTES,</span><br><span class="line">    Runtime.getRuntime().maxMemory()) * maxInMemCopyUse);</span><br></pre></td></tr></table></figure></li>
<li>创建内存合并器 org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl#createInMemoryMerger<ul>
<li>实现类 org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl.InMemoryMerger</li>
<li>合并方法org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl.InMemoryMerger#merge</li>
<li>Combiner + 溢写org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl#combineAndSpill</li>
</ul>
</li>
<li>创建磁盘合并器 org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl.OnDiskMerger</li>
</ul>
</li>
</ul>
</li>
<li>开始抓取数据org.apache.hadoop.mapreduce.task.reduce.Shuffle:107  <code>eventFetcher.start();</code></li>
<li>抓取数据结束org.apache.hadoop.mapreduce.task.reduce.Shuffle:141 <code>eventFetcher.shutDown();</code></li>
<li>copy阶段完成，启动下一个阶段sort org.apache.hadoop.mapreduce.task.reduce.Shuffle:151 <code>// copyPhase.complete();</code></li>
<li>标记进入sort阶段 org.apache.hadoop.mapreduce.task.reduce.Shuffle:152 <code>taskStatus.setPhase(TaskStatus.Phase.SORT);</code></li>
</ul>
</li>
<li>sort阶段完成 开启下一阶段reduce org.apache.hadoop.mapred.ReduceTask:382 <code>sortPhase.complete();</code></li>
</ul>
</li>
<li>reduce();  //reduce阶段调用的就是我们自定义的reduce方法，会被调用多次</li>
<li>cleanup(context); //reduce完成之前，会最后调用一次Reducer里面的cleanup方法</li>
</ul>
</li>
</ul>
</li>
</ol>
<h3 id="3-9-1分析Job提交流程的源码"><a href="#3-9-1分析Job提交流程的源码" class="headerlink" title="3.9.1分析Job提交流程的源码"></a>3.9.1分析Job提交流程的源码</h3><ul>
<li>定位提交入口 org.apache.hadoop.mapreduce.Job#waitForCompletion</li>
<li>跟进 <code>org.apache.hadoop.mapreduce.Job#waitForCompletion</code>  <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">waitForCompletion</span><span class="params">(<span class="keyword">boolean</span> verbose</span></span></span><br><span class="line"><span class="params"><span class="function">                              )</span> <span class="keyword">throws</span> IOException, InterruptedException, ClassNotFoundException </span>&#123;</span><br><span class="line">  <span class="comment">// 判断当前Job的状态是否为定义阶段</span></span><br><span class="line">  <span class="keyword">if</span> (state == JobState.DEFINE) &#123;</span><br><span class="line">    <span class="comment">//提交方法</span></span><br><span class="line">    submit();</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span> (verbose) &#123;</span><br><span class="line">    monitorAndPrintJob();</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="comment">// get the completion poll interval from the client.</span></span><br><span class="line">    <span class="keyword">int</span> completionPollIntervalMillis = </span><br><span class="line">      Job.getCompletionPollInterval(cluster.getConf());</span><br><span class="line">    <span class="keyword">while</span> (!isComplete()) &#123;</span><br><span class="line">          <span class="keyword">try</span> &#123;</span><br><span class="line">            Thread.sleep(completionPollIntervalMillis);</span><br><span class="line">          &#125; <span class="keyword">catch</span> (InterruptedException ie) &#123;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> isSuccessful();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>跟进<code>org.apache.hadoop.mapreduce.Job#submit</code>  <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">submit</span><span class="params">()</span> </span></span><br><span class="line"><span class="function">       <span class="keyword">throws</span> IOException, InterruptedException, ClassNotFoundException </span>&#123;</span><br><span class="line">  <span class="comment">//判断当前为定义阶段</span></span><br><span class="line">  ensureState(JobState.DEFINE);</span><br><span class="line">  <span class="comment">//兼容老版本API</span></span><br><span class="line">  setUseNewAPI();</span><br><span class="line">  <span class="comment">//连接集群（如果是本地模式结果就是LocalRunner, 如果Yarn集群结果就是YARNRuuner）</span></span><br><span class="line">  connect();</span><br><span class="line">  <span class="comment">// 开始提交Job</span></span><br><span class="line">  <span class="keyword">final</span> JobSubmitter submitter = </span><br><span class="line">      getJobSubmitter(cluster.getFileSystem(), cluster.getClient());</span><br><span class="line">  status = ugi.doAs(<span class="keyword">new</span> PrivilegedExceptionAction&lt;JobStatus&gt;() &#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> JobStatus <span class="title">run</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException, </span></span><br><span class="line"><span class="function">    ClassNotFoundException </span>&#123;</span><br><span class="line">      <span class="comment">//执行提交动作</span></span><br><span class="line">      <span class="keyword">return</span> submitter.submitJobInternal(Job.<span class="keyword">this</span>, cluster);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;);</span><br><span class="line">  state = JobState.RUNNING;</span><br><span class="line">  LOG.info(<span class="string">&quot;The url to track the job: &quot;</span> + getTrackingURL());</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure></li>
<li>跟进<code>org.apache.hadoop.mapreduce.JobSubmitter#submitJobInternal</code>  <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Internal method for submitting jobs to the system.</span></span><br><span class="line"><span class="comment"> * </span></span><br><span class="line"><span class="comment"> * &lt;p&gt;The job submission process involves:</span></span><br><span class="line"><span class="comment"> * &lt;ol&gt;</span></span><br><span class="line"><span class="comment"> *   &lt;li&gt;</span></span><br><span class="line"><span class="comment"> *   检测输入输出路径的合法性</span></span><br><span class="line"><span class="comment"> *   Checking the input and output specifications of the job.</span></span><br><span class="line"><span class="comment"> *   &lt;/li&gt;</span></span><br><span class="line"><span class="comment"> *   &lt;li&gt;</span></span><br><span class="line"><span class="comment"> *   给当前Job计算切片信息</span></span><br><span class="line"><span class="comment"> *   Computing the &#123;<span class="doctag">@link</span> InputSplit&#125;s for the job.</span></span><br><span class="line"><span class="comment"> *   &lt;/li&gt;</span></span><br><span class="line"><span class="comment"> *   &lt;li&gt;</span></span><br><span class="line"><span class="comment"> *   添加分布式缓存文件</span></span><br><span class="line"><span class="comment"> *   Setup the requisite accounting information for the </span></span><br><span class="line"><span class="comment"> *   &#123;<span class="doctag">@link</span> DistributedCache&#125; of the job, if necessary.</span></span><br><span class="line"><span class="comment"> *   &lt;/li&gt;</span></span><br><span class="line"><span class="comment"> *   &lt;li&gt;</span></span><br><span class="line"><span class="comment"> *   将必要的内容都拷贝到 job执行的临时目录（jar包、切片信息、配置文件）</span></span><br><span class="line"><span class="comment"> *   Copying the job&#x27;s jar and configuration to the map-reduce system</span></span><br><span class="line"><span class="comment"> *   directory on the distributed file-system. </span></span><br><span class="line"><span class="comment"> *   &lt;/li&gt;</span></span><br><span class="line"><span class="comment"> *   &lt;li&gt;</span></span><br><span class="line"><span class="comment"> *   提交Job</span></span><br><span class="line"><span class="comment"> *   Submitting the job to the &lt;code&gt;JobTracker&lt;/code&gt; and optionally</span></span><br><span class="line"><span class="comment"> *   monitoring it&#x27;s status.</span></span><br><span class="line"><span class="comment"> *   &lt;/li&gt;</span></span><br><span class="line"><span class="comment"> * &lt;/ol&gt;&lt;/p&gt;</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> job the configuration to submit</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> cluster the handle to the Cluster</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@throws</span> ClassNotFoundException</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@throws</span> InterruptedException</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@throws</span> IOException</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function">JobStatus <span class="title">submitJobInternal</span><span class="params">(Job job, Cluster cluster)</span> </span></span><br><span class="line"><span class="function"><span class="keyword">throws</span> ClassNotFoundException, InterruptedException, IOException </span>&#123;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul>
<hr>
<h2 id="3-8-MapReduce开发总结"><a href="#3-8-MapReduce开发总结" class="headerlink" title="3.8 MapReduce开发总结"></a>3.8 MapReduce开发总结</h2><ol>
<li>输入数据接口：InputFormat<ul>
<li>默认使用的实现类是：TextInputFormat</li>
<li>TextInputFormat 的功能逻辑是：一次读一行文本，然后将该行的起始偏移量作为key，行内容作为 value 返回。</li>
<li>CombineTextInputFormat 可以把多个小文件合并成一个切片处理，提高处理效率。</li>
</ul>
</li>
<li>map逻辑处理接口：Mapper <ul>
<li>用户根据业务需求实现其中三个方法：map() setup() cleanup () </li>
</ul>
</li>
<li>Partitioner 分区<ul>
<li>有默认实现 HashPartitioner，逻辑是根据 key 的哈希值和 numReduces 来返回一个分区号；key.hashCode()&amp;Integer.MAXVALUE % numReduces</li>
<li>如果业务上有特别的需求，可以自定义分区。</li>
</ul>
</li>
<li>Comparable 排序<ul>
<li>当我们用自定义的对象作为 key 来输出时，就必须要实现 WritableComparable 接口，重写其中的 compareTo()方法。</li>
<li>部分排序：对最终输出的每一个文件进行内部排序。</li>
<li>全排序：对所有数据进行排序，通常只有一个 Reduce。 （4）二次排序：排序的条件有两个。</li>
</ul>
</li>
<li>Combiner 合并<ul>
<li>Combiner 合并可以提高程序执行效率，减少 IO 传输。但是使用时必须不能影响原有的业务处理结果。</li>
</ul>
</li>
<li>reduce逻辑处理接口：Reducer<ul>
<li>用户根据业务需求实现其中三个方法：reduce() setup() cleanup () </li>
</ul>
</li>
<li>输出数据接口：OutputFormat<ul>
<li>默认实现类是 TextOutputFormat，功能逻辑是：将每一个 KV 对，向目标文本文件输出一行。</li>
<li>用户还可以自定义 OutputFormat。</li>
</ul>
</li>
</ol>
<h2 id="问答"><a href="#问答" class="headerlink" title="问答"></a>问答</h2><h4 id="一、描述一下手写MR的大概流程和规范"><a href="#一、描述一下手写MR的大概流程和规范" class="headerlink" title="一、描述一下手写MR的大概流程和规范"></a>一、描述一下手写MR的大概流程和规范</h4><ol>
<li>继承Mapper重写map方法</li>
<li>继承Reducer重写reduce方法 </li>
<li>编写Driver配置Job参数</li>
<li>提交Job</li>
</ol>
<h4 id="二、如何实现Hadoop中的序列化，以及Hadoop的序列化和Java的序列化有什么区别？"><a href="#二、如何实现Hadoop中的序列化，以及Hadoop的序列化和Java的序列化有什么区别？" class="headerlink" title="二、如何实现Hadoop中的序列化，以及Hadoop的序列化和Java的序列化有什么区别？"></a>二、如何实现Hadoop中的序列化，以及Hadoop的序列化和Java的序列化有什么区别？</h4><ol>
<li>实现Writeable接口</li>
<li>无参构造</li>
<li>重写序列化方法write</li>
<li>重写反序列化方法readFields</li>
<li>write 方法和readFields方法保持一致</li>
</ol>
<h4 id="三、概述一下MR程序的执行流程"><a href="#三、概述一下MR程序的执行流程" class="headerlink" title="三、概述一下MR程序的执行流程"></a>三、概述一下MR程序的执行流程</h4><ol>
<li>数据读取阶段：InputFormat进行切片读取</li>
<li>map阶段：执行map方法业务逻辑，输出处理后的kv数据</li>
<li>shuffle阶段：对map阶段输出的kv进行分区，排序，分组，通知reduce取数据</li>
<li>reduce阶段：执行reduce方法业务逻辑，输出数据</li>
<li>输出阶段：OutputFormat处理输出数据，写入文件</li>
</ol>
<h4 id="四、InputFormat负责数据写的时候要进行切片，为什么切片大小默认是128M"><a href="#四、InputFormat负责数据写的时候要进行切片，为什么切片大小默认是128M" class="headerlink" title="四、InputFormat负责数据写的时候要进行切片，为什么切片大小默认是128M"></a>四、InputFormat负责数据写的时候要进行切片，为什么切片大小默认是128M</h4><ol>
<li>HDFS默认的Block大小为128M</li>
<li>默认128M切片可以从单个数据块读取到全部数据</li>
<li>避免了跨机器读取导致大量IO</li>
</ol>
<h4 id="五、描述一下切片的逻辑（从源码角度描述）"><a href="#五、描述一下切片的逻辑（从源码角度描述）" class="headerlink" title="五、描述一下切片的逻辑（从源码角度描述）"></a>五、描述一下切片的逻辑（从源码角度描述）</h4><ol>
<li>定位入口InputFormat#getSplits</li>
<li>由FileInputFormat#getSplits具体实现</li>
<li>确定最小切片大小默认1，最大切片大小默认Long.MAX_VALUE</li>
<li>获取是否对输入路径递归执行的参数默认false，递归处理输入路径下的所有文件</li>
<li>判断是否能够切分，压缩文件不进行切分</li>
<li>获取文件大小和块大小</li>
<li>计算切片大小max(最小切块大小，min(块大小，最大切块大小))</li>
<li>判断剩余文件大小是否可以继续切分，大于1.1倍的切片大小则继续切分</li>
</ol>
<h4 id="六、CombineTextInputFormat机制是怎么实现的"><a href="#六、CombineTextInputFormat机制是怎么实现的" class="headerlink" title="六、CombineTextInputFormat机制是怎么实现的"></a>六、CombineTextInputFormat机制是怎么实现的</h4><ol>
<li>CombineTextInputFormat默认切片大小为4m</li>
<li>虚拟切片过程：文件和切片大小进行比较<ol>
<li>当前文件&gt;切片大小 且 小于2倍的切片大小，就切成2片</li>
<li>当前文件&gt;大于2倍的切片大小，直接切出切片大小的文件，重复执行虚拟切片过程</li>
</ol>
</li>
<li>实际切片过程：比较虚拟切片的结果文件大小和设置切片大小<ol>
<li>如果大于等于切片大小就单独行程一个切片</li>
<li>如果小于设置切片大小就和下一个虚拟文件进行合并，重复执行至大于切片大小</li>
<li>合并后大于设置切片大小单独就形成一个切片</li>
</ol>
</li>
</ol>
<h4 id="七、阐述一下-Shuffle机制-流程？"><a href="#七、阐述一下-Shuffle机制-流程？" class="headerlink" title="七、阐述一下 Shuffle机制 流程？"></a>七、阐述一下 Shuffle机制 流程？</h4><ol>
<li>Shuffle机制处于Map过程和Reduce过程的中间阶段</li>
<li>具体实现的功能包括，分区，分组，排序，合并</li>
<li>map端的Shuffle<ol>
<li>partition: 获取分区编号保存到元数据中，数据写入环形缓冲区</li>
<li>spill: 环形缓冲区默认100M，到达80%时触发spill溢写，剩余20%继续执行写入</li>
<li>sort: spill溢写过程根据分区编号，Key比较规则进行排序（升序，快排），溢写文件保证分区内有序</li>
<li>combine: 触发spill进行sort之后，写入文件之前会进行combine操作；溢写文件大于3个时merge的过程中也会执行combine</li>
<li>merge: 对多个溢写文件进行合并，算法为多路归并排序，最终生成一个文件作为map阶段的输出</li>
<li>通知reduce拉取数据</li>
</ol>
</li>
<li>reduce端的Shuffle<ol>
<li>copy过程：拉取MapTask处理完的数据，使用0.7 × maxHeap大小的堆内存空间作为内存缓冲区</li>
<li>merge过程：内存中数据到达阈值会触发内存到磁盘的合并；map端数据读取完成后触发磁盘到磁盘的合并，算法为归并排序</li>
<li>reduce输入：merge阶段合并为一个大文件作Reduce数据文件执行Reduce逻辑，Shuffle阶段结束</li>
</ol>
</li>
</ol>
<h4 id="八、在MR程序中由谁来决定分区的数量，哪个阶段环节会开始往分区中写数据？"><a href="#八、在MR程序中由谁来决定分区的数量，哪个阶段环节会开始往分区中写数据？" class="headerlink" title="八、在MR程序中由谁来决定分区的数量，哪个阶段环节会开始往分区中写数据？"></a>八、在MR程序中由谁来决定分区的数量，哪个阶段环节会开始往分区中写数据？</h4><ol>
<li>分区由业务逻辑决定</li>
<li>分区规则有ReduceTask数量控制</li>
<li>Shuffle阶段确定分区编号</li>
<li>溢写阶段进行分区</li>
<li>Reduce执行结束，写入分区文件</li>
</ol>
<h4 id="九、阐述MR中实现分区的思路（从源码角度分析）"><a href="#九、阐述MR中实现分区的思路（从源码角度分析）" class="headerlink" title="九、阐述MR中实现分区的思路（从源码角度分析）"></a>九、阐述MR中实现分区的思路（从源码角度分析）</h4><ol>
<li>Shuffle阶段确定分区编号</li>
<li>溢写阶段根据分区编号生成不同的溢写文件</li>
<li>Reduce从多个map输出的文件中取自己分区的数据，处理后生成该分区的结果文件</li>
<li>默认分区规则为根据Key.hashcode 取模，作为分区编号</li>
</ol>
<h4 id="十、描述一下Hadoop中实现排序比较的规则"><a href="#十、描述一下Hadoop中实现排序比较的规则" class="headerlink" title="十、描述一下Hadoop中实现排序比较的规则"></a>十、描述一下Hadoop中实现排序比较的规则</h4><ol>
<li>Hadoop中实现排序依赖Comparator#compare方法</li>
<li>Comparator获取逻辑如下<ol>
<li>首先从jobContext中获取配置比较器的类名</li>
<li>如果获取到直接通过反射创建比较器，流程结束</li>
<li>如果未配置比较器类名，先从比较器缓存Map中根据输出key类对象获取比较器，如果获取到，直接返回比较器，流程结束</li>
<li>如果比较器缓存中未获取到比较器，强制加载后重新获取</li>
<li>如果还获取不到，Hadoop会使用输出key的class对象向创建一个比较器（要求必须实现了WritableComparable）</li>
</ol>
</li>
</ol>
<h4 id="十一、Hadoop中实现排序的两种方案分别是什么？"><a href="#十一、Hadoop中实现排序的两种方案分别是什么？" class="headerlink" title="十一、Hadoop中实现排序的两种方案分别是什么？"></a>十一、Hadoop中实现排序的两种方案分别是什么？</h4><ol>
<li>比较对象实现WritableComparable接口，重写compareTo方法</li>
<li>自定义Comparator，继承WritableComparator，重写compare方法，在Driver中指定</li>
<li>自定义Comparator，继承WritableComparator，重写compare方法，静态代码注册比较器</li>
</ol>
<h4 id="十二、编写MR的时候什么情况下使用Combiner-具体实现流程是什么？"><a href="#十二、编写MR的时候什么情况下使用Combiner-具体实现流程是什么？" class="headerlink" title="十二、编写MR的时候什么情况下使用Combiner 具体实现流程是什么？"></a>十二、编写MR的时候什么情况下使用Combiner 具体实现流程是什么？</h4><ol>
<li>为了提高MR运行效率，减轻ReduceTask压力，减少copy环节IO开销</li>
<li>Combiner执行不影响最终的业务逻辑</li>
<li>Reduce端对MaoTask数据的整体性没有要求</li>
<li>Combiner实现流程<ol>
<li>自定义Combiner类，继承Reducer，重写reduce方法</li>
<li>Job中指定Combiner</li>
<li>输入k-v为map的输出，输出k-v为reduce的输入</li>
</ol>
</li>
</ol>
<h4 id="十三、OutputFormat自定义实现流程描述一下"><a href="#十三、OutputFormat自定义实现流程描述一下" class="headerlink" title="十三、OutputFormat自定义实现流程描述一下"></a>十三、OutputFormat自定义实现流程描述一下</h4><ol>
<li>自定义OutputFormat类，继承OutputFormat，实现getRecordWriter抽象方法，返回自定义RecordWriter</li>
<li>自定义RecordWriter类，继承RecordWriter，重写write实现数据写出的逻辑，重写close方法对资源进行关闭</li>
<li>Job中指定OutputFormat处理类</li>
</ol>
<h4 id="十四、MR实现-ReduceJoin-的思路，以及ReduceJoin方案有哪些不足？"><a href="#十四、MR实现-ReduceJoin-的思路，以及ReduceJoin方案有哪些不足？" class="headerlink" title="十四、MR实现 ReduceJoin 的思路，以及ReduceJoin方案有哪些不足？"></a>十四、MR实现 ReduceJoin 的思路，以及ReduceJoin方案有哪些不足？</h4><ul>
<li>思路<ol>
<li>分析文件关联，确定关联字段</li>
<li>定义统一对象，包含关联字段和数据来源</li>
<li>map端参与文件输出统一对象，key为关联字段</li>
<li>reduce端以关联字段为key，统一对象为value，从统一对象中根据数据来源拆分对象</li>
<li>根据拆分对象进行关联</li>
</ol>
</li>
<li>不足<ol>
<li>耗费性能，需要参与数据全部遍历才能进行join</li>
<li>无法解决数据倾斜问题</li>
<li>海量数据容易造成reduce崩溃，任务失败</li>
</ol>
</li>
</ul>
<h4 id="十五、MR实现-MapJoin-的思路，以及MapJoin的局限性是什么？"><a href="#十五、MR实现-MapJoin-的思路，以及MapJoin的局限性是什么？" class="headerlink" title="十五、MR实现 MapJoin 的思路，以及MapJoin的局限性是什么？"></a>十五、MR实现 MapJoin 的思路，以及MapJoin的局限性是什么？</h4><ul>
<li>思路：<ol>
<li>分析文件关联，确定关联字段</li>
<li>小文件使用DistributedCache加载到缓存中</li>
<li>map端每读取一行数据，都根据关联字段，在缓存中获取对应关联数据</li>
<li>输出包换关联信息的完整数据给reduce</li>
</ol>
</li>
<li>局限<ol>
<li>适用数据量差异比较大的两个数据集</li>
</ol>
</li>
</ul>
]]></content>
      <categories>
        <category>MR</category>
      </categories>
      <tags>
        <tag>MR</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux</title>
    <url>/2021/11/07/Linux/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="Linux"><a href="#Linux" class="headerlink" title="Linux"></a>Linux</h1><h3 id="1、Linux简介"><a href="#1、Linux简介" class="headerlink" title="1、Linux简介"></a>1、Linux简介</h3><p><strong>什么是操作系统？</strong></p>
<p>操作系统是管理计算机硬件与软件资源的计算机程序，同时也是计算机系统的内核与基石。操作系统需要处理如管理与配置内存、决定系统资源供需的优先次序、控制输入设备与输出设备、操作网络与管理文件系统等基本事务。操作系统也提供一个让用户与系统交互的操作界面</p>
<p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/image20200211120630067.png?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10" alt="image-20200211120630067"></p>
<p><strong>常见的操作系统</strong></p>
<ul>
<li>2</li>
<li>MAC OS</li>
<li>Android</li>
<li>iOS</li>
</ul>
<p><strong>操作系统的发展史</strong></p>
<p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/image20200211120911148.png?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10" alt="image-20200211120911148"></p>
<ul>
<li><p>Unix</p>
<p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/image20200211120956539.png?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10" alt="image-20200211120956539"></p>
<p>1965年之前的时候，电脑并不像现在一样普遍，它可不是一般人能碰的起的，除非是军事或者学院的研究机构，而且当时大型主机至多能提供30台终端（30个键盘、显示器)，连接一台电脑</p>
<p>为了解决数量不够的问题：</p>
<ul>
<li><p>1965年左后由贝尔实验室、麻省理工学院 以及 通用电气共同发起了Multics项目，想让大型主机支持300台终端</p>
</li>
<li><p>1969年前后这个项目进度缓慢，资金短缺，贝尔实验室退出了研究</p>
</li>
<li><p>1969年从这个项目中退出的Ken Thompson当时在实验室无聊时，为了让一台空闲的电脑上能够运行“星际旅行”游行，在8月份左右趁着其妻子探亲的时间，用了1个月的时间 编写出了 Unix操作系统的原型</p>
</li>
<li><p>1970年，美国贝尔实验室的 Ken Thompson，以 BCPL语言 为基础，设计出很简单且很接近硬件的 B语言（取BCPL的首字母），并且他用B语言写了第一个UNIX操作系统</p>
</li>
<li><p>因为B语言的跨平台性较差，为了能够在其他的电脑上也能够运行这个非常棒的Unix操作系统，Dennis Ritchie和Ken Thompson 从B语言的基础上准备研究一个更好的语言</p>
<p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/jie-ping20200211121129.png?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10" alt="截屏2020-02-1112.11.29"></p>
</li>
<li><p>1972年，美国贝尔实验室的 Dennis Ritchie在B语言的基础上最终设计出了一种新的语言，他取了BCPL的第二个字母作为这种语言的名字，这就是C语言</p>
</li>
<li><p>1973年初，C语言的主体完成。Thompson和Ritchie迫不及待地开始用它完全重写了现在大名鼎鼎的Unix操作系统</p>
</li>
</ul>
</li>
<li><p>Minix</p>
<p>简介：因为AT&amp;T(通用电气)的政策改变，在Version 7 Unix推出之后，发布新的使用条款，将UNIX源代码私有化，在大学中不再能使用UNIX源代码。Andrew S. Tanenbaum(塔能鲍姆)教授为了能在课堂上教授学生操作系统运作的实务细节，决定在不使用任何AT&amp;T的源代码前提下，自行开发与UNIX兼容的操作系统，以避免版权上的争议。他以小型UNIX（mini-UNIX）之意，将它称为MINIX</p>
<p>没有火的原因：Minix的创始人说，MINIX 3没有统治世界是源于他在1992年犯下的一个错误，当时他认为BSD必然会一统天下，因为它是一个更稳定和更成熟的系统，其它操作系统难以与之竞争。因此他的MINIX的重心集中在教育上。四名BSD开发者已经成立了一家公司销售BSD系统，他们甚至还有一个有趣的电话号码1-800-ITS-UNIX。然而他们正因为这个电话号码而惹火上身。美国电话电报公司因电话号码而提起诉讼。官司打了三年才解决。在此期间，BSD陷于停滞，而Linux则借此一飞冲天。他的错误在于没有意识官司竟然持续了如此长的时间，以及BSD会因此受到削弱。如果美国电话电报公司没有起诉，Linux永远不会流行起来，BSD将统治世界</p>
</li>
<li><p>Linux</p>
<p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/image20200211121238635.png?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10" alt="image-20200211121238635"></p>
<p>因为Minix只是教学使用，因此功能并不强，因此Torvalds利用GNU的bash当做开发环境，gcc当做编译工具，编写了Linux内核-v0.02，但是一开始Linux并不能兼容Unix，即Unix上跑的应用程序不能在Linux上跑，即应用程序与内核之间的接口不一致，因为Unix是遵循POSIX规范的，因此Torvalds修改了Linux，并遵循POSIX（Portable Operating System Interface，他规范了应用程序与内核的接口规范）； 一开始Linux只适用于386，后来经过全世界的网友的帮助，最终能够兼容多种硬件</p>
<p>Linux发展的重要里程碑：</p>
<ul>
<li>1990, Linus Torvalds 首次接触 MINIX</li>
<li>1991, Linus Torvalds 开始在 MINIX 上编写各种驱动程序等操作系统内核组件</li>
<li>1991 底, Linus Torvalds 公开了 Linux 内核</li>
<li>1993, Linux 1.0 版发行，Linux 转向 GPL 版权协议</li>
<li>1994, Linux 的第一个商业发行版 Slackware 问世</li>
<li>1996, 美国国家标准技术局的计算机系统实验室确认 Linux 版本1.2.13（由 Open Linux 公司打包）符合 POSIX 标准</li>
<li>1999, Linux 的简体中文发行版相继问世</li>
</ul>
</li>
</ul>
<p><strong>Linux版本</strong></p>
<ul>
<li><p>Linux内核版本</p>
<p>内核(kernel)是系统的心脏，是运行程序和管理像磁盘和打印机等硬件设备的核心程序，它提供了一个在裸设备与应用程序间的抽象层</p>
<p>Linux内核版本又分为稳定版和开发版，两种版本是相互关联，相互循环</p>
<table>
<thead>
<tr>
<th align="left">版本</th>
<th align="left">说明</th>
</tr>
</thead>
<tbody><tr>
<td align="left">稳定版</td>
<td align="left">具有工业级强度，可以广泛地应用和部署。新的稳定版相对于较旧的只是修正一些bug或加入一些新的驱动程序</td>
</tr>
<tr>
<td align="left">开发版</td>
<td align="left">由于要试验各种解决方案，所以变化很快</td>
</tr>
</tbody></table>
<p>内核源码网址[<a href="http://www.kernel.org]，所有来自全世界的对Linux源码的修改最终都会汇总到这个网站，由Linus领导的开源社区对其进行甄别和修改最终决定是否进入到Linux主线内核源码中">http://www.kernel.org]，所有来自全世界的对Linux源码的修改最终都会汇总到这个网站，由Linus领导的开源社区对其进行甄别和修改最终决定是否进入到Linux主线内核源码中</a></p>
</li>
<li><p>Linux发行版本</p>
<p>通常包含了包括桌面环境、办公套件、媒体播放器、数据库等应用软件</p>
<p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/image20200211121551282.png?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10" alt="image-20200211121551282"></p>
<p>常见发行版本：</p>
<ul>
<li>Fedora</li>
<li>Redhat</li>
<li>Ubuntu</li>
<li>CentOS</li>
</ul>
</li>
</ul>
<p><strong>Linux应用领域</strong></p>
<table>
<thead>
<tr>
<th>领域</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>个人桌面领域</td>
<td>此领域是传统linux应用最薄弱的环节，传统linux由于界面简单、操作复杂、应用软件少的缺点，一直被windows所压制，但近些年来随着ubuntu、fedora等优秀桌面环境的兴起，同时各大硬件厂商对其支持的加大，linux在个人桌面领域的占有率在逐渐的提高</td>
</tr>
<tr>
<td>服务器领域</td>
<td>linux免费、稳定、高效等特点在这里得到了很好的体现，但早期因为维护、运行等原因同样受到了很大的限制，但近些年来linux服务器市场得到了飞速的提升，尤其在一些高端领域尤为广泛</td>
</tr>
<tr>
<td>嵌入式领域</td>
<td>linux运行稳定、对网络的良好支持性、低成本，且可以根据需要进行软件裁剪，内核最小可以达到几百KB等特点，使其近些年来在嵌入式领域的应用得到非常大的提高</td>
</tr>
</tbody></table>
<p><strong>Linux和Windows区别</strong></p>
<p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/bu-huo.PNG?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10" alt="捕获"></p>
<p><strong>CentOS下载地址</strong></p>
<p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/bu-huo1627694182903.PNG?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10" alt="捕获"></p>
<h3 id="2、安装VMware虚拟机"><a href="#2、安装VMware虚拟机" class="headerlink" title="2、安装VMware虚拟机"></a>2、安装VMware虚拟机</h3><h3 id="3、安装linux系统"><a href="#3、安装linux系统" class="headerlink" title="3、安装linux系统"></a>3、安装linux系统</h3><h3 id="4、文件和目录"><a href="#4、文件和目录" class="headerlink" title="4、文件和目录"></a>4、文件和目录</h3><p><strong>Windows文件系统</strong></p>
<p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/image20200211134106179.png?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10" alt="image-20200211134106179"></p>
<p>在 windows 平台下，打开“计算机”，我们看到的是一个个的驱动器盘符</p>
<p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/image20200211134123743.png?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10" alt="image-20200211134123743"></p>
<p>每个驱动器都有自己的根目录结构，这样形成了多个树并列的情形</p>
<p><strong>Linux文件系统</strong></p>
<p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/image20200211145837589.png?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10" alt="image-20200211145837589"></p>
<p>centos没有盘符这个概念，只有一个根目录/，所有文件都在它下面</p>
<p><strong>目录</strong></p>
<table>
<thead>
<tr>
<th>路径</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>/</td>
<td>根目录，一般根目录下只存放目录，在Linux下有且只有一个根目录。所有的东西都是从这里开始。当你在终端里输入“/home”，你其实是在告诉电脑，先从/（根目录）开始，再进入到home目录</td>
</tr>
<tr>
<td>/bin<br />/usr/bin</td>
<td>可执行二进制文件的目录，如常用的命令ls、tar、mv、cat等</td>
</tr>
<tr>
<td>/boot</td>
<td>放置linux系统启动时用到的一些文件，如Linux的内核文件：/boot/vmlinuz，系统引导管理器：/boot/grub</td>
</tr>
<tr>
<td>/dev</td>
<td>存放linux系统下的设备文件，访问该目录下某个文件，相当于访问某个设备，常用的是挂载光驱 mount /dev/cdrom /mnt</td>
</tr>
<tr>
<td>/etc</td>
<td>系统配置文件存放的目录，不建议在此目录下存放可执行文件，重要的配置文件有 /etc/inittab、/etc/fstab、/etc/init.d、/etc/X11、/etc/sysconfig、/etc/xinetd.d</td>
</tr>
<tr>
<td>/home</td>
<td>系统默认的用户家目录，新增用户账号时，用户的家目录都存放在此目录下，<del>表示当前用户的家目录，</del>edu 表示用户 edu 的家目录</td>
</tr>
<tr>
<td>/lib<br />/usr/lib<br />/usr/local/lib</td>
<td>系统使用的函数库的目录，程序在执行过程中，需要调用一些额外的参数时需要函数库的协助</td>
</tr>
<tr>
<td>/lost+fount</td>
<td>系统异常产生错误时，会将一些遗失的片段放置于此目录下</td>
</tr>
<tr>
<td>/mnt</td>
<td>/media：光盘默认挂载点，通常光盘挂载于 /mnt/cdrom 下，也不一定，可以选择任意位置进行挂载</td>
</tr>
<tr>
<td>/opt</td>
<td>给主机额外安装软件所摆放的目录</td>
</tr>
<tr>
<td>/proc</td>
<td>此目录的数据都在内存中，如系统核心，外部设备，网络状态，由于数据都存放于内存中，所以不占用磁盘空间，比较重要的目录有 /proc/cpuinfo、/proc/interrupts、/proc/dma、/proc/ioports、/proc/net/* 等</td>
</tr>
<tr>
<td>/root</td>
<td>系统管理员root的家目录</td>
</tr>
<tr>
<td>/sbin<br />/usr/sbin<br />/usr/local/sbin</td>
<td>放置系统管理员使用的可执行命令，如fdisk、shutdown、mount 等。与 /bin 不同的是，这几个目录是给系统管理员 root使用的命令，一般用户只能”查看”而不能设置和使用</td>
</tr>
<tr>
<td>/tmp</td>
<td>一般用户或正在执行的程序临时存放文件的目录，任何人都可以访问，重要数据不可放置在此目录下</td>
</tr>
<tr>
<td>/srv</td>
<td>服务启动之后需要访问的数据目录，如 www 服务需要访问的网页数据存放在 /srv/www 内</td>
</tr>
<tr>
<td>/usr</td>
<td>应用程序存放目录，/usr/bin 存放应用程序，/usr/share 存放共享数据，/usr/lib 存放不能直接运行的，却是许多程序运行所必需的一些函数库文件。/usr/local: 存放软件升级包。/usr/share/doc: 系统说明文件存放目录。/usr/share/man: 程序说明文件存放目录</td>
</tr>
<tr>
<td>/var</td>
<td>放置系统执行过程中经常变化的文件，如随时更改的日志文件 /var/log，/var/log/message：所有的登录文件存放目录，/var/spool/mail：邮件存放的目录，/var/run:程序或服务启动后，其PID存放在该目录下</td>
</tr>
</tbody></table>
<p><strong>路径</strong></p>
<ul>
<li><p>绝对路径</p>
<p>从/目录开始描述的路径为绝对路径</p>
</li>
<li><p>相对路径</p>
<p>从当前位置开始描述的路径为相对路径</p>
</li>
<li><p>.</p>
<p>代表当前目录</p>
</li>
<li><p>..</p>
<p>表示上一级目录</p>
<p>注意：根目录下的.和..都代表当前目录</p>
</li>
</ul>
<h3 id="5、命令概述"><a href="#5、命令概述" class="headerlink" title="5、命令概述"></a>5、命令概述</h3><p>Linux 提供了大量的命令，利用它可以有效地完成大量的工作，如磁盘操作、文件存取、目录操作、进程管理、文件权限设定等。Linux 发行版本最少的命令也有 200 多个，这里只介绍比较重要和使用频率最多的命令</p>
<ul>
<li><p>命令的使用方法</p>
<p>格式：command  [-options]  [parameter1]  …</p>
<p>command：命令名，相应功能的英文单词或单词的缩写</p>
<p>[-options]：选项，可用来对命令进行控制，也可以省略</p>
<p>[parameter1]  …：传给命令的参数，可以是零个一个或多个</p>
</li>
<li><p>查看帮助文档</p>
<ul>
<li><p>man</p>
<p>man是linux提供的一个手册，包含了绝大部分的命令、函数使用说明。该手册分成很多章节（section），使用man时可以指定不同的章节来浏览</p>
<img src="Linux.assets/image-20200211174846722.png" alt="image-20200211174846722" style="zoom:150%;" />

<p>功能键<img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/image20200211174857959.png?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10" alt="image-20200211174857959"></p>
</li>
<li><p>help</p>
<p>获得shell内置命令的帮助信息</p>
<p>语法：help 命令</p>
<p><code>help cd</code></p>
</li>
<li><p>–help</p>
<p>一般是linux命令自带的帮助信息</p>
<p>例如：ls  –help</p>
</li>
</ul>
</li>
<li><p>自动补全</p>
<p>在敲出命令的前几个字母的同时，按下tab键，系统会自动帮我们补全命令</p>
</li>
<li><p>历史命令</p>
<p>当系统执行过一些命令后，可按上下键翻看以前的命令，history将执行过的命令列举出来</p>
</li>
</ul>
<h3 id="6、常用快捷键"><a href="#6、常用快捷键" class="headerlink" title="6、常用快捷键"></a>6、常用快捷键</h3><table>
<thead>
<tr>
<th>常用快捷键</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>ctrl + c</td>
<td>停止进程</td>
</tr>
<tr>
<td>ctrl+l</td>
<td>清屏:clear；彻底清屏是：reset</td>
</tr>
<tr>
<td>ctrl + q</td>
<td>退出</td>
</tr>
<tr>
<td>善于用tab键</td>
<td>提示(更重要的是可以防止敲错)</td>
</tr>
<tr>
<td>上下键</td>
<td>查找执行过的命令</td>
</tr>
<tr>
<td>ctrl +alt</td>
<td>linux和Windows之间切换</td>
</tr>
</tbody></table>
<h3 id="7、文件管理"><a href="#7、文件管理" class="headerlink" title="7、文件管理"></a>7、文件管理</h3><ul>
<li><p><code>ls</code></p>
<p>作用：显示指定目录下所有的文件和目录</p>
<p>选项：</p>
<ul>
<li><p>-a</p>
<p>显示指定目录下所有子目录与文件，包括隐藏文件。Linux文件或者目录名称最长可以有265个字符，“.”代表当前目录，“..”代表上一级目录，以“.”开头的文件为隐藏文件，需要用 -a 参数才能显示</p>
</li>
<li><p>-l</p>
<p>以列表方式显示文件的详细信息</p>
<p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/image20200211181530984.png?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10" alt="image-20200211181530984"></p>
<p>文件类型：</p>
<table>
<thead>
<tr>
<th>符号</th>
<th>类型</th>
</tr>
</thead>
<tbody><tr>
<td>-</td>
<td>普通文件</td>
</tr>
<tr>
<td>d</td>
<td>目录文件</td>
</tr>
<tr>
<td>l</td>
<td>链接文件</td>
</tr>
<tr>
<td>c</td>
<td>字符设备</td>
</tr>
<tr>
<td>b</td>
<td>块设备</td>
</tr>
</tbody></table>
</li>
<li><p>-h</p>
<p>配合 -l 以人性化的方式显示文件大小</p>
</li>
</ul>
<p>通配符：</p>
<table>
<thead>
<tr>
<th>符号</th>
<th align="left">说明</th>
</tr>
</thead>
<tbody><tr>
<td>*</td>
<td align="left">文件代表文件名中所有字符</td>
</tr>
<tr>
<td>ls te*</td>
<td align="left">查找以te开头的文件</td>
</tr>
<tr>
<td>ls *html</td>
<td align="left">查找结尾为html的文件</td>
</tr>
<tr>
<td>？</td>
<td align="left">代表文件名中任意一个字符</td>
</tr>
<tr>
<td>ls ?.c</td>
<td align="left">只找第一个字符任意，后缀为.c的文件</td>
</tr>
<tr>
<td>ls a.?</td>
<td align="left">只找只有3个字符，前2字符为a.，最后一个字符任意的文件</td>
</tr>
<tr>
<td>[]</td>
<td align="left">[”和“]”将字符组括起来，表示可以匹配字符组中的任意一个。“-”用于表示字符范围</td>
</tr>
<tr>
<td>[abc]</td>
<td align="left">匹配a、b、c中的任意一个</td>
</tr>
<tr>
<td>[a-f]</td>
<td align="left">匹配从a到f范围内的的任意一个字符</td>
</tr>
<tr>
<td>ls [a-f]*</td>
<td align="left">找到从a到f范围内的的任意一个字符开头的文件</td>
</tr>
<tr>
<td>ls a-f</td>
<td align="left">查找文件名为a-f的文件,当“-”处于方括号之外失去通配符的作用</td>
</tr>
<tr>
<td>\</td>
<td align="left">如果要使通配符作为普通字符使用，可以在其前面加上转义字符。“?”和“*”处于方括号内时不用使用转义字符就失去通配符的作用</td>
</tr>
<tr>
<td><code>ls \*a</code></td>
<td align="left">查找文件名为*a的文件</td>
</tr>
</tbody></table>
</li>
<li><p><code>pwd</code></p>
<p>作用：显示当前的工作目录</p>
</li>
<li><p><code>cd</code></p>
<p>作用：切换工作目录</p>
<p>注意：cd后面可跟绝对路径，也可以跟相对路径</p>
<p>特殊写法：</p>
<table>
<thead>
<tr>
<th>写法</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>cd</td>
<td>切换到当前用户的主目录(/home/用户目录)，用户登陆的时候，默认的目录就是用户的主目录</td>
</tr>
<tr>
<td>cd ~</td>
<td>切换到当前用户的主目录(/home/用户目录)</td>
</tr>
<tr>
<td>cd .</td>
<td>切换到当前目录</td>
</tr>
<tr>
<td>cd ..</td>
<td>切换到上级目录</td>
</tr>
<tr>
<td>cd -</td>
<td>可进入上次所在的目录</td>
</tr>
<tr>
<td>cd -P</td>
<td>跳转到实际物理路径，而非快捷方式路径</td>
</tr>
</tbody></table>
</li>
<li><p><code>&gt;</code></p>
<p>作用：输出重定向，Linux允许将命令执行结果重定向到一个文件，本应显示在终端上的内容保存到指定文件中</p>
<p>示例：<code>ls &gt; test.txt</code></p>
<p>注意：如果文件不存在，则创建，存在则覆盖其内容</p>
</li>
<li><p><code>&gt;&gt;</code></p>
<p>作用：输出重定向，Linux允许将命令执行结果重定向到一个文件，本应显示在终端上的内容保存到指定文件中</p>
<p>示例：<code>ls &gt;&gt; test.txt</code></p>
<p>注意：如果文件不存在，则创建，存在则追加到文件的尾部</p>
</li>
<li><p><code>cat</code></p>
<p>作用：查看或者合并文件内容</p>
<p>合并文件示例：<code>cat  test1.txt  test2.txt &gt; test.txt</code></p>
</li>
<li><p><code>head</code></p>
<p>作用：查看文件</p>
<p>默认显示前10行：<code>head  test.txt</code></p>
<p>显示前n行：<code>head  -n  test.txt</code></p>
</li>
<li><p><code>tail</code></p>
<p>作用：查看文件  </p>
<p>默认显示后10行：<code>tail  test.txt</code></p>
<p>显示后n行：<code>tail -n  test.txt</code></p>
<p>监控文件变化：<code>tail -f  test.txt</code></p>
</li>
<li><p><code>more</code></p>
<p>作用：分屏显示，查看内容时，在信息过长无法在一屏上显示时，会出现快速滚屏，使得用户无法看清文件的内容，此时可以使用more命令，每次只显示一页，按下空格键可以显示下一页，按下q键退出显示，按下h键可以获取帮助</p>
</li>
<li><p><code>less</code></p>
<p>作用：分屏查看文件，它的功能与more指令类似，但是比more指令更加强大，支持各种显示终端。less指令在显示文件内容时，并不是一次将整个文件加载之后才显示，而是根据显示需要加载内容，对于显示大型文件具有较高的效率。</p>
<p>说明：敲enter键往下走一行，敲空格键，往下走一页，可以向上翻页，键盘上的pageup，pagedown</p>
</li>
<li><p><code>wc</code></p>
<p>作用：一次显示文件行数、字节数、文件名信息</p>
</li>
<li><p><code>echo</code></p>
<p>使用：echo [选项] [输出内容]</p>
<p>作用：输出内容</p>
<p>-e： 支持反斜线控制的字符转换</p>
<table>
<thead>
<tr>
<th>控制字符</th>
<th>作用</th>
</tr>
</thead>
<tbody><tr>
<td>\</td>
<td>输出\本身</td>
</tr>
<tr>
<td>\n</td>
<td>换行符</td>
</tr>
<tr>
<td>\t</td>
<td>制表符，也就是Tab键</td>
</tr>
</tbody></table>
<p><code>echo “hello\tworld”</code></p>
<p><code>echo -e “hello\tworld”</code></p>
</li>
<li><p><code>clear</code></p>
<p>作用：清除终端上的显示清除终端上的显示</p>
</li>
<li><p><code>mkdir</code></p>
<p>作用：创建一个新的目录</p>
<p>注意：新建目录的名称不能与当前目录中已有的目录或文件同名，并且目录创建者必须对当前目录具有写权限</p>
<p>递归创建目录：mkdir  -p  a/b/c/d  </p>
</li>
<li><p><code>touch</code></p>
<p>作用：创建一个新的普通文件</p>
</li>
<li><p><code>rmdir</code></p>
<p>作用：删除一个目录</p>
<p>注意：目录必须为空目录</p>
</li>
<li><p><code>rm</code></p>
<p>使用：rm  [选项]  deleteFile</p>
<p>作用：删除文件或目录，删除的文件不能恢复</p>
<p>参数：</p>
<table>
<thead>
<tr>
<th>值</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>-i</td>
<td>以进行交互式方式执行</td>
</tr>
<tr>
<td>-f</td>
<td>强制删除，忽略不存在的文件，无需提示</td>
</tr>
<tr>
<td>-r</td>
<td>递归地删除目录下的内容，删除文件夹时必须加此参数</td>
</tr>
<tr>
<td>-v</td>
<td>显示指令的详细执行过程</td>
</tr>
</tbody></table>
</li>
<li><p><code>cp</code></p>
<p>作用：将给出的文件或目录复制到另一个文件或目录中</p>
<p>格式：cp  文件名 目标目录</p>
<p>选项：</p>
<table>
<thead>
<tr>
<th>值</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>-a</td>
<td>该选项通常在复制目录时使用，它保留链接、文件属性，并递归地复制目录，简单而言，保持文件原有属性</td>
</tr>
<tr>
<td>-f</td>
<td>已经存在的目标文件而不提示</td>
</tr>
<tr>
<td>-i</td>
<td>交互式复制，在覆盖目标文件之前将给出提示要求用户确认</td>
</tr>
<tr>
<td>-r</td>
<td>若给出的源文件是目录文件，则cp将递归复制该目录下的所有子目录和文件，目标文件必须为一个目录名</td>
</tr>
<tr>
<td>-v</td>
<td>显示拷贝进度</td>
</tr>
</tbody></table>
</li>
<li><p><code>mv</code></p>
<p>作用:</p>
<table>
<thead>
<tr>
<th>说明</th>
<th>使用格式</th>
</tr>
</thead>
<tbody><tr>
<td>移动文件或目录</td>
<td>mv 文件  目标目录</td>
</tr>
<tr>
<td>重命名</td>
<td>mv  文件名  文件名</td>
</tr>
</tbody></table>
<p>参数：</p>
<table>
<thead>
<tr>
<th>值</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>-f</td>
<td>禁止交互式操作，如有覆盖也不会给出提示</td>
</tr>
<tr>
<td>-i</td>
<td>确认交互方式操作，如果mv操作将导致对已存在的目标文件的覆盖，系统会询问是否重写，要求用户回答以避免误覆盖文件</td>
</tr>
<tr>
<td>-v</td>
<td>显示移动进度</td>
</tr>
</tbody></table>
</li>
<li><p><code>ln</code></p>
<p>作用：建立链接文件，Linux链接文件类似于Windows下的快捷方式</p>
<table>
<thead>
<tr>
<th>链接文件分类</th>
<th>说明</th>
<th>创建格式</th>
<th>注意事项</th>
</tr>
</thead>
<tbody><tr>
<td>软连接</td>
<td>软链接不占用磁盘空间，源文件删除则软链接失效</td>
<td>ln  -s  源文件  链接文件</td>
<td>如果软链接文件和源文件不在同一个目录，源文件要使用绝对路径，不能使用相对路径</td>
</tr>
<tr>
<td>硬链接</td>
<td>硬链接只能链接普通文件，不能链接目录</td>
<td>ln  源文件  链接文件</td>
<td>两个文件占用相同大小的硬盘空间，即使删除了源文件，链接文件还是存在，所以-s选项是更常见的形式</td>
</tr>
</tbody></table>
<p>注意：删除软链接： rm -rf 软链接名，而不是rm -rf 软链接名/。如果使用 rm -rf 软链接名/ 删除，会把软链接对应的真实目录下内容删掉。</p>
</li>
</ul>
<h3 id="8、文件查找"><a href="#8、文件查找" class="headerlink" title="8、文件查找"></a>8、文件查找</h3><ul>
<li><p><code>find</code></p>
<p>作用：查找文件</p>
<p>格式：find [搜索范围] [选项]</p>
<table>
<thead>
<tr>
<th>示例</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>find  ./  -name  test.sh</td>
<td>查找当前目录下所有名为test.sh的文件</td>
</tr>
<tr>
<td>find  ./  -name  ‘*.sh’</td>
<td>查找当前目录下所有后缀为.sh的文件</td>
</tr>
<tr>
<td>find  ./  -name  “[A-Z]*”</td>
<td>查找当前目录下所有以大写字母开头的文件</td>
</tr>
<tr>
<td>find  /tmp  -size  2M</td>
<td>查找在/tmp 目录下等于2M的文件</td>
</tr>
<tr>
<td>find  /tmp  -size  +2M</td>
<td>查找在/tmp 目录下大于2M的文件</td>
</tr>
<tr>
<td>find  /tmp  -size  -2M</td>
<td>查找在/tmp 目录下小于2M的文件</td>
</tr>
<tr>
<td>find  ./  -size  +4k  -size  -5M</td>
<td>查找当前目录下大于4k，小于5M的文件</td>
</tr>
<tr>
<td>find  ./  -perm  0777</td>
<td>查找当前目录下权限为 777 的文件或目录</td>
</tr>
<tr>
<td>find ./ -uers atguigu</td>
<td>查找当前目录下属于atguigu用户的所有文件</td>
</tr>
</tbody></table>
<p>-size单位：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">b —— 块（512字节）</span><br><span class="line">c —— 字节</span><br><span class="line">w —— 字（2字节）</span><br><span class="line">k —— 千字节</span><br><span class="line">M —— 兆字节</span><br><span class="line">G —— 吉字节</span><br></pre></td></tr></table></figure></li>
<li><p>locate</p>
<p>作用：快速定位文件路径，locate指令利用事先建立的系统中所有文件名称及路径的locate数据库实现快速定位给定的文件。Locate指令无需遍历整个文件系统，查询速度较快。为了保证查询结果的准确度，管理员必须定期更新locate时刻。</p>
<p>格式：locate 搜索文件</p>
<p>注意：由于locate指令基于数据库进行查询，所以第一次运行前，必须使用updatedb指令创建locate数据库。<br>注意：tmp目录不会简历索引</p>
</li>
<li><p><code>which</code></p>
<p>作用：查看命令位置</p>
</li>
<li><p><code>|</code></p>
<p>名称：管道</p>
<p>说明：一个命令的输出可以通过管道做为另一个命令的输入</p>
<p>简述：管道我们可以理解现实生活中的管子，管子的一头塞东西进去，另一头取出来，这里“ | ”的左右分为两端，左端塞东西(写)，右端取东西(读)</p>
</li>
<li><p><code>grep</code></p>
<p>作用：文本搜索，强大的文本搜索工具，grep允许对文本文件进行模式查找，如果找到匹配模式， grep打印包含模式的所有行</p>
<p>格式：grep  [-选项]  ‘搜索内容串’  文件名</p>
<p>注意：搜索内容串可以是正则表达式</p>
<p>选项：</p>
<table>
<thead>
<tr>
<th>值</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>-v</td>
<td>显示不包含匹配文本的所有行（相当于求反）</td>
</tr>
<tr>
<td>-n</td>
<td>显示匹配行及行号</td>
</tr>
<tr>
<td>-i</td>
<td>忽略大小写</td>
</tr>
</tbody></table>
<p>示例：<code>ps -aux | grep java</code> </p>
</li>
</ul>
<h3 id="9、解压和压缩"><a href="#9、解压和压缩" class="headerlink" title="9、解压和压缩"></a>9、解压和压缩</h3><ul>
<li><p><code>tar</code></p>
<p>作用：归档管理，计算机中的数据经常需要备份，tar是Unix/Linux中最常用的备份工具，此命令可以把一系列文件归档到一个大文件中，也可以把档案文件解开以恢复数据</p>
<p>格式：tar  [参数]  打包文件名  文件</p>
<p>参数：</p>
<table>
<thead>
<tr>
<th>值</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>-c</td>
<td>生成档案文件，创建打包文件</td>
</tr>
<tr>
<td>-v</td>
<td>列出归档解档的详细过程，显示进度</td>
</tr>
<tr>
<td>-f</td>
<td>指定档案文件名称，f后面一定是.tar文件，所以必须放选项最后</td>
</tr>
<tr>
<td>-t</td>
<td>列出档案中包含的文件</td>
</tr>
<tr>
<td>-x</td>
<td>解开档案文件</td>
</tr>
</tbody></table>
<p>注意：</p>
<p>​        参数前面可以使用“-”，也可以不使用</p>
<p>​        除了f需要放在参数的最后，其它参数的顺序任意</p>
</li>
<li><p><code>gzip</code></p>
<p>作用：tar与gzip命令结合使用实现文件打包、压缩。 tar只负责打包文件，但不压缩，用gzip压缩tar打包后的文件，其扩展名一般用xxxx.tar.gz</p>
<p>解压格式：gzip  [选项]  待解压文件</p>
<p>压缩格式：gzip  [选项]  被压缩文件  压缩后文件名</p>
<p>选项：</p>
<table>
<thead>
<tr>
<th>值</th>
<th>说明</th>
<th>示例</th>
</tr>
</thead>
<tbody><tr>
<td>-r</td>
<td>压缩所有子目录</td>
<td>gzip  -r  1.tar  1.tar.gz</td>
</tr>
<tr>
<td>-d</td>
<td>解压</td>
<td>gzip  -d  1.tar.gz</td>
</tr>
</tbody></table>
<p>注意：tar这个命令并没有压缩的功能，它只是一个打包的命令，但是在tar命令中增加一个选项(-z)可以调用gzip实现了一个压缩的功能，实行一个先打包后压缩的过程</p>
<p>结合tar使用：</p>
<p>​        压缩：tar  -cvzf  1.tar.gz  *</p>
<p>​        解压：</p>
<table>
<thead>
<tr>
<th>示例</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>tar  -xvzf  1.tar.gz</td>
<td>解压到当前目录</td>
</tr>
<tr>
<td>tar  -xvzf  1.tar.gz  -C  /temp</td>
<td>解压到指定目录</td>
</tr>
</tbody></table>
</li>
<li><p><code>bzip2</code></p>
<p>作用：tar与bzip2命令结合使用实现文件打包、压缩(用法和gzip一样)。tar只负责打包文件，但不压缩，用bzip2压缩tar打包后的文件，其扩展名一般用xxxx.tar.bz2。在tar命令中增加一个选项(-j)可以调用bzip2实现了一个压缩的功能，实行一个先打包后压缩的过程</p>
<p>结合tar使用：</p>
<p>压缩：tar  -jcvf  压缩包包名  文件…(tar  jcvf  bk.tar.bz2  *.c)</p>
<p>解压：tar  -jxvf  压缩包包名  (tar  jxvf  bk.tar.bz2)</p>
</li>
<li><p><code>zip、unzip</code></p>
<p>作用：通过zip压缩文件的目标文件不需要指定扩展名，默认扩展名为zip</p>
<p>压缩：zip  [-r]  目标文件(没有扩展名)  源文件</p>
<p>解压：unzip  -d  解压后目录文件  压缩文件</p>
</li>
</ul>
<h3 id="10、vi编辑器"><a href="#10、vi编辑器" class="headerlink" title="10、vi编辑器"></a>10、vi编辑器</h3><p><code>gedit</code>：是一个Linux环境下的文本编辑器，类似windows下的写字板程序，在不需要特别复杂的编程环境下，作为基本的文本编辑器比较合适</p>
<ul>
<li><p>作用</p>
<p>打开文件编辑并保存退出文件</p>
</li>
<li><p>打开文件</p>
<p>格式：vim  文件名</p>
<p>说明：如果文件不存在则先打开，当关闭保存时自动创建该文件</p>
<p>示例：</p>
<table>
<thead>
<tr>
<th>示例</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>vim  sunck.txt</td>
<td>打开文件，光标在第一行</td>
</tr>
<tr>
<td>vim  sunck.txt  +5</td>
<td>打开文件，将光标移动到第四行<br />如果文件不存在则没有效果</td>
</tr>
<tr>
<td>vim sunck.txt +</td>
<td>打开文件，将光标移动到最后一行<br />如果文件不存在则没有效果</td>
</tr>
</tbody></table>
</li>
<li><p>模式</p>
<ul>
<li><p>命令模式</p>
<p>作用：可以实行特定命令，快速操作文本</p>
<p>进入与退出命令模式：打开文件即进入命令模式，按ESC即退出</p>
<p>移动光标命令：</p>
<table>
<thead>
<tr>
<th>命令</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>上、下、左、右方向键</td>
<td>移动光标</td>
</tr>
<tr>
<td>G</td>
<td>光标快速的定位到末行行首</td>
</tr>
<tr>
<td>$</td>
<td>光标快速定位到该行行尾</td>
</tr>
<tr>
<td>^</td>
<td>光标快速定位到该行行首</td>
</tr>
<tr>
<td>gg</td>
<td>光标快速定位到第一行行首</td>
</tr>
<tr>
<td>ngg</td>
<td>光标快速定位到第n行行首</td>
</tr>
<tr>
<td>M</td>
<td>光标移动到中间行</td>
</tr>
<tr>
<td>L</td>
<td>光标移动到屏幕最后一行行首</td>
</tr>
<tr>
<td>w</td>
<td>向后一次移动一个字</td>
</tr>
<tr>
<td>b</td>
<td>向前一次移动一个字</td>
</tr>
<tr>
<td>ctr+d、ctr+u</td>
<td>向下、上翻半屏</td>
</tr>
<tr>
<td>ctr+f、ctr+b</td>
<td>向下、上翻一屏</td>
</tr>
<tr>
<td>h、j、k、l</td>
<td>左、下、上、右移动光标</td>
</tr>
</tbody></table>
<p>删除命令:</p>
<table>
<thead>
<tr>
<th>命令</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>x</td>
<td>删除光标后一个字符</td>
</tr>
<tr>
<td>X</td>
<td>删除光标前一个字符</td>
</tr>
<tr>
<td>dd</td>
<td>删除光标所在行</td>
</tr>
<tr>
<td>ndd</td>
<td>删除指定的行数</td>
</tr>
<tr>
<td>d0</td>
<td>删除光标前本行所有内容,不包含光标所在字符</td>
</tr>
<tr>
<td>dw</td>
<td>删除光标开始位置的字,包含光标所在字符</td>
</tr>
</tbody></table>
<p>撤销命令:</p>
<table>
<thead>
<tr>
<th>命令</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>u</td>
<td>一步一步撤销</td>
</tr>
<tr>
<td>ctr+r</td>
<td>反撤销</td>
</tr>
</tbody></table>
<p>重复命令:</p>
<table>
<thead>
<tr>
<th>命令</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>.</td>
<td>重复上一次操作的命令</td>
</tr>
</tbody></table>
<p>文本行移动命令:</p>
<table>
<thead>
<tr>
<th>命令</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>shift+&gt;&gt;</td>
<td>文本行右移</td>
</tr>
<tr>
<td>shift+&gt;&gt;</td>
<td>问本行左移</td>
</tr>
</tbody></table>
<p>复制粘贴命令:</p>
<table>
<thead>
<tr>
<th>命令</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>yy</td>
<td>复制当前行</td>
</tr>
<tr>
<td>nyy</td>
<td>复制n行</td>
</tr>
<tr>
<td>p</td>
<td>在光标所在位置向下新开辟一行,粘贴</td>
</tr>
</tbody></table>
<p>剪切粘贴命令:</p>
<table>
<thead>
<tr>
<th>命令</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>dd、ndd</td>
<td>删除命令相当于剪切</td>
</tr>
<tr>
<td>p</td>
<td>在光标所在位置向下新开辟一行,粘贴</td>
</tr>
</tbody></table>
<p>可视模式命令:</p>
<table>
<thead>
<tr>
<th>命令</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>v</td>
<td>按字符移动,选中文本</td>
</tr>
<tr>
<td>V</td>
<td>按行移动,选中文本可视模式可以配合 d, y, &gt;&gt;, &lt;&lt; 实现对文本块的删除,复制,左右移动</td>
</tr>
</tbody></table>
</li>
<li><p>输入模式</p>
<p>作用：向文件中输入内容</p>
<table>
<thead>
<tr>
<th>进入方式</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>按ESC后按a</td>
<td>从光标之后开始输入</td>
</tr>
<tr>
<td>按ESC后按A</td>
<td>在光标所在行的末尾开始输入</td>
</tr>
<tr>
<td>按ESC后按i</td>
<td>从光标之前开始输入</td>
</tr>
<tr>
<td>按ESC后按I</td>
<td>从光标所在行第一个非空字符开始输入</td>
</tr>
<tr>
<td>按ESC后按o</td>
<td>在光标所在行下一行，另起一行开始输入</td>
</tr>
<tr>
<td>按ESC后按O</td>
<td>在光标所在行上一行，另起一行开始输入</td>
</tr>
<tr>
<td>按ESC后按s</td>
<td>删除光标所在字符开始输入</td>
</tr>
<tr>
<td>按ESC后按S</td>
<td>删除光标所在行开始输入</td>
</tr>
</tbody></table>
</li>
<li><p>末行模式</p>
<p>作用：可以实行特定命令，可用于查找替换、保存退出等</p>
<p>进入方式：按ESC后按Shift+冒号</p>
<p>光标命令：</p>
<table>
<thead>
<tr>
<th>命令</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>n</td>
<td>光标跳转到第n行</td>
</tr>
</tbody></table>
<p>存储命令：</p>
<table>
<thead>
<tr>
<th>命令</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>w</td>
<td>保存文件</td>
</tr>
<tr>
<td>wq</td>
<td>保存并退出文件</td>
</tr>
<tr>
<td>x</td>
<td>保存并退出文件</td>
</tr>
<tr>
<td>!</td>
<td>表示强制</td>
</tr>
<tr>
<td>w!</td>
<td>强制保存</td>
</tr>
<tr>
<td>q!</td>
<td>强制退出</td>
</tr>
<tr>
<td>wq!</td>
<td>强制保存退出</td>
</tr>
</tbody></table>
<p>查找命令：</p>
<table>
<thead>
<tr>
<th>命令</th>
<th>说明</th>
<th>示例</th>
</tr>
</thead>
<tbody><tr>
<td>/</td>
<td>正向查找，按n查看下一个</td>
<td>/sunck</td>
</tr>
<tr>
<td>?</td>
<td>反向查找，按n查看上一个</td>
<td>?sunck</td>
</tr>
</tbody></table>
<p>替换命令：</p>
<table>
<thead>
<tr>
<th>示例</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>s/sunck/kaige</td>
<td>将光标所在行的第一个sunck替换为kaige</td>
</tr>
<tr>
<td>s/sunck/kaige/g</td>
<td>将光标所在行的所有sunck替换为kaige</td>
</tr>
<tr>
<td>n,s/sunck/kaige</td>
<td>将指定行的第一个sunck替换为kaige</td>
</tr>
<tr>
<td>n,s/sunck/kaige/g</td>
<td>将指定行的所有sunck替换为kaige</td>
</tr>
<tr>
<td>%s/sunck/kaige</td>
<td>将每一行的第一个sunck替换为kaige</td>
</tr>
<tr>
<td>%s/sunck/kaige/g</td>
<td>将每一行的所有sunck替换为kaige</td>
</tr>
</tbody></table>
<p>设置命令：</p>
<table>
<thead>
<tr>
<th>示例</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>set  nu</td>
<td>显示行号</td>
</tr>
<tr>
<td>set  nonu</td>
<td>取消显示行号</td>
</tr>
</tbody></table>
</li>
</ul>
</li>
<li><p>转换关系</p>
<p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/image20200212103923712.png?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10" alt="image-20200212103923712"></p>
</li>
<li><p>非法关闭处理</p>
<p>说明：当非法关闭正在编辑的文件的时候，再次打开文件会有提示信息</p>
<p>解决：</p>
<p>​        敲击enter：进入文件</p>
<p>​        保存上次写的内容：vim -r 1.txt</p>
<p>​        将产生的交换文件删除掉：rm .1.txt.swp</p>
</li>
<li><p>配置</p>
<table>
<thead>
<tr>
<th>打开文件</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>vim  ~/.vimrc</td>
<td>修改当前用户配置</td>
</tr>
<tr>
<td>sudo vim /etc/vim/vimrc</td>
<td>修改所有用户配置</td>
</tr>
</tbody></table>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">syntax on</span><br><span class="line">set nu</span><br><span class="line">set autoindent</span><br><span class="line">set smartindent</span><br><span class="line">set tabstop=4</span><br><span class="line">set shiftwidth=4</span><br><span class="line">set showmatch</span><br><span class="line">set ruler</span><br><span class="line">set cindent</span><br><span class="line">set background=dark</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="11、用户、权限管理"><a href="#11、用户、权限管理" class="headerlink" title="11、用户、权限管理"></a>11、用户、权限管理</h3><ul>
<li><p>概述</p>
<ul>
<li>用户是Unix/Linux系统工作中重要的一环，用户管理包括用户与组账号的管理</li>
<li>在Unix/Linux系统中，不论是由本机或是远程登录系统，每个系统都必须拥有一个账号，并且对于不同的系统资源拥有不同的使用权限</li>
<li>Unix/Linux系统中的root账号通常用于系统的维护和管理，它对Unix/Linux操作系统的所有部分具有不受限制的访问权限</li>
<li>在Unix/Linux安装的过程中，系统会自动创建许多用户账号，而这些默认的用户就称为“标准用户”</li>
<li>在大多数版本的Unix/Linux中，都不推荐直接使用root账号登录系统</li>
</ul>
</li>
<li><p><code>whoami</code></p>
<p>作用：查看当前系统当前账号的用户名。可通过cat /etc/passwd查看系统用户信息</p>
</li>
<li><p><code>who</code></p>
<p>作用：查看当前所有登录系统的用户信息</p>
<p>选项：</p>
<table>
<thead>
<tr>
<th>选项值</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>-m或am  I</td>
<td>只显示运行who命令的用户名、登录终端和登录时间</td>
</tr>
<tr>
<td>-q或–count</td>
<td>只显示用户的登录账号和登录用户的数量</td>
</tr>
<tr>
<td>-u或–heading</td>
<td>显示列标题</td>
</tr>
</tbody></table>
</li>
<li><p><code>exit</code></p>
<table>
<thead>
<tr>
<th>作用</th>
</tr>
</thead>
<tbody><tr>
<td>如果是图形界面，退出当前终端</td>
</tr>
<tr>
<td>如果是使用ssh远程登录，退出登陆账户</td>
</tr>
<tr>
<td>如果是切换后的登陆用户，退出则返回上一个登陆账号</td>
</tr>
</tbody></table>
</li>
<li><p><code>groupadd</code></p>
<p>作用：新建组账号</p>
<p>格式：groupadd 组名</p>
</li>
<li><p><code>groupdel</code></p>
<p>作用：删除组账号</p>
<p>格式：groupdel 组名</p>
</li>
<li><p><code>useradd</code></p>
<p>作用：在Unix/Linux中添加用户账号可以使用adduser或useradd命令，因为adduser命令是指向useradd命令的一个链接，因此，这两个命令的使用格式完全一样</p>
<p>格式：useradd  [参数]  新建用户账号</p>
<table>
<thead>
<tr>
<th>参数值</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>-d</td>
<td>指定用户登录系统时的主目录，如果不使用该参数，系统自动在/home目录下建立与用户名同名目录为主目录</td>
</tr>
<tr>
<td>-m</td>
<td>自动建立目录</td>
</tr>
<tr>
<td>-g</td>
<td>指定组名称</td>
</tr>
</tbody></table>
<table>
<thead>
<tr>
<th>使用说明</th>
</tr>
</thead>
<tbody><tr>
<td>Linux每个用户都要有一个主目录，主目录就是第一次登陆系统，用户的默认当前目录(/home/用户)</td>
</tr>
<tr>
<td>每一个用户必须有一个主目录，所以用useradd创建用户的时候，一定给用户指定一个主目录</td>
</tr>
<tr>
<td>用户的主目录一般要放到根目录的home目录下，用户的主目录和用户名是相同的</td>
</tr>
<tr>
<td>如果创建用户的时候，不指定组名，那么系统会自动创建一个和用户名一样的组名</td>
</tr>
</tbody></table>
</li>
<li><p><code>passwd</code></p>
<p>作用：在Unix/Linux中，超级用户可以使用passwd命令为普通用户设置或修改用户口令。用户也可以直接使用该命令来修改自己的口令，而无需在命令后面使用用户名</p>
</li>
<li><p><code>userdel</code></p>
<p>作用：删除用户</p>
<table>
<thead>
<tr>
<th>示例</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>userdel  kaige</td>
<td>删除kaige用户，但不会自动删除用户的主目录</td>
</tr>
<tr>
<td>userdel  -r  kaige</td>
<td>删除用户，同时删除用户的主目录</td>
</tr>
</tbody></table>
</li>
<li><p><code>su</code></p>
<p>作用：切换用户</p>
<p>注意：su后面可以加“-”。su和su –命令不同之处在于，su -切换到对应的用户时会将当前的工作目录自动转换到切换后的用户主目录</p>
</li>
<li><p>查看有哪些用户组</p>
<p>cat  /etc/group</p>
<p>groupmod  +  三次tab键</p>
</li>
<li><p><code>groups</code></p>
<p>作用：查看用户在哪些组</p>
<p>示例：groups  sunck</p>
</li>
<li><p><code>usermod</code></p>
<p>作用：修改用户所在组</p>
<p>格式：usermod  选项  用户组  用户名</p>
<table>
<thead>
<tr>
<th>选项值</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>-g</td>
<td>用来制定这个用户默认的用户组</td>
</tr>
<tr>
<td>-G</td>
<td>一般配合’-a’来完成向其它组添加</td>
</tr>
</tbody></table>
</li>
<li><p>为创建的普通用户添加sudo权限</p>
<p>注意：新创建的用户，默认不能sudo，需要进行一下操作</p>
<p>操作：</p>
<ul>
<li>修改/etc/sudoers<img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/image20200212105606513.png?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10" alt="image-20200212105606513"></li>
<li>需要强制保存退出</li>
</ul>
</li>
<li><p><code>chmod</code></p>
<p>作用：修改文件权限</p>
<p>权限：<img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/image20200212105732765.png?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10" alt="image-20200212105732765"></p>
<table>
<thead>
<tr>
<th>值</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>r</td>
<td>read 表示可读取，对于一个目录，如果没有r权限，那么就意味着不能通过ls查看这个目录的内容</td>
</tr>
<tr>
<td>w</td>
<td>write 表示可写入，对于一个目录，如果没有w权限，那么就意味着不能在目录下创建新的文件</td>
</tr>
<tr>
<td>x</td>
<td>excute 表示可执行，对于一个目录，如果没有x权限，那么就意味着不能通过cd进入这个目录</td>
</tr>
</tbody></table>
<p>修改:</p>
<ul>
<li><p>字母法</p>
<p>格式：chmod  u/g/o/a  +/-/=  rwx  文件</p>
<table>
<thead>
<tr>
<th>u/g/o/a</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>u</td>
<td>user 表示该文件的所有者</td>
</tr>
<tr>
<td>g</td>
<td>group 表示与该文件的所有者属于同一组( group )者，即用户组</td>
</tr>
<tr>
<td>o</td>
<td>other 表示其他以外的人</td>
</tr>
<tr>
<td>a</td>
<td>all 表示这三者皆是</td>
</tr>
</tbody></table>
<p>[ +-= ]说明：</p>
<table>
<thead>
<tr>
<th>+-=</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>+</td>
<td>增加权限</td>
</tr>
<tr>
<td>-</td>
<td>撤销权限</td>
</tr>
<tr>
<td>=</td>
<td>设定权限</td>
</tr>
</tbody></table>
</li>
<li><p>数字法</p>
<table>
<thead>
<tr>
<th>rwx-</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>r</td>
<td>读取权限，数字代号为 “4”</td>
</tr>
<tr>
<td>w</td>
<td>写入权限，数字代号为 “2”</td>
</tr>
<tr>
<td>x</td>
<td>执行权限，数字代号为 “1”</td>
</tr>
<tr>
<td>-</td>
<td>不具任何权限，数字代号为 “0”</td>
</tr>
</tbody></table>
<p>示例：chmod  751  file</p>
<p>说明：</p>
<p>​        文件所有者：读、写、执行权限</p>
<p>​        同组用户：读、执行的权限</p>
<p>​        其它用户：执行的权限</p>
</li>
</ul>
</li>
<li><p><code>chown</code></p>
<p>作用：修改文件所有者</p>
<p>格式：chown  新用户名  文件名</p>
</li>
<li><p><code>chgrp</code></p>
<p>作用：修改文件所属组</p>
<p>格式：chgrp  新组名  文件名</p>
</li>
</ul>
<h3 id="12、时间日期命令"><a href="#12、时间日期命令" class="headerlink" title="12、时间日期命令"></a>12、时间日期命令</h3><ul>
<li><p><code>cal</code></p>
<p>作用：查看当前日历</p>
<p>显示整年日历：cal  -y</p>
</li>
<li><p>date</p>
<p>作用：显示或设置时间</p>
<p><code>date  [MMDDhhmm[[CC]YY][.ss]] +format</code></p>
<p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/image20200212110626684.png?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10" alt="image-20200212110626684"></p>
<p>显示当前：</p>
<figure class="highlight powershell"><table><tr><td class="code"><pre><span class="line">[<span class="type">root</span>@<span class="type">centos7_base</span> ~]<span class="comment"># date</span></span><br><span class="line"><span class="number">2021</span>年 <span class="number">07</span>月 <span class="number">31</span>日 星期六 <span class="number">19</span>:<span class="number">42</span>:<span class="number">24</span> CST</span><br><span class="line">[<span class="type">root</span>@<span class="type">centos7_base</span> ~]<span class="comment"># date +%Y</span></span><br><span class="line"><span class="number">2021</span></span><br><span class="line">[<span class="type">root</span>@<span class="type">centos7_base</span> ~]<span class="comment"># date +%Y-%m-%d</span></span><br><span class="line"><span class="number">2021</span><span class="literal">-07</span><span class="literal">-31</span></span><br><span class="line">[<span class="type">root</span>@<span class="type">centos7_base</span> ~]<span class="comment"># date +&quot;%Y-%m-%d %H:%M:%S&quot;</span></span><br><span class="line"><span class="number">2021</span><span class="literal">-07</span><span class="literal">-31</span> <span class="number">19</span>:<span class="number">43</span>:<span class="number">03</span></span><br><span class="line">[<span class="type">root</span>@<span class="type">centos7_base</span> ~]<span class="comment">#</span></span><br></pre></td></tr></table></figure>

<p>显示非当前：</p>
<figure class="highlight powershell"><table><tr><td class="code"><pre><span class="line">[<span class="type">root</span>@<span class="type">centos7_base</span> ~]<span class="comment"># date -d &quot;1 days ago&quot;</span></span><br><span class="line"><span class="number">2021</span>年 <span class="number">07</span>月 <span class="number">30</span>日 星期五 <span class="number">19</span>:<span class="number">44</span>:<span class="number">16</span> CST</span><br><span class="line">[<span class="type">root</span>@<span class="type">centos7_base</span> ~]<span class="comment"># date -d &quot;-1 days ago&quot;</span></span><br><span class="line"><span class="number">2021</span>年 <span class="number">08</span>月 <span class="number">01</span>日 星期日 <span class="number">19</span>:<span class="number">44</span>:<span class="number">25</span> CST</span><br></pre></td></tr></table></figure>

<p>设置：</p>
<figure class="highlight powershell"><table><tr><td class="code"><pre><span class="line">[<span class="type">root</span>@<span class="type">centos7_base</span> ~]<span class="comment"># date</span></span><br><span class="line"><span class="number">2021</span>年 <span class="number">07</span>月 <span class="number">31</span>日 星期六 <span class="number">19</span>:<span class="number">45</span>:<span class="number">40</span> CST</span><br><span class="line">[<span class="type">root</span>@<span class="type">centos7_base</span> ~]<span class="comment"># date -s &quot;2020-11-23 20:12:03&quot;</span></span><br><span class="line"><span class="number">2020</span>年 <span class="number">11</span>月 <span class="number">23</span>日 星期一 <span class="number">20</span>:<span class="number">12</span>:<span class="number">03</span> CST</span><br><span class="line">[<span class="type">root</span>@<span class="type">centos7_base</span> ~]<span class="comment"># date</span></span><br><span class="line"><span class="number">2020</span>年 <span class="number">11</span>月 <span class="number">23</span>日 星期一 <span class="number">20</span>:<span class="number">12</span>:<span class="number">05</span> CST</span><br><span class="line">[<span class="type">root</span>@<span class="type">centos7_base</span> ~]<span class="comment"># date -s &quot;2021-07-31 11:46:45&quot;</span></span><br><span class="line"><span class="number">2021</span>年 <span class="number">07</span>月 <span class="number">31</span>日 星期六 <span class="number">11</span>:<span class="number">46</span>:<span class="number">45</span> CST</span><br><span class="line">[<span class="type">root</span>@<span class="type">centos7_base</span> ~]<span class="comment"># date</span></span><br><span class="line"><span class="number">2021</span>年 <span class="number">07</span>月 <span class="number">31</span>日 星期六 <span class="number">11</span>:<span class="number">46</span>:<span class="number">47</span> CST</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="13、磁盘分区命令"><a href="#13、磁盘分区命令" class="headerlink" title="13、磁盘分区命令"></a>13、磁盘分区命令</h3><ul>
<li><p><code>df</code>（disk free）</p>
<p>作用：检测文件系统的磁盘空间占用和空余情况，可以显示所有文件系统对节点和磁盘块的使用情况（查看磁盘空间使用情况）</p>
<p>格式：df  选项</p>
<table>
<thead>
<tr>
<th>选项</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>-a</td>
<td>显示所有文件系统的磁盘使用情况</td>
</tr>
<tr>
<td>-m</td>
<td>以1024字节为单位显示</td>
</tr>
<tr>
<td>-T</td>
<td>显示文件系统</td>
</tr>
<tr>
<td>-h</td>
<td>以人们较易阅读的 GBytes, MBytes, KBytes 等格式自行显示；</td>
</tr>
</tbody></table>
</li>
<li><p><code>fdisk</code></p>
<p>作用：查看磁盘分区详情</p>
<p>注意：该命令必须在root用户下才能使用</p>
<p>使用：<code>fdisk -l</code></p>
<table>
<thead>
<tr>
<th>选项</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>-l</td>
<td>显示所有硬盘的分区列表</td>
</tr>
</tbody></table>
<p>Boot：引导</p>
<p>Start：从X磁柱开始</p>
<p>End：到Y磁柱结束</p>
<p>Blocks：容量</p>
<p>Id：分区类型ID</p>
<p>System：分区类型</p>
</li>
<li><p><code>lsblk</code></p>
<p>作用：查看设备挂载情况</p>
<table>
<thead>
<tr>
<th>选项</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>-f</td>
<td>查看详细的设备挂载情况，显示文件系统信息</td>
</tr>
</tbody></table>
</li>
<li><p><code>mount/umount</code></p>
<p>对于Linux用户来讲，不论有几个分区，分别分给哪一个目录使用，它总归就是一个根目录、一个独立且唯一的文件结构。Linux中每个分区都是用来组成整个文件系统的一部分，它在用一种叫做“挂载”的处理方法，它整个文件系统中包含了一整套的文件和目录，并将一个分区和一个目录联系起来，要载入的那个分区将使它的存储空间在这个目录下获得。</p>
<p>功能：挂载设备</p>
<p>格式：<code>mount [-t vfstype] [-o options] device dir</code></p>
<table>
<thead>
<tr>
<th>参数</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>-t vfstype</td>
<td>指定文件系统的类型，通常不必指定。mount 会自动选择正确的类型。常用类型有：  光盘或光盘镜像：iso9660  DOS fat16文件系统：msdos  <a href="http://blog.csdn.net/hancunai0017/article/details/6995284">Windows</a> 9x fat32文件系统：vfat  Windows NT ntfs文件系统：ntfs  Mount Windows文件<a href="http://blog.csdn.net/hancunai0017/article/details/6995284">网络</a>共享：smbfs  <a href="http://blog.csdn.net/hancunai0017/article/details/6995284">UNIX</a>(LINUX) 文件网络共享：nfs</td>
</tr>
<tr>
<td>-o options</td>
<td>主要用来描述设备或档案的挂接方式。常用的参数有：  loop：用来把一个文件当成硬盘分区挂接上系统  ro：采用只读方式挂接设备  rw：采用读写方式挂接设备  　 iocharset：指定访问文件系统所用字符集</td>
</tr>
<tr>
<td>device</td>
<td>要挂接(mount)的设备</td>
</tr>
<tr>
<td>dir</td>
<td>设备在系统上的挂接点(mount point)</td>
</tr>
</tbody></table>
<p>挂载U盘：</p>
<figure class="highlight powershell"><table><tr><td class="code"><pre><span class="line">[<span class="type">root</span>@<span class="type">centos7_base</span> ~]<span class="comment"># ls /mnt/</span></span><br><span class="line">[<span class="type">root</span>@<span class="type">centos7_base</span> ~]<span class="comment"># mkdir /mnt/upan</span></span><br><span class="line">[<span class="type">root</span>@<span class="type">centos7_base</span> ~]<span class="comment"># ls /dev | grep sd</span></span><br><span class="line">sda</span><br><span class="line">sda1</span><br><span class="line">sda2</span><br></pre></td></tr></table></figure>

<p>虚拟机–&gt;可移动设备–&gt;设备名称–&gt;连接</p>
<figure class="highlight powershell"><table><tr><td class="code"><pre><span class="line">[<span class="type">root</span>@<span class="type">centos7_base</span> ~]<span class="comment"># ls /dev | grep sd</span></span><br><span class="line">sda</span><br><span class="line">sda1</span><br><span class="line">sda2</span><br><span class="line">sdb</span><br><span class="line">sdb1</span><br><span class="line">[<span class="type">root</span>@<span class="type">centos7_base</span> ~]<span class="comment"># ls /mnt/upan/</span></span><br><span class="line">[<span class="type">root</span>@<span class="type">centos7_base</span> ~]<span class="comment"># mount /dev/sdb1 /mnt/upan/</span></span><br><span class="line">[<span class="type">root</span>@<span class="type">centos7_base</span> ~]<span class="comment"># ls /mnt/upan/</span></span><br><span class="line"><span class="number">1</span>?Linux  bigData2105  System Volume Information</span><br><span class="line">[<span class="type">root</span>@<span class="type">centos7_base</span> ~]<span class="comment"># umount /dev/sdb1</span></span><br><span class="line">[<span class="type">root</span>@<span class="type">centos7_base</span> ~]<span class="comment"># ls /mnt/upan/</span></span><br><span class="line">[<span class="type">root</span>@<span class="type">centos7_base</span> ~]<span class="comment"># </span></span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="14、系统管理"><a href="#14、系统管理" class="headerlink" title="14、系统管理"></a>14、系统管理</h3><ul>
<li><p><code>ps</code></p>
<p>作用：查看进程信息</p>
<table>
<thead>
<tr>
<th>选项值</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>-a</td>
<td>显示终端上的所有进程，包括其他用户的进程</td>
</tr>
<tr>
<td>-u</td>
<td>显示进程的详细状态</td>
</tr>
<tr>
<td>-x</td>
<td>显示没有控制终端的进程</td>
</tr>
<tr>
<td>-w</td>
<td>显示加宽，以便显示更多的信息</td>
</tr>
<tr>
<td>-r</td>
<td>只显示正在运行的进程</td>
</tr>
</tbody></table>
<p>常用格式：</p>
<p><code>ps -aux</code>：查看系统中所有进程</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">USER：该进程是由哪个用户产生的</span><br><span class="line">PID：进程的ID号</span><br><span class="line">%CPU：该进程占用CPU资源的百分比，占用越高，进程越耗费资源；</span><br><span class="line">%MEM：该进程占用物理内存的百分比，占用越高，进程越耗费资源；</span><br><span class="line">VSZ：该进程占用虚拟内存的大小，单位KB；</span><br><span class="line">RSS：该进程占用实际物理内存的大小，单位KB；</span><br><span class="line">TTY：该进程是在哪个终端中运行的。其中tty1-tty7代表本地控制台终端，tty1-tty6是本地的字符界面终端，tty7是图形终端。pts/0-255代表虚拟终端。</span><br><span class="line">STAT：进程状态。常见的状态有：R：运行、S：睡眠、T：停止状态、s：包含子进程、+：位于后台</span><br><span class="line">START：该进程的启动时间</span><br><span class="line">TIME：该进程占用CPU的运算时间，注意不是系统时间</span><br><span class="line">COMMAND：产生此进程的命令名</span><br></pre></td></tr></table></figure>

<p><code>ps -ef</code>：可以查看子父进程之间的关系</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">UID：用户ID </span><br><span class="line">PID：进程ID </span><br><span class="line">PPID：父进程ID </span><br><span class="line">C：CPU用于计算执行优先级的因子。数值越大，表明进程是CPU密集型运算，执行优先级会降低；数值越小，表明进程是I/O密集型运算，执行优先级会提高 </span><br><span class="line">STIME：进程启动的时间 </span><br><span class="line">TTY：完整的终端名称 </span><br><span class="line">TIME：CPU时间 </span><br><span class="line">CMD：启动进程所用的命令和参数</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
<li><p><code>kill</code></p>
<p>作用：终止进程</p>
<p>格式：kill  [-signal]  pid</p>
<p>注意：信号值从0到15，其中9为绝对终止，可以处理一般信号无法终止的进程</p>
</li>
<li><p><code>pstree </code></p>
<p>作用：查看进程树</p>
<p>格式：pstree [选项]</p>
<table>
<thead>
<tr>
<th>选项</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>-p</td>
<td>显示进程的PID</td>
</tr>
<tr>
<td>-u</td>
<td>显示进程的所属用户</td>
</tr>
</tbody></table>
</li>
<li><p><code>top</code></p>
<p>作用：动态显示进程</p>
<p>格式：top [选项]</p>
<table>
<thead>
<tr>
<th>选项</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>-d 秒数</td>
<td>指定top命令每隔几秒更新。默认是3秒在top命令的交互模式当中可以执行的命令：</td>
</tr>
<tr>
<td>-i</td>
<td>使top不显示任何闲置或者僵死进程。</td>
</tr>
<tr>
<td>-p</td>
<td>通过指定监控进程ID来仅仅监控某个进程的状态。</td>
</tr>
</tbody></table>
<p>说明：能够在运行后，在指定的时间间隔更新显示信息。可以在使用top命令时加上-d 来指定显示信息更新的时间间隔</p>
<table>
<thead>
<tr>
<th>按键</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>M</td>
<td>根据内存使用量来排序</td>
</tr>
<tr>
<td>P</td>
<td>根据CPU占有率来排序</td>
</tr>
<tr>
<td>T</td>
<td>根据进程运行时间的长短来排序</td>
</tr>
<tr>
<td>U</td>
<td>可以根据后面输入的用户名来筛选进程</td>
</tr>
<tr>
<td>k</td>
<td>可以根据后面输入的PID来杀死进程</td>
</tr>
<tr>
<td>q</td>
<td>退出</td>
</tr>
<tr>
<td>h</td>
<td>获得帮助</td>
</tr>
</tbody></table>
<ol>
<li><p>第一行信息为任务队列信息</p>
<table>
<thead>
<tr>
<th align="left">内容</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td align="left">12:26:46</td>
<td>系统当前时间</td>
</tr>
<tr>
<td align="left">up 1 day, 13:32</td>
<td>系统的运行时间，本机已经运行1天  13小时32分钟</td>
</tr>
<tr>
<td align="left">2 users</td>
<td>当前登录了两个用户</td>
</tr>
<tr>
<td align="left">load average:   0.00, 0.00, 0.00</td>
<td>系统在之前1分钟，5分钟，15分钟的平均负载。一般认为小于1时，负载较小。如果大于1，系统已经超出负荷。</td>
</tr>
</tbody></table>
</li>
<li><p>第二行信息为任务队列信息</p>
<table>
<thead>
<tr>
<th></th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>Tasks: 95 total</td>
<td>系统中的进程总数</td>
</tr>
<tr>
<td>1 running</td>
<td>正在运行的进程数</td>
</tr>
<tr>
<td>94 sleeping</td>
<td>睡眠的进程</td>
</tr>
<tr>
<td>0 stopped</td>
<td>正在停止的进程</td>
</tr>
<tr>
<td>0 zombie</td>
<td>僵尸进程。如果不是0，需要手工检查僵尸进程</td>
</tr>
</tbody></table>
</li>
<li><p>第三信息为任务队列信息</p>
<table>
<thead>
<tr>
<th>内容</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>Cpu(s): 0.1%us</td>
<td>用户模式占用的CPU百分比</td>
</tr>
<tr>
<td>0.1%sy</td>
<td>系统模式占用的CPU百分比</td>
</tr>
<tr>
<td>0.0%ni</td>
<td>改变过优先级的用户进程占用的CPU百分比</td>
</tr>
<tr>
<td>99.7%id</td>
<td>空闲CPU的CPU百分比</td>
</tr>
<tr>
<td>0.1%wa</td>
<td>等待输入/输出的进程的占用CPU百分比</td>
</tr>
<tr>
<td>0.0%hi</td>
<td>硬中断请求服务占用的CPU百分比</td>
</tr>
<tr>
<td>0.1%si</td>
<td>软中断请求服务占用的CPU百分比</td>
</tr>
<tr>
<td>0.0%st</td>
<td>st（Steal time）虚拟时间百分比。就是当有虚拟机时，虚拟CPU等待实际CPU的时间百分比。</td>
</tr>
</tbody></table>
</li>
<li><p>第四行信息为任务队列信息</p>
<table>
<thead>
<tr>
<th>内容</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>Mem:  625344k total</td>
<td>物理内存的总量，单位KB</td>
</tr>
<tr>
<td>571504k used</td>
<td>已经使用的物理内存数量</td>
</tr>
<tr>
<td>53840k free</td>
<td>空闲的物理内存数量，我们使用的是虚拟机，总共只分配了628MB内存，所以只有53MB的空闲内存了</td>
</tr>
<tr>
<td>65800k buffers</td>
<td>作为缓冲的内存数量</td>
</tr>
</tbody></table>
</li>
<li><p>第五行为交换分区（swap）信息</p>
<table>
<thead>
<tr>
<th>Swap:  524280k total</th>
<th>交换分区（虚拟内存）的总大小</th>
</tr>
</thead>
<tbody><tr>
<td>0k used</td>
<td>已经使用的交互分区的大小</td>
</tr>
<tr>
<td>524280k free</td>
<td>空闲交换分区的大小</td>
</tr>
<tr>
<td>409280k cached</td>
<td>作为缓存的交互分区的大小</td>
</tr>
</tbody></table>
</li>
</ol>
</li>
<li><p><code>netstat</code></p>
<p>作用：显示网络统计信息和端口占用情况</p>
<ul>
<li><p><code>netstat -anp | grep 进程号</code>   </p>
<p>功能描述：查看该进程网络信息</p>
</li>
<li><p><code>netstat –nlp | grep 端口号</code>   </p>
<p>能描述：查看网络端口号占用情况</p>
</li>
</ul>
<table>
<thead>
<tr>
<th>选项</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>-n</td>
<td>拒绝显示别名，能显示数字的全部转化成数字</td>
</tr>
<tr>
<td>-l</td>
<td>仅列出有在listen（监听）的服务状态</td>
</tr>
<tr>
<td>-p</td>
<td>表示显示哪个进程在调用</td>
</tr>
</tbody></table>
<p>查看某端口号是否被占用：</p>
<figure class="highlight powershell"><table><tr><td class="code"><pre><span class="line">[<span class="type">root</span>@<span class="type">centos7_base</span> <span class="type">etc</span>]<span class="comment"># netstat -nltp | grep 22</span></span><br><span class="line">tcp        <span class="number">0</span>      <span class="number">0</span> <span class="number">192.168</span>.<span class="number">122.1</span>:<span class="number">53</span>        <span class="number">0.0</span>.<span class="number">0.0</span>:*               LISTEN      <span class="number">1384</span>/dnsmasq        </span><br><span class="line">tcp        <span class="number">0</span>      <span class="number">0</span> <span class="number">0.0</span>.<span class="number">0.0</span>:<span class="number">22</span>              <span class="number">0.0</span>.<span class="number">0.0</span>:*               LISTEN      <span class="number">1009</span>/sshd           </span><br><span class="line">tcp6       <span class="number">0</span>      <span class="number">0</span> :::<span class="number">22</span>                   :::*                    LISTEN      <span class="number">1009</span>/sshd </span><br></pre></td></tr></table></figure>

<p>通过进程号查看该进程的网络信息：</p>
<figure class="highlight powershell"><table><tr><td class="code"><pre><span class="line">[<span class="type">root</span>@<span class="type">centos7_base</span> <span class="type">etc</span>]<span class="comment"># netstat -anp | grep sshd</span></span><br><span class="line">tcp        <span class="number">0</span>      <span class="number">0</span> <span class="number">0.0</span>.<span class="number">0.0</span>:<span class="number">22</span>              <span class="number">0.0</span>.<span class="number">0.0</span>:*               LISTEN      <span class="number">1009</span>/sshd           </span><br><span class="line">tcp        <span class="number">0</span>      <span class="number">0</span> <span class="number">192.168</span>.<span class="number">100.5</span>:<span class="number">22</span>        <span class="number">192.168</span>.<span class="number">100.1</span>:<span class="number">52900</span>     ESTABLISHED <span class="number">1775</span>/sshd: root@pts </span><br><span class="line">tcp6       <span class="number">0</span>      <span class="number">0</span> :::<span class="number">22</span>                   :::*                    LISTEN      <span class="number">1009</span>/sshd           </span><br><span class="line">unix  <span class="number">3</span>      [ ]         STREAM     CONNECTED     <span class="number">23573</span>    <span class="number">1009</span>/sshd            </span><br><span class="line">unix  <span class="number">2</span>      [ ]         DGRAM                    <span class="number">31107</span>    <span class="number">1775</span>/sshd: root@pts  </span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
<li><p><code>reboot、shutdown、init</code></p>
<p>作用：关机重启</p>
<table>
<thead>
<tr>
<th>示例</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>reboot</td>
<td>重新启动操作系统</td>
</tr>
<tr>
<td>shutdown –r now</td>
<td>重新启动操作系统，shutdown会给别的用户提示</td>
</tr>
<tr>
<td>shutdown -h now</td>
<td>立刻关机，其中now相当于时间为0的状态</td>
</tr>
<tr>
<td>shutdown -h 20:25</td>
<td>系统在今天的20:25 会关机</td>
</tr>
<tr>
<td>shutdown -h +10</td>
<td>系统再过十分钟后自动关机</td>
</tr>
<tr>
<td>init 0</td>
<td>关机</td>
</tr>
<tr>
<td>init 6</td>
<td>重启</td>
</tr>
</tbody></table>
</li>
</ul>
<h3 id="15、网络配置"><a href="#15、网络配置" class="headerlink" title="15、网络配置"></a>15、网络配置</h3><h3 id="16、系统定时任务"><a href="#16、系统定时任务" class="headerlink" title="16、系统定时任务"></a>16、系统定时任务</h3><ul>
<li><p>crond 服务</p>
<p><code>systemctl status crond</code></p>
<p><code>systemctl stop crond</code></p>
<p><code>systemctl start crond</code></p>
<p><code>systemctl restart crond</code></p>
</li>
<li><p>crontab 定时任务设置</p>
<p>语法：crontab [选项]</p>
<table>
<thead>
<tr>
<th>选项</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>-e</td>
<td>编辑crontab定时任务</td>
</tr>
<tr>
<td>-l</td>
<td>查询crontab任务</td>
</tr>
<tr>
<td>-r</td>
<td>删除当前用户所有的crontab任务</td>
</tr>
</tbody></table>
<p>进入crontab编辑界面。会打开vim编辑你的工作</p>
<p>格式：* * * * * 执行的任务</p>
<table>
<thead>
<tr>
<th>项目</th>
<th>含义</th>
<th>范围</th>
</tr>
</thead>
<tbody><tr>
<td>第一个“*”</td>
<td>一小时当中的第几分钟</td>
<td>0-59</td>
</tr>
<tr>
<td>第二个“*”</td>
<td>一天当中的第几小时</td>
<td>0-23</td>
</tr>
<tr>
<td>第三个“*”</td>
<td>一个月当中的第几天</td>
<td>1-31</td>
</tr>
<tr>
<td>第四个“*”</td>
<td>一年当中的第几月</td>
<td>1-12</td>
</tr>
<tr>
<td>第五个“*”</td>
<td>一周当中的星期几</td>
<td>0-7（0和7都代表星期日）</td>
</tr>
</tbody></table>
<p>特殊符号:</p>
<table>
<thead>
<tr>
<th>特殊符号</th>
<th>含义</th>
</tr>
</thead>
<tbody><tr>
<td>*</td>
<td>代表任何时间。比如第一个“*”就代表一小时中每分钟都执行一次的意思。</td>
</tr>
<tr>
<td>，</td>
<td>代表不连续的时间。比如“0 8,12,16 * * * 命令”，就代表在每天的8点0分，12点0分，16点0分都执行一次命令</td>
</tr>
<tr>
<td>-</td>
<td>代表连续的时间范围。比如“0 5 *   * 1-6命令”，代表在周一到周六的凌晨5点0分执行命令</td>
</tr>
<tr>
<td>*/n</td>
<td>代表每隔多久执行一次。比如“*/10 *   * * * 命令”，代表每隔10分钟就执行一遍命令</td>
</tr>
</tbody></table>
<p>特定时间执行命令:</p>
<ul>
<li><p><code>45 22 * * * 命令</code></p>
<p>在22点45分执行命令</p>
</li>
<li><p><code>0 17 * * 1 命令</code></p>
<p>每周1 的17点0分执行命令</p>
</li>
<li><p><code>0 5 1,15 * * 命令</code></p>
<p>每月1号和15号的凌晨5点0分执行命令</p>
</li>
<li><p><code>40 4 * * 1-5 命令</code></p>
<p>每周一到周五的凌晨4点40分执行命令</p>
</li>
<li><p><code>*/10 4 * * * 命令</code></p>
<p>每天的凌晨4点，每隔10分钟执行一次命令</p>
</li>
<li><p><code>0 0 1,15 * 1 命令</code></p>
<p>每月1号和15号，每周1的0点0分都会执行命令。注意：星期几和几号最好不要同时出现，因为他们定义的都是天。非常容易让管理员混乱。</p>
</li>
</ul>
</li>
</ul>
<h3 id="17、软件包管理"><a href="#17、软件包管理" class="headerlink" title="17、软件包管理"></a>17、软件包管理</h3><h3 id="18、远程连接与拷贝"><a href="#18、远程连接与拷贝" class="headerlink" title="18、远程连接与拷贝"></a>18、远程连接与拷贝</h3><p>通常在工作过程中，公司中使用的真实服务器或者是云服务器，都不允许除运维人员之外的员工直接接触，因此就需要通过远程登录的方式来操作。所以，远程登录工具就是必不可缺的，目前，比较主流的有Xshell, SSH Secure Shell, SecureCRT,FinalShell等，同学们可以根据自己的习惯自行选择</p>
<ul>
<li><p>ssh</p>
<ul>
<li><p>概述</p>
<p>SSH为Secure Shell的缩写，由 IETF 的网络工作小组（Network Working Group）所制定；SSH 为建立在应用层和传输层基础上的安全协议</p>
<p>SSH是目前较可靠，专为远程登录会话和其他网络服务提供安全性的协议。常用于远程登录，以及用户之间进行资料拷贝</p>
<p>利用SSH协议可以有效防止远程管理过程中的信息泄露问题。SSH最初是 UNIX 系统上的一个程序，后来又迅速扩展到其他操作平台。SSH 在正确使用时可弥补网络中的漏洞。SSH 客户端适用于多种平台。几乎所有 UNIX 平台—包括 HP-UX、Linux、AIX、Solaris、Digital UNIX、Irix，以及其他平台，都可运行SSH</p>
<p>使用SSH服务，需要安装相应的服务器和客户端。客户端和服务器的关系：如果，A机器想被B机器远程控制，那么，A机器需要安装SSH服务器，B机器需要安装SSH客户端</p>
</li>
<li><p>远程连接</p>
<p>格式：ssh  用户名@IP</p>
<p>示例：ssh  <a href="mailto:&#114;&#x6f;&#111;&#x74;&#x40;&#x31;&#x39;&#x32;&#46;&#49;&#x36;&#56;&#46;&#x30;&#46;&#49;">&#114;&#x6f;&#111;&#x74;&#x40;&#x31;&#x39;&#x32;&#46;&#49;&#x36;&#56;&#46;&#x30;&#46;&#49;</a></p>
<p>注意：第一次连接会出现问题，输入yes后回车，以后ssh不会出现</p>
<p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/image20200212112656835.png?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10" alt="image-20200212112656835"></p>
<p>可能存在的问题：使用ssh访问，如访问出现错误。可查看是否有该文件 ～/.ssh/known_ssh 尝试删除该文件解决</p>
</li>
<li><p>注意</p>
<p>Windows可以通过使用Xshell实现远程连接Linux</p>
<p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/20200212113130.png?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10" alt="2020-02-12_113130"></p>
<p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/20200212113220.png?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10" alt="2020-02-12_113220"></p>
</li>
</ul>
</li>
<li><p>scp</p>
<p>作用：远程拷贝文件</p>
<p>本地文件复制到远程：scp  -r  本机文件的绝对路径或者相对路径  目标用户名@目标主机IP地址:目标文件的绝对路径</p>
<p>远程文件复制到本地：scp  -r  目标用户名@目标主机IP地址:目标文件的绝对路径  保存到本机的绝对路径或者相对路径</p>
<p>注意：Windows可以通过winscp实现远程拷贝</p>
<p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/20200212113738.png?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10" alt="2020-02-12_113738"></p>
<p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/20200212113751.png?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10" alt="2020-02-12_113751"></p>
<p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/20200212113807.png?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10" alt="2020-02-12_113807"></p>
<p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/20200212113832.png?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10" alt="2020-02-12_113832"></p>
</li>
</ul>
<h3 id="19、克隆虚拟机"><a href="#19、克隆虚拟机" class="headerlink" title="19、克隆虚拟机"></a>19、克隆虚拟机</h3><p><strong>创建基础系统(界面)</strong></p>
<ol>
<li><p>创建流程</p>
</li>
<li><p>切换到root用户</p>
</li>
<li><p>配置静态IP</p>
<p><code>vim /etc/sysconfig/network-scripts/ifcfg-ens33</code></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">TYPE=Ethernet</span><br><span class="line">PROXY_METHOD=none</span><br><span class="line">BROWSER_ONLY=no</span><br><span class="line">BOOTPROTO=static</span><br><span class="line">DEFROUTE=yes</span><br><span class="line">IPV4_FAILURE_FATAL=no</span><br><span class="line">IPV6INIT=yes</span><br><span class="line">IPV6_AUTOCONF=yes</span><br><span class="line">IPV6_DEFROUTE=yes</span><br><span class="line">IPV6_FAILURE_FATAL=no</span><br><span class="line">IPV6_ADDR_GEN_MODE=stable-privacy</span><br><span class="line">NAME=ens33</span><br><span class="line">UUID=d901d918-f6af-48cc-845c-dfc0e3e41f71</span><br><span class="line">DEVICE=ens33</span><br><span class="line">ONBOOT=yes</span><br><span class="line">IPADDR=192.168.100.5</span><br><span class="line">GATEWAY=192.168.100.2</span><br><span class="line">DNS1=192.168.100.2</span><br></pre></td></tr></table></figure></li>
<li><p>修改主机名</p>
<p><code>vim /etc/hostname</code></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">centos7_base</span><br></pre></td></tr></table></figure></li>
<li><p>关闭防火墙</p>
<p><code>systemctl stop firewalld</code></p>
<p><code>systemctl disable firewalld</code></p>
</li>
<li><p>atguigu用户添加管理员权限</p>
<p><code>vim /etc/sudoers</code></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">%</span><span class="bash">wheel  ALL=(ALL)       ALL</span></span><br><span class="line">atguigu ALL=(ALL)       ALL</span><br></pre></td></tr></table></figure></li>
<li><p>添加vim配置</p>
<p><code>vim  ~/.vimrc</code>针对当前用户生效</p>
<p><code>vim /etc/vimrc</code>针对所有用户生效</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">syntax on</span><br><span class="line">set nu</span><br><span class="line">set autoindent</span><br><span class="line">set smartindent</span><br><span class="line">set tabstop=4</span><br><span class="line">set shiftwidth=4</span><br><span class="line">set showmatch</span><br><span class="line">set ruler</span><br><span class="line">set cindent</span><br><span class="line">set background=dark</span><br></pre></td></tr></table></figure></li>
<li><p>重启</p>
</li>
<li><p>安装epel-release</p>
<p><code>yum install -y epel-release</code></p>
</li>
</ol>
<p><strong>创建基础系统(最小化)</strong></p>
<ol>
<li><p>创建流程（使用最小化）</p>
</li>
<li><p>root用户登录</p>
</li>
<li><p>配置静态IP</p>
<p><code>vi /etc/sysconfig/network-scripts/ifcfg-ens33</code></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">TYPE=Ethernet</span><br><span class="line">PROXY_METHOD=none</span><br><span class="line">BROWSER_ONLY=no</span><br><span class="line">BOOTPROTO=static</span><br><span class="line">DEFROUTE=yes</span><br><span class="line">IPV4_FAILURE_FATAL=no</span><br><span class="line">IPV6INIT=yes</span><br><span class="line">IPV6_AUTOCONF=yes</span><br><span class="line">IPV6_DEFROUTE=yes</span><br><span class="line">IPV6_FAILURE_FATAL=no</span><br><span class="line">IPV6_ADDR_GEN_MODE=stable-privacy</span><br><span class="line">NAME=ens33</span><br><span class="line">UUID=d901d918-f6af-48cc-845c-dfc0e3e41f71</span><br><span class="line">DEVICE=ens33</span><br><span class="line">ONBOOT=yes</span><br><span class="line">IPADDR=192.168.100.6</span><br><span class="line">GATEWAY=192.168.100.2</span><br><span class="line">DNS1=192.168.100.2</span><br></pre></td></tr></table></figure></li>
<li><p>修改主机名</p>
<p><code>vi /etc/hostname</code></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">centos7_mix</span><br></pre></td></tr></table></figure></li>
<li><p>关闭防火墙</p>
<p><code>systemctl stop firewalld</code></p>
<p><code>systemctl disable firewalld</code></p>
</li>
<li><p>atguigu用户添加管理员权限</p>
<p><code>vi /etc/sudoers</code></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">%</span><span class="bash">wheel  ALL=(ALL)       ALL</span></span><br><span class="line">atguigu ALL=(ALL)       ALL</span><br></pre></td></tr></table></figure></li>
<li><p>重启</p>
<p>reboot</p>
</li>
<li><p>安装epel-release</p>
<p><code>yum install -y epel-release</code></p>
</li>
<li><p>net-tool：工具包集合，包含ifconfig等命令</p>
<p><code>yum install -y net-tools</code></p>
</li>
<li><p>vim：编辑器</p>
<p><code>yum install -y vim</code></p>
</li>
<li><p>添加vim配置</p>
<p><code>vim  ~/.vimrc</code>针对当前用户生效</p>
<p><code>vim /etc/vimrc</code>针对所有用户生效</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">syntax on</span><br><span class="line">set nu</span><br><span class="line">set autoindent</span><br><span class="line">set smartindent</span><br><span class="line">set tabstop=4</span><br><span class="line">set shiftwidth=4</span><br><span class="line">set showmatch</span><br><span class="line">set ruler</span><br><span class="line">set cindent</span><br><span class="line">set background=dark</span><br></pre></td></tr></table></figure></li>
</ol>
<p><strong>克隆虚拟机</strong></p>
<ol>
<li><code>su - root</code></li>
<li><code>vim /etc/sysconfig/network-scripts/ifcfg-ens33</code></li>
<li><code>vim /etc/hostname</code></li>
<li><code>vim /etc/hosts</code></li>
<li><code>reboot</code></li>
<li><code>C:\Windows\System32\drivers\etc</code></li>
</ol>
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title>Hive</title>
    <url>/2021/11/07/Hive/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="Hive"><a href="#Hive" class="headerlink" title="Hive"></a>Hive</h1><h1 id="一、Hive基本概念"><a href="#一、Hive基本概念" class="headerlink" title="一、Hive基本概念"></a>一、Hive基本概念</h1><h2 id="1-1-什么是Hive"><a href="#1-1-什么是Hive" class="headerlink" title="1.1 什么是Hive"></a>1.1 什么是Hive</h2><p><a href="https://cloud.tencent.com/developer/article/1456786">通俗易懂理解hive是什么</a></p>
<ol>
<li>Hive简介<ul>
<li>Hive：由Facebook开源用于解决海量结构化日志的数据统计工具。</li>
<li>Hive是基于Hadoop的一个数据仓库工具，可以将结构化的数据文件映射为一张表，并提供类SQL查询功能。</li>
</ul>
</li>
<li>Hive本质：将HQL转化成MapReduce程序<ol>
<li>Hive处理的数据存储在HDFS</li>
<li>Hive分析数据底层的实现是MapReduce</li>
<li>执行程序运行在Yarn上<br><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16346965217937.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></li>
</ol>
</li>
</ol>
<h2 id="1-2Hive的优缺点"><a href="#1-2Hive的优缺点" class="headerlink" title="1.2Hive的优缺点"></a>1.2Hive的优缺点</h2><h3 id="1-2-1-优点"><a href="#1-2-1-优点" class="headerlink" title="1.2.1 优点"></a>1.2.1 优点</h3><ol>
<li>操作接口采用类SQL语法，提供快速开发的能力（简单、容易上手）。</li>
<li>避免了去写MapReduce，减少开发人员的学习成本。</li>
<li>Hive的执行延迟比较高，因此Hive常用于数据分析，对实时性要求不高的场合。</li>
<li>Hive优势在于处理大数据，对于处理小数据没有优势，因为Hive的执行延迟比较高。</li>
<li>Hive支持用户自定义函数，用户可以根据自己的需求来实现自己的函数。<h3 id="1-2-2-缺点"><a href="#1-2-2-缺点" class="headerlink" title="1.2.2 缺点"></a>1.2.2 缺点</h3></li>
<li>Hive的HQL表达能力有限<ol>
<li>迭代式算法无法表达</li>
<li>数据挖掘方面不擅长，由于MapReduce数据处理流程的限制，效率更高的算法却无法实现。</li>
</ol>
</li>
<li>Hive的效率比较低<ol>
<li>Hive自动生成的MapReduce作业，通常情况下不够智能化</li>
<li>Hive调优比较困难，粒度较粗</li>
</ol>
</li>
</ol>
<h2 id="1-3-Hive架构原理"><a href="#1-3-Hive架构原理" class="headerlink" title="1.3 Hive架构原理"></a>1.3 Hive架构原理</h2><p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16346968536473.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
<ol>
<li>用户接口：Client<ul>
<li>CLI（command-line interface）、JDBC/ODBC(jdbc访问hive)、WEBUI（浏览器访问hive）</li>
</ul>
</li>
<li>元数据：Metastore<ul>
<li>元数据包括：表名、表所属的数据库（默认是default）、表的拥有者、列/分区字段、表的类型（是否是外部表）、表的数据所在目录等；</li>
<li>默认存储在自带的derby数据库中，推荐使用MySQL存储Metastore</li>
</ul>
</li>
<li>Hadoop<ul>
<li>使用HDFS进行存储，使用MapReduce进行计算。</li>
</ul>
</li>
<li>驱动器：Driver<ul>
<li>解析器（SQL Parser）：将SQL字符串转换成抽象语法树AST，这一步一般都用第三方工具库完成，比如antlr；对AST进行语法分析，比如表是否存在、字段是否存在、SQL语义是否有误。</li>
<li>编译器（Physical Plan）：将AST编译生成逻辑执行计划。</li>
<li>优化器（Query Optimizer）：对逻辑执行计划进行优化。</li>
<li>执行器（Execution）：把逻辑执行计划转换成可以运行的物理计划。对于Hive来说，就是MR/Spark。</li>
</ul>
</li>
</ol>
<h2 id="1-4-Hive和-数据库比较"><a href="#1-4-Hive和-数据库比较" class="headerlink" title="1.4 Hive和 数据库比较"></a>1.4 Hive和 数据库比较</h2><ul>
<li>由于 Hive 采用了类似SQL 的查询语言 HQL(Hive Query Language)，因此很容易将 Hive 理解为数据库。其实从结构上来看，Hive 和数据库除了拥有类似的查询语言，再无类似之处。本文将从多个方面来阐述 Hive 和数据库的差异。数据库可以用在 Online 的应用中，但是Hive 是为数据仓库而设计的，清楚这一点，有助于从应用角度理解 Hive 的特性。<h3 id="1-4-1-查询语言"><a href="#1-4-1-查询语言" class="headerlink" title="1.4.1 查询语言"></a>1.4.1 查询语言</h3></li>
<li>由于SQL被广泛的应用在数据仓库中，因此，专门针对Hive的特性设计了类SQL的查询语言HQL。熟悉SQL开发的开发者可以很方便的使用Hive进行开发。<h3 id="1-4-2-数据更新"><a href="#1-4-2-数据更新" class="headerlink" title="1.4.2 数据更新"></a>1.4.2 数据更新</h3></li>
<li>由于Hive是针对数据仓库应用设计的，而数据仓库的内容是读多写少的。因此，Hive中不建议对数据的改写，所有的数据都是在加载的时候确定好的。而数据库中的数据通常是需要经常进行修改的，因此可以使用 INSERT INTO …  VALUES 添加数据，使用 UPDATE … SET修改数据。<h3 id="1-4-3-执行延迟"><a href="#1-4-3-执行延迟" class="headerlink" title="1.4.3 执行延迟"></a>1.4.3 执行延迟</h3></li>
<li>Hive 在查询数据的时候，由于没有索引，需要扫描整个表，因此延迟较高。另外一个导致 Hive 执行延迟高的因素是 MapReduce框架。由于MapReduce 本身具有较高的延迟，因此在利用MapReduce 执行Hive查询时，也会有较高的延迟。相对的，数据库的执行延迟较低。当然，这个低是有条件的，即数据规模较小，当数据规模大到超过数据库的处理能力的时候，Hive的并行计算显然能体现出优势。<h3 id="1-4-4-数据规模"><a href="#1-4-4-数据规模" class="headerlink" title="1.4.4 数据规模"></a>1.4.4 数据规模</h3></li>
<li>由于Hive建立在集群上并可以利用MapReduce进行并行计算，因此可以支持很大规模的数据；对应的，数据库可以支持的数据规模较小。</li>
</ul>
<h1 id="二、Hive安装"><a href="#二、Hive安装" class="headerlink" title="二、Hive安装"></a>二、Hive安装</h1><h2 id="2-1-Hive安装地址"><a href="#2-1-Hive安装地址" class="headerlink" title="2.1 Hive安装地址"></a>2.1 Hive安装地址</h2><ol>
<li>Hive官网地址<ul>
<li><a href="http://hive.apache.org/">http://hive.apache.org/</a></li>
</ul>
</li>
<li>文档查看地址<ul>
<li><a href="https://cwiki.apache.org/confluence/display/Hive/GettingStarted">https://cwiki.apache.org/confluence/display/Hive/GettingStarted</a></li>
</ul>
</li>
<li>下载地址<ul>
<li><a href="http://archive.apache.org/dist/hive/">http://archive.apache.org/dist/hive/</a></li>
</ul>
</li>
<li>github地址<ul>
<li><a href="https://github.com/apache/hive">https://github.com/apache/hive</a></li>
</ul>
</li>
</ol>
<h2 id="2-2-MySql安装"><a href="#2-2-MySql安装" class="headerlink" title="2.2 MySql安装"></a>2.2 MySql安装</h2><ul>
<li>参考<a href="mweblib://16347028415943">Mysql安装</a></li>
</ul>
<h2 id="2-3-Hive安装部署"><a href="#2-3-Hive安装部署" class="headerlink" title="2.3 Hive安装部署"></a>2.3 Hive安装部署</h2><ol>
<li>把apache-hive-3.1.2-bin.tar.gz上传到linux的/opt/software目录下  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 software]$ <span class="built_in">pwd</span></span><br><span class="line">/opt/software</span><br><span class="line">[atguigu@hadoop001 software]$ ll</span><br><span class="line">total 835216</span><br><span class="line">-rw-r--r--. 1 atguigu atguigu 312850286 Apr 20  2020 apache-hive-3.1.2-bin.tar.gz</span><br><span class="line">-rw-r--r--. 1 atguigu atguigu   9311744 Apr 20  2020 apache-zookeeper-3.5.7-bin.tar.gz</span><br><span class="line">-rw-r--r--. 1 atguigu atguigu 338075860 Oct  9 12:59 hadoop-3.1.3.tar.gz</span><br><span class="line">-rw-r--r--. 1 atguigu atguigu 195013152 Oct  9 12:59 jdk-8u212-linux-x64.tar.gz</span><br><span class="line">[atguigu@hadoop001 software]$</span><br></pre></td></tr></table></figure></li>
<li>解压apache-hive-3.1.2-bin.tar.gz到/opt/module/目录下面 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 software]$ tar -zxvf /opt/software/apache-hive-3.1.2-bin.tar.gz -C /opt/module/</span><br><span class="line">···</span><br><span class="line">[atguigu@hadoop001 module]$ ll</span><br><span class="line">total 0</span><br><span class="line">drwxrwxr-x.  9 atguigu atguigu 153 Oct 20 13:55 apache-hive-3.1.2-bin</span><br><span class="line">drwxr-xr-x.  9 atguigu atguigu 149 Oct 19 14:29 hadoop-3.1.3</span><br><span class="line">drwxr-xr-x. 11 atguigu atguigu 173 Oct 19 14:45 ha-hadoop-3.1.3</span><br><span class="line">drwxr-xr-x.  7 atguigu atguigu 245 Apr  2  2019 jdk1.8.0_212</span><br><span class="line">drwxrwxr-x.  8 atguigu atguigu 160 Oct 18 13:44 zookeeper-3.5.7</span><br><span class="line">[atguigu@hadoop001 module]$</span><br></pre></td></tr></table></figure></li>
<li>修改apache-hive-3.1.2-bin.tar.gz的名称为hive<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 module]$ mv apache-hive-3.1.2-bin hive-3.1.2</span><br><span class="line">[atguigu@hadoop001 module]$ ll</span><br><span class="line">total 0</span><br><span class="line">drwxr-xr-x.  9 atguigu atguigu 149 Oct 19 14:29 hadoop-3.1.3</span><br><span class="line">drwxr-xr-x. 11 atguigu atguigu 173 Oct 19 14:45 ha-hadoop-3.1.3</span><br><span class="line">drwxrwxr-x.  9 atguigu atguigu 153 Oct 20 13:55 hive-3.1.2</span><br><span class="line">drwxr-xr-x.  7 atguigu atguigu 245 Apr  2  2019 jdk1.8.0_212</span><br><span class="line">drwxrwxr-x.  8 atguigu atguigu 160 Oct 18 13:44 zookeeper-3.5.7</span><br><span class="line">[atguigu@hadoop001 module]$</span><br></pre></td></tr></table></figure></li>
<li>修改/etc/profile.d/set_env.sh，添加环境变量 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 module]$ cat /etc/profile.d/set_env.sh</span><br><span class="line"><span class="comment">#JAVA_HOME</span></span><br><span class="line"><span class="built_in">export</span> JAVA_HOME=/opt/module/jdk1.8.0_212</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$JAVA_HOME</span>/bin</span><br><span class="line"></span><br><span class="line"><span class="comment">#HADOOP_HOME</span></span><br><span class="line"><span class="built_in">export</span> HADOOP_HOME=/opt/module/ha-hadoop-3.1.3</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$HADOOP_HOME</span>/bin</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$HADOOP_HOME</span>/sbin</span><br><span class="line"></span><br><span class="line"><span class="comment">#HIVE_HOME</span></span><br><span class="line"><span class="built_in">export</span> HIVE_HOME=/opt/module/hive-3.1.2</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$HIVE_HOME</span>/bin</span><br><span class="line">[atguigu@hadoop001 module]$</span><br></pre></td></tr></table></figure></li>
<li>解决日志Jar包冲突 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 ~]$ ll <span class="variable">$HIVE_HOME</span>/lib/log4j-slf4j-impl-2.10.0.jar</span><br><span class="line">-rw-rw-r--. 1 atguigu atguigu 24173 Apr 15  2020 /opt/module/hive-3.1.2/lib/log4j-slf4j-impl-2.10.0.jar</span><br><span class="line">[atguigu@hadoop001 ~]$ mv <span class="variable">$HIVE_HOME</span>/lib/log4j-slf4j-impl-2.10.0.jar <span class="variable">$HIVE_HOME</span>/lib/log4j-slf4j-impl-2.10.0.jar_bak</span><br><span class="line">[atguigu@hadoop001 ~]$ ll <span class="variable">$HIVE_HOME</span>/lib/log4j-slf4j-impl-2.10.0.jar_bak</span><br><span class="line">-rw-rw-r--. 1 atguigu atguigu 24173 Apr 15  2020 /opt/module/hive-3.1.2/lib/log4j-slf4j-impl-2.10.0.jar_bak</span><br><span class="line">[atguigu@hadoop001 ~]$</span><br></pre></td></tr></table></figure>
<h2 id="2-4-Hive元数据配置到MySql"><a href="#2-4-Hive元数据配置到MySql" class="headerlink" title="2.4 Hive元数据配置到MySql"></a>2.4 Hive元数据配置到MySql</h2><h3 id="2-4-1-拷贝驱动"><a href="#2-4-1-拷贝驱动" class="headerlink" title="2.4.1 拷贝驱动"></a>2.4.1 拷贝驱动</h3></li>
<li>将MySQL的JDBC驱动拷贝到Hive的lib目录下 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 software]$ cp /opt/software/mysql-connector-java-8.0.25.jar <span class="variable">$HIVE_HOME</span>/lib</span><br><span class="line">[atguigu@hadoop001 software]$ ll <span class="variable">$HIVE_HOME</span>/lib/mysql-connector-java-8.0.25.jar</span><br><span class="line">-rw-r--r--. 1 atguigu atguigu 2428320 Oct 20 14:14 /opt/module/hive-3.1.2/lib/mysql-connector-java-8.0.25.jar</span><br><span class="line">[atguigu@hadoop001 software]$</span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="2-4-2-配置Metastore到MySql"><a href="#2-4-2-配置Metastore到MySql" class="headerlink" title="2.4.2 配置Metastore到MySql"></a>2.4.2 配置Metastore到MySql</h3><ol>
<li>在$HIVE_HOME/conf目录下新建hive-site.xml文件添加如下内容 <figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version=&quot;1.0&quot;?&gt;</span></span><br><span class="line"><span class="meta">&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- jdbc连接的URL --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionURL<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>jdbc:mysql://hadoop102:3306/metastore?useSSL=false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- jdbc连接的Driver--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionDriverName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>com.mysql.jdbc.Driver<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- jdbc连接的username--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionUserName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>root<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- jdbc连接的password --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionPassword<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>root<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- Hive默认在HDFS的工作目录 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.warehouse.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/user/hive/warehouse<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- Hive元数据存储的验证 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.schema.verification<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- 元数据存储授权  --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.event.db.notification.api.auth<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="2-5-启动Hive"><a href="#2-5-启动Hive" class="headerlink" title="2.5 启动Hive"></a>2.5 启动Hive</h2><h3 id="2-5-1-初始化元数据库"><a href="#2-5-1-初始化元数据库" class="headerlink" title="2.5.1 初始化元数据库"></a>2.5.1 初始化元数据库</h3><ol>
<li>登陆MySQL创建Hive元数据库 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">show</span> databases;</span><br><span class="line"><span class="operator">+</span><span class="comment">--------------------+</span></span><br><span class="line"><span class="operator">|</span> Database           <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">--------------------+</span></span><br><span class="line"><span class="operator">|</span> information_schema <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> mysql              <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> performance_schema <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> sys                <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">--------------------+</span></span><br><span class="line"><span class="number">4</span> <span class="keyword">rows</span> <span class="keyword">in</span> <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span><br><span class="line"></span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">create</span> database metastore;</span><br><span class="line">Query OK, <span class="number">1</span> <span class="type">row</span> affected (<span class="number">0.00</span> sec)</span><br><span class="line"></span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">show</span> databases;</span><br><span class="line"><span class="operator">+</span><span class="comment">--------------------+</span></span><br><span class="line"><span class="operator">|</span> Database           <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">--------------------+</span></span><br><span class="line"><span class="operator">|</span> information_schema <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> metastore          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> mysql              <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> performance_schema <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> sys                <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">--------------------+</span></span><br><span class="line"><span class="number">5</span> <span class="keyword">rows</span> <span class="keyword">in</span> <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span><br><span class="line"></span><br><span class="line">mysql<span class="operator">&gt;</span></span><br></pre></td></tr></table></figure></li>
<li>初始化Hive元数据库 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 conf]$ schematool -initSchema -dbType mysql -verbose</span><br><span class="line">Metastore connection URL:	 jdbc:mysql://mysql:3306/metastore?useSSL=<span class="literal">false</span></span><br><span class="line">Metastore Connection Driver :	 com.mysql.jdbc.Driver</span><br><span class="line">Metastore connection User:	 root</span><br><span class="line">Loading class `com.mysql.jdbc.Driver<span class="string">&#x27;. This is deprecated. The new driver class is `com.mysql.cj.jdbc.Driver&#x27;</span>. The driver is automatically registered via the SPI and manual loading of the driver class is generally unnecessary.</span><br><span class="line">Starting metastore schema initialization to 3.1.0</span><br><span class="line">Initialization script hive-schema-3.1.0.mysql.sql</span><br><span class="line">Connecting to jdbc:mysql://mysql:3306/metastore?useSSL=<span class="literal">false</span></span><br><span class="line">Connected to: MySQL (version 8.0.25)</span><br><span class="line">Driver: MySQL Connector/J (version mysql-connector-java-8.0.25 (Revision: 08be9e9b4cba6aa115f9b27b215887af40b159e0))</span><br><span class="line">Transaction isolation: TRANSACTION_READ_COMMITTED</span><br><span class="line">0: jdbc:mysql://mysql:3306/metastore&gt; !autocommit on</span><br><span class="line">Autocommit status: <span class="literal">true</span></span><br><span class="line">···</span><br><span class="line">···</span><br><span class="line">Closing: 0: jdbc:mysql://mysql:3306/metastore?useSSL=<span class="literal">false</span></span><br><span class="line">beeline&gt;</span><br><span class="line">beeline&gt; Initialization script completed</span><br><span class="line">schemaTool completed</span><br><span class="line">[atguigu@hadoop001 conf]$</span><br></pre></td></tr></table></figure></li>
<li>元数据相关表 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">desc</span> dbs;</span><br><span class="line"><span class="operator">+</span><span class="comment">-----------------+---------------+------+-----+---------+-------+</span></span><br><span class="line"><span class="operator">|</span> Field           <span class="operator">|</span> Type          <span class="operator">|</span> <span class="keyword">Null</span> <span class="operator">|</span> Key <span class="operator">|</span> <span class="keyword">Default</span> <span class="operator">|</span> Extra <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-----------------+---------------+------+-----+---------+-------+</span></span><br><span class="line"><span class="operator">|</span> DB_ID           <span class="operator">|</span> <span class="type">bigint</span>        <span class="operator">|</span> <span class="keyword">NO</span>   <span class="operator">|</span> PRI <span class="operator">|</span> <span class="keyword">NULL</span>    <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="keyword">DESC</span>            <span class="operator">|</span> <span class="type">varchar</span>(<span class="number">4000</span>) <span class="operator">|</span> YES  <span class="operator">|</span>     <span class="operator">|</span> <span class="keyword">NULL</span>    <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> DB_LOCATION_URI <span class="operator">|</span> <span class="type">varchar</span>(<span class="number">4000</span>) <span class="operator">|</span> <span class="keyword">NO</span>   <span class="operator">|</span>     <span class="operator">|</span> <span class="keyword">NULL</span>    <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> NAME            <span class="operator">|</span> <span class="type">varchar</span>(<span class="number">128</span>)  <span class="operator">|</span> YES  <span class="operator">|</span> MUL <span class="operator">|</span> <span class="keyword">NULL</span>    <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> OWNER_NAME      <span class="operator">|</span> <span class="type">varchar</span>(<span class="number">128</span>)  <span class="operator">|</span> YES  <span class="operator">|</span>     <span class="operator">|</span> <span class="keyword">NULL</span>    <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> OWNER_TYPE      <span class="operator">|</span> <span class="type">varchar</span>(<span class="number">10</span>)   <span class="operator">|</span> YES  <span class="operator">|</span>     <span class="operator">|</span> <span class="keyword">NULL</span>    <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> CTLG_NAME       <span class="operator">|</span> <span class="type">varchar</span>(<span class="number">256</span>)  <span class="operator">|</span> <span class="keyword">NO</span>   <span class="operator">|</span> MUL <span class="operator">|</span> hive    <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-----------------+---------------+------+-----+---------+-------+</span></span><br><span class="line"><span class="number">7</span> <span class="keyword">rows</span> <span class="keyword">in</span> <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span><br><span class="line"></span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">desc</span> tbls;</span><br><span class="line"><span class="operator">+</span><span class="comment">--------------------+--------------+------+-----+---------+-------+</span></span><br><span class="line"><span class="operator">|</span> Field              <span class="operator">|</span> Type         <span class="operator">|</span> <span class="keyword">Null</span> <span class="operator">|</span> Key <span class="operator">|</span> <span class="keyword">Default</span> <span class="operator">|</span> Extra <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">--------------------+--------------+------+-----+---------+-------+</span></span><br><span class="line"><span class="operator">|</span> TBL_ID             <span class="operator">|</span> <span class="type">bigint</span>       <span class="operator">|</span> <span class="keyword">NO</span>   <span class="operator">|</span> PRI <span class="operator">|</span> <span class="keyword">NULL</span>    <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> CREATE_TIME        <span class="operator">|</span> <span class="type">int</span>          <span class="operator">|</span> <span class="keyword">NO</span>   <span class="operator">|</span>     <span class="operator">|</span> <span class="keyword">NULL</span>    <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> DB_ID              <span class="operator">|</span> <span class="type">bigint</span>       <span class="operator">|</span> YES  <span class="operator">|</span> MUL <span class="operator">|</span> <span class="keyword">NULL</span>    <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> LAST_ACCESS_TIME   <span class="operator">|</span> <span class="type">int</span>          <span class="operator">|</span> <span class="keyword">NO</span>   <span class="operator">|</span>     <span class="operator">|</span> <span class="keyword">NULL</span>    <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> OWNER              <span class="operator">|</span> <span class="type">varchar</span>(<span class="number">767</span>) <span class="operator">|</span> YES  <span class="operator">|</span>     <span class="operator">|</span> <span class="keyword">NULL</span>    <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> OWNER_TYPE         <span class="operator">|</span> <span class="type">varchar</span>(<span class="number">10</span>)  <span class="operator">|</span> YES  <span class="operator">|</span>     <span class="operator">|</span> <span class="keyword">NULL</span>    <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> RETENTION          <span class="operator">|</span> <span class="type">int</span>          <span class="operator">|</span> <span class="keyword">NO</span>   <span class="operator">|</span>     <span class="operator">|</span> <span class="keyword">NULL</span>    <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> SD_ID              <span class="operator">|</span> <span class="type">bigint</span>       <span class="operator">|</span> YES  <span class="operator">|</span> MUL <span class="operator">|</span> <span class="keyword">NULL</span>    <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> TBL_NAME           <span class="operator">|</span> <span class="type">varchar</span>(<span class="number">256</span>) <span class="operator">|</span> YES  <span class="operator">|</span> MUL <span class="operator">|</span> <span class="keyword">NULL</span>    <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> TBL_TYPE           <span class="operator">|</span> <span class="type">varchar</span>(<span class="number">128</span>) <span class="operator">|</span> YES  <span class="operator">|</span>     <span class="operator">|</span> <span class="keyword">NULL</span>    <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> VIEW_EXPANDED_TEXT <span class="operator">|</span> mediumtext   <span class="operator">|</span> YES  <span class="operator">|</span>     <span class="operator">|</span> <span class="keyword">NULL</span>    <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> VIEW_ORIGINAL_TEXT <span class="operator">|</span> mediumtext   <span class="operator">|</span> YES  <span class="operator">|</span>     <span class="operator">|</span> <span class="keyword">NULL</span>    <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> IS_REWRITE_ENABLED <span class="operator">|</span> bit(<span class="number">1</span>)       <span class="operator">|</span> <span class="keyword">NO</span>   <span class="operator">|</span>     <span class="operator">|</span> b<span class="string">&#x27;0&#x27;</span>    <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">--------------------+--------------+------+-----+---------+-------+</span></span><br><span class="line"><span class="number">13</span> <span class="keyword">rows</span> <span class="keyword">in</span> <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span><br><span class="line"></span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">desc</span> sds;</span><br><span class="line"><span class="operator">+</span><span class="comment">---------------------------+---------------+------+-----+---------+-------+</span></span><br><span class="line"><span class="operator">|</span> Field                     <span class="operator">|</span> Type          <span class="operator">|</span> <span class="keyword">Null</span> <span class="operator">|</span> Key <span class="operator">|</span> <span class="keyword">Default</span> <span class="operator">|</span> Extra <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">---------------------------+---------------+------+-----+---------+-------+</span></span><br><span class="line"><span class="operator">|</span> SD_ID                     <span class="operator">|</span> <span class="type">bigint</span>        <span class="operator">|</span> <span class="keyword">NO</span>   <span class="operator">|</span> PRI <span class="operator">|</span> <span class="keyword">NULL</span>    <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> CD_ID                     <span class="operator">|</span> <span class="type">bigint</span>        <span class="operator">|</span> YES  <span class="operator">|</span> MUL <span class="operator">|</span> <span class="keyword">NULL</span>    <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> INPUT_FORMAT              <span class="operator">|</span> <span class="type">varchar</span>(<span class="number">4000</span>) <span class="operator">|</span> YES  <span class="operator">|</span>     <span class="operator">|</span> <span class="keyword">NULL</span>    <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> IS_COMPRESSED             <span class="operator">|</span> bit(<span class="number">1</span>)        <span class="operator">|</span> <span class="keyword">NO</span>   <span class="operator">|</span>     <span class="operator">|</span> <span class="keyword">NULL</span>    <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> IS_STOREDASSUBDIRECTORIES <span class="operator">|</span> bit(<span class="number">1</span>)        <span class="operator">|</span> <span class="keyword">NO</span>   <span class="operator">|</span>     <span class="operator">|</span> <span class="keyword">NULL</span>    <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> LOCATION                  <span class="operator">|</span> <span class="type">varchar</span>(<span class="number">4000</span>) <span class="operator">|</span> YES  <span class="operator">|</span>     <span class="operator">|</span> <span class="keyword">NULL</span>    <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> NUM_BUCKETS               <span class="operator">|</span> <span class="type">int</span>           <span class="operator">|</span> <span class="keyword">NO</span>   <span class="operator">|</span>     <span class="operator">|</span> <span class="keyword">NULL</span>    <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> OUTPUT_FORMAT             <span class="operator">|</span> <span class="type">varchar</span>(<span class="number">4000</span>) <span class="operator">|</span> YES  <span class="operator">|</span>     <span class="operator">|</span> <span class="keyword">NULL</span>    <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> SERDE_ID                  <span class="operator">|</span> <span class="type">bigint</span>        <span class="operator">|</span> YES  <span class="operator">|</span> MUL <span class="operator">|</span> <span class="keyword">NULL</span>    <span class="operator">|</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">---------------------------+---------------+------+-----+---------+-------+</span></span><br><span class="line"><span class="number">9</span> <span class="keyword">rows</span> <span class="keyword">in</span> <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span><br><span class="line"></span><br><span class="line">mysql<span class="operator">&gt;</span></span><br></pre></td></tr></table></figure>
<h3 id="2-5-2-启动Hive"><a href="#2-5-2-启动Hive" class="headerlink" title="2.5.2 启动Hive"></a>2.5.2 启动Hive</h3></li>
<li>先启动hadoop集群</li>
<li>启动Hive <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 conf]$ hive</span><br><span class="line"><span class="built_in">which</span>: no hbase <span class="keyword">in</span> (/usr/<span class="built_in">local</span>/bin:/usr/bin:/usr/<span class="built_in">local</span>/sbin:/usr/sbin:/opt/module/jdk1.8.0_212/bin:/opt/module/ha-hadoop-3.1.3/bin:/opt/module/ha-hadoop-3.1.3/sbin:/opt/module/hive-3.1.2/bin:/home/atguigu/.<span class="built_in">local</span>/bin:/home/atguigu/bin)</span><br><span class="line">Hive Session ID = 6e003b50-4b21-418c-9983-d13aea1037a4</span><br><span class="line"></span><br><span class="line">Logging initialized using configuration <span class="keyword">in</span> jar:file:/opt/module/hive-3.1.2/lib/hive-common-3.1.2.jar!/hive-log4j2.properties Async: <span class="literal">true</span></span><br><span class="line">Loading class `com.mysql.jdbc.Driver<span class="string">&#x27;. This is deprecated. The new driver class is `com.mysql.cj.jdbc.Driver&#x27;</span>. The driver is automatically registered via the SPI and manual loading of the driver class is generally unnecessary.</span><br><span class="line">Hive-on-MR is deprecated <span class="keyword">in</span> Hive 2 and may not be available <span class="keyword">in</span> the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.</span><br><span class="line">Hive Session ID = 19720043-aa70-4ba3-8eb9-7ff94beb4379</span><br><span class="line">hive&gt; </span><br></pre></td></tr></table></figure></li>
<li>使用Hive <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive<span class="operator">&gt;</span> <span class="keyword">show</span> databases;</span><br><span class="line">OK</span><br><span class="line"><span class="keyword">default</span></span><br><span class="line"><span class="type">Time</span> taken: <span class="number">0.58</span> seconds, Fetched: <span class="number">1</span> <span class="type">row</span>(s)</span><br><span class="line">hive<span class="operator">&gt;</span> <span class="keyword">show</span> tables;</span><br><span class="line">OK</span><br><span class="line">test</span><br><span class="line">test_2</span><br><span class="line"><span class="type">Time</span> taken: <span class="number">0.029</span> seconds, Fetched: <span class="number">2</span> <span class="type">row</span>(s)</span><br><span class="line">hive<span class="operator">&gt;</span> <span class="keyword">create</span> <span class="keyword">table</span> test_3 (id <span class="type">int</span>,name string,age <span class="type">int</span>);</span><br><span class="line">OK</span><br><span class="line"><span class="type">Time</span> taken: <span class="number">0.346</span> seconds</span><br><span class="line">hive<span class="operator">&gt;</span> <span class="keyword">show</span> tables;</span><br><span class="line">OK</span><br><span class="line">test</span><br><span class="line">test_2</span><br><span class="line">test_3</span><br><span class="line"><span class="type">Time</span> taken: <span class="number">0.034</span> seconds, Fetched: <span class="number">3</span> <span class="type">row</span>(s)</span><br><span class="line">hive<span class="operator">&gt;</span> <span class="keyword">insert</span> <span class="keyword">into</span> test_3 <span class="keyword">values</span> (<span class="number">123</span>,<span class="string">&#x27;zhangsan&#x27;</span>,<span class="number">18</span>);</span><br><span class="line">Query ID <span class="operator">=</span> atguigu_20211020154850_d4a395ef<span class="operator">-</span>b85d<span class="number">-466</span>a<span class="number">-852</span>f<span class="operator">-</span>b6707a260960</span><br><span class="line">Total jobs <span class="operator">=</span> <span class="number">3</span></span><br><span class="line">Launching Job <span class="number">1</span> <span class="keyword">out</span> <span class="keyword">of</span> <span class="number">3</span></span><br><span class="line">Number <span class="keyword">of</span> reduce tasks determined <span class="keyword">at</span> compile <span class="type">time</span>: <span class="number">1</span></span><br><span class="line"><span class="keyword">In</span> <span class="keyword">order</span> <span class="keyword">to</span> change the average load <span class="keyword">for</span> a reducer (<span class="keyword">in</span> bytes):</span><br><span class="line">  <span class="keyword">set</span> hive.exec.reducers.bytes.per.reducer<span class="operator">=</span><span class="operator">&lt;</span>number<span class="operator">&gt;</span></span><br><span class="line"><span class="keyword">In</span> <span class="keyword">order</span> <span class="keyword">to</span> limit the maximum number <span class="keyword">of</span> reducers:</span><br><span class="line">  <span class="keyword">set</span> hive.exec.reducers.max<span class="operator">=</span><span class="operator">&lt;</span>number<span class="operator">&gt;</span></span><br><span class="line"><span class="keyword">In</span> <span class="keyword">order</span> <span class="keyword">to</span> <span class="keyword">set</span> a constant number <span class="keyword">of</span> reducers:</span><br><span class="line">  <span class="keyword">set</span> mapreduce.job.reduces<span class="operator">=</span><span class="operator">&lt;</span>number<span class="operator">&gt;</span></span><br><span class="line">Starting Job <span class="operator">=</span> job_1634689350447_0003, Tracking URL <span class="operator">=</span> http:<span class="operator">/</span><span class="operator">/</span>hadoop003:<span class="number">8088</span><span class="operator">/</span>proxy<span class="operator">/</span>application_1634689350447_0003<span class="operator">/</span></span><br><span class="line">Kill Command <span class="operator">=</span> <span class="operator">/</span>opt<span class="operator">/</span><span class="keyword">module</span><span class="operator">/</span>ha<span class="operator">-</span>hadoop<span class="number">-3.1</span><span class="number">.3</span><span class="operator">/</span>bin<span class="operator">/</span>mapred job  <span class="operator">-</span>kill job_1634689350447_0003</span><br><span class="line">Hadoop job information <span class="keyword">for</span> Stage<span class="number">-1</span>: number <span class="keyword">of</span> mappers: <span class="number">1</span>; number <span class="keyword">of</span> reducers: <span class="number">1</span></span><br><span class="line"><span class="number">2021</span><span class="number">-10</span><span class="number">-20</span> <span class="number">15</span>:<span class="number">50</span>:<span class="number">00</span>,<span class="number">166</span> Stage<span class="number">-1</span> map <span class="operator">=</span> <span class="number">0</span><span class="operator">%</span>,  reduce <span class="operator">=</span> <span class="number">0</span><span class="operator">%</span></span><br><span class="line"><span class="number">2021</span><span class="number">-10</span><span class="number">-20</span> <span class="number">15</span>:<span class="number">50</span>:<span class="number">05</span>,<span class="number">319</span> Stage<span class="number">-1</span> map <span class="operator">=</span> <span class="number">100</span><span class="operator">%</span>,  reduce <span class="operator">=</span> <span class="number">0</span><span class="operator">%</span>, Cumulative CPU <span class="number">1.37</span> sec</span><br><span class="line"><span class="number">2021</span><span class="number">-10</span><span class="number">-20</span> <span class="number">15</span>:<span class="number">50</span>:<span class="number">10</span>,<span class="number">425</span> Stage<span class="number">-1</span> map <span class="operator">=</span> <span class="number">100</span><span class="operator">%</span>,  reduce <span class="operator">=</span> <span class="number">100</span><span class="operator">%</span>, Cumulative CPU <span class="number">2.49</span> sec</span><br><span class="line">MapReduce Total cumulative CPU <span class="type">time</span>: <span class="number">2</span> seconds <span class="number">490</span> msec</span><br><span class="line">Ended Job <span class="operator">=</span> job_1634689350447_0003</span><br><span class="line">Stage<span class="number">-4</span> <span class="keyword">is</span> selected <span class="keyword">by</span> <span class="keyword">condition</span> resolver.</span><br><span class="line">Stage<span class="number">-3</span> <span class="keyword">is</span> filtered <span class="keyword">out</span> <span class="keyword">by</span> <span class="keyword">condition</span> resolver.</span><br><span class="line">Stage<span class="number">-5</span> <span class="keyword">is</span> filtered <span class="keyword">out</span> <span class="keyword">by</span> <span class="keyword">condition</span> resolver.</span><br><span class="line">Moving data <span class="keyword">to</span> directory hdfs:<span class="operator">/</span><span class="operator">/</span>mycluster<span class="operator">/</span><span class="keyword">user</span><span class="operator">/</span>hive<span class="operator">/</span>warehouse<span class="operator">/</span>test_3<span class="operator">/</span>.hive<span class="operator">-</span>staging_hive_2021<span class="number">-10</span><span class="number">-20</span>_15<span class="number">-48</span><span class="number">-50</span>_976_8532078537182566570<span class="number">-1</span><span class="operator">/</span><span class="operator">-</span>ext<span class="number">-10000</span></span><br><span class="line">Loading data <span class="keyword">to</span> <span class="keyword">table</span> default.test_3</span><br><span class="line">MapReduce Jobs Launched:</span><br><span class="line">Stage<span class="operator">-</span>Stage<span class="number">-1</span>: Map: <span class="number">1</span>  Reduce: <span class="number">1</span>   Cumulative CPU: <span class="number">2.49</span> sec   HDFS Read: <span class="number">16281</span> HDFS Write: <span class="number">285</span> SUCCESS</span><br><span class="line">Total MapReduce CPU <span class="type">Time</span> Spent: <span class="number">2</span> seconds <span class="number">490</span> msec</span><br><span class="line">OK</span><br><span class="line"><span class="type">Time</span> taken: <span class="number">80.881</span> seconds</span><br><span class="line">hive<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> test_3;</span><br><span class="line">OK</span><br><span class="line"><span class="number">123</span>	zhangsan	<span class="number">18</span></span><br><span class="line"><span class="type">Time</span> taken: <span class="number">0.109</span> seconds, Fetched: <span class="number">1</span> <span class="type">row</span>(s)</span><br><span class="line">hive<span class="operator">&gt;</span></span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="2-5-3-使用独立的元数据服务的方式访问Hive"><a href="#2-5-3-使用独立的元数据服务的方式访问Hive" class="headerlink" title="2.5.3 使用独立的元数据服务的方式访问Hive"></a>2.5.3 使用独立的元数据服务的方式访问Hive</h3><ol>
<li>在hive-site.xml文件中添加如下配置信息 <figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 指定存储元数据要连接的地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.uris<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>thrift://hadoop102:9083<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li>
<li>启动metastore <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 conf]$ hive --service metastore</span><br><span class="line">2021-10-20 16:19:53: Starting Hive Metastore Server</span><br><span class="line">Loading class `com.mysql.jdbc.Driver<span class="string">&#x27;. This is deprecated. The new driver class is `com.mysql.cj.jdbc.Driver&#x27;</span>. The driver is automatically registered via the SPI and manual loading of the driver class is generally unnecessary.</span><br></pre></td></tr></table></figure>
 注意: 启动后窗口不能再操作，需打开一个新的shell窗口做别的操作</li>
<li>启动 hive <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 ~]$ hive</span><br><span class="line"><span class="built_in">which</span>: no hbase <span class="keyword">in</span> (/usr/<span class="built_in">local</span>/bin:/usr/bin:/usr/<span class="built_in">local</span>/sbin:/usr/sbin:/opt/module/jdk1.8.0_212/bin:/opt/module/ha-hadoop-3.1.3/bin:/opt/module/ha-hadoop-3.1.3/sbin:/opt/module/hive-3.1.2/bin:/home/atguigu/.<span class="built_in">local</span>/bin:/home/atguigu/bin)</span><br><span class="line">Hive Session ID = cc81e23a-657b-47e7-b211-bc4364e64fc2</span><br><span class="line"></span><br><span class="line">Logging initialized using configuration <span class="keyword">in</span> jar:file:/opt/module/hive-3.1.2/lib/hive-common-3.1.2.jar!/hive-log4j2.properties Async: <span class="literal">true</span></span><br><span class="line">Hive-on-MR is deprecated <span class="keyword">in</span> Hive 2 and may not be available <span class="keyword">in</span> the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.</span><br><span class="line">Hive Session ID = 60aa1b01-a2b8-4766-b9fc-4246f8c5fee6</span><br><span class="line">hive&gt;</span><br></pre></td></tr></table></figure>
<h3 id="2-5-4-使用JDBC方式访问Hive"><a href="#2-5-4-使用JDBC方式访问Hive" class="headerlink" title="2.5.4 使用JDBC方式访问Hive"></a>2.5.4 使用JDBC方式访问Hive</h3></li>
<li>在hive-site.xml文件中添加如下配置信息 <figure class="highlight xml"><table><tr><td class="code"><pre><span class="line">    <span class="comment">&lt;!-- 指定hiveserver2连接的host --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.server2.thrift.bind.host<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop102<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">&lt;!-- 指定hiveserver2连接的端口号 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.server2.thrift.port<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>10000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    ```    </span><br><span class="line">2. 启动hiveserver2</span><br><span class="line">    ```bash</span><br><span class="line">    [atguigu@hadoop001 ~]$ hive --service hiveserver2</span><br><span class="line">    which: no hbase in (/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/opt/module/jdk1.8.0_212/bin:/opt/module/ha-hadoop-3.1.3/bin:/opt/module/ha-hadoop-3.1.3/sbin:/opt/module/hive-3.1.2/bin:/home/atguigu/.local/bin:/home/atguigu/bin)</span><br><span class="line">    2021-10-20 16:23:25: Starting HiveServer2</span><br><span class="line">    Hive Session ID = 9191577f-34bc-478c-a4ae-8b0f2d20f945</span><br><span class="line">    Hive Session ID = 769ddf72-053a-4f0b-aa48-5723090f513b</span><br></pre></td></tr></table></figure></li>
<li>启动beeline客户端（需要多等待一会） <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 ~]$ beeline -u jdbc:hive2://hadoop001:10000 -n atguigu</span><br><span class="line">Connecting to jdbc:hive2://hadoop001:10000</span><br><span class="line">Connected to: Apache Hive (version 3.1.2)</span><br><span class="line">Driver: Hive JDBC (version 3.1.2)</span><br><span class="line">Transaction isolation: TRANSACTION_REPEATABLE_READ</span><br><span class="line">Beeline version 3.1.2 by Apache Hive</span><br><span class="line">0: jdbc:hive2://hadoop001:10000&gt;</span><br><span class="line">0: jdbc:hive2://hadoop001:10000&gt;</span><br><span class="line">0: jdbc:hive2://hadoop001:10000&gt;</span><br><span class="line">0: jdbc:hive2://hadoop001:10000&gt; show databases;</span><br><span class="line">INFO  : Compiling <span class="built_in">command</span>(queryId=atguigu_20211020162645_fd2a8faf-9209-4bec-b36d-ec4c63bd8149): show databases</span><br><span class="line">INFO  : Concurrency mode is disabled, not creating a lock manager</span><br><span class="line">INFO  : Semantic Analysis Completed (retrial = <span class="literal">false</span>)</span><br><span class="line">INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:database_name, <span class="built_in">type</span>:string, comment:from deserializer)], properties:null)</span><br><span class="line">INFO  : Completed compiling <span class="built_in">command</span>(queryId=atguigu_20211020162645_fd2a8faf-9209-4bec-b36d-ec4c63bd8149); Time taken: 0.589 seconds</span><br><span class="line">INFO  : Concurrency mode is disabled, not creating a lock manager</span><br><span class="line">INFO  : Executing <span class="built_in">command</span>(queryId=atguigu_20211020162645_fd2a8faf-9209-4bec-b36d-ec4c63bd8149): show databases</span><br><span class="line">INFO  : Starting task [Stage-0:DDL] <span class="keyword">in</span> serial mode</span><br><span class="line">INFO  : Completed executing <span class="built_in">command</span>(queryId=atguigu_20211020162645_fd2a8faf-9209-4bec-b36d-ec4c63bd8149); Time taken: 0.016 seconds</span><br><span class="line">INFO  : OK</span><br><span class="line">INFO  : Concurrency mode is disabled, not creating a lock manager</span><br><span class="line">+----------------+</span><br><span class="line">| database_name  |</span><br><span class="line">+----------------+</span><br><span class="line">| default        |</span><br><span class="line">+----------------+</span><br><span class="line">1 row selected (0.84 seconds)</span><br><span class="line">0: jdbc:hive2://hadoop001:10000&gt;</span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="2-5-5-编写启动metastore和hiveserver2脚本"><a href="#2-5-5-编写启动metastore和hiveserver2脚本" class="headerlink" title="2.5.5 编写启动metastore和hiveserver2脚本"></a>2.5.5 编写启动metastore和hiveserver2脚本</h3><ol>
<li><p>前台启动的方式导致需要打开多个shell窗口，可以使用如下方式后台方式启动</p>
<ul>
<li>nohup: 放在命令开头，表示不挂起,也就是关闭终端进程也继续保持运行状态<ul>
<li>0:标准输入</li>
<li>1:标准输出</li>
<li>2:错误输出</li>
<li>2&gt;&amp;1 : 表示将错误重定向到标准输出上</li>
<li>&amp;: 放在命令结尾,表示后台运行</li>
<li>一般会组合使用: nohup  [xxx命令操作]&gt; file  2&gt;&amp;1 &amp;  ， 表示将xxx命令运行的<br>结果输出到file中，并保持命令启动的进程在后台运行。<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 ~]$ nohup hive --service metastore 2&gt;&amp;1 &amp;</span><br><span class="line">[1] 19488</span><br><span class="line">[atguigu@hadoop001 ~]$ nohup: ignoring input and appending output to ‘nohup.out’</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop001 ~]$</span><br><span class="line">[atguigu@hadoop001 ~]$ nohup hive --service hiveserver2 2&gt;&amp;1 &amp;</span><br><span class="line">[2] 19603</span><br><span class="line">[atguigu@hadoop001 ~]$ nohup: ignoring input and appending output to ‘nohup.out’</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop001 ~]$</span><br><span class="line">[atguigu@hadoop001 ~]$ tail -f nohup.out</span><br><span class="line">2021-10-20 16:46:00: Starting Hive Metastore Server</span><br><span class="line">Loading class `com.mysql.jdbc.Driver<span class="string">&#x27;. This is deprecated. The new driver class is `com.mysql.cj.jdbc.Driver&#x27;</span>. The driver is automatically registered via the SPI and manual loading of the driver class is generally unnecessary.</span><br><span class="line"><span class="built_in">which</span>: no hbase <span class="keyword">in</span> (/usr/<span class="built_in">local</span>/bin:/usr/bin:/usr/<span class="built_in">local</span>/sbin:/usr/sbin:/opt/module/jdk1.8.0_212/bin:/opt/module/ha-hadoop-3.1.3/bin:/opt/module/ha-hadoop-3.1.3/sbin:/opt/module/hive-3.1.2/bin:/home/atguigu/.<span class="built_in">local</span>/bin:/home/atguigu/bin)</span><br><span class="line">2021-10-20 16:46:07: Starting HiveServer2</span><br><span class="line">Hive Session ID = e305ccdf-dd51-48e1-a740-fbc309de0a31</span><br><span class="line">Hive Session ID = b9a65f04-071e-493d-b89e-774daf7cac0f</span><br><span class="line">Hive Session ID = 985ef9a9-6d00-47f6-a0e4-178eed45c7f8</span><br><span class="line">Hive Session ID = dae41fea-63a9-4bda-84e6-c4e171fc0ecc</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>
</li>
<li><p>编写脚本hiveservices.sh</p>
 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line">HIVE_LOG_DIR=<span class="variable">$HIVE_HOME</span>/logs</span><br><span class="line"><span class="keyword">if</span> [ ! -d <span class="variable">$HIVE_LOG_DIR</span> ]; <span class="keyword">then</span></span><br><span class="line">    mkdir -p <span class="variable">$HIVE_LOG_DIR</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"><span class="comment">#检查进程是否运行正常，参数1为进程名，参数2为进程端口</span></span><br><span class="line"><span class="keyword">function</span> <span class="function"><span class="title">check_process</span></span>() &#123;</span><br><span class="line">    pid=$(ps -ef 2&gt;/dev/null | grep -v grep | grep -i <span class="variable">$1</span> | awk <span class="string">&#x27;&#123;print $2&#125;&#x27;</span>)</span><br><span class="line">    ppid=$(netstat -nltp 2&gt;/dev/null | grep <span class="variable">$2</span> | awk <span class="string">&#x27;&#123;print $7&#125;&#x27;</span> | cut -d <span class="string">&#x27;/&#x27;</span> -f 1)</span><br><span class="line">    <span class="built_in">echo</span> <span class="variable">$pid</span></span><br><span class="line">    [[ <span class="string">&quot;<span class="variable">$pid</span>&quot;</span> =~ <span class="string">&quot;<span class="variable">$ppid</span>&quot;</span> ]] &amp;&amp; [ <span class="string">&quot;<span class="variable">$ppid</span>&quot;</span> ] &amp;&amp; <span class="built_in">return</span> 0 || <span class="built_in">return</span> 1</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">function</span> <span class="function"><span class="title">hive_start</span></span>() &#123;</span><br><span class="line">    metapid=$(check_process HiveMetastore 9083)</span><br><span class="line">    cmd=<span class="string">&quot;nohup hive --service metastore &gt;<span class="variable">$HIVE_LOG_DIR</span>/metastore.log 2&gt;&amp;1 &amp;&quot;</span></span><br><span class="line">    cmd=<span class="variable">$cmd</span><span class="string">&quot; sleep 4; hdfs dfsadmin -safemode wait &gt;/dev/null 2&gt;&amp;1&quot;</span></span><br><span class="line">    [ -z <span class="string">&quot;<span class="variable">$metapid</span>&quot;</span> ] &amp;&amp; <span class="built_in">eval</span> <span class="variable">$cmd</span> || <span class="built_in">echo</span> <span class="string">&quot;Metastroe服务已启动&quot;</span></span><br><span class="line">    server2pid=$(check_process HiveServer2 10000)</span><br><span class="line">    cmd=<span class="string">&quot;nohup hive --service hiveserver2 &gt;<span class="variable">$HIVE_LOG_DIR</span>/hiveServer2.log 2&gt;&amp;1 &amp;&quot;</span></span><br><span class="line">    [ -z <span class="string">&quot;<span class="variable">$server2pid</span>&quot;</span> ] &amp;&amp; <span class="built_in">eval</span> <span class="variable">$cmd</span> || <span class="built_in">echo</span> <span class="string">&quot;HiveServer2服务已启动&quot;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">function</span> <span class="function"><span class="title">hive_stop</span></span>() &#123;</span><br><span class="line">    metapid=$(check_process HiveMetastore 9083)</span><br><span class="line">    [ <span class="string">&quot;<span class="variable">$metapid</span>&quot;</span> ] &amp;&amp; <span class="built_in">kill</span> <span class="variable">$metapid</span> || <span class="built_in">echo</span> <span class="string">&quot;Metastore服务未启动&quot;</span></span><br><span class="line">    server2pid=$(check_process HiveServer2 10000)</span><br><span class="line">    [ <span class="string">&quot;<span class="variable">$server2pid</span>&quot;</span> ] &amp;&amp; <span class="built_in">kill</span> <span class="variable">$server2pid</span> || <span class="built_in">echo</span> <span class="string">&quot;HiveServer2服务未启动&quot;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="variable">$1</span> <span class="keyword">in</span></span><br><span class="line"><span class="string">&quot;start&quot;</span>)</span><br><span class="line">    hive_start</span><br><span class="line">    ;;</span><br><span class="line"><span class="string">&quot;stop&quot;</span>)</span><br><span class="line">    hive_stop</span><br><span class="line">    ;;</span><br><span class="line"><span class="string">&quot;restart&quot;</span>)</span><br><span class="line">    hive_stop</span><br><span class="line">    sleep 2</span><br><span class="line">    hive_start</span><br><span class="line">    ;;</span><br><span class="line"><span class="string">&quot;status&quot;</span>)</span><br><span class="line">    check_process HiveMetastore 9083 &gt;/dev/null &amp;&amp; <span class="built_in">echo</span> <span class="string">&quot;Metastore服务运行正常&quot;</span> || <span class="built_in">echo</span> <span class="string">&quot;Metastore服务运行异常&quot;</span></span><br><span class="line">    check_process HiveServer2 10000 &gt;/dev/null &amp;&amp; <span class="built_in">echo</span> <span class="string">&quot;HiveServer2服务运行正常&quot;</span> || <span class="built_in">echo</span> <span class="string">&quot;HiveServer2服务运行异常&quot;</span></span><br><span class="line">    ;;</span><br><span class="line">*)</span><br><span class="line">    <span class="built_in">echo</span> Invalid Args!</span><br><span class="line">    <span class="built_in">echo</span> <span class="string">&#x27;Usage: &#x27;</span>$(basename <span class="variable">$0</span>)<span class="string">&#x27; start|stop|restart|status&#x27;</span></span><br><span class="line">    ;;</span><br><span class="line"><span class="keyword">esac</span></span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
<li><p>添加执行权限</p>
 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 bin]$ ll</span><br><span class="line">total 20</span><br><span class="line">-rw-rw-r--. 1 atguigu atguigu 1723 Oct 20 17:02 hive_server.sh</span><br><span class="line">[atguigu@hadoop001 bin]$ chmod a+x hive_server.sh</span><br><span class="line">[atguigu@hadoop001 bin]$ ll</span><br><span class="line">total 20</span><br><span class="line">-rwxrwxr-x. 1 atguigu atguigu 1723 Oct 20 17:02 hive_server.sh</span><br><span class="line">[atguigu@hadoop001 bin]$</span><br></pre></td></tr></table></figure></li>
<li><p>启动Hive后台服务</p>
 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 bin]$ hive_server.sh start</span><br><span class="line">[atguigu@hadoop001 bin]$ hive_server.sh status</span><br><span class="line">Metastore服务运行正常</span><br><span class="line">HiveServer2服务运行正常</span><br><span class="line">[atguigu@hadoop001 bin]$</span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="2-6-Hive常用交互命令"><a href="#2-6-Hive常用交互命令" class="headerlink" title="2.6 Hive常用交互命令"></a>2.6 Hive常用交互命令</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 hive]$ hive -<span class="built_in">help</span></span><br><span class="line"><span class="built_in">which</span>: no hbase <span class="keyword">in</span> (/usr/<span class="built_in">local</span>/bin:/usr/bin:/usr/<span class="built_in">local</span>/sbin:/usr/sbin:/opt/module/jdk1.8.0_212/bin:/opt/module/ha-hadoop-3.1.3/bin:/opt/module/ha-hadoop-3.1.3/sbin:/opt/module/hive-3.1.2/bin:/home/atguigu/.<span class="built_in">local</span>/bin:/home/atguigu/bin)</span><br><span class="line">Hive Session ID = ccd1a72b-c24a-4b49-9583-1601fa1f7a34</span><br><span class="line">usage: hive</span><br><span class="line"> -d,--define &lt;key=value&gt;          Variable substitution to apply to Hive</span><br><span class="line">                                  commands. e.g. -d A=B or --define A=B</span><br><span class="line">    --database &lt;databasename&gt;     Specify the database to use</span><br><span class="line"> -e &lt;quoted-query-string&gt;         SQL from <span class="built_in">command</span> line</span><br><span class="line"> -f &lt;filename&gt;                    SQL from files</span><br><span class="line"> -H,--<span class="built_in">help</span>                        Print <span class="built_in">help</span> information</span><br><span class="line">    --hiveconf &lt;property=value&gt;   Use value <span class="keyword">for</span> given property</span><br><span class="line">    --hivevar &lt;key=value&gt;         Variable substitution to apply to Hive</span><br><span class="line">                                  commands. e.g. --hivevar A=B</span><br><span class="line"> -i &lt;filename&gt;                    Initialization SQL file</span><br><span class="line"> -S,--silent                      Silent mode <span class="keyword">in</span> interactive shell</span><br><span class="line"> -v,--verbose                     Verbose mode (<span class="built_in">echo</span> executed SQL to the</span><br><span class="line">                                  console)</span><br><span class="line">[atguigu@hadoop001 hive]$</span><br></pre></td></tr></table></figure>
<ol>
<li>“-e”不进入hive的交互窗口执行sql语句 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 hive]$ hive -e <span class="string">&#x27;select count(1) from test_3&#x27;</span></span><br><span class="line"><span class="built_in">which</span>: no hbase <span class="keyword">in</span> (/usr/<span class="built_in">local</span>/bin:/usr/bin:/usr/<span class="built_in">local</span>/sbin:/usr/sbin:/opt/module/jdk1.8.0_212/bin:/opt/module/ha-hadoop-3.1.3/bin:/opt/module/ha-hadoop-3.1.3/sbin:/opt/module/hive-3.1.2/bin:/home/atguigu/.<span class="built_in">local</span>/bin:/home/atguigu/bin)</span><br><span class="line">Hive Session ID = 8eeed838-5d5e-4ed4-a252-29f85e7f15f3</span><br><span class="line"></span><br><span class="line">Logging initialized using configuration <span class="keyword">in</span> jar:file:/opt/module/hive-3.1.2/lib/hive-common-3.1.2.jar!/hive-log4j2.properties Async: <span class="literal">true</span></span><br><span class="line">Hive Session ID = c1c58f1c-71c4-4869-9f5e-8dc2a4ac52f4</span><br><span class="line">OK</span><br><span class="line">32</span><br><span class="line">Time taken: 1.972 seconds, Fetched: 1 row(s)</span><br><span class="line">[atguigu@hadoop001 hive]$</span><br></pre></td></tr></table></figure></li>
<li>-f”执行脚本中sql语句 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 hive]$ ls</span><br><span class="line">selectall.sql</span><br><span class="line"><span class="comment"># 编写sql脚本</span></span><br><span class="line">[atguigu@hadoop001 hive]$ cat selectall.sql</span><br><span class="line">select count(1) from test_3</span><br><span class="line"><span class="comment"># 使用-f执行脚本</span></span><br><span class="line">[atguigu@hadoop001 hive]$ hive -f selectall.sql</span><br><span class="line"><span class="built_in">which</span>: no hbase <span class="keyword">in</span> (/usr/<span class="built_in">local</span>/bin:/usr/bin:/usr/<span class="built_in">local</span>/sbin:/usr/sbin:/opt/module/jdk1.8.0_212/bin:/opt/module/ha-hadoop-3.1.3/bin:/opt/module/ha-hadoop-3.1.3/sbin:/opt/module/hive-3.1.2/bin:/home/atguigu/.<span class="built_in">local</span>/bin:/home/atguigu/bin)</span><br><span class="line">Hive Session ID = 2b43ecf4-1fd8-4bd1-be10-60adaad0ffa4</span><br><span class="line"></span><br><span class="line">Logging initialized using configuration <span class="keyword">in</span> jar:file:/opt/module/hive-3.1.2/lib/hive-common-3.1.2.jar!/hive-log4j2.properties Async: <span class="literal">true</span></span><br><span class="line">Hive Session ID = a07efeea-d038-4ca8-b342-7b001f1493d7</span><br><span class="line">OK</span><br><span class="line">32</span><br><span class="line">Time taken: 1.954 seconds, Fetched: 1 row(s)</span><br><span class="line">[atguigu@hadoop001 hive]$</span><br><span class="line"><span class="comment"># 执行并导出文件</span></span><br><span class="line">[atguigu@hadoop001 hive]$ hive -f selectall.sql &gt; selectall.txt</span><br><span class="line"><span class="built_in">which</span>: no hbase <span class="keyword">in</span> (/usr/<span class="built_in">local</span>/bin:/usr/bin:/usr/<span class="built_in">local</span>/sbin:/usr/sbin:/opt/module/jdk1.8.0_212/bin:/opt/module/ha-hadoop-3.1.3/bin:/opt/module/ha-hadoop-3.1.3/sbin:/opt/module/hive-3.1.2/bin:/home/atguigu/.<span class="built_in">local</span>/bin:/home/atguigu/bin)</span><br><span class="line">Hive Session ID = 13cd6a52-4af8-4234-86bf-70841d565b98</span><br><span class="line"></span><br><span class="line">Logging initialized using configuration <span class="keyword">in</span> jar:file:/opt/module/hive-3.1.2/lib/hive-common-3.1.2.jar!/hive-log4j2.properties Async: <span class="literal">true</span></span><br><span class="line">Hive Session ID = 9279ee4e-997c-4a92-b258-11ac8c0beaa6</span><br><span class="line">OK</span><br><span class="line">Time taken: 1.953 seconds, Fetched: 1 row(s)</span><br><span class="line">[atguigu@hadoop001 hive]$ cat selectall.txt</span><br><span class="line">32</span><br><span class="line">[atguigu@hadoop001 hive]$</span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="2-7-Hive其他命令操作"><a href="#2-7-Hive其他命令操作" class="headerlink" title="2.7 Hive其他命令操作"></a>2.7 Hive其他命令操作</h2><ol>
<li>退出hive窗口： <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hive(default)&gt;<span class="built_in">exit</span>;</span><br><span class="line">hive(default)&gt;quit;</span><br></pre></td></tr></table></figure>
<ul>
<li>在新版的hive中没区别了，在以前的版本是有的：</li>
<li>exit:先隐性提交数据，再退出；</li>
<li>quit:不提交数据，退出；</li>
</ul>
</li>
<li>在hive cli命令窗口中如何查看hdfs文件系统 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hive&gt; dfs -ls /user;</span><br><span class="line">Found 1 items</span><br><span class="line">drwxr-xr-x   - atguigu supergroup          0 2021-10-20 14:52 /user/hive</span><br><span class="line">hive&gt; dfs -ls /user/hive;</span><br><span class="line">Found 1 items</span><br><span class="line">drwxr-xr-x   - atguigu supergroup          0 2021-10-20 15:48 /user/hive/warehouse</span><br><span class="line">hive&gt; dfs -ls /user/hive/warehouse;</span><br><span class="line">Found 3 items</span><br><span class="line">drwxr-xr-x   - atguigu supergroup          0 2021-10-20 15:40 /user/hive/warehouse/<span class="built_in">test</span></span><br><span class="line">drwxr-xr-x   - atguigu supergroup          0 2021-10-20 15:43 /user/hive/warehouse/test_2</span><br><span class="line">drwxr-xr-x   - atguigu supergroup          0 2021-10-22 15:06 /user/hive/warehouse/test_3</span><br><span class="line">hive&gt;</span><br></pre></td></tr></table></figure></li>
<li>查看在hive中输入的所有历史命令,用户home目录下.hivehistory文件,~/.hivehistory <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 ~]$ head -n 10 ~/.hivehistory</span><br><span class="line">show databases;</span><br><span class="line">create table <span class="built_in">test</span> (id int);</span><br><span class="line">show database;</span><br><span class="line">show databases;</span><br><span class="line">ls</span><br><span class="line">show databases;</span><br><span class="line">quit;</span><br><span class="line">show databases;</span><br><span class="line">quit</span><br><span class="line">;</span><br><span class="line">[atguigu@hadoop001 ~]$</span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="2-8-Hive常见属性配置"><a href="#2-8-Hive常见属性配置" class="headerlink" title="2.8 Hive常见属性配置"></a>2.8 Hive常见属性配置</h2><h3 id="2-8-1-hive窗口打印默认库和表头"><a href="#2-8-1-hive窗口打印默认库和表头" class="headerlink" title="2.8.1 hive窗口打印默认库和表头"></a>2.8.1 hive窗口打印默认库和表头</h3><ol>
<li>打印 当前库 和 表头 hive-site.xml中加入如下两个配置:  <figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.cli.print.header<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.cli.print.current.db<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> test_2;</span><br><span class="line">OK</span><br><span class="line">test_2.id	test_2.name</span><br><span class="line"><span class="number">123</span>	zhangsan</span><br><span class="line"><span class="type">Time</span> taken: <span class="number">0.106</span> seconds, Fetched: <span class="number">1</span> <span class="type">row</span>(s)</span><br><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span></span><br></pre></td></tr></table></figure>
<h3 id="2-8-2-Hive运行日志信息配置"><a href="#2-8-2-Hive运行日志信息配置" class="headerlink" title="2.8.2 Hive运行日志信息配置"></a>2.8.2 Hive运行日志信息配置</h3></li>
<li>Hive的log默认存放在/tmp/atguigu/hive.log目录下（当前用户名下）</li>
<li>修改hive的log存放日志到/opt/module/hive-3.1.2/logs<ol>
<li>修改/opt/module/hive/conf/hive-log4j2.properties.template文件名称为hive-log4j2.properties <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 conf]$ <span class="built_in">pwd</span></span><br><span class="line">/opt/module/hive/conf</span><br><span class="line">[atguigu@hadoop102 conf]$ mv hive-log4j2.properties.template hive-log4j2.properties</span><br></pre></td></tr></table></figure></li>
<li>在hive-log4j.properties文件中修改log存放位置 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">property.hive.log.dir=/opt/module/hive/logs</span><br></pre></td></tr></table></figure>
<h3 id="2-8-2-参数配置方式"><a href="#2-8-2-参数配置方式" class="headerlink" title="2.8.2 参数配置方式"></a>2.8.2 参数配置方式</h3></li>
</ol>
</li>
<li>查看当前所有的配置信息 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive<span class="operator">&gt;</span><span class="keyword">set</span>;</span><br></pre></td></tr></table></figure></li>
<li>参数的配置三种方式<ol>
<li>配置文件方式<ul>
<li>默认配置文件：hive-default.xml </li>
<li>用户自定义配置文件：hive-site.xml</li>
<li>注意：用户自定义配置会覆盖默认配置。另外，Hive也会读入Hadoop的配置，因为Hive是作为Hadoop的客户端启动的，Hive的配置会覆盖Hadoop的配置。配置文件的设定对本机启动的所有Hive进程都有效。</li>
</ul>
</li>
<li>命令行参数方式<ul>
<li>启动Hive时，可以在命令行添加-hiveconf param=value来设定参数。</li>
<li>例如：  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop103 hive]$ bin/hive -hiveconf mapred.reduce.tasks=10;</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
<li>注意：仅对本次hive启动有效。查看参数设置：  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">set</span> mapred.reduce.tasks;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>参数声明方式<ul>
<li>可以在HQL中使用SET关键字设定参数</li>
<li>例如：  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">set</span> mapred.reduce.tasks<span class="operator">=</span><span class="number">100</span>;</span><br></pre></td></tr></table></figure></li>
<li>注意：仅对本次hive启动有效。查看参数设置  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">set</span> mapred.reduce.tasks;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>上述三种设定方式的优先级依次递增。即default配置文件&lt;hive-site.x配置文件&lt;命令行参数&lt;参数声明。注意某些系统级的参数，例如log4j相关的设定，必须用前两种方式设定，因为那些参数的读取在会话建立以前已经完成了。</li>
</ol>
</li>
</ol>
<h1 id="三、Hive数据类型"><a href="#三、Hive数据类型" class="headerlink" title="三、Hive数据类型"></a>三、Hive数据类型</h1><h2 id="3-1-基本数据类型"><a href="#3-1-基本数据类型" class="headerlink" title="3.1 基本数据类型"></a>3.1 基本数据类型</h2><table>
<thead>
<tr>
<th>Hive数据类型</th>
<th>Java数据类型</th>
<th>长度</th>
</tr>
</thead>
<tbody><tr>
<td>TINYINT</td>
<td>byte</td>
<td>1byte有符号整数</td>
</tr>
<tr>
<td>SMALINT</td>
<td>short</td>
<td>2byte有符号整数</td>
</tr>
<tr>
<td>INT</td>
<td>int</td>
<td>4byte有符号整数</td>
</tr>
<tr>
<td>BIGINT</td>
<td>long</td>
<td>8byte有符号整数</td>
</tr>
<tr>
<td>BOOLEAN</td>
<td>boolean</td>
<td>布尔类型</td>
</tr>
<tr>
<td>FLOAT</td>
<td>float</td>
<td>单精度浮点数</td>
</tr>
<tr>
<td>DOUBLE</td>
<td>double</td>
<td>双精度浮点数</td>
</tr>
<tr>
<td>STRING</td>
<td>string</td>
<td>字符系列。可以指定字符集。使用单引号或者双引号</td>
</tr>
<tr>
<td>TIMESTAMP</td>
<td>Dat</td>
<td>时间类型</td>
</tr>
<tr>
<td>BINARY</td>
<td></td>
<td>字节数组</td>
</tr>
</tbody></table>
<ul>
<li>对于Hive的String类型相当于数据库的varchar类型，该类型是一个可变的字符串，不过它不能声明其中最多能存储多少个字符，理论上它可以存储2GB的字符数</li>
</ul>
<h2 id="3-2-集合数据类型"><a href="#3-2-集合数据类型" class="headerlink" title="3.2 集合数据类型"></a>3.2 集合数据类型</h2><table>
<thead>
<tr>
<th>数据类型</th>
<th>描述</th>
<th>语法示例</th>
</tr>
</thead>
<tbody><tr>
<td>STRUCT</td>
<td>和c语言中的struct类似，都可以通过“点”符号访问元素内容。例如，如果某个列的数据类型是STRUCT{first STRING, last STRING},那么第1个元素可以通过字段.first来引用。</td>
<td>struct()<br> 例如struct&lt;street:string, city:string&gt;</td>
</tr>
<tr>
<td>MAP</td>
<td>MAP是一组键-值对元组集合，使用数组表示法可以访问数据。例如，如果某个列的数据类型是MAP，其中键-&gt;值对是’first’-&gt;’John’和’last’-&gt;’Doe’，那么可以通过字段名[‘last’]获取最后一个元素</td>
<td>map()<br>例如map&lt;string, int&gt;</td>
</tr>
<tr>
<td>ARRAY</td>
<td>数组是一组具有相同类型和名称的变量的集合。这些变量称为数组的元素，每个数组元素都有一个编号，编号从零开始。例如，数组值为[‘John’, ‘Doe’]，那么第2个元素可以通过数组名[1]进行引用。</td>
<td>Array()<br>例如array<string></td>
</tr>
</tbody></table>
<ul>
<li>Hive有三种复杂数据类型ARRAY、MAP 和 STRUCT。ARRAY和MAP与Java中的Array和Map类似，而STRUCT与C语言中的Struct类似，它封装了一个命名字段集合，复杂数据类型允许任意层次的嵌套</li>
</ul>
<ol>
<li>数据类型实操<ol>
<li>数据示例 <figure class="highlight json"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">&quot;name&quot;</span>: <span class="string">&quot;songsong&quot;</span>,</span><br><span class="line">    </span><br><span class="line">    <span class="attr">&quot;friends&quot;</span>: [</span><br><span class="line">        <span class="string">&quot;bingbing&quot;</span>,</span><br><span class="line">        <span class="string">&quot;lili&quot;</span></span><br><span class="line">    ], </span><br><span class="line">    <span class="attr">&quot;children&quot;</span>: &#123; </span><br><span class="line">        <span class="attr">&quot;xiao song&quot;</span>: <span class="number">19</span>,</span><br><span class="line">        <span class="attr">&quot;xiaoxiao song&quot;</span>: <span class="number">18</span></span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="attr">&quot;address&quot;</span>: &#123; </span><br><span class="line">        <span class="attr">&quot;street&quot;</span>: <span class="string">&quot;hui long guan&quot;</span>,</span><br><span class="line">        <span class="attr">&quot;city&quot;</span>: <span class="string">&quot;beijing&quot;</span>,</span><br><span class="line">        <span class="attr">&quot;zip&quot;</span>: <span class="number">1000001</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>基于上述数据结构，在Hive里创建对应的表，并导入数据 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">test.txt</span><br><span class="line">songsong,bingbing_lili,xiao song:18_xiaoxiao song:19,hui long guan_beijing_1000001</span><br><span class="line">yangyang,caicai_susu,xiao yang:18_xiaoxiao yang:19,chao yang_beijing_1000001</span><br></pre></td></tr></table></figure>
<ul>
<li>注意：MAP，STRUCT和ARRAY里的元素间关系都可以用同一个字符表示，这里用“_”。</li>
</ul>
</li>
<li>Hive上创建测试表user <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> person_info (</span><br><span class="line">    name     string,</span><br><span class="line">    friends  <span class="keyword">array</span><span class="operator">&lt;</span>string<span class="operator">&gt;</span>,</span><br><span class="line">    children map<span class="operator">&lt;</span>string, <span class="type">int</span><span class="operator">&gt;</span>,</span><br><span class="line">    address  struct<span class="operator">&lt;</span>street:string, city:string, zip:<span class="type">int</span><span class="operator">&gt;</span></span><br><span class="line">)</span><br><span class="line"><span class="comment">-- 列分隔符</span></span><br><span class="line"><span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;,&#x27;</span></span><br><span class="line"><span class="comment">-- MAP STRUCT 和 ARRAY 的分隔符(数据分割符号)</span></span><br><span class="line">collection items terminated <span class="keyword">by</span> <span class="string">&#x27;_&#x27;</span></span><br><span class="line"><span class="comment">-- MAP中的key与value的分隔符</span></span><br><span class="line">map keys terminated <span class="keyword">by</span> <span class="string">&#x27;:&#x27;</span></span><br><span class="line"><span class="comment">-- 行分隔符</span></span><br><span class="line">lines terminated <span class="keyword">by</span> <span class="string">&#x27;\n&#x27;</span>;</span><br></pre></td></tr></table></figure></li>
<li>导入数据 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> load data <span class="keyword">local</span> inpath <span class="string">&#x27;/home/atguigu/hive/user.txt&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span> person_info;</span><br><span class="line">Loading data <span class="keyword">to</span> <span class="keyword">table</span> default.user_test</span><br><span class="line">OK</span><br><span class="line"><span class="type">Time</span> taken: <span class="number">0.873</span> seconds</span><br><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span></span><br></pre></td></tr></table></figure></li>
<li>查看数据 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> person_info <span class="keyword">where</span> friends[<span class="number">1</span>]<span class="operator">=</span><span class="string">&#x27;lili&#x27;</span> <span class="keyword">and</span> address.city<span class="operator">=</span><span class="string">&#x27;beijing&#x27;</span>;</span><br><span class="line"><span class="operator">+</span><span class="comment">-------------------+----------------------+--------------------------------------+----------------------------------------------------+</span></span><br><span class="line"><span class="operator">|</span> person_info.name  <span class="operator">|</span> person_info.friends  <span class="operator">|</span>         person_info.children         <span class="operator">|</span>                person_info.address                 <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-------------------+----------------------+--------------------------------------+----------------------------------------------------+</span></span><br><span class="line"><span class="operator">|</span> songsong          <span class="operator">|</span> [&quot;bingbing&quot;,&quot;lili&quot;]  <span class="operator">|</span> &#123;&quot;xiao song&quot;:<span class="number">18</span>,&quot;xiaoxiao song&quot;:<span class="number">19</span>&#125;  <span class="operator">|</span> &#123;&quot;street&quot;:&quot;hui long guan&quot;,&quot;city&quot;:&quot;beijing&quot;,&quot;zip&quot;:<span class="number">1000001</span>&#125; <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-------------------+----------------------+--------------------------------------+----------------------------------------------------+</span></span><br><span class="line"><span class="number">1</span> <span class="type">row</span> selected (<span class="number">0.076</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span></span><br></pre></td></tr></table></figure></li>
</ol>
</li>
</ol>
<h2 id="3-3-类型转化"><a href="#3-3-类型转化" class="headerlink" title="3.3 类型转化"></a>3.3 类型转化</h2><p>Hive的原生数据类型是可以进行隐式转换的，类似于Java的类型转换，例如某表达式使用INT类型，TINYINT会自动转换为INT类型，但是Hive不会进行反向转化，例如，某表达式使用TINYINT类型，INT不会自动转换为TINYINT类型，它会返回错误，除非使用CAST操作。</p>
<ol>
<li>隐式类型转换规则如下<ul>
<li>任何整数类型都可以隐式地转换为一个范围更广的类型，如TINYINT可以转换成INT，INT可以转换成BIGINT。</li>
<li>所有整数类型、FLOAT和STRING类型都可以隐式地转换成DOUBLE。</li>
<li>TINYINT、SMALLINT、INT都可以转换为FLOAT。</li>
<li>BOOLEAN类型不可以转换为任何其它的类型。</li>
</ul>
</li>
<li>可以使用CAST操作显示进行数据类型转换<ul>
<li>例如CAST(‘1’ AS INT)将把字符串’1’ 转换成整数1；如果强制类型转换失败，如执行CAST(‘X’ AS INT)，表达式返回空值 NULL。  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">select</span> <span class="number">1</span><span class="operator">+</span><span class="string">&#x27;1&#x27;</span>;</span><br><span class="line"><span class="operator">+</span><span class="comment">------+</span></span><br><span class="line"><span class="operator">|</span> _c0  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">------+</span></span><br><span class="line"><span class="operator">|</span> <span class="number">2.0</span>  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">------+</span></span><br><span class="line"><span class="number">1</span> <span class="type">row</span> selected (<span class="number">0.098</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">select</span> <span class="number">1</span><span class="operator">+</span><span class="built_in">cast</span>(<span class="string">&#x27;1&#x27;</span> <span class="keyword">as</span> <span class="type">int</span>);</span><br><span class="line"><span class="operator">+</span><span class="comment">------+</span></span><br><span class="line"><span class="operator">|</span> _c0  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">------+</span></span><br><span class="line"><span class="operator">|</span> <span class="number">2</span>    <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">------+</span></span><br><span class="line"><span class="number">1</span> <span class="type">row</span> selected (<span class="number">0.087</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">select</span> <span class="number">1</span><span class="operator">+</span><span class="built_in">cast</span>(<span class="string">&#x27;x&#x27;</span> <span class="keyword">as</span> <span class="type">int</span>);</span><br><span class="line"><span class="operator">+</span><span class="comment">-------+</span></span><br><span class="line"><span class="operator">|</span>  _c0  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-------+</span></span><br><span class="line"><span class="operator">|</span> <span class="keyword">NULL</span>  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-------+</span></span><br><span class="line"><span class="number">1</span> <span class="type">row</span> selected (<span class="number">0.096</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span></span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ol>
<h1 id="四、DDL数据库定义操作"><a href="#四、DDL数据库定义操作" class="headerlink" title="四、DDL数据库定义操作"></a>四、DDL数据库定义操作</h1><h2 id="4-1-创建数据库"><a href="#4-1-创建数据库" class="headerlink" title="4.1 创建数据库"></a>4.1 创建数据库</h2><ul>
<li>格式  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> DATABASE [IF <span class="keyword">NOT</span> <span class="keyword">EXISTS</span>] database_name</span><br><span class="line">[COMMENT database_comment]</span><br><span class="line">[LOCATION hdfs_path]</span><br><span class="line">[<span class="keyword">WITH</span> DBPROPERTIES (property_name<span class="operator">=</span>property_value, ...)];</span><br></pre></td></tr></table></figure></li>
</ul>
<ol>
<li>创建一个数据库，数据库在HDFS上的默认存储路径是/user/hive/warehouse/*.db。 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">create</span> database hive_id;</span><br><span class="line"><span class="keyword">No</span> <span class="keyword">rows</span> affected (<span class="number">0.108</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">show</span> databases;</span><br><span class="line"><span class="operator">+</span><span class="comment">----------------+</span></span><br><span class="line"><span class="operator">|</span> database_name  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------------+</span></span><br><span class="line"><span class="operator">|</span> <span class="keyword">default</span>        <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> hive_id        <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------------+</span></span><br><span class="line"><span class="number">2</span> <span class="keyword">rows</span> selected (<span class="number">0.044</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span></span><br></pre></td></tr></table></figure></li>
<li>避免要创建的数据库已经存在错误，增加if not exists判断。（标准写法） <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">show</span> databases;</span><br><span class="line"><span class="operator">+</span><span class="comment">----------------+</span></span><br><span class="line"><span class="operator">|</span> database_name  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------------+</span></span><br><span class="line"><span class="operator">|</span> <span class="keyword">default</span>        <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> hive_id        <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------------+</span></span><br><span class="line"><span class="number">2</span> <span class="keyword">rows</span> selected (<span class="number">0.044</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">create</span> database hive_id;</span><br><span class="line">Error: Error while processing statement: FAILED: Execution Error, <span class="keyword">return</span> code <span class="number">1</span> <span class="keyword">from</span> org.apache.hadoop.hive.ql.exec.DDLTask. Database hive_id already <span class="keyword">exists</span> (state<span class="operator">=</span><span class="number">42000</span>,code<span class="operator">=</span><span class="number">1</span>)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">create</span> database if <span class="keyword">not</span> <span class="keyword">exists</span> hive_id;</span><br><span class="line"><span class="keyword">No</span> <span class="keyword">rows</span> affected (<span class="number">0.031</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">show</span> databases;</span><br><span class="line"><span class="operator">+</span><span class="comment">----------------+</span></span><br><span class="line"><span class="operator">|</span> database_name  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------------+</span></span><br><span class="line"><span class="operator">|</span> <span class="keyword">default</span>        <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> hive_id        <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------------+</span></span><br><span class="line"><span class="number">2</span> <span class="keyword">rows</span> selected (<span class="number">0.03</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span></span><br></pre></td></tr></table></figure></li>
<li>创建一个数据库，指定数据库在HDFS上存放的位置 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">create</span> database if <span class="keyword">not</span> <span class="keyword">exists</span> database_test location <span class="string">&#x27;/hive/db/test/location/location_test_db.db&#x27;</span> <span class="keyword">with</span> dbproperties (<span class="string">&#x27;author&#x27;</span><span class="operator">=</span><span class="string">&#x27;anzhen&#x27;</span>, <span class="string">&#x27;create_time&#x27;</span><span class="operator">=</span><span class="string">&#x27;2021/10/25&#x27;</span>);</span><br><span class="line"><span class="keyword">No</span> <span class="keyword">rows</span> affected (<span class="number">0.057</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">show</span> databases;</span><br><span class="line"><span class="operator">+</span><span class="comment">-------------------+</span></span><br><span class="line"><span class="operator">|</span>   database_name   <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-------------------+</span></span><br><span class="line"><span class="operator">|</span> <span class="keyword">default</span>           <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> hive_id           <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> location_test_db  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-------------------+</span></span><br><span class="line"><span class="number">3</span> <span class="keyword">rows</span> selected (<span class="number">0.027</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> dfs <span class="operator">-</span>ls <span class="operator">/</span>hive<span class="operator">/</span>db<span class="operator">/</span>test<span class="operator">/</span>location<span class="operator">/</span>;</span><br><span class="line"><span class="operator">+</span><span class="comment">----------------------------------------------------+</span></span><br><span class="line"><span class="operator">|</span>                     DFS Output                     <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------------------------------------------------+</span></span><br><span class="line"><span class="operator">|</span> Found <span class="number">1</span> items                                      <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> drwxr<span class="operator">-</span>xr<span class="operator">-</span>x   <span class="operator">-</span> atguigu supergroup          <span class="number">0</span> <span class="number">2021</span><span class="number">-10</span><span class="number">-22</span> <span class="number">17</span>:<span class="number">11</span> <span class="operator">/</span>hive<span class="operator">/</span>db<span class="operator">/</span>test<span class="operator">/</span>location<span class="operator">/</span>location_test_db.db <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------------------------------------------------+</span></span><br><span class="line"><span class="number">2</span> <span class="keyword">rows</span> selected (<span class="number">0.011</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span></span><br></pre></td></tr></table></figure>
<h2 id="4-2-查询数据库"><a href="#4-2-查询数据库" class="headerlink" title="4.2 查询数据库"></a>4.2 查询数据库</h2></li>
<li>2.1 显示数据库</li>
<li>显示数据库 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">show</span> databases;</span><br><span class="line"><span class="operator">+</span><span class="comment">-------------------+</span></span><br><span class="line"><span class="operator">|</span>   database_name   <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-------------------+</span></span><br><span class="line"><span class="operator">|</span> <span class="keyword">default</span>           <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> hive_id           <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> location_test_db  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-------------------+</span></span><br><span class="line"><span class="number">3</span> <span class="keyword">rows</span> selected (<span class="number">0.096</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span></span><br></pre></td></tr></table></figure></li>
<li>过滤显示查询的数据库 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">show</span> databases <span class="keyword">like</span> <span class="string">&#x27;l*&#x27;</span>;</span><br><span class="line"><span class="operator">+</span><span class="comment">-------------------+</span></span><br><span class="line"><span class="operator">|</span>   database_name   <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-------------------+</span></span><br><span class="line"><span class="operator">|</span> location_test_db  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-------------------+</span></span><br><span class="line"><span class="number">1</span> <span class="type">row</span> selected (<span class="number">0.025</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span></span><br></pre></td></tr></table></figure>
<h3 id="4-2-2-查看数据库详情"><a href="#4-2-2-查看数据库详情" class="headerlink" title="4.2.2 查看数据库详情"></a>4.2.2 查看数据库详情</h3></li>
<li>显示数据库信息 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">desc</span> database location_test_db;</span><br><span class="line"><span class="operator">+</span><span class="comment">-------------------+----------+----------------------------------------------------+-------------+-------------+-------------+</span></span><br><span class="line"><span class="operator">|</span>      db_name      <span class="operator">|</span> comment  <span class="operator">|</span>                      location                      <span class="operator">|</span> owner_name  <span class="operator">|</span> owner_type  <span class="operator">|</span> parameters  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-------------------+----------+----------------------------------------------------+-------------+-------------+-------------+</span></span><br><span class="line"><span class="operator">|</span> location_test_db  <span class="operator">|</span>          <span class="operator">|</span> hdfs:<span class="operator">/</span><span class="operator">/</span>mycluster<span class="operator">/</span>hive<span class="operator">/</span>db<span class="operator">/</span>test<span class="operator">/</span>location<span class="operator">/</span>location_test_db.db <span class="operator">|</span> atguigu     <span class="operator">|</span> <span class="keyword">USER</span>        <span class="operator">|</span>             <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-------------------+----------+----------------------------------------------------+-------------+-------------+-------------+</span></span><br><span class="line"><span class="number">1</span> <span class="type">row</span> selected (<span class="number">0.037</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span></span><br></pre></td></tr></table></figure></li>
<li>显示数据库详细信息，extended <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">desc</span> database EXTENDED database_test;</span><br><span class="line"><span class="operator">+</span><span class="comment">----------------+----------+----------------------------------------------------+-------------+-------------+------------------------------------------+</span></span><br><span class="line"><span class="operator">|</span>    db_name     <span class="operator">|</span> comment  <span class="operator">|</span>                      location                      <span class="operator">|</span> owner_name  <span class="operator">|</span> owner_type  <span class="operator">|</span>                parameters                <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------------+----------+----------------------------------------------------+-------------+-------------+------------------------------------------+</span></span><br><span class="line"><span class="operator">|</span> database_test  <span class="operator">|</span>          <span class="operator">|</span> hdfs:<span class="operator">/</span><span class="operator">/</span>mycluster<span class="operator">/</span>hive<span class="operator">/</span>db<span class="operator">/</span>test<span class="operator">/</span>location<span class="operator">/</span>location_test_db.db <span class="operator">|</span> atguigu     <span class="operator">|</span> <span class="keyword">USER</span>        <span class="operator">|</span> &#123;create_time<span class="operator">=</span><span class="number">2021</span><span class="operator">/</span><span class="number">10</span><span class="operator">/</span><span class="number">25</span>, author<span class="operator">=</span>anzhen&#125;  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------------+----------+----------------------------------------------------+-------------+-------------+------------------------------------------+</span></span><br><span class="line"><span class="number">1</span> <span class="type">row</span> selected (<span class="number">0.019</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span></span><br></pre></td></tr></table></figure>
<h3 id="4-2-3-切换当前数据库"><a href="#4-2-3-切换当前数据库" class="headerlink" title="4.2.3 切换当前数据库"></a>4.2.3 切换当前数据库</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> use location_test_db;</span><br><span class="line"><span class="keyword">No</span> <span class="keyword">rows</span> affected (<span class="number">0.026</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> use hive_id;</span><br><span class="line"><span class="keyword">No</span> <span class="keyword">rows</span> affected (<span class="number">0.022</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span></span><br></pre></td></tr></table></figure>
<h2 id="4-3-修改数据库"><a href="#4-3-修改数据库" class="headerlink" title="4.3 修改数据库"></a>4.3 修改数据库</h2></li>
<li>用户可以使用ALTER DATABASE命令为某个数据库的DBPROPERTIES设置键-值对属性值，来描述这个数据库的属性信息。 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">alter</span> database location_test_db <span class="keyword">set</span> dbproperties(<span class="string">&#x27;createtile&#x27;</span><span class="operator">=</span><span class="string">&#x27;20211022&#x27;</span>);</span><br><span class="line"><span class="keyword">No</span> <span class="keyword">rows</span> affected (<span class="number">0.069</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">desc</span> database extended location_test_db;</span><br><span class="line"><span class="operator">+</span><span class="comment">-------------------+----------+----------------------------------------------------+-------------+-------------+------------------------+</span></span><br><span class="line"><span class="operator">|</span>      db_name      <span class="operator">|</span> comment  <span class="operator">|</span>                      location                      <span class="operator">|</span> owner_name  <span class="operator">|</span> owner_type  <span class="operator">|</span>       parameters       <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-------------------+----------+----------------------------------------------------+-------------+-------------+------------------------+</span></span><br><span class="line"><span class="operator">|</span> location_test_db  <span class="operator">|</span>          <span class="operator">|</span> hdfs:<span class="operator">/</span><span class="operator">/</span>mycluster<span class="operator">/</span>hive<span class="operator">/</span>db<span class="operator">/</span>test<span class="operator">/</span>location<span class="operator">/</span>location_test_db.db <span class="operator">|</span> atguigu     <span class="operator">|</span> <span class="keyword">USER</span>        <span class="operator">|</span> &#123;createtile<span class="operator">=</span><span class="number">20211022</span>&#125;  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-------------------+----------+----------------------------------------------------+-------------+-------------+------------------------+</span></span><br><span class="line"><span class="number">1</span> <span class="type">row</span> selected (<span class="number">0.023</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span></span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="4-4-删除数据库"><a href="#4-4-删除数据库" class="headerlink" title="4.4 删除数据库"></a>4.4 删除数据库</h2><ol>
<li>删除空数据库 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">show</span> databases;</span><br><span class="line"><span class="operator">+</span><span class="comment">-------------------+</span></span><br><span class="line"><span class="operator">|</span>   database_name   <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-------------------+</span></span><br><span class="line"><span class="operator">|</span> <span class="keyword">default</span>           <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> hive_id           <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> location_test_db  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-------------------+</span></span><br><span class="line"><span class="number">3</span> <span class="keyword">rows</span> selected (<span class="number">0.026</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">drop</span> database location_test_db;</span><br><span class="line"><span class="keyword">No</span> <span class="keyword">rows</span> affected (<span class="number">0.12</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">show</span> databases;</span><br><span class="line"><span class="operator">+</span><span class="comment">----------------+</span></span><br><span class="line"><span class="operator">|</span> database_name  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------------+</span></span><br><span class="line"><span class="operator">|</span> <span class="keyword">default</span>        <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> hive_id        <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------------+</span></span><br><span class="line"><span class="number">2</span> <span class="keyword">rows</span> selected (<span class="number">0.022</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span></span><br></pre></td></tr></table></figure></li>
<li>如果删除的数据库不存在，最好采用 if exists判断数据库是否存在 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">show</span> databases;</span><br><span class="line"><span class="operator">+</span><span class="comment">----------------+</span></span><br><span class="line"><span class="operator">|</span> database_name  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------------+</span></span><br><span class="line"><span class="operator">|</span> <span class="keyword">default</span>        <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> hive_id        <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------------+</span></span><br><span class="line"><span class="number">2</span> <span class="keyword">rows</span> selected (<span class="number">0.022</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">drop</span> database location_test_db;</span><br><span class="line">Error: Error while compiling statement: FAILED: SemanticException [Error <span class="number">10072</span>]: Database does <span class="keyword">not</span> exist: location_test_db (state<span class="operator">=</span><span class="number">42000</span>,code<span class="operator">=</span><span class="number">10072</span>)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">drop</span> database if <span class="keyword">exists</span> location_test_db;</span><br><span class="line"><span class="keyword">No</span> <span class="keyword">rows</span> affected (<span class="number">0.011</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">show</span> databases;</span><br><span class="line"><span class="operator">+</span><span class="comment">----------------+</span></span><br><span class="line"><span class="operator">|</span> database_name  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------------+</span></span><br><span class="line"><span class="operator">|</span> <span class="keyword">default</span>        <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> hive_id        <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------------+</span></span><br><span class="line"><span class="number">2</span> <span class="keyword">rows</span> selected (<span class="number">0.02</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span></span><br></pre></td></tr></table></figure></li>
<li>如果数据库不为空，可以采用cascade命令，强制删除 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">show</span> databases;</span><br><span class="line"><span class="operator">+</span><span class="comment">----------------+</span></span><br><span class="line"><span class="operator">|</span> database_name  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------------+</span></span><br><span class="line"><span class="operator">|</span> <span class="keyword">default</span>        <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> hive_id        <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------------+</span></span><br><span class="line"><span class="number">2</span> <span class="keyword">rows</span> selected (<span class="number">0.02</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">create</span> database location_test_db;</span><br><span class="line"><span class="keyword">No</span> <span class="keyword">rows</span> affected (<span class="number">0.051</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> use location_test_db;</span><br><span class="line"><span class="keyword">No</span> <span class="keyword">rows</span> affected (<span class="number">0.02</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">create</span> <span class="keyword">table</span> test(id <span class="type">int</span>,name string);</span><br><span class="line"><span class="keyword">No</span> <span class="keyword">rows</span> affected (<span class="number">0.053</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">drop</span> database location_test_db;</span><br><span class="line">Error: Error while processing statement: FAILED: Execution Error, <span class="keyword">return</span> code <span class="number">1</span> <span class="keyword">from</span> org.apache.hadoop.hive.ql.exec.DDLTask. InvalidOperationException(message:Database location_test_db <span class="keyword">is</span> <span class="keyword">not</span> empty. <span class="keyword">One</span> <span class="keyword">or</span> more tables exist.) (state<span class="operator">=</span><span class="number">08</span>S01,code<span class="operator">=</span><span class="number">1</span>)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">drop</span> database location_test_db cascade;</span><br><span class="line"><span class="keyword">No</span> <span class="keyword">rows</span> affected (<span class="number">0.31</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">show</span> databases;</span><br><span class="line"><span class="operator">+</span><span class="comment">----------------+</span></span><br><span class="line"><span class="operator">|</span> database_name  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------------+</span></span><br><span class="line"><span class="operator">|</span> <span class="keyword">default</span>        <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> hive_id        <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------------+</span></span><br><span class="line"><span class="number">2</span> <span class="keyword">rows</span> selected (<span class="number">0.02</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span></span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="4-5-创建表"><a href="#4-5-创建表" class="headerlink" title="4.5 创建表"></a>4.5 创建表</h2><ol>
<li>建表语法 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> [<span class="keyword">EXTERNAL</span>] <span class="keyword">TABLE</span> [IF <span class="keyword">NOT</span> <span class="keyword">EXISTS</span>] table_name </span><br><span class="line">[(col_name data_type [COMMENT col_comment], ...)] </span><br><span class="line">[COMMENT table_comment] </span><br><span class="line">[PARTITIONED <span class="keyword">BY</span> (col_name data_type [COMMENT col_comment], ...)] </span><br><span class="line">[CLUSTERED <span class="keyword">BY</span> (col_name, col_name, ...) </span><br><span class="line">[SORTED <span class="keyword">BY</span> (col_name [<span class="keyword">ASC</span><span class="operator">|</span><span class="keyword">DESC</span>], ...)] <span class="keyword">INTO</span> num_buckets BUCKETS] </span><br><span class="line">[<span class="type">ROW</span> FORMAT row_format] </span><br><span class="line">[STORED <span class="keyword">AS</span> file_format] </span><br><span class="line">[LOCATION hdfs_path]</span><br><span class="line">[TBLPROPERTIES (property_name<span class="operator">=</span>property_value, ...)]</span><br><span class="line">[<span class="keyword">AS</span> select_statement]</span><br></pre></td></tr></table></figure></li>
<li>字段解释说明 <ol>
<li><code>CREATE TABLE</code>: 创建一个指定名字的表。如果相同名字的表已经存在，则抛出异常；用户可以用<code>IF NOT EXISTS</code>选项来忽略这个异常。</li>
<li><code>EXTERNAL</code>: 关键字可以让用户创建一个外部表，在建表的同时可以指定一个指向实际数据的路径（<code>LOCATION</code>），<font color ='red' >在删除表的时候，内部表的元数据和数据会被一起删除，而外部表只删除元数据，不删除数据</font>。</li>
<li><code>COMMENT</code>: 为表和列添加注释。</li>
<li><code>PARTITIONED BY</code>: 指定分区表的规范</li>
<li><code>CLUSTERED BY</code>: 指定分桶表的规范</li>
<li><code>SORTED BY</code>: 指定单签表默认排序规则及粪桶数量</li>
<li><code>ROW FORMAT DELIMITED</code>: 定义每一行中字段之间的分隔符</li>
<li><code>collection items terminated by</code>: 集合数据类型，元素之间的分隔符</li>
<li><code>map keys terminated by</code>: map数据类型key-value分隔符</li>
<li><code>lines terminated by</code>: 每一行的分隔符</li>
<li><code>STORED AS</code>: 指定存储文件类型<ul>
<li>常用的存储文件类型：SEQUENCEFILE（二进制序列文件）、TEXTFILE（文本）、RCFILE（列式存储格式文件）</li>
<li>如果文件数据是纯文本，可以使用STORED AS TEXTFILE。如果数据需要压缩，使用 STORED AS SEQUENCEFILE。</li>
</ul>
</li>
<li><code>LOCATION</code>: 指定表在HDFS上的存储位置。</li>
<li><code>AS select_statement</code>: 后跟查询语句，根据查询结果创建表。</li>
<li><code>LIKE</code>: 允许用户复制现有的表结构，但是不复制数据。</li>
</ol>
</li>
</ol>
<h3 id="4-5-1-管理表-内部表"><a href="#4-5-1-管理表-内部表" class="headerlink" title="4.5.1 管理表(内部表)"></a>4.5.1 管理表(内部表)</h3><ol>
<li>理论<ul>
<li>默认创建的表都是所谓的管理表，有时也被称为内部表。因为这种表，Hive会（或多或少地）控制着数据的生命周期。Hive默认情况下会将这些表的数据存储在由配置项hive.metastore.warehouse.dir(例如，/user/hive/warehouse)所定义的目录的子目录下。当我们<font color ='red' >删除一个管理表时，Hive也会删除这个表中数据。管理表不适合和其他工具共享数据</font>。</li>
</ul>
</li>
<li>案例实操<ol>
<li>原始数据 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">1001	ss1</span><br><span class="line">1002	ss2</span><br><span class="line">1003	ss3</span><br><span class="line">1004	ss4</span><br><span class="line">1005	ss5</span><br><span class="line">1006	ss6</span><br><span class="line">1007	ss7</span><br><span class="line">1008	ss8</span><br><span class="line">1009	ss9</span><br><span class="line">1010	ss10</span><br><span class="line">1011	ss11</span><br><span class="line">1012	ss12</span><br><span class="line">1013	ss13</span><br><span class="line">1014	ss14</span><br><span class="line">1015	ss15</span><br><span class="line">1016	ss16</span><br></pre></td></tr></table></figure></li>
<li>普通创建表 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> if <span class="keyword">not</span> <span class="keyword">exists</span> student (</span><br><span class="line">    id   <span class="type">int</span>,</span><br><span class="line">    name string</span><br><span class="line">)</span><br><span class="line"><span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span></span><br><span class="line">stored <span class="keyword">as</span> textfile</span><br><span class="line">location <span class="string">&#x27;/user/hive/warehouse/student&#x27;</span>;</span><br></pre></td></tr></table></figure></li>
<li>导入数据 </li>
<li>根据查询结果创建表（查询的结果会添加到新创建的表中） <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> if <span class="keyword">not</span> <span class="keyword">exists</span> student_copy <span class="keyword">as</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> student;</span><br></pre></td></tr></table></figure></li>
<li>根据已经存在的表结构创建表 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> if <span class="keyword">not</span> <span class="keyword">exists</span> student_copy_like <span class="keyword">like</span> student;</span><br></pre></td></tr></table></figure></li>
<li>查询表的类型 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">desc</span> student;</span><br><span class="line"><span class="operator">+</span><span class="comment">-----------+------------+----------+</span></span><br><span class="line"><span class="operator">|</span> col_name  <span class="operator">|</span> data_type  <span class="operator">|</span> comment  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-----------+------------+----------+</span></span><br><span class="line"><span class="operator">|</span> id        <span class="operator">|</span> <span class="type">int</span>        <span class="operator">|</span>          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> name      <span class="operator">|</span> string     <span class="operator">|</span>          <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-----------+------------+----------+</span></span><br><span class="line"><span class="number">2</span> <span class="keyword">rows</span> selected (<span class="number">0.101</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span></span><br></pre></td></tr></table></figure>
<h3 id="4-5-2-外部表"><a href="#4-5-2-外部表" class="headerlink" title="4.5.2 外部表"></a>4.5.2 外部表</h3></li>
</ol>
</li>
<li>理论<ul>
<li>因为表是外部表，所以Hive并非认为其完全拥有这份数据。<font color ='red' >删除该表并不会删除掉这份数据，不过描述表的元数据信息会被删除掉</font>。</li>
</ul>
</li>
<li>管理表和外部表的使用场景<ul>
<li>每天将收集到的网站日志定期流入HDFS文本文件。在外部表（原始日志表）的基础上做大量的统计分析，用到的中间表、结果表使用内部表存储，数据通过SELECT+INSERT进入内部表。</li>
</ul>
</li>
<li>案例实操<ul>
<li>分别创建部门和员工外部表，并向表中导入数据。</li>
</ul>
<ol>
<li>原始数据<ul>
<li>dept:  <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">10	ACCOUNTING	1700</span><br><span class="line">20	RESEARCH	1800</span><br><span class="line">30	SALES	1900</span><br><span class="line">40	OPERATIONS	1700</span><br></pre></td></tr></table></figure></li>
<li>emp：  <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">7369	SMITH	CLERK	7902	1980-12-17	800.00		20</span><br><span class="line">7499	ALLEN	SALESMAN	7698	1981-2-20	1600.00	300.00	30</span><br><span class="line">7521	WARD	SALESMAN	7698	1981-2-22	1250.00	500.00	30</span><br><span class="line">7566	JONES	MANAGER	7839	1981-4-2	2975.00		20</span><br><span class="line">7654	MARTIN	SALESMAN	7698	1981-9-28	1250.00	1400.00	30</span><br><span class="line">7698	BLAKE	MANAGER	7839	1981-5-1	2850.00		30</span><br><span class="line">7782	CLARK	MANAGER	7839	1981-6-9	2450.00		10</span><br><span class="line">7788	SCOTT	ANALYST	7566	1987-4-19	3000.00		20</span><br><span class="line">7839	KING	PRESIDENT		1981-11-17	5000.00		10</span><br><span class="line">7844	TURNER	SALESMAN	7698	1981-9-8	1500.00	0.00	30</span><br><span class="line">7876	ADAMS	CLERK	7788	1987-5-23	1100.00		20</span><br><span class="line">7900	JAMES	CLERK	7698	1981-12-3	950.00		30</span><br><span class="line">7902	FORD	ANALYST	7566	1981-12-3	3000.00		20</span><br><span class="line">7934	MILLER	CLERK	7782	1982-1-23	1300.00		10</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>上传数据到HDFS <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 hive]$ hadoop fs -put dep.txt /hive/source_data/</span><br><span class="line">[atguigu@hadoop001 hive]$ hadoop fs -put emp.txt /hive/source_data/</span><br><span class="line">[atguigu@hadoop001 hive]$ hadoop fs -ls /hive/source_data</span><br><span class="line">Found 2 items</span><br><span class="line">-rw-r--r--   3 atguigu supergroup         69 2021-10-25 13:15 /hive/source_data/dep.txt</span><br><span class="line">-rw-r--r--   3 atguigu supergroup        657 2021-10-25 13:15 /hive/source_data/emp.txt</span><br><span class="line">[atguigu@hadoop001 hive]$</span><br></pre></td></tr></table></figure></li>
<li>建表语句，创建外部表<ul>
<li>创建部门表  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> if <span class="keyword">not</span> <span class="keyword">exists</span> dept (</span><br><span class="line">    dept_no <span class="type">int</span>,</span><br><span class="line">    dept_name  string,</span><br><span class="line">    loc    <span class="type">int</span></span><br><span class="line">)</span><br><span class="line"><span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span>;</span><br></pre></td></tr></table></figure></li>
<li>创建员工表  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> if <span class="keyword">not</span> <span class="keyword">exists</span> emp (</span><br><span class="line">    emp_no    <span class="type">int</span>,</span><br><span class="line">    emp_name  string,</span><br><span class="line">    job       string,</span><br><span class="line">    mgr       <span class="type">int</span>,</span><br><span class="line">    hire_date string,</span><br><span class="line">    sal       <span class="keyword">double</span>,</span><br><span class="line">    comm      <span class="keyword">double</span>,</span><br><span class="line">    dept_no   <span class="type">int</span></span><br><span class="line">)</span><br><span class="line"><span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span>;</span><br></pre></td></tr></table></figure></li>
<li>查看表格式化数据  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">0: jdbc:hive2://hadoop001:10000&gt; desc formatted dept;</span><br><span class="line">+---------------+------------------+--------+</span><br><span class="line">| col_name      | data_type        | comment|</span><br><span class="line">+---------------+------------------+--------+</span><br><span class="line">···</span><br><span class="line">···</span><br><span class="line">| Table Type:   | EXTERNAL_TABLE   | NULL   |</span><br><span class="line">···</span><br><span class="line">···</span><br><span class="line">+---------------+------------------+--------+</span><br><span class="line">35 rows selected (0.054 seconds)</span><br><span class="line">0: jdbc:hive2://hadoop001:10000&gt;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>导入数据 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">0: jdbc:hive2://hadoop001:10000&gt; load data inpath <span class="string">&#x27;/hive/source_data/dep.txt&#x27;</span> into table mock_data.dept;</span><br><span class="line">No rows affected (0.132 seconds)</span><br><span class="line">0: jdbc:hive2://hadoop001:10000&gt; load data inpath <span class="string">&#x27;/hive/source_data/emp.txt&#x27;</span> into table mock_data.emp;</span><br><span class="line">No rows affected (0.095 seconds)</span><br><span class="line">0: jdbc:hive2://hadoop001:10000&gt; select count(1) from mock_data.emp;</span><br><span class="line">+------+</span><br><span class="line">| _c0  |</span><br><span class="line">+------+</span><br><span class="line">| 14   |</span><br><span class="line">+------+</span><br><span class="line">1 row selected (14.939 seconds)</span><br><span class="line">0: jdbc:hive2://hadoop001:10000&gt; select count(1) from mock_data.dept;</span><br><span class="line">+------+</span><br><span class="line">| _c0  |</span><br><span class="line">+------+</span><br><span class="line">| 4    |</span><br><span class="line">+------+</span><br><span class="line">1 row selected (13.836 seconds)</span><br><span class="line">0: jdbc:hive2://hadoop001:10000&gt;</span><br></pre></td></tr></table></figure></li>
<li>删除外部表后，hdfs中的数据还在，但是metadata中dept的元数据已被删除 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">drop</span> <span class="keyword">table</span> mock_data.dept;</span><br><span class="line"><span class="keyword">No</span> <span class="keyword">rows</span> affected (<span class="number">0.097</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">show</span> tables;</span><br><span class="line"><span class="operator">+</span><span class="comment">--------------------+</span></span><br><span class="line"><span class="operator">|</span>      tab_name      <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">--------------------+</span></span><br><span class="line"><span class="operator">|</span> emp                <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> person             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> person_info        <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> student            <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> student_copy_as    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> student_copy_like  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">--------------------+</span></span><br><span class="line"><span class="number">6</span> <span class="keyword">rows</span> selected (<span class="number">0.02</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span></span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> dfs <span class="operator">-</span>ls <span class="operator">/</span><span class="keyword">user</span><span class="operator">/</span>hive<span class="operator">/</span>warehouse<span class="operator">/</span>mock_data.db<span class="operator">/</span>dept;</span><br><span class="line"><span class="operator">+</span><span class="comment">----------------------------------------------------+</span></span><br><span class="line"><span class="operator">|</span>                     DFS Output                     <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------------------------------------------------+</span></span><br><span class="line"><span class="operator">|</span> Found <span class="number">1</span> items                                      <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="operator">-</span>rw<span class="operator">-</span>r<span class="comment">--r--   3 atguigu supergroup         69 2021-10-25 13:15 /user/hive/warehouse/mock_data.db/dept/dep.txt |</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------------------------------------------------+</span></span><br><span class="line"><span class="number">2</span> <span class="keyword">rows</span> selected (<span class="number">0.042</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span></span><br></pre></td></tr></table></figure></li>
<li>重建表，再次查询 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> if <span class="keyword">not</span> <span class="keyword">exists</span> dept (</span><br><span class="line">     dept_no <span class="type">int</span>,</span><br><span class="line">     dept_name  string,</span><br><span class="line">     loc    <span class="type">int</span></span><br><span class="line"> )</span><br><span class="line"> <span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span>;</span><br><span class="line"><span class="keyword">No</span> <span class="keyword">rows</span> affected (<span class="number">0.039</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">show</span> tables ;</span><br><span class="line"><span class="operator">+</span><span class="comment">--------------------+</span></span><br><span class="line"><span class="operator">|</span>      tab_name      <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">--------------------+</span></span><br><span class="line"><span class="operator">|</span> dept               <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> emp                <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> person             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> person_info        <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> student            <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> student_copy_as    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> student_copy_like  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">--------------------+</span></span><br><span class="line"><span class="number">7</span> <span class="keyword">rows</span> selected (<span class="number">0.021</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> dept;</span><br><span class="line"><span class="operator">+</span><span class="comment">---------------+-----------------+-----------+</span></span><br><span class="line"><span class="operator">|</span> dept.dept_no  <span class="operator">|</span> dept.dept_name  <span class="operator">|</span> dept.loc  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">---------------+-----------------+-----------+</span></span><br><span class="line"><span class="operator">|</span> <span class="number">10</span>            <span class="operator">|</span> ACCOUNTING      <span class="operator">|</span> <span class="number">1700</span>      <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">20</span>            <span class="operator">|</span> RESEARCH        <span class="operator">|</span> <span class="number">1800</span>      <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">30</span>            <span class="operator">|</span> SALES           <span class="operator">|</span> <span class="number">1900</span>      <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">40</span>            <span class="operator">|</span> OPERATIONS      <span class="operator">|</span> <span class="number">1700</span>      <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">---------------+-----------------+-----------+</span></span><br><span class="line"><span class="number">4</span> <span class="keyword">rows</span> selected (<span class="number">0.059</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span></span><br></pre></td></tr></table></figure></li>
</ol>
</li>
</ol>
<h3 id="4-5-3-管理表-内部表-与外部表的互相转换"><a href="#4-5-3-管理表-内部表-与外部表的互相转换" class="headerlink" title="4.5.3 管理表(内部表)与外部表的互相转换"></a>4.5.3 管理表(内部表)与外部表的互相转换</h3><ol>
<li>查询表的类型 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 hive]$ hive -e <span class="string">&#x27;desc formatted mock_data.student;&#x27;</span>| grep <span class="string">&#x27;Table Type&#x27;</span></span><br><span class="line"><span class="built_in">which</span>: no hbase <span class="keyword">in</span> (/usr/<span class="built_in">local</span>/bin:/usr/bin:/usr/<span class="built_in">local</span>/sbin:/usr/sbin:/opt/module/jdk1.8.0_212/bin:/opt/module/ha-hadoop-3.1.3/bin:/opt/module/ha-hadoop-3.1.3/sbin:/opt/module/hive-3.1.2/bin:/home/atguigu/.<span class="built_in">local</span>/bin:/home/atguigu/bin)</span><br><span class="line">Hive Session ID = 8eb096f9-31db-49d1-b22b-5eb6e458c0c4</span><br><span class="line"></span><br><span class="line">Logging initialized using configuration <span class="keyword">in</span> file:/opt/module/hive-3.1.2/conf/hive-log4j2.properties Async: <span class="literal">true</span></span><br><span class="line">Hive Session ID = a1d905c5-6885-493a-bd5e-db945af0b3ac</span><br><span class="line">OK</span><br><span class="line">Table Type:         	MANAGED_TABLE</span><br><span class="line">Time taken: 0.679 seconds, Fetched: 32 row(s)</span><br><span class="line">[atguigu@hadoop001 hive]$</span><br></pre></td></tr></table></figure></li>
<li>内部表外部表转换 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"># 外部表</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">alter</span> <span class="keyword">table</span> student <span class="keyword">set</span> tblproperties(<span class="string">&#x27;EXTERNAL&#x27;</span><span class="operator">=</span><span class="string">&#x27;TRUE&#x27;</span>);</span><br><span class="line"><span class="keyword">No</span> <span class="keyword">rows</span> affected (<span class="number">0.048</span> seconds)</span><br><span class="line"># 内部表</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">alter</span> <span class="keyword">table</span> student <span class="keyword">set</span> tblproperties(<span class="string">&#x27;EXTERNAL&#x27;</span><span class="operator">=</span><span class="string">&#x27;FALSE&#x27;</span>);</span><br><span class="line"><span class="keyword">No</span> <span class="keyword">rows</span> affected (<span class="number">0.053</span> seconds)</span><br></pre></td></tr></table></figure>
<ul>
<li>注意：(‘EXTERNAL’=’TRUE’)和(‘EXTERNAL’=’FALSE’)为固定写法，区分大小写！</li>
</ul>
</li>
</ol>
<h3 id="4-5-3-内部表外部表使用场景"><a href="#4-5-3-内部表外部表使用场景" class="headerlink" title="4.5.3 内部表外部表使用场景"></a>4.5.3 内部表外部表使用场景</h3><ul>
<li>内部表：不常用，通常一些中间表(运算过程产生)会使用内部表</li>
<li>外部表：常用，主要考虑HDFS源数据安全性，数据分析一般采用外部表</li>
</ul>
<h2 id="4-6-修改表"><a href="#4-6-修改表" class="headerlink" title="4.6 修改表"></a>4.6 修改表</h2><h3 id="4-6-1-重命名表"><a href="#4-6-1-重命名表" class="headerlink" title="4.6.1 重命名表"></a>4.6.1 重命名表</h3><ol>
<li>语法 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> table_name RENAME <span class="keyword">TO</span> new_table_name</span><br></pre></td></tr></table></figure></li>
<li>实操案例 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">show</span> tables;</span><br><span class="line"><span class="operator">+</span><span class="comment">--------------------+</span></span><br><span class="line"><span class="operator">|</span>      tab_name      <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">--------------------+</span></span><br><span class="line"><span class="operator">|</span> dept               <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> emp                <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> person             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> person_info        <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> student            <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> student_copy_as    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> student_copy_like  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">--------------------+</span></span><br><span class="line"><span class="number">7</span> <span class="keyword">rows</span> selected (<span class="number">0.035</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">alter</span> <span class="keyword">table</span> student_copy_as rename <span class="keyword">to</span> student_copy_as_rename;</span><br><span class="line"><span class="keyword">No</span> <span class="keyword">rows</span> affected (<span class="number">0.069</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">show</span> tables;</span><br><span class="line"><span class="operator">+</span><span class="comment">-------------------------+</span></span><br><span class="line"><span class="operator">|</span>        tab_name         <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-------------------------+</span></span><br><span class="line"><span class="operator">|</span> dept                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> emp                     <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> person                  <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> person_info             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> student                 <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> student_copy_as_rename  <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> student_copy_like       <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-------------------------+</span></span><br><span class="line"><span class="number">7</span> <span class="keyword">rows</span> selected (<span class="number">0.019</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span></span><br></pre></td></tr></table></figure>
<h3 id="4-6-2-增加、修改和删除表分区"><a href="#4-6-2-增加、修改和删除表分区" class="headerlink" title="4.6.2 增加、修改和删除表分区"></a>4.6.2 增加、修改和删除表分区</h3> 详见7.1章分区表基本操作。</li>
</ol>
<h3 id="4-6-3-增加-修改-替换列信息"><a href="#4-6-3-增加-修改-替换列信息" class="headerlink" title="4.6.3 增加/修改/替换列信息"></a>4.6.3 增加/修改/替换列信息</h3><ol>
<li>语法<ul>
<li>更新列  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> table_name CHANGE [<span class="keyword">COLUMN</span>] col_old_name col_new_name column_type [COMMENT col_comment] [<span class="keyword">FIRST</span><span class="operator">|</span>AFTER column_name]</span><br></pre></td></tr></table></figure></li>
<li>增加和替换列  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> table_name <span class="keyword">ADD</span><span class="operator">|</span>REPLACE COLUMNS (col_name data_type [COMMENT col_comment], ...) </span><br></pre></td></tr></table></figure>
<ul>
<li>注：ADD是代表新增一字段，字段位置在所有列后面(partition列前)，REPLACE则是表示替换表中所有字段。</li>
</ul>
</li>
</ul>
</li>
<li>实操案例<ul>
<li>查询表结构  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">desc</span> student;</span><br><span class="line"><span class="operator">+</span><span class="comment">-----------+------------+----------+</span></span><br><span class="line"><span class="operator">|</span> col_name  <span class="operator">|</span> data_type  <span class="operator">|</span> comment  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-----------+------------+----------+</span></span><br><span class="line"><span class="operator">|</span> id        <span class="operator">|</span> <span class="type">int</span>        <span class="operator">|</span>          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> name      <span class="operator">|</span> string     <span class="operator">|</span>          <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-----------+------------+----------+</span></span><br><span class="line"><span class="number">2</span> <span class="keyword">rows</span> selected (<span class="number">0.027</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span></span><br></pre></td></tr></table></figure></li>
<li>添加列  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">alter</span> <span class="keyword">table</span> dept <span class="keyword">add</span> columns(dept_desc string, dept_create_time string);</span><br><span class="line"><span class="keyword">No</span> <span class="keyword">rows</span> affected (<span class="number">0.058</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">desc</span> dept;</span><br><span class="line"><span class="operator">+</span><span class="comment">-------------------+------------+----------+</span></span><br><span class="line"><span class="operator">|</span>     col_name      <span class="operator">|</span> data_type  <span class="operator">|</span> comment  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-------------------+------------+----------+</span></span><br><span class="line"><span class="operator">|</span> dept_no           <span class="operator">|</span> <span class="type">int</span>        <span class="operator">|</span>          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> dept_name         <span class="operator">|</span> string     <span class="operator">|</span>          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> loc               <span class="operator">|</span> <span class="type">int</span>        <span class="operator">|</span>          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> dept_desc         <span class="operator">|</span> string     <span class="operator">|</span>          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> dept_create_time  <span class="operator">|</span> string     <span class="operator">|</span>          <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-------------------+------------+----------+</span></span><br><span class="line"><span class="number">5</span> <span class="keyword">rows</span> selected (<span class="number">0.026</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span></span><br></pre></td></tr></table></figure></li>
<li>更新列  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">0: jdbc:hive2://hadoop001:10000&gt; alter table dept change column dept_desc desc string;</span><br><span class="line">No rows affected (0.055 seconds)</span><br><span class="line">0: jdbc:hive2://hadoop001:10000&gt; desc dept;</span><br><span class="line">+-------------------+------------+----------+</span><br><span class="line">|     col_name      | data_type  | comment  |</span><br><span class="line">+-------------------+------------+----------+</span><br><span class="line">| dept_no           | int        |          |</span><br><span class="line">| dept_name         | string     |          |</span><br><span class="line">| loc               | int        |          |</span><br><span class="line">| desc              | string     |          |</span><br><span class="line">| dept_create_time  | string     |          |</span><br><span class="line">+-------------------+------------+----------+</span><br><span class="line">5 rows selected (0.026 seconds)</span><br><span class="line">0: jdbc:hive2://hadoop001:10000&gt;</span><br></pre></td></tr></table></figure></li>
<li>替换列  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">alter</span> <span class="keyword">table</span> dept replace columns(deptno string, dname string, loc string);</span><br><span class="line"><span class="keyword">No</span> <span class="keyword">rows</span> affected (<span class="number">0.083</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">desc</span> dept;</span><br><span class="line"><span class="operator">+</span><span class="comment">-----------+------------+----------+</span></span><br><span class="line"><span class="operator">|</span> col_name  <span class="operator">|</span> data_type  <span class="operator">|</span> comment  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-----------+------------+----------+</span></span><br><span class="line"><span class="operator">|</span> deptno    <span class="operator">|</span> string     <span class="operator">|</span>          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> dname     <span class="operator">|</span> string     <span class="operator">|</span>          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> loc       <span class="operator">|</span> string     <span class="operator">|</span>          <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-----------+------------+----------+</span></span><br><span class="line"><span class="number">3</span> <span class="keyword">rows</span> selected (<span class="number">0.027</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span></span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ol>
<h1 id="五、DML数据库操作"><a href="#五、DML数据库操作" class="headerlink" title="五、DML数据库操作"></a>五、DML数据库操作</h1><h2 id="5-1-数据导入"><a href="#5-1-数据导入" class="headerlink" title="5.1 数据导入"></a>5.1 数据导入</h2><h3 id="5-1-1-向表中装载数据（Load）"><a href="#5-1-1-向表中装载数据（Load）" class="headerlink" title="5.1.1 向表中装载数据（Load）"></a>5.1.1 向表中装载数据（Load）</h3><ol>
<li>语法 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">load data [<span class="built_in">local</span>] inpath <span class="string">&#x27;数据的path&#x27;</span> [overwrite] into table student [partition (partcol1=val1,…)];</span><br></pre></td></tr></table></figure>
<ul>
<li>load data:表示加载数据</li>
<li>local:表示从本地加载数据到hive表；否则从HDFS加载数据到hive表</li>
<li>inpath:表示加载数据的路径</li>
<li>overwrite:表示覆盖表中已有数据，否则表示追加</li>
<li>into table:表示加载到哪张表</li>
<li>student:表示具体的表</li>
<li>partition:表示上传到指定分区</li>
</ul>
</li>
<li>实操案例<ul>
<li>加载本地文件到hive  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> load data <span class="keyword">local</span> inpath <span class="string">&#x27;/home/atguigu/hive/student.txt&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span> mock_data.student;</span><br><span class="line"><span class="keyword">No</span> <span class="keyword">rows</span> affected (<span class="number">0.148</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span></span><br></pre></td></tr></table></figure></li>
<li>加载HDFS文件到hive中  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> dfs <span class="operator">-</span>ls <span class="operator">/</span>hive<span class="operator">/</span>source_data;</span><br><span class="line"><span class="operator">+</span><span class="comment">----------------------------------------------------+</span></span><br><span class="line"><span class="operator">|</span>                     DFS Output                     <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------------------------------------------------+</span></span><br><span class="line"><span class="operator">|</span> Found <span class="number">1</span> items                                      <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="operator">-</span>rw<span class="operator">-</span>r<span class="comment">--r--   3 atguigu supergroup        150 2021-10-25 15:22 /hive/source_data/student.txt |</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------------------------------------------------+</span></span><br><span class="line"><span class="number">2</span> <span class="keyword">rows</span> selected (<span class="number">0.055</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> load data inpath <span class="string">&#x27;/hive/source_data/student.txt&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span> mock_data.student;</span><br><span class="line"><span class="keyword">No</span> <span class="keyword">rows</span> affected (<span class="number">0.121</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span></span><br></pre></td></tr></table></figure></li>
<li>加载数据覆盖表中已有的数据  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">0: jdbc:hive2://hadoop001:10000&gt; select count(*) from mock_data.student;</span><br><span class="line">+------+</span><br><span class="line">| _c0  |</span><br><span class="line">+------+</span><br><span class="line">| 48   |</span><br><span class="line">+------+</span><br><span class="line">1 row selected (16.219 seconds)</span><br><span class="line">0: jdbc:hive2://hadoop001:10000&gt; load data <span class="built_in">local</span> inpath <span class="string">&#x27;/home/atguigu/hive/student.txt&#x27;</span> overwrite into table mock_data.student;</span><br><span class="line">No rows affected (0.104 seconds)</span><br><span class="line">0: jdbc:hive2://hadoop001:10000&gt; select count(*) from mock_data.student;</span><br><span class="line">+------+</span><br><span class="line">| _c0  |</span><br><span class="line">+------+</span><br><span class="line">| 16   |</span><br><span class="line">+------+</span><br><span class="line">1 row selected (14.727 seconds)</span><br><span class="line">0: jdbc:hive2://hadoop001:10000&gt;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>导入顺序<ul>
<li>追加导入数据根据文件名字典序排序</li>
</ul>
</li>
</ol>
<h3 id="5-1-2-通过查询语句向表中插入数据（Insert）"><a href="#5-1-2-通过查询语句向表中插入数据（Insert）" class="headerlink" title="5.1.2 通过查询语句向表中插入数据（Insert）"></a>5.1.2 通过查询语句向表中插入数据（Insert）</h3><ol>
<li>根据单张表查询结果插入 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> student_from_select <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> student;</span><br></pre></td></tr></table></figure>
<ul>
<li>insert into：以追加数据的方式插入到表或分区，原有数据不会删除</li>
<li>insert overwrite：会覆盖表中已存在的数据</li>
<li>注意：insert不支持插入部分字段</li>
</ul>
</li>
<li>多表（多分区）插入模式（根据多张表查询结果） <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> student</span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> student_from_select</span><br><span class="line"><span class="keyword">select</span> id, name <span class="keyword">where</span> id<span class="operator">=</span><span class="number">1001</span></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> student_copy_like</span><br><span class="line"><span class="keyword">select</span> id, name <span class="keyword">where</span> id<span class="operator">=</span><span class="number">1009</span></span><br></pre></td></tr></table></figure>
<h3 id="5-1-3-查询语句中创建表并加载数据（As-Select）"><a href="#5-1-3-查询语句中创建表并加载数据（As-Select）" class="headerlink" title="5.1.3 查询语句中创建表并加载数据（As Select）"></a>5.1.3 查询语句中创建表并加载数据（As Select）</h3></li>
<li>根据查询结果创建表（查询的结果会添加到新创建的表中） <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> if <span class="keyword">not</span> <span class="keyword">exists</span> student3 <span class="keyword">as</span> <span class="keyword">select</span> id, name <span class="keyword">from</span> student; </span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="5-1-4-创建表时通过Location指定加载数据路径"><a href="#5-1-4-创建表时通过Location指定加载数据路径" class="headerlink" title="5.1.4 创建表时通过Location指定加载数据路径"></a>5.1.4 创建表时通过Location指定加载数据路径</h3><ol>
<li>上传数据到hdfs上 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> dfs <span class="operator">-</span>put <span class="operator">/</span>home<span class="operator">/</span>atguigu<span class="operator">/</span>hive<span class="operator">/</span>load_student.txt <span class="operator">/</span>hive<span class="operator">/</span>db<span class="operator">/</span>mock_data<span class="operator">/</span>load_student<span class="operator">/</span>load_student.txt;</span><br><span class="line"><span class="operator">+</span><span class="comment">-------------+</span></span><br><span class="line"><span class="operator">|</span> DFS Output  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-------------+</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-------------+</span></span><br><span class="line"><span class="keyword">No</span> <span class="keyword">rows</span> selected (<span class="number">0.019</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span></span><br></pre></td></tr></table></figure></li>
<li>创建表，并指定在hdfs上的位置 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> if <span class="keyword">not</span> <span class="keyword">exists</span> load_student (</span><br><span class="line">    id   <span class="type">int</span>,</span><br><span class="line">    name string</span><br><span class="line">)</span><br><span class="line"><span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span></span><br><span class="line">stored <span class="keyword">as</span> textfile</span><br><span class="line">location <span class="string">&#x27;/hive/db/mock_data/load_student&#x27;</span>;</span><br></pre></td></tr></table></figure></li>
<li>查询数据 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> mock_data.load_student limit <span class="number">10</span>;</span><br><span class="line"><span class="operator">+</span><span class="comment">------------------+--------------------+</span></span><br><span class="line"><span class="operator">|</span> load_student.id  <span class="operator">|</span> load_student.name  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">------------------+--------------------+</span></span><br><span class="line"><span class="operator">|</span> <span class="number">22852294</span>         <span class="operator">|</span> 司寇俊                <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">22852295</span>         <span class="operator">|</span> 安投悬                <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">22852296</span>         <span class="operator">|</span> 申燎赢                <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">22852297</span>         <span class="operator">|</span> 洪谎                 <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">22852298</span>         <span class="operator">|</span> 丰觅                 <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">22852299</span>         <span class="operator">|</span> 拓跋卷                <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">22852300</span>         <span class="operator">|</span> 汪舶                 <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">22852301</span>         <span class="operator">|</span> 涂钦羡                <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">22852302</span>         <span class="operator">|</span> 濮崭                 <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">22852303</span>         <span class="operator">|</span> 阚撑                 <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">------------------+--------------------+</span></span><br><span class="line"><span class="number">10</span> <span class="keyword">rows</span> selected (<span class="number">0.057</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span></span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="5-1-5-Import数据到指定Hive表中"><a href="#5-1-5-Import数据到指定Hive表中" class="headerlink" title="5.1.5 Import数据到指定Hive表中"></a>5.1.5 Import数据到指定Hive表中</h3><ul>
<li>注意：先用export导出后，再将数据导入。多用于小规模迁移，会自动创建表。<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> export <span class="keyword">table</span> mock_data.person <span class="keyword">to</span> <span class="string">&#x27;/hive/source_data/person/export&#x27;</span>;</span><br><span class="line"><span class="keyword">No</span> <span class="keyword">rows</span> affected (<span class="number">17.558</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> import <span class="keyword">table</span> mock_data.import_person <span class="keyword">from</span> <span class="string">&#x27;/hive/source_data/person/export&#x27;</span>;</span><br><span class="line"><span class="keyword">No</span> <span class="keyword">rows</span> affected (<span class="number">4.487</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">select</span> <span class="built_in">count</span>(<span class="operator">*</span>) <span class="keyword">from</span> mock_data.import_person;</span><br><span class="line"><span class="operator">+</span><span class="comment">-----------+</span></span><br><span class="line"><span class="operator">|</span>    _c0    <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-----------+</span></span><br><span class="line"><span class="operator">|</span> <span class="number">24307000</span>  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-----------+</span></span><br><span class="line"><span class="number">1</span> <span class="type">row</span> selected (<span class="number">19.358</span> seconds)</span><br></pre></td></tr></table></figure></li>
</ul>
<h2 id="5-2-数据导出"><a href="#5-2-数据导出" class="headerlink" title="5.2 数据导出"></a>5.2 数据导出</h2><h3 id="5-2-1-Insert导出"><a href="#5-2-1-Insert导出" class="headerlink" title="5.2.1 Insert导出"></a>5.2.1 Insert导出</h3><ol>
<li>将查询的结果导出到本地 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">local</span> directory <span class="string">&#x27;/home/atguigu/hive/export/person&#x27;</span> <span class="keyword">select</span> id,name <span class="keyword">from</span> mock_data.person;</span><br></pre></td></tr></table></figure></li>
<li>将查询的结果格式化导出到本地 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">local</span> directory <span class="string">&#x27;/home/atguigu/hive/export/person&#x27;</span> <span class="type">ROW</span> FORMAT DELIMITED FIELDS TERMINATED <span class="keyword">BY</span> <span class="string">&#x27;\t&#x27;</span> <span class="keyword">select</span> id,name <span class="keyword">from</span> mock_data.person;</span><br></pre></td></tr></table></figure></li>
<li>将查询的结果导出到HDFS上(没有local) <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">insert</span> overwrite directory <span class="string">&#x27;/hive/source_data/person&#x27;</span> <span class="type">ROW</span> FORMAT DELIMITED FIELDS TERMINATED <span class="keyword">BY</span> <span class="string">&#x27;\t&#x27;</span> <span class="keyword">select</span> id,name <span class="keyword">from</span> mock_data.person;</span><br><span class="line"><span class="keyword">No</span> <span class="keyword">rows</span> affected (<span class="number">21.398</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> dfs <span class="operator">-</span>ls <span class="operator">/</span>hive<span class="operator">/</span>source_data<span class="operator">/</span>person;</span><br><span class="line"><span class="operator">+</span><span class="comment">----------------------------------------------------+</span></span><br><span class="line"><span class="operator">|</span>                     DFS Output                     <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------------------------------------------------+</span></span><br><span class="line"><span class="operator">|</span> Found <span class="number">5</span> items                                      <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="operator">-</span>rw<span class="operator">-</span>r<span class="comment">--r--   3 atguigu supergroup  115767751 2021-10-25 19:13 /hive/source_data/person/000000_0 |</span></span><br><span class="line"><span class="operator">|</span> <span class="operator">-</span>rw<span class="operator">-</span>r<span class="comment">--r--   3 atguigu supergroup  115656456 2021-10-25 19:13 /hive/source_data/person/000001_0 |</span></span><br><span class="line"><span class="operator">|</span> <span class="operator">-</span>rw<span class="operator">-</span>r<span class="comment">--r--   3 atguigu supergroup  111382098 2021-10-25 19:13 /hive/source_data/person/000002_0 |</span></span><br><span class="line"><span class="operator">|</span> <span class="operator">-</span>rw<span class="operator">-</span>r<span class="comment">--r--   3 atguigu supergroup   55441386 2021-10-25 19:13 /hive/source_data/person/000003_0 |</span></span><br><span class="line"><span class="operator">|</span> <span class="operator">-</span>rw<span class="operator">-</span>r<span class="comment">--r--   3 atguigu supergroup   27719097 2021-10-25 19:13 /hive/source_data/person/000004_0 |</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------------------------------------------------+</span></span><br><span class="line"><span class="number">6</span> <span class="keyword">rows</span> selected (<span class="number">0.012</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span></span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="5-2-2-Export导出到HDFS上"><a href="#5-2-2-Export导出到HDFS上" class="headerlink" title="5.2.2 Export导出到HDFS上"></a>5.2.2 Export导出到HDFS上</h3><p>export和import主要用于两个Hadoop平台集群之间Hive表迁移。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">export <span class="keyword">table</span> mock_data.person <span class="keyword">to</span> <span class="string">&#x27;/hive/source_data/person/export&#x27;</span>;</span><br></pre></td></tr></table></figure>

<h3 id="5-2-3-Hive-Shell-命令导出"><a href="#5-2-3-Hive-Shell-命令导出" class="headerlink" title="5.2.3 Hive Shell 命令导出"></a>5.2.3 Hive Shell 命令导出</h3><ul>
<li>基本语法：（hive -f/-e 执行语句或者脚本 &gt; file）  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 hive]$ bin/hive -e <span class="string">&#x27;select * from default.student;&#x27;</span> &gt;</span><br><span class="line"> /opt/module/hive/datas/<span class="built_in">export</span>/student4.txt;</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="5-2-4-Export导出到HDFS上"><a href="#5-2-4-Export导出到HDFS上" class="headerlink" title="5.2.4 Export导出到HDFS上"></a>5.2.4 Export导出到HDFS上</h3><ul>
<li>export和import主要用于两个Hadoop平台集群之间Hive表迁移。  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">(defahiveult)<span class="operator">&gt;</span> export <span class="keyword">table</span> default.student <span class="keyword">to</span></span><br><span class="line"> <span class="string">&#x27;/user/hive/warehouse/export/student&#x27;</span>;</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="5-2-5-清除表中数据（Truncate）"><a href="#5-2-5-清除表中数据（Truncate）" class="headerlink" title="5.2.5 清除表中数据（Truncate）"></a>5.2.5 清除表中数据（Truncate）</h3><ul>
<li>注意：Truncate只能删除管理表，不能删除外部表中数据<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">truncate</span> <span class="keyword">table</span> student;</span><br></pre></td></tr></table></figure></li>
</ul>
<h1 id="六、查询"><a href="#六、查询" class="headerlink" title="六、查询"></a>六、查询</h1><h3 id="6-1-1-全表和特定列查询"><a href="#6-1-1-全表和特定列查询" class="headerlink" title="6.1.1 全表和特定列查询"></a>6.1.1 全表和特定列查询</h3><ol>
<li>数据准备 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">dept:</span><br><span class="line">10	ACCOUNTING	1700</span><br><span class="line">20	RESEARCH	1800</span><br><span class="line">30	SALES	1900</span><br><span class="line">40	OPERATIONS	1700</span><br><span class="line">emp：</span><br><span class="line">7369	SMITH	CLERK	7902	1980-12-17	800.00		20</span><br><span class="line">7499	ALLEN	SALESMAN	7698	1981-2-20	1600.00	300.00	30</span><br><span class="line">7521	WARD	SALESMAN	7698	1981-2-22	1250.00	500.00	30</span><br><span class="line">7566	JONES	MANAGER	7839	1981-4-2	2975.00		20</span><br><span class="line">7654	MARTIN	SALESMAN	7698	1981-9-28	1250.00	1400.00	30</span><br><span class="line">7698	BLAKE	MANAGER	7839	1981-5-1	2850.00		30</span><br><span class="line">7782	CLARK	MANAGER	7839	1981-6-9	2450.00		10</span><br><span class="line">7788	SCOTT	ANALYST	7566	1987-4-19	3000.00		20</span><br><span class="line">7839	KING	PRESIDENT		1981-11-17	5000.00		10</span><br><span class="line">7844	TURNER	SALESMAN	7698	1981-9-8	1500.00	0.00	30</span><br><span class="line">7876	ADAMS	CLERK	7788	1987-5-23	1100.00		20</span><br><span class="line">7900	JAMES	CLERK	7698	1981-12-3	950.00		30</span><br><span class="line">7902	FORD	ANALYST	7566	1981-12-3	3000.00		20</span><br><span class="line">7934	MILLER	CLERK	7782	1982-1-23	1300.00		10</span><br></pre></td></tr></table></figure></li>
<li>创建表结构 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 部门表</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> if <span class="keyword">not</span> <span class="keyword">exists</span> dept(</span><br><span class="line">    deptno <span class="type">int</span>,</span><br><span class="line">    dname string,</span><br><span class="line">    loc <span class="type">int</span></span><br><span class="line">)</span><br><span class="line"><span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span>;</span><br><span class="line"><span class="comment">-- 员工表</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> if <span class="keyword">not</span> <span class="keyword">exists</span> emp(</span><br><span class="line">    empno <span class="type">int</span>,</span><br><span class="line">    ename string,</span><br><span class="line">    job string,</span><br><span class="line">    mgr <span class="type">int</span>,</span><br><span class="line">    hiredate string, </span><br><span class="line">    sal <span class="keyword">double</span>, </span><br><span class="line">    comm <span class="keyword">double</span>,</span><br><span class="line">    deptno <span class="type">int</span></span><br><span class="line">)</span><br><span class="line"><span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span>;</span><br></pre></td></tr></table></figure></li>
<li>导入数据 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">load data <span class="keyword">local</span> inpath <span class="string">&#x27;/opt/module/datas/dept.txt&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span></span><br><span class="line">dept;	</span><br><span class="line">load data <span class="keyword">local</span> inpath <span class="string">&#x27;/opt/module/datas/emp.txt&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span> emp;</span><br></pre></td></tr></table></figure></li>
<li>全表查询 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> emp;</span><br><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> empno,ename,job,mgr,hiredate,sal,comm,deptno <span class="keyword">from</span> emp ;</span><br></pre></td></tr></table></figure></li>
<li>选择特定列查询 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> empno, ename <span class="keyword">from</span> emp;</span><br></pre></td></tr></table></figure></li>
<li>注意：<ul>
<li>（1）SQL 语言大小写不敏感。 </li>
<li>（2）SQL 可以写在一行或者多行</li>
<li>（3）关键字不能被缩写也不能分行</li>
<li>（4）各子句一般要分行写。</li>
<li>（5）使用缩进提高语句的可读性。</li>
</ul>
</li>
</ol>
<h3 id="6-1-2-列别名"><a href="#6-1-2-列别名" class="headerlink" title="6.1.2 列别名"></a>6.1.2 列别名</h3><ol>
<li>用途<ol>
<li>重命名一个列</li>
<li>便于计算</li>
<li>紧跟列名，也可以在列名和别名之间加入关键字‘AS’ </li>
<li>案例实操 </li>
</ol>
</li>
<li>查询名称和部门 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> ename <span class="keyword">AS</span> name, deptno dn <span class="keyword">from</span> emp;</span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="6-1-3-算术运算符"><a href="#6-1-3-算术运算符" class="headerlink" title="6.1.3 算术运算符"></a>6.1.3 算术运算符</h3><table>
<thead>
<tr>
<th>运算符</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>A+B</td>
<td>A和B 相加</td>
</tr>
<tr>
<td>A-B</td>
<td>A减去B</td>
</tr>
<tr>
<td>A*B</td>
<td>A和B 相乘</td>
</tr>
<tr>
<td>A/B</td>
<td>A除以B</td>
</tr>
<tr>
<td>A%B</td>
<td>A对B取余</td>
</tr>
<tr>
<td>A&amp;B</td>
<td>A和B按位取与</td>
</tr>
<tr>
<td>A</td>
<td>B</td>
</tr>
<tr>
<td>A^B</td>
<td>A和B按位取异或</td>
</tr>
<tr>
<td>~A</td>
<td>A按位取反</td>
</tr>
</tbody></table>
<ul>
<li>案例实操：查询出所有员工的薪水后加1显示。<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> sal <span class="operator">+</span><span class="number">1</span> <span class="keyword">from</span> emp;</span><br></pre></td></tr></table></figure>
<h3 id="6-1-4-常用函数"><a href="#6-1-4-常用函数" class="headerlink" title="6.1.4 常用函数"></a>6.1.4 常用函数</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 求总行数（count）</span></span><br><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="built_in">count</span>(<span class="operator">*</span>) cnt <span class="keyword">from</span> emp;</span><br><span class="line"><span class="comment">-- 求工资的最大值（max）</span></span><br><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="built_in">max</span>(sal) max_sal <span class="keyword">from</span> emp;</span><br><span class="line"><span class="comment">-- 求工资的最小值（min）</span></span><br><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="built_in">min</span>(sal) min_sal <span class="keyword">from</span> emp;</span><br><span class="line"><span class="comment">-- 求工资的总和（sum）</span></span><br><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="built_in">sum</span>(sal) sum_sal <span class="keyword">from</span> emp; </span><br><span class="line"><span class="comment">-- 求工资的平均值（avg）</span></span><br><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="built_in">avg</span>(sal) avg_sal <span class="keyword">from</span> emp;</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="6-1-5-Limit语句"><a href="#6-1-5-Limit语句" class="headerlink" title="6.1.5 Limit语句"></a>6.1.5 Limit语句</h3><ul>
<li>LIMIT子句用于限制返回的行数。  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> emp limit <span class="number">5</span>;</span><br><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> emp limit <span class="number">2</span>,<span class="number">3</span>;</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="6-1-6-Where语句"><a href="#6-1-6-Where语句" class="headerlink" title="6.1.6 Where语句"></a>6.1.6 Where语句</h3><ol>
<li>使用WHERE子句，将不满足条件的行过滤掉</li>
<li>WHERE子句紧随FROM子句</li>
<li>案例实操<ul>
<li>查询出薪水大于1000的所有员工  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> emp <span class="keyword">where</span> sal <span class="operator">&gt;</span><span class="number">1000</span>;</span><br></pre></td></tr></table></figure></li>
<li>注意：where子句中不能使用字段别名。</li>
</ul>
</li>
</ol>
<h3 id="6-1-7-比较运算符（Between-In-Is-Null）"><a href="#6-1-7-比较运算符（Between-In-Is-Null）" class="headerlink" title="6.1.7 比较运算符（Between/In/ Is Null）"></a>6.1.7 比较运算符（Between/In/ Is Null）</h3><ol>
<li><p>下面表中描述了谓词操作符，这些操作符同样可以用于JOIN…ON和HAVING语句中。</p>
<table>
<thead>
<tr>
<th>操作符</th>
<th>支持的数据类型</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>A=B</td>
<td>基本数据类型</td>
<td>如果A等于B则返回TRUE，反之返回FALSE</td>
</tr>
<tr>
<td>A&lt;=&gt;B</td>
<td>基本数据类型</td>
<td>如果A和B都为NULL，则返回TRUE，如果一边为NULL，返回False</td>
</tr>
<tr>
<td>A&lt;&gt;B, A!=B</td>
<td>基本数据类型</td>
<td>A或者B为NULL则返回NULL；如果A不等于B，则返回TRUE，反之返回FALSE</td>
</tr>
<tr>
<td>A&lt;B</td>
<td>基本数据类型</td>
<td>A或者B为NULL，则返回NULL；如果A小于B，则返回TRUE，反之返回FALSE</td>
</tr>
<tr>
<td>A&lt;=B</td>
<td>基本数据类型</td>
<td>A或者B为NULL，则返回NULL；如果A小于等于B，则返回TRUE，反之返回FALSE</td>
</tr>
<tr>
<td>A&gt;B</td>
<td>基本数据类型</td>
<td>A或者B为NULL，则返回NULL；如果A大于B，则返回TRUE，反之返回FALSE</td>
</tr>
<tr>
<td>A&gt;=B</td>
<td>基本数据类型</td>
<td>A或者B为NULL，则返回NULL；如果A大于等于B，则返回TRUE，反之返回FALSE</td>
</tr>
<tr>
<td>A [NOT] BETWEEN B AND C</td>
<td>基本数据类型</td>
<td>如果A，B或者C任一为NULL，则结果为NULL。如果A的值大于等于B而且小于或等于C，则结果为TRUE，反之为FALSE。如果使用NOT关键字则可达到相反的效果。</td>
</tr>
<tr>
<td>A IS NULL</td>
<td>所有数据类型</td>
<td>如果A等于NULL，则返回TRUE，反之返回FALSE</td>
</tr>
<tr>
<td>A IS NOT NULL</td>
<td>所有数据类型</td>
<td>如果A不等于NULL，则返回TRUE，反之返回FALSE</td>
</tr>
<tr>
<td>IN(数值1, 数值2)</td>
<td>所有数据类型</td>
<td>使用 IN运算显示列表中的值</td>
</tr>
<tr>
<td>A [NOT] LIKE B</td>
<td>STRING 类型</td>
<td>B是一个SQL下的简单正则表达式，也叫通配符模式，如果A与其匹配的话，则返回TRUE；反之返回FALSE。B的表达式说明如下：‘x%’表示A必须以字母‘x’开头，‘%x’表示A必须以字母’x’结尾，而‘%x%’表示A包含有字母’x’,可以位于开头，结尾或者字符串中间。如果使用NOT关键字则可达到相反的效果。</td>
</tr>
<tr>
<td>A RLIKE B, A REGEXP B</td>
<td>STRING 类型</td>
<td>B是基于java的正则表达式，如果A与其匹配，则返回TRUE；反之返回FALSE。匹配使用的是JDK中的正则表达式接口实现的，因为正则也依据其中的规则。例如，正则表达式必须和整个字符串A相匹配，而不是只需与其字符串匹配。</td>
</tr>
</tbody></table>
</li>
<li><p>案例实操</p>
<ul>
<li>查询出薪水等于5000的所有员工  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; select * from emp <span class="built_in">where</span> sal =5000;</span><br></pre></td></tr></table></figure></li>
<li>查询工资在500到1000的员工信息  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> emp <span class="keyword">where</span> sal <span class="keyword">between</span> <span class="number">500</span> <span class="keyword">and</span> <span class="number">1000</span>;</span><br></pre></td></tr></table></figure></li>
<li>查询comm为空的所有员工信息  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> emp <span class="keyword">where</span> comm <span class="keyword">is</span> <span class="keyword">null</span>;</span><br></pre></td></tr></table></figure></li>
<li>查询工资是1500或5000的员工信息  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> emp <span class="keyword">where</span> sal <span class="keyword">IN</span> (<span class="number">1500</span>, <span class="number">5000</span>);</span><br></pre></td></tr></table></figure>
<h3 id="6-1-8-Like和RLike"><a href="#6-1-8-Like和RLike" class="headerlink" title="6.1.8 Like和RLike"></a>6.1.8 Like和RLike</h3></li>
</ul>
</li>
<li><p>使用LIKE运算选择类似的值</p>
</li>
<li><p>选择条件可以包含字符或数字:</p>
<ul>
<li>% 代表零个或多个字符(任意个字符)。</li>
<li>_ 代表一个字符。</li>
</ul>
</li>
<li><p>RLIKE子句</p>
<ul>
<li>RLIKE子句是Hive中这个功能的一个扩展，其可以通过Java的正则表达式这个更强大的语言来指定匹配条件。</li>
</ul>
</li>
<li><p>案例实操</p>
<ol>
<li>查找名字以A开头的员工信息 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> emp <span class="keyword">where</span> ename <span class="keyword">LIKE</span> <span class="string">&#x27;A%&#x27;</span>;</span><br></pre></td></tr></table></figure></li>
<li>查找名字中第二个字母为A的员工信息 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> emp <span class="keyword">where</span> ename <span class="keyword">LIKE</span> <span class="string">&#x27;_A%&#x27;</span>;</span><br></pre></td></tr></table></figure></li>
<li>查找名字中带有A-N的员工信息 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> emp <span class="keyword">where</span> ename  RLIKE <span class="string">&#x27;[A-N]&#x27;</span>;</span><br></pre></td></tr></table></figure>
<h3 id="6-1-9-逻辑运算符（And-Or-Not）"><a href="#6-1-9-逻辑运算符（And-Or-Not）" class="headerlink" title="6.1.9 逻辑运算符（And/Or/Not）"></a>6.1.9 逻辑运算符（And/Or/Not）</h3><table>
<thead>
<tr>
<th>操作符</th>
<th>含义</th>
</tr>
</thead>
<tbody><tr>
<td>AND</td>
<td>逻辑并</td>
</tr>
<tr>
<td>OR</td>
<td>逻辑或</td>
</tr>
<tr>
<td>NOT</td>
<td>逻辑否</td>
</tr>
</tbody></table>
</li>
</ol>
</li>
<li><p>案例实操</p>
<ol>
<li>查询薪水大于1000，部门是30 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> emp <span class="keyword">where</span> sal<span class="operator">&gt;</span><span class="number">1000</span> <span class="keyword">and</span> deptno<span class="operator">=</span><span class="number">30</span>;</span><br></pre></td></tr></table></figure></li>
<li>查询薪水大于1000，或者部门是30 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> emp <span class="keyword">where</span> sal<span class="operator">&gt;</span><span class="number">1000</span> <span class="keyword">or</span> deptno<span class="operator">=</span><span class="number">30</span>;</span><br></pre></td></tr></table></figure></li>
<li>查询除了20部门和30部门以外的员工信息 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> emp <span class="keyword">where</span> deptno <span class="keyword">not</span> <span class="keyword">IN</span>(<span class="number">30</span>, <span class="number">20</span>);</span><br></pre></td></tr></table></figure></li>
</ol>
</li>
</ol>
<h2 id="6-2-分组"><a href="#6-2-分组" class="headerlink" title="6.2 分组"></a>6.2 分组</h2><h3 id="6-2-1-Group-By语句"><a href="#6-2-1-Group-By语句" class="headerlink" title="6.2.1 Group By语句"></a>6.2.1 Group By语句</h3><p>GROUP BY语句通常会和聚合函数一起使用，按照一个或者多个列队结果进行分组，然后对每个组执行聚合操作。</p>
<ol>
<li>案例实操：<ol>
<li>计算emp表每个部门的平均工资 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> t.deptno, <span class="built_in">avg</span>(t.sal) avg_sal <span class="keyword">from</span> emp t <span class="keyword">group</span> <span class="keyword">by</span> t.deptno;</span><br></pre></td></tr></table></figure></li>
<li>计算emp每个部门中每个岗位的最高薪水 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> t.deptno, t.job, <span class="built_in">max</span>(t.sal) max_sal <span class="keyword">from</span> emp t <span class="keyword">group</span> <span class="keyword">by</span> t.deptno, t.job;</span><br></pre></td></tr></table></figure>
<h3 id="6-2-2-Having语句"><a href="#6-2-2-Having语句" class="headerlink" title="6.2.2 Having语句"></a>6.2.2 Having语句</h3></li>
</ol>
</li>
<li>having与where不同点<ol>
<li>where后面不能写分组函数，而having后面可以使用分组函数。</li>
<li>having只用于group by分组统计语句。</li>
</ol>
</li>
<li>案例实操<ol>
<li>求每个部门的平均薪水大于2000的部门<ol>
<li>求每个部门的平均工资 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> deptno, <span class="built_in">avg</span>(sal) <span class="keyword">from</span> emp <span class="keyword">group</span> <span class="keyword">by</span> deptno;</span><br></pre></td></tr></table></figure></li>
<li>求每个部门的平均薪水大于2000的部门  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> deptno, <span class="built_in">avg</span>(sal) avg_sal <span class="keyword">from</span> emp <span class="keyword">group</span> <span class="keyword">by</span> deptno <span class="keyword">having</span> avg_sal <span class="operator">&gt;</span> <span class="number">2000</span>;</span><br></pre></td></tr></table></figure>
<h2 id="6-3-Join语句"><a href="#6-3-Join语句" class="headerlink" title="6.3 Join语句"></a>6.3 Join语句</h2><h3 id="6-3-1-等值Join"><a href="#6-3-1-等值Join" class="headerlink" title="6.3.1 等值Join"></a>6.3.1 等值Join</h3>Hive支持通常的SQL JOIN语句。 </li>
</ol>
</li>
</ol>
</li>
<li>案例实操<ol>
<li>根据员工表和部门表中的部门编号相等，查询员工编号、员工名称和部门名称； <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> e.empno, e.ename, d.deptno, d.dname <span class="keyword">from</span> emp e <span class="keyword">join</span> dept d <span class="keyword">on</span> e.deptno <span class="operator">=</span> d.deptno;</span><br></pre></td></tr></table></figure></li>
</ol>
</li>
</ol>
<h3 id="6-3-2-表的别名"><a href="#6-3-2-表的别名" class="headerlink" title="6.3.2 表的别名"></a>6.3.2 表的别名</h3><ol>
<li>好处<ol>
<li>使用别名可以简化查询。</li>
<li>使用表名前缀可以提高执行效率。</li>
</ol>
</li>
<li>案例实操<ol>
<li>合并员工表和部门表 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> e.empno, e.ename, d.deptno <span class="keyword">from</span> emp e <span class="keyword">join</span> dept d <span class="keyword">on</span> e.deptno <span class="operator">=</span> d.deptno;</span><br></pre></td></tr></table></figure>
<h3 id="6-3-3-内连接"><a href="#6-3-3-内连接" class="headerlink" title="6.3.3 内连接"></a>6.3.3 内连接</h3></li>
</ol>
</li>
</ol>
<ul>
<li>只有进行连接的两个表中都存在与连接条件相匹配的数据才会被保留下来。  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> e.empno, e.ename, d.deptno <span class="keyword">from</span> emp e <span class="keyword">join</span> dept d <span class="keyword">on</span> e.deptno <span class="operator">=</span> d.deptno;</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="6-3-4-左外连接"><a href="#6-3-4-左外连接" class="headerlink" title="6.3.4 左外连接"></a>6.3.4 左外连接</h3><ul>
<li>JOIN操作符左边表中符合WHERE子句的所有记录将会被返回。  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> e.empno, e.ename, d.deptno <span class="keyword">from</span> emp e <span class="keyword">left</span> <span class="keyword">join</span> dept d <span class="keyword">on</span> e.deptno <span class="operator">=</span> d.deptno;</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="6-3-5-右外连接"><a href="#6-3-5-右外连接" class="headerlink" title="6.3.5 右外连接"></a>6.3.5 右外连接</h3><ul>
<li>JOIN操作符右边表中符合WHERE子句的所有记录将会被返回。  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> e.empno, e.ename, d.deptno <span class="keyword">from</span> emp e <span class="keyword">right</span> <span class="keyword">join</span> dept d <span class="keyword">on</span> e.deptno <span class="operator">=</span> d.deptno;</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="6-3-6-满外连接"><a href="#6-3-6-满外连接" class="headerlink" title="6.3.6 满外连接"></a>6.3.6 满外连接</h3><ul>
<li>将会返回所有表中符合WHERE语句条件的所有记录。如果任一表的指定字段没有符合条件的值的话，那么就使用NULL值替代。  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> e.empno, e.ename, d.deptno <span class="keyword">from</span> emp e <span class="keyword">full</span> <span class="keyword">join</span> dept d <span class="keyword">on</span> e.deptno <span class="operator">=</span> d.deptno;</span><br></pre></td></tr></table></figure>
<h3 id="6-3-7-多表连接"><a href="#6-3-7-多表连接" class="headerlink" title="6.3.7 多表连接"></a>6.3.7 多表连接</h3></li>
<li>注意：连接 n个表，至少需要n-1个连接条件。例如：连接三个表，至少需要两个连接条件。</li>
<li>数据准备  <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">location</span><br><span class="line">1700	Beijing</span><br><span class="line">1800	London</span><br><span class="line">1900	Tokyo</span><br></pre></td></tr></table></figure></li>
</ul>
<ol>
<li>创建位置表 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> if <span class="keyword">not</span> <span class="keyword">exists</span> location(</span><br><span class="line">    loc <span class="type">int</span>,</span><br><span class="line">    loc_name string</span><br><span class="line">)</span><br><span class="line"><span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span>;</span><br></pre></td></tr></table></figure></li>
<li>导入数据 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> load data <span class="keyword">local</span> inpath <span class="string">&#x27;/opt/module/datas/location.txt&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span> location;</span><br></pre></td></tr></table></figure></li>
<li>多表连接查询 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span><span class="keyword">SELECT</span></span><br><span class="line">       e.emp_name,</span><br><span class="line">       d.dept_name,</span><br><span class="line">       l.loc_name</span><br><span class="line"><span class="keyword">FROM</span> emp e</span><br><span class="line"><span class="keyword">JOIN</span> dept d <span class="keyword">ON</span> d.dept_no <span class="operator">=</span> e.dept_no</span><br><span class="line"><span class="keyword">JOIN</span> location l <span class="keyword">ON</span> d.loc <span class="operator">=</span> l.loc;</span><br></pre></td></tr></table></figure>
<ul>
<li>大多数情况下，Hive会对每对JOIN连接对象启动一个MapReduce任务。本例中会首先启动一个MapReduce job对表e和表d进行连接操作，然后会再启动一个MapReduce job将第一个MapReduce job的输出和表l;进行连接操作。</li>
<li>注意：为什么不是表d和表l先进行连接操作呢？这是因为Hive总是按照从左到右的顺序执行的。</li>
<li>优化：当对3个或者更多表进行join连接时，如果每个on子句都使用相同的连接键的话，那么只会产生一个MapReduce job。</li>
</ul>
</li>
</ol>
<h3 id="6-3-8-笛卡尔积"><a href="#6-3-8-笛卡尔积" class="headerlink" title="6.3.8 笛卡尔积"></a>6.3.8 笛卡尔积</h3><ol>
<li>笛卡尔积会在下面条件下产生<ol>
<li>省略连接条件</li>
<li>连接条件无效</li>
<li>所有表中的所有行互相连接</li>
</ol>
</li>
<li>案例实操 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> emp_name, dept_name <span class="keyword">from</span> emp, dept;</span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="6-4-排序"><a href="#6-4-排序" class="headerlink" title="6.4 排序"></a>6.4 排序</h2><h3 id="6-4-1-全局排序（Order-By）"><a href="#6-4-1-全局排序（Order-By）" class="headerlink" title="6.4.1 全局排序（Order By）"></a>6.4.1 全局排序（Order By）</h3><p>Order By：全局排序，只有一个Reducer</p>
<ol>
<li>使用 ORDER BY 子句排序<ul>
<li>ASC（ascend）: 升序（默认）</li>
<li>DESC（descend）: 降序</li>
</ul>
</li>
<li>ORDER BY 子句在SELECT语句的结尾</li>
<li>案例实操 <ol>
<li>查询员工信息按工资升序排列 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> emp <span class="keyword">order</span> <span class="keyword">by</span> sal;</span><br></pre></td></tr></table></figure></li>
<li>查询员工信息按工资降序排列 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> emp <span class="keyword">order</span> <span class="keyword">by</span> sal <span class="keyword">desc</span>;</span><br></pre></td></tr></table></figure>
<h3 id="6-4-2-按照别名排序"><a href="#6-4-2-按照别名排序" class="headerlink" title="6.4.2 按照别名排序"></a>6.4.2 按照别名排序</h3></li>
</ol>
</li>
</ol>
<ul>
<li>按照员工薪水的2倍排序  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> ename, sal<span class="operator">*</span><span class="number">2</span> twosal <span class="keyword">from</span> emp <span class="keyword">order</span> <span class="keyword">by</span> twosal;</span><br></pre></td></tr></table></figure>
<h3 id="6-4-3-多个列排序"><a href="#6-4-3-多个列排序" class="headerlink" title="6.4.3 多个列排序"></a>6.4.3 多个列排序</h3></li>
<li>按照部门和工资升序排序  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> ename, deptno, sal <span class="keyword">from</span> emp <span class="keyword">order</span> <span class="keyword">by</span> deptno, sal ;</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="6-4-4-Sort-By-每个Reduce内部排序"><a href="#6-4-4-Sort-By-每个Reduce内部排序" class="headerlink" title="6.4.4 Sort By 每个Reduce内部排序"></a>6.4.4 Sort By 每个Reduce内部排序</h3><ul>
<li>Sort By：对于大规模的数据集order by的效率非常低。在很多情况下，并不需要全局排序，此时可以使用sort by。</li>
<li>Sort by为每个reducer产生一个排序文件。每个Reducer内部进行排序，对全局结果集来说不是排序。</li>
</ul>
<ol>
<li>设置reduce个数 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">set</span> mapreduce.job.reduces<span class="operator">=</span><span class="number">2</span>;</span><br><span class="line"><span class="keyword">No</span> <span class="keyword">rows</span> affected (<span class="number">0.005</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">set</span> mapreduce.job.reduces;</span><br><span class="line"><span class="operator">+</span><span class="comment">--------------------------+</span></span><br><span class="line"><span class="operator">|</span>           <span class="keyword">set</span>            <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">--------------------------+</span></span><br><span class="line"><span class="operator">|</span> mapreduce.job.reduces<span class="operator">=</span><span class="number">2</span>  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">--------------------------+</span></span><br><span class="line"><span class="number">1</span> <span class="type">row</span> selected (<span class="number">0.204</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span></span><br></pre></td></tr></table></figure></li>
<li>根据部门编号降序查看员工信息 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> mock_data.emp sort <span class="keyword">by</span> dept_no <span class="keyword">desc</span>;</span><br><span class="line"><span class="operator">+</span><span class="comment">-------------+---------------+------------+----------+----------------+----------+-----------+--------------+</span></span><br><span class="line"><span class="operator">|</span> emp.emp_no  <span class="operator">|</span> emp.emp_name  <span class="operator">|</span>  emp.job   <span class="operator">|</span> emp.mgr  <span class="operator">|</span> emp.hire_date  <span class="operator">|</span> emp.sal  <span class="operator">|</span> emp.comm  <span class="operator">|</span> emp.dept_no  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-------------+---------------+------------+----------+----------------+----------+-----------+--------------+</span></span><br><span class="line"><span class="operator">|</span> <span class="number">7521</span>        <span class="operator">|</span> WARD          <span class="operator">|</span> SALESMAN   <span class="operator">|</span> <span class="number">7698</span>     <span class="operator">|</span> <span class="number">1981</span><span class="number">-2</span><span class="number">-22</span>      <span class="operator">|</span> <span class="number">1250.0</span>   <span class="operator">|</span> <span class="number">500.0</span>     <span class="operator">|</span> <span class="number">30</span>           <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">7900</span>        <span class="operator">|</span> JAMES         <span class="operator">|</span> CLERK      <span class="operator">|</span> <span class="number">7698</span>     <span class="operator">|</span> <span class="number">1981</span><span class="number">-12</span><span class="number">-3</span>      <span class="operator">|</span> <span class="number">950.0</span>    <span class="operator">|</span> <span class="keyword">NULL</span>      <span class="operator">|</span> <span class="number">30</span>           <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">7844</span>        <span class="operator">|</span> TURNER        <span class="operator">|</span> SALESMAN   <span class="operator">|</span> <span class="number">7698</span>     <span class="operator">|</span> <span class="number">1981</span><span class="number">-9</span><span class="number">-8</span>       <span class="operator">|</span> <span class="number">1500.0</span>   <span class="operator">|</span> <span class="number">0.0</span>       <span class="operator">|</span> <span class="number">30</span>           <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">7654</span>        <span class="operator">|</span> MARTIN        <span class="operator">|</span> SALESMAN   <span class="operator">|</span> <span class="number">7698</span>     <span class="operator">|</span> <span class="number">1981</span><span class="number">-9</span><span class="number">-28</span>      <span class="operator">|</span> <span class="number">1250.0</span>   <span class="operator">|</span> <span class="number">1400.0</span>    <span class="operator">|</span> <span class="number">30</span>           <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">7876</span>        <span class="operator">|</span> ADAMS         <span class="operator">|</span> CLERK      <span class="operator">|</span> <span class="number">7788</span>     <span class="operator">|</span> <span class="number">1987</span><span class="number">-5</span><span class="number">-23</span>      <span class="operator">|</span> <span class="number">1100.0</span>   <span class="operator">|</span> <span class="keyword">NULL</span>      <span class="operator">|</span> <span class="number">20</span>           <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">7902</span>        <span class="operator">|</span> FORD          <span class="operator">|</span> ANALYST    <span class="operator">|</span> <span class="number">7566</span>     <span class="operator">|</span> <span class="number">1981</span><span class="number">-12</span><span class="number">-3</span>      <span class="operator">|</span> <span class="number">3000.0</span>   <span class="operator">|</span> <span class="keyword">NULL</span>      <span class="operator">|</span> <span class="number">20</span>           <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">7788</span>        <span class="operator">|</span> SCOTT         <span class="operator">|</span> ANALYST    <span class="operator">|</span> <span class="number">7566</span>     <span class="operator">|</span> <span class="number">1987</span><span class="number">-4</span><span class="number">-19</span>      <span class="operator">|</span> <span class="number">3000.0</span>   <span class="operator">|</span> <span class="keyword">NULL</span>      <span class="operator">|</span> <span class="number">20</span>           <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">7369</span>        <span class="operator">|</span> SMITH         <span class="operator">|</span> CLERK      <span class="operator">|</span> <span class="number">7902</span>     <span class="operator">|</span> <span class="number">1980</span><span class="number">-12</span><span class="number">-17</span>     <span class="operator">|</span> <span class="number">800.0</span>    <span class="operator">|</span> <span class="keyword">NULL</span>      <span class="operator">|</span> <span class="number">20</span>           <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">7566</span>        <span class="operator">|</span> JONES         <span class="operator">|</span> MANAGER    <span class="operator">|</span> <span class="number">7839</span>     <span class="operator">|</span> <span class="number">1981</span><span class="number">-4</span><span class="number">-2</span>       <span class="operator">|</span> <span class="number">2975.0</span>   <span class="operator">|</span> <span class="keyword">NULL</span>      <span class="operator">|</span> <span class="number">20</span>           <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">7499</span>        <span class="operator">|</span> ALLEN         <span class="operator">|</span> SALESMAN   <span class="operator">|</span> <span class="number">7698</span>     <span class="operator">|</span> <span class="number">1981</span><span class="number">-2</span><span class="number">-20</span>      <span class="operator">|</span> <span class="number">1600.0</span>   <span class="operator">|</span> <span class="number">300.0</span>     <span class="operator">|</span> <span class="number">30</span>           <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">7698</span>        <span class="operator">|</span> BLAKE         <span class="operator">|</span> MANAGER    <span class="operator">|</span> <span class="number">7839</span>     <span class="operator">|</span> <span class="number">1981</span><span class="number">-5</span><span class="number">-1</span>       <span class="operator">|</span> <span class="number">2850.0</span>   <span class="operator">|</span> <span class="keyword">NULL</span>      <span class="operator">|</span> <span class="number">30</span>           <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">7934</span>        <span class="operator">|</span> MILLER        <span class="operator">|</span> CLERK      <span class="operator">|</span> <span class="number">7782</span>     <span class="operator">|</span> <span class="number">1982</span><span class="number">-1</span><span class="number">-23</span>      <span class="operator">|</span> <span class="number">1300.0</span>   <span class="operator">|</span> <span class="keyword">NULL</span>      <span class="operator">|</span> <span class="number">10</span>           <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">7839</span>        <span class="operator">|</span> KING          <span class="operator">|</span> PRESIDENT  <span class="operator">|</span> <span class="keyword">NULL</span>     <span class="operator">|</span> <span class="number">1981</span><span class="number">-11</span><span class="number">-17</span>     <span class="operator">|</span> <span class="number">5000.0</span>   <span class="operator">|</span> <span class="keyword">NULL</span>      <span class="operator">|</span> <span class="number">10</span>           <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">7782</span>        <span class="operator">|</span> CLARK         <span class="operator">|</span> MANAGER    <span class="operator">|</span> <span class="number">7839</span>     <span class="operator">|</span> <span class="number">1981</span><span class="number">-6</span><span class="number">-9</span>       <span class="operator">|</span> <span class="number">2450.0</span>   <span class="operator">|</span> <span class="keyword">NULL</span>      <span class="operator">|</span> <span class="number">10</span>           <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-------------+---------------+------------+----------+----------------+----------+-----------+--------------+</span></span><br><span class="line"><span class="number">14</span> <span class="keyword">rows</span> selected (<span class="number">16.877</span> seconds)</span><br></pre></td></tr></table></figure></li>
<li>将查询结果导入到文件中（按照部门编号降序排序） <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">set</span> mapreduce.job.reduces<span class="operator">=</span><span class="number">2</span>;</span><br><span class="line"><span class="keyword">No</span> <span class="keyword">rows</span> affected (<span class="number">0.037</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">insert</span> overwrite <span class="keyword">local</span> directory <span class="string">&#x27;/home/atguigu/hive/emp&#x27;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> mock_data.emp sort <span class="keyword">by</span> dept_no <span class="keyword">desc</span>;</span><br><span class="line"><span class="keyword">No</span> <span class="keyword">rows</span> affected (<span class="number">17.73</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="operator">!</span>quit</span><br><span class="line">Closing: <span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span></span><br><span class="line">[atguigu<span class="variable">@hadoop001</span> emp]$ ls</span><br><span class="line"><span class="number">000000</span>_0  <span class="number">000001</span>_0</span><br><span class="line">[atguigu<span class="variable">@hadoop001</span> emp]$</span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="6-4-5-分区（Distribute-By）"><a href="#6-4-5-分区（Distribute-By）" class="headerlink" title="6.4.5 分区（Distribute By）"></a>6.4.5 分区（Distribute By）</h3><ul>
<li>Distribute By： 在有些情况下，我们需要控制某个特定行应该到哪个reducer，通常是为了进行后续的聚集操作。distribute by 子句可以做这件事。</li>
<li>Distribute by类似MR中partition（自定义分区），进行分区，结合sort by使用。 </li>
<li>对于distribute by进行测试，一定要分配多reduce进行处理，否则无法看到distribute by的效果。</li>
</ul>
<ol>
<li>案例实操：<ol>
<li>先按照部门编号分区，再按照员工编号降序排序。 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">set</span> mapreduce.job.reduces<span class="operator">=</span><span class="number">3</span>;</span><br><span class="line"><span class="keyword">No</span> <span class="keyword">rows</span> affected (<span class="number">0.035</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">insert</span> overwrite <span class="keyword">local</span> directory <span class="string">&#x27;/home/atguigu/hive/emp/distribute&#x27;</span> <span class="type">ROW</span> FORMAT DELIMITED FIELDS TERMINATED <span class="keyword">BY</span> <span class="string">&#x27;\t&#x27;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> mock_data.emp distribute <span class="keyword">by</span> dept_no sort <span class="keyword">by</span> emp_no <span class="keyword">desc</span> ;</span><br><span class="line"><span class="keyword">No</span> <span class="keyword">rows</span> affected (<span class="number">18.69</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="operator">!</span>quit</span><br><span class="line">Closing: <span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span></span><br><span class="line">[atguigu<span class="variable">@hadoop001</span> distribute]$ ls</span><br><span class="line"><span class="number">000000</span>_0  <span class="number">000001</span>_0  <span class="number">000002</span>_0</span><br><span class="line">[atguigu<span class="variable">@hadoop001</span> distribute]$ cat <span class="number">000000</span>_0</span><br><span class="line"><span class="number">7900</span>	JAMES	CLERK	<span class="number">7698</span>	<span class="number">1981</span><span class="number">-12</span><span class="number">-3</span>	<span class="number">950.0</span>	\N	<span class="number">30</span></span><br><span class="line"><span class="number">7844</span>	TURNER	SALESMAN	<span class="number">7698</span>	<span class="number">1981</span><span class="number">-9</span><span class="number">-8</span>	<span class="number">1500.0</span>	<span class="number">0.0</span>	<span class="number">30</span></span><br><span class="line"><span class="number">7698</span>	BLAKE	MANAGER	<span class="number">7839</span>	<span class="number">1981</span><span class="number">-5</span><span class="number">-1</span>	<span class="number">2850.0</span>	\N	<span class="number">30</span></span><br><span class="line"><span class="number">7654</span>	MARTIN	SALESMAN	<span class="number">7698</span>	<span class="number">1981</span><span class="number">-9</span><span class="number">-28</span>	<span class="number">1250.0</span>	<span class="number">1400.0</span>	<span class="number">30</span></span><br><span class="line"><span class="number">7521</span>	WARD	SALESMAN	<span class="number">7698</span>	<span class="number">1981</span><span class="number">-2</span><span class="number">-22</span>	<span class="number">1250.0</span>	<span class="number">500.0</span>	<span class="number">30</span></span><br><span class="line"><span class="number">7499</span>	ALLEN	SALESMAN	<span class="number">7698</span>	<span class="number">1981</span><span class="number">-2</span><span class="number">-20</span>	<span class="number">1600.0</span>	<span class="number">300.0</span>	<span class="number">30</span></span><br><span class="line">[atguigu<span class="variable">@hadoop001</span> distribute]$ cat <span class="number">000001</span>_0</span><br><span class="line"><span class="number">7934</span>	MILLER	CLERK	<span class="number">7782</span>	<span class="number">1982</span><span class="number">-1</span><span class="number">-23</span>	<span class="number">1300.0</span>	\N	<span class="number">10</span></span><br><span class="line"><span class="number">7839</span>	KING	PRESIDENT	\N	<span class="number">1981</span><span class="number">-11</span><span class="number">-17</span>	<span class="number">5000.0</span>	\N	<span class="number">10</span></span><br><span class="line"><span class="number">7782</span>	CLARK	MANAGER	<span class="number">7839</span>	<span class="number">1981</span><span class="number">-6</span><span class="number">-9</span>	<span class="number">2450.0</span>	\N	<span class="number">10</span></span><br><span class="line">[atguigu<span class="variable">@hadoop001</span> distribute]$ cat <span class="number">000002</span>_0</span><br><span class="line"><span class="number">7902</span>	FORD	ANALYST	<span class="number">7566</span>	<span class="number">1981</span><span class="number">-12</span><span class="number">-3</span>	<span class="number">3000.0</span>	\N	<span class="number">20</span></span><br><span class="line"><span class="number">7876</span>	ADAMS	CLERK	<span class="number">7788</span>	<span class="number">1987</span><span class="number">-5</span><span class="number">-23</span>	<span class="number">1100.0</span>	\N	<span class="number">20</span></span><br><span class="line"><span class="number">7788</span>	SCOTT	ANALYST	<span class="number">7566</span>	<span class="number">1987</span><span class="number">-4</span><span class="number">-19</span>	<span class="number">3000.0</span>	\N	<span class="number">20</span></span><br><span class="line"><span class="number">7566</span>	JONES	MANAGER	<span class="number">7839</span>	<span class="number">1981</span><span class="number">-4</span><span class="number">-2</span>	<span class="number">2975.0</span>	\N	<span class="number">20</span></span><br><span class="line"><span class="number">7369</span>	SMITH	CLERK	<span class="number">7902</span>	<span class="number">1980</span><span class="number">-12</span><span class="number">-17</span>	<span class="number">800.0</span>	\N	<span class="number">20</span></span><br><span class="line">[atguigu<span class="variable">@hadoop001</span> distribute]$</span><br></pre></td></tr></table></figure></li>
</ol>
</li>
<li>注意：<ul>
<li>distribute by的分区规则是根据分区字段的hash码与reduce的个数进行模除后，余数相同的分到一个区。</li>
<li>Hive要求DISTRIBUTE BY语句要写在SORT BY语句之前。</li>
</ul>
</li>
</ol>
<h3 id="6-4-6-Cluster-By"><a href="#6-4-6-Cluster-By" class="headerlink" title="6.4.6 Cluster By"></a>6.4.6 Cluster By</h3><ul>
<li>当distribute by和sort by字段相同时，可以使用cluster by方式。</li>
<li>cluster by除了具有distribute by的功能外还兼具sort by的功能。但是排序只能是升序排序，不能指定排序规则为ASC或者DESC。</li>
<li>以下两种写法等价  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> emp cluster <span class="keyword">by</span> deptno;</span><br><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> emp distribute <span class="keyword">by</span> deptno sort <span class="keyword">by</span> deptno;</span><br></pre></td></tr></table></figure></li>
<li>注意：按照部门编号分区，不一定就是固定死的数值，可以是20号和30号部门分到一个分区里面去。</li>
</ul>
<h1 id="七、分区表和分桶表"><a href="#七、分区表和分桶表" class="headerlink" title="七、分区表和分桶表"></a>七、分区表和分桶表</h1><h2 id="7-1-分区表"><a href="#7-1-分区表" class="headerlink" title="7.1 分区表"></a>7.1 分区表</h2><ul>
<li>概念：<ol>
<li>出于对数据查询的优化，考虑Hive常规查询会进行全表扫描，效率低下。在建表的时候使用分区表规划更合理的存储结构</li>
<li>分区表实际上就是对应一个HDFS文件系统上的独立的文件夹，该文件夹下是该分区所有的数据文件。</li>
<li>Hive中的分区就是分目录，把一个大的数据集根据业务需要分割成小的数据集。在查询时通过WHERE子句中的表达式选择查询所需要的指定的分区，这样的查询效率会提高很多。</li>
</ol>
</li>
</ul>
<h3 id="7-1-1-分区表基本操作"><a href="#7-1-1-分区表基本操作" class="headerlink" title="7.1.1 分区表基本操作"></a>7.1.1 分区表基本操作</h3><ol>
<li><p>创建分区表语法</p>
 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> dept_partition (</span><br><span class="line">    deptno <span class="type">int</span>,</span><br><span class="line">    dname  string,</span><br><span class="line">    loc    string</span><br><span class="line">)</span><br><span class="line">partitioned <span class="keyword">by</span> (<span class="keyword">day</span> string)</span><br><span class="line"><span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span>;</span><br></pre></td></tr></table></figure>
<ul>
<li>注意：分区字段不能是表中已经存在的数据，可以将分区字段看作表的伪列。</li>
</ul>
</li>
<li><p>加载数据到分区表中</p>
<ul>
<li>数据准备  <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">dept_20200401.log</span><br><span class="line">10	ACCOUNTING	1700</span><br><span class="line">20	RESEARCH	1800</span><br><span class="line"></span><br><span class="line">dept_20200402.log</span><br><span class="line">30	SALES	1900</span><br><span class="line">40	OPERATIONS	1700</span><br><span class="line"></span><br><span class="line">dept_20200403.log</span><br><span class="line">50	TEST	2000</span><br><span class="line">60	DEV	1900</span><br></pre></td></tr></table></figure></li>
<li>加载数据  <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">0: jdbc:hive2://hadoop001:10000&gt; load data local inpath &#x27;/home/atguigu/hive/emp/partitioned/dept_20200401.log&#x27; into table mock_data.dept_partition partition(day=&#x27;20200401&#x27;);</span><br><span class="line">No rows affected (1.197 seconds)</span><br><span class="line">0: jdbc:hive2://hadoop001:10000&gt; load data local inpath &#x27;/home/atguigu/hive/emp/partitioned/dept_20200402.log&#x27; into table mock_data.dept_partition partition(day=&#x27;20200402&#x27;);</span><br><span class="line">No rows affected (0.363 seconds)</span><br><span class="line">0: jdbc:hive2://hadoop001:10000&gt; load data local inpath &#x27;/home/atguigu/hive/emp/partitioned/dept_20200403.log&#x27; into table mock_data.dept_partition partition(day=&#x27;20200403&#x27;);</span><br><span class="line">No rows affected (0.307 seconds)</span><br><span class="line">0: jdbc:hive2://hadoop001:10000&gt; dfs -ls /user/hive/warehouse/mock_data.db/dept_partition;</span><br><span class="line">+----------------------------------------------------+</span><br><span class="line">|                     DFS Output                     |</span><br><span class="line">+----------------------------------------------------+</span><br><span class="line">| Found 3 items                                      |</span><br><span class="line">| drwxr-xr-x   - atguigu supergroup          0 2021-10-26 14:18 /user/hive/warehouse/mock_data.db/dept_partition/day=20200401 |</span><br><span class="line">| drwxr-xr-x   - atguigu supergroup          0 2021-10-26 14:18 /user/hive/warehouse/mock_data.db/dept_partition/day=20200402 |</span><br><span class="line">| drwxr-xr-x   - atguigu supergroup          0 2021-10-26 14:18 /user/hive/warehouse/mock_data.db/dept_partition/day=20200403 |</span><br><span class="line">+----------------------------------------------------+</span><br><span class="line">4 rows selected (0.01 seconds)</span><br><span class="line">0: jdbc:hive2://hadoop001:10000&gt;</span><br></pre></td></tr></table></figure></li>
<li>注意：分区表加载数据时，必须指定分区</li>
</ul>
</li>
<li><p>查询分区表中数据</p>
<ul>
<li>单分区查询  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> dept_partition <span class="keyword">where</span> <span class="keyword">day</span><span class="operator">=</span><span class="string">&#x27;20200401&#x27;</span>;</span><br></pre></td></tr></table></figure></li>
<li>多分区联合查询  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> dept_partition <span class="keyword">where</span> <span class="keyword">day</span><span class="operator">=</span><span class="string">&#x27;20200401&#x27;</span></span><br><span class="line"><span class="keyword">union</span> <span class="keyword">all</span></span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> dept_partition <span class="keyword">where</span> <span class="keyword">day</span><span class="operator">=</span><span class="string">&#x27;20200402&#x27;</span></span><br><span class="line"><span class="keyword">union</span> <span class="keyword">all</span></span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> dept_partition <span class="keyword">where</span> <span class="keyword">day</span><span class="operator">=</span><span class="string">&#x27;20200403&#x27;</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> dept_partition <span class="keyword">where</span> <span class="keyword">day</span> <span class="keyword">in</span> (<span class="string">&#x27;20200401&#x27;</span>,<span class="string">&#x27;20200402&#x27;</span>,<span class="string">&#x27;20200403&#x27;</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> dept_partition <span class="keyword">where</span> <span class="keyword">day</span> <span class="keyword">between</span> <span class="number">20200401</span> <span class="keyword">and</span> <span class="number">20200403</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> dept_partition <span class="keyword">where</span> (<span class="keyword">day</span> <span class="operator">=</span> <span class="string">&#x27;20200401&#x27;</span> <span class="keyword">or</span> <span class="keyword">day</span> <span class="operator">=</span><span class="string">&#x27;20200402&#x27;</span> <span class="keyword">or</span> <span class="keyword">day</span> <span class="operator">=</span> <span class="string">&#x27;20200403&#x27;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>增加分区</p>
<ul>
<li>创建单个分区  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">alter</span> <span class="keyword">table</span> mock_data.dept_partition <span class="keyword">add</span> <span class="keyword">partition</span>(<span class="keyword">day</span><span class="operator">=</span><span class="string">&#x27;20200404&#x27;</span>) ;</span><br><span class="line"><span class="keyword">No</span> <span class="keyword">rows</span> affected (<span class="number">0.099</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">show</span> partitions mock_data.dept_partition;</span><br><span class="line"><span class="operator">+</span><span class="comment">---------------+</span></span><br><span class="line"><span class="operator">|</span>   <span class="keyword">partition</span>   <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">---------------+</span></span><br><span class="line"><span class="operator">|</span> <span class="keyword">day</span><span class="operator">=</span><span class="number">20200401</span>  <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="keyword">day</span><span class="operator">=</span><span class="number">20200402</span>  <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="keyword">day</span><span class="operator">=</span><span class="number">20200403</span>  <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="keyword">day</span><span class="operator">=</span><span class="number">20200404</span>  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">---------------+</span></span><br><span class="line"><span class="number">4</span> <span class="keyword">rows</span> selected (<span class="number">0.075</span> seconds)</span><br></pre></td></tr></table></figure></li>
<li>同时创建多个分区  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">alter</span> <span class="keyword">table</span> mock_data.dept_partition <span class="keyword">add</span> <span class="keyword">partition</span>(<span class="keyword">day</span><span class="operator">=</span><span class="string">&#x27;20200405&#x27;</span>) <span class="keyword">partition</span>(<span class="keyword">day</span><span class="operator">=</span><span class="string">&#x27;20200406&#x27;</span>);</span><br><span class="line"><span class="keyword">No</span> <span class="keyword">rows</span> affected (<span class="number">0.135</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">show</span> partitions mock_data.dept_partition;</span><br><span class="line"><span class="operator">+</span><span class="comment">---------------+</span></span><br><span class="line"><span class="operator">|</span>   <span class="keyword">partition</span>   <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">---------------+</span></span><br><span class="line"><span class="operator">|</span> <span class="keyword">day</span><span class="operator">=</span><span class="number">20200401</span>  <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="keyword">day</span><span class="operator">=</span><span class="number">20200402</span>  <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="keyword">day</span><span class="operator">=</span><span class="number">20200403</span>  <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="keyword">day</span><span class="operator">=</span><span class="number">20200404</span>  <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="keyword">day</span><span class="operator">=</span><span class="number">20200405</span>  <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="keyword">day</span><span class="operator">=</span><span class="number">20200406</span>  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">---------------+</span></span><br><span class="line"><span class="number">6</span> <span class="keyword">rows</span> selected (<span class="number">0.094</span> seconds)</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>删除分区</p>
<ul>
<li>删除单个分区  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">alter</span> <span class="keyword">table</span> mock_data.dept_partition <span class="keyword">drop</span> <span class="keyword">partition</span>(<span class="keyword">day</span><span class="operator">=</span><span class="string">&#x27;20200404&#x27;</span>) ;</span><br><span class="line"><span class="keyword">No</span> <span class="keyword">rows</span> affected (<span class="number">0.154</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">show</span> partitions mock_data.dept_partition;</span><br><span class="line"><span class="operator">+</span><span class="comment">---------------+</span></span><br><span class="line"><span class="operator">|</span>   <span class="keyword">partition</span>   <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">---------------+</span></span><br><span class="line"><span class="operator">|</span> <span class="keyword">day</span><span class="operator">=</span><span class="number">20200401</span>  <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="keyword">day</span><span class="operator">=</span><span class="number">20200402</span>  <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="keyword">day</span><span class="operator">=</span><span class="number">20200403</span>  <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="keyword">day</span><span class="operator">=</span><span class="number">20200405</span>  <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="keyword">day</span><span class="operator">=</span><span class="number">20200406</span>  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">---------------+</span></span><br><span class="line"><span class="number">5</span> <span class="keyword">rows</span> selected (<span class="number">0.065</span> seconds)</span><br></pre></td></tr></table></figure></li>
<li>同时删除多个分区  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">alter</span> <span class="keyword">table</span> mock_data.dept_partition <span class="keyword">drop</span> <span class="keyword">partition</span>(<span class="keyword">day</span><span class="operator">=</span><span class="string">&#x27;20200405&#x27;</span>), <span class="keyword">partition</span>(<span class="keyword">day</span><span class="operator">=</span><span class="string">&#x27;20200406&#x27;</span>);</span><br><span class="line"><span class="keyword">No</span> <span class="keyword">rows</span> affected (<span class="number">0.223</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">show</span> partitions mock_data.dept_partition;</span><br><span class="line"><span class="operator">+</span><span class="comment">---------------+</span></span><br><span class="line"><span class="operator">|</span>   <span class="keyword">partition</span>   <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">---------------+</span></span><br><span class="line"><span class="operator">|</span> <span class="keyword">day</span><span class="operator">=</span><span class="number">20200401</span>  <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="keyword">day</span><span class="operator">=</span><span class="number">20200402</span>  <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="keyword">day</span><span class="operator">=</span><span class="number">20200403</span>  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">---------------+</span></span><br><span class="line"><span class="number">3</span> <span class="keyword">rows</span> selected (<span class="number">0.068</span> seconds)</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>查看分区表有多少分区</p>
 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">show</span> partitions mock_data.dept_partition;</span><br><span class="line"><span class="operator">+</span><span class="comment">---------------+</span></span><br><span class="line"><span class="operator">|</span>   <span class="keyword">partition</span>   <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">---------------+</span></span><br><span class="line"><span class="operator">|</span> <span class="keyword">day</span><span class="operator">=</span><span class="number">20200401</span>  <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="keyword">day</span><span class="operator">=</span><span class="number">20200402</span>  <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="keyword">day</span><span class="operator">=</span><span class="number">20200403</span>  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">---------------+</span></span><br><span class="line"><span class="number">3</span> <span class="keyword">rows</span> selected (<span class="number">0.068</span> seconds)</span><br></pre></td></tr></table></figure></li>
<li><p>查看分区表结构(关注Partition Information.numPartitions)</p>
 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">desc</span> formatted mock_data.dept_partition;</span><br><span class="line"><span class="operator">+</span><span class="comment">-------------------------------+----------------------------------------------------+-----------------------+</span></span><br><span class="line"><span class="operator">|</span>           col_name            <span class="operator">|</span>                     data_type                      <span class="operator">|</span>        comment        <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-------------------------------+----------------------------------------------------+-----------------------+</span></span><br><span class="line"><span class="operator">|</span> # col_name                    <span class="operator">|</span> data_type                                          <span class="operator">|</span> comment               <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> deptno                        <span class="operator">|</span> <span class="type">int</span>                                                <span class="operator">|</span>                       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> dname                         <span class="operator">|</span> string                                             <span class="operator">|</span>                       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> loc                           <span class="operator">|</span> string                                             <span class="operator">|</span>                       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>                               <span class="operator">|</span> <span class="keyword">NULL</span>                                               <span class="operator">|</span> <span class="keyword">NULL</span>                  <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> # <span class="keyword">Partition</span> Information       <span class="operator">|</span> <span class="keyword">NULL</span>                                               <span class="operator">|</span> <span class="keyword">NULL</span>                  <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> # col_name                    <span class="operator">|</span> data_type                                          <span class="operator">|</span> comment               <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="keyword">day</span>                           <span class="operator">|</span> string                                             <span class="operator">|</span>                       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>                               <span class="operator">|</span> <span class="keyword">NULL</span>                                               <span class="operator">|</span> <span class="keyword">NULL</span>                  <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> # Detailed <span class="keyword">Table</span> Information  <span class="operator">|</span> <span class="keyword">NULL</span>                                               <span class="operator">|</span> <span class="keyword">NULL</span>                  <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> Database:                     <span class="operator">|</span> mock_data                                          <span class="operator">|</span> <span class="keyword">NULL</span>                  <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> OwnerType:                    <span class="operator">|</span> <span class="keyword">USER</span>                                               <span class="operator">|</span> <span class="keyword">NULL</span>                  <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> Owner:                        <span class="operator">|</span> atguigu                                            <span class="operator">|</span> <span class="keyword">NULL</span>                  <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> CreateTime:                   <span class="operator">|</span> Tue Oct <span class="number">26</span> <span class="number">14</span>:<span class="number">10</span>:<span class="number">53</span> CST <span class="number">2021</span>                       <span class="operator">|</span> <span class="keyword">NULL</span>                  <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> LastAccessTime:               <span class="operator">|</span> <span class="literal">UNKNOWN</span>                                            <span class="operator">|</span> <span class="keyword">NULL</span>                  <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> Retention:                    <span class="operator">|</span> <span class="number">0</span>                                                  <span class="operator">|</span> <span class="keyword">NULL</span>                  <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> Location:                     <span class="operator">|</span> hdfs:<span class="operator">/</span><span class="operator">/</span>mycluster<span class="operator">/</span><span class="keyword">user</span><span class="operator">/</span>hive<span class="operator">/</span>warehouse<span class="operator">/</span>mock_data.db<span class="operator">/</span>dept_partition <span class="operator">|</span> <span class="keyword">NULL</span>                  <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="keyword">Table</span> Type:                   <span class="operator">|</span> MANAGED_TABLE                                      <span class="operator">|</span> <span class="keyword">NULL</span>                  <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="keyword">Table</span> Parameters:             <span class="operator">|</span> <span class="keyword">NULL</span>                                               <span class="operator">|</span> <span class="keyword">NULL</span>                  <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>                               <span class="operator">|</span> bucketing_version                                  <span class="operator">|</span> <span class="number">2</span>                     <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>                               <span class="operator">|</span> numFiles                                           <span class="operator">|</span> <span class="number">3</span>                     <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>                               <span class="operator">|</span> numPartitions                                      <span class="operator">|</span> <span class="number">3</span>                     <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>                               <span class="operator">|</span> numRows                                            <span class="operator">|</span> <span class="number">0</span>                     <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>                               <span class="operator">|</span> rawDataSize                                        <span class="operator">|</span> <span class="number">0</span>                     <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>                               <span class="operator">|</span> totalSize                                          <span class="operator">|</span> <span class="number">95</span>                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>                               <span class="operator">|</span> transient_lastDdlTime                              <span class="operator">|</span> <span class="number">1635228653</span>            <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>                               <span class="operator">|</span> <span class="keyword">NULL</span>                                               <span class="operator">|</span> <span class="keyword">NULL</span>                  <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> # Storage Information         <span class="operator">|</span> <span class="keyword">NULL</span>                                               <span class="operator">|</span> <span class="keyword">NULL</span>                  <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> SerDe Library:                <span class="operator">|</span> org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe <span class="operator">|</span> <span class="keyword">NULL</span>                  <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> InputFormat:                  <span class="operator">|</span> org.apache.hadoop.mapred.TextInputFormat           <span class="operator">|</span> <span class="keyword">NULL</span>                  <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> OutputFormat:                 <span class="operator">|</span> org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat <span class="operator">|</span> <span class="keyword">NULL</span>                  <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> Compressed:                   <span class="operator">|</span> <span class="keyword">No</span>                                                 <span class="operator">|</span> <span class="keyword">NULL</span>                  <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> Num Buckets:                  <span class="operator">|</span> <span class="number">-1</span>                                                 <span class="operator">|</span> <span class="keyword">NULL</span>                  <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> Bucket Columns:               <span class="operator">|</span> []                                                 <span class="operator">|</span> <span class="keyword">NULL</span>                  <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> Sort Columns:                 <span class="operator">|</span> []                                                 <span class="operator">|</span> <span class="keyword">NULL</span>                  <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> Storage <span class="keyword">Desc</span> Params:          <span class="operator">|</span> <span class="keyword">NULL</span>                                               <span class="operator">|</span> <span class="keyword">NULL</span>                  <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>                               <span class="operator">|</span> field.delim                                        <span class="operator">|</span> \t                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>                               <span class="operator">|</span> serialization.format                               <span class="operator">|</span> \t                    <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-------------------------------+----------------------------------------------------+-----------------------+</span></span><br><span class="line"><span class="number">38</span> <span class="keyword">rows</span> selected (<span class="number">0.151</span> seconds)</span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="7-1-2-二级分区"><a href="#7-1-2-二级分区" class="headerlink" title="7.1.2 二级分区"></a>7.1.2 二级分区</h3><p>思考: 如何一天的日志数据量也很大，如何再将数据拆分?</p>
<ol>
<li>创建多级级分区表 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> dept_partition_level (</span><br><span class="line">    deptno <span class="type">int</span>,</span><br><span class="line">    dname  string,</span><br><span class="line">    loc    string</span><br><span class="line">)</span><br><span class="line">partitioned <span class="keyword">by</span> (<span class="keyword">year</span> string,<span class="keyword">month</span> string, <span class="keyword">day</span> string)</span><br><span class="line"><span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span>;</span><br></pre></td></tr></table></figure></li>
<li>正常的加载数据<ul>
<li>加载数据到分区表中  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> load data <span class="keyword">local</span> inpath <span class="string">&#x27;/home/atguigu/hive/emp/partitioned/dept_20200401.log&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span> mock_data.dept_partition_level <span class="keyword">partition</span>(<span class="keyword">year</span><span class="operator">=</span><span class="string">&#x27;2020&#x27;</span>, <span class="keyword">month</span><span class="operator">=</span><span class="string">&#x27;04&#x27;</span>,<span class="keyword">day</span><span class="operator">=</span><span class="string">&#x27;01&#x27;</span>);</span><br><span class="line"><span class="keyword">No</span> <span class="keyword">rows</span> affected (<span class="number">0.292</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> load data <span class="keyword">local</span> inpath <span class="string">&#x27;/home/atguigu/hive/emp/partitioned/dept_20200402.log&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span> mock_data.dept_partition_level <span class="keyword">partition</span>(<span class="keyword">year</span><span class="operator">=</span><span class="string">&#x27;2020&#x27;</span>, <span class="keyword">month</span><span class="operator">=</span><span class="string">&#x27;04&#x27;</span>,<span class="keyword">day</span><span class="operator">=</span><span class="string">&#x27;02&#x27;</span>);</span><br><span class="line"><span class="keyword">No</span> <span class="keyword">rows</span> affected (<span class="number">0.295</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> load data <span class="keyword">local</span> inpath <span class="string">&#x27;/home/atguigu/hive/emp/partitioned/dept_20200403.log&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span> mock_data.dept_partition_level <span class="keyword">partition</span>(<span class="keyword">year</span><span class="operator">=</span><span class="string">&#x27;2020&#x27;</span>, <span class="keyword">month</span><span class="operator">=</span><span class="string">&#x27;04&#x27;</span>,<span class="keyword">day</span><span class="operator">=</span><span class="string">&#x27;03&#x27;</span>);</span><br><span class="line"><span class="keyword">No</span> <span class="keyword">rows</span> affected (<span class="number">0.279</span> seconds)</span><br></pre></td></tr></table></figure></li>
<li>查询分区数据  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> </span><br><span class="line">    <span class="operator">*</span></span><br><span class="line"><span class="keyword">from</span> dept_partition_level</span><br><span class="line"><span class="keyword">where</span> <span class="keyword">year</span> <span class="operator">=</span> <span class="number">2020</span></span><br><span class="line">  <span class="keyword">and</span> <span class="keyword">month</span> <span class="operator">=</span> <span class="number">04</span></span><br><span class="line">  <span class="keyword">and</span> <span class="keyword">day</span> <span class="operator">=</span> <span class="number">01</span></span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>把数据直接上传到分区目录上，让分区表和数据产生关联的三种方式<ul>
<li>方式一：上传数据后修复<ul>
<li>上传数据  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 partitioned]$ hadoop fs -mkdir -p /user/hive/warehouse/mock_data.db/dept_partition_level/year=2021/month=06/day=09</span><br><span class="line">[atguigu@hadoop001 partitioned]$ hadoop fs -put dept_20210609.log /user/hive/warehouse/mock_data.db/dept_partition_level/year=2021/month=06/day=09</span><br><span class="line">2021-10-26 15:36:57,908 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = <span class="literal">false</span>, remoteHostTrusted = <span class="literal">false</span></span><br><span class="line">[atguigu@hadoop001 partitioned]$</span><br></pre></td></tr></table></figure></li>
<li>查询数据（查询不到刚上传的数据）  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">0: jdbc:hive2://hadoop001:10000&gt; select *</span><br><span class="line"> from mock_data.dept_partition_level</span><br><span class="line"> <span class="built_in">where</span> year = 2021;</span><br><span class="line">+------------------------------+-----------------------------+---------------------------+----------------------------+-----------------------------+---------------------------+</span><br><span class="line">| dept_partition_level.deptno  | dept_partition_level.dname  | dept_partition_level.loc  | dept_partition_level.year  | dept_partition_level.month  | dept_partition_level.day  |</span><br><span class="line">+------------------------------+-----------------------------+---------------------------+----------------------------+-----------------------------+---------------------------+</span><br><span class="line">| 60                           | ACCOUNTING                  | 1700                      | 2021                       | 04                          | 01                        |</span><br><span class="line">+------------------------------+-----------------------------+---------------------------+----------------------------+-----------------------------+---------------------------+</span><br><span class="line">1 row selected (0.118 seconds)</span><br><span class="line">0: jdbc:hive2://hadoop001:10000&gt;</span><br></pre></td></tr></table></figure></li>
<li>执行修复命令再次查询数据  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> msck repair <span class="keyword">table</span> mock_data.dept_partition_level;</span><br><span class="line"><span class="keyword">No</span> <span class="keyword">rows</span> affected (<span class="number">0.139</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span></span><br><span class="line"> <span class="keyword">from</span> mock_data.dept_partition_level</span><br><span class="line"> <span class="keyword">where</span> <span class="keyword">year</span> <span class="operator">=</span> <span class="number">2021</span>;</span><br><span class="line"><span class="operator">+</span><span class="comment">------------------------------+-----------------------------+---------------------------+----------------------------+-----------------------------+---------------------------+</span></span><br><span class="line"><span class="operator">|</span> dept_partition_level.deptno  <span class="operator">|</span> dept_partition_level.dname  <span class="operator">|</span> dept_partition_level.loc  <span class="operator">|</span> dept_partition_level.year  <span class="operator">|</span> dept_partition_level.month  <span class="operator">|</span> dept_partition_level.day  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">------------------------------+-----------------------------+---------------------------+----------------------------+-----------------------------+---------------------------+</span></span><br><span class="line"><span class="operator">|</span> <span class="number">60</span>                           <span class="operator">|</span> ACCOUNTING                  <span class="operator">|</span> <span class="number">1700</span>                      <span class="operator">|</span> <span class="number">2021</span>                       <span class="operator">|</span> <span class="number">04</span>                          <span class="operator">|</span> <span class="number">01</span>                        <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">10</span>                           <span class="operator">|</span> ACCOUNTING                  <span class="operator">|</span> <span class="number">1700</span>                      <span class="operator">|</span> <span class="number">2021</span>                       <span class="operator">|</span> <span class="number">06</span>                          <span class="operator">|</span> <span class="number">09</span>                        <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">20</span>                           <span class="operator">|</span> RESEARCH                    <span class="operator">|</span> <span class="number">1800</span>                      <span class="operator">|</span> <span class="number">2021</span>                       <span class="operator">|</span> <span class="number">06</span>                          <span class="operator">|</span> <span class="number">09</span>                        <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">------------------------------+-----------------------------+---------------------------+----------------------------+-----------------------------+---------------------------+</span></span><br><span class="line"><span class="number">3</span> <span class="keyword">rows</span> selected (<span class="number">0.114</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span></span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>方式二：上传数据后添加分区<ul>
<li>上传数据  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 partitioned]$ hadoop fs -mkdir -p /user/hive/warehouse/mock_data.db/dept_partition_level/year=2021/month=10/day=26</span><br><span class="line">[atguigu@hadoop001 partitioned]$ hadoop fs -put dept_20211026.log /user/hive/warehouse/mock_data.db/dept_partition_level/year=2021/month=10/day=26</span><br><span class="line">2021-10-26 15:51:40,926 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = <span class="literal">false</span>, remoteHostTrusted = <span class="literal">false</span></span><br></pre></td></tr></table></figure></li>
<li>执行添加分区  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">alter</span> <span class="keyword">table</span> mock_data.dept_partition_level <span class="keyword">add</span> <span class="keyword">partition</span>(<span class="keyword">year</span><span class="operator">=</span><span class="string">&#x27;2021&#x27;</span>, <span class="keyword">month</span><span class="operator">=</span><span class="string">&#x27;10&#x27;</span>, <span class="keyword">day</span><span class="operator">=</span><span class="string">&#x27;26&#x27;</span>) ;</span><br><span class="line"><span class="keyword">No</span> <span class="keyword">rows</span> affected (<span class="number">0.062</span> seconds)</span><br></pre></td></tr></table></figure></li>
<li>查询数据  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span></span><br><span class="line"> <span class="keyword">from</span> mock_data.dept_partition_level</span><br><span class="line"> <span class="keyword">where</span> <span class="keyword">year</span> <span class="operator">=</span> <span class="number">2021</span>;</span><br><span class="line"><span class="operator">+</span><span class="comment">------------------------------+-----------------------------+---------------------------+----------------------------+-----------------------------+---------------------------+</span></span><br><span class="line"><span class="operator">|</span> dept_partition_level.deptno  <span class="operator">|</span> dept_partition_level.dname  <span class="operator">|</span> dept_partition_level.loc  <span class="operator">|</span> dept_partition_level.year  <span class="operator">|</span> dept_partition_level.month  <span class="operator">|</span> dept_partition_level.day  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">------------------------------+-----------------------------+---------------------------+----------------------------+-----------------------------+---------------------------+</span></span><br><span class="line"><span class="operator">|</span> <span class="number">60</span>                           <span class="operator">|</span> ACCOUNTING                  <span class="operator">|</span> <span class="number">1700</span>                      <span class="operator">|</span> <span class="number">2021</span>                       <span class="operator">|</span> <span class="number">04</span>                          <span class="operator">|</span> <span class="number">01</span>                        <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">10</span>                           <span class="operator">|</span> ACCOUNTING                  <span class="operator">|</span> <span class="number">1700</span>                      <span class="operator">|</span> <span class="number">2021</span>                       <span class="operator">|</span> <span class="number">06</span>                          <span class="operator">|</span> <span class="number">09</span>                        <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">20</span>                           <span class="operator">|</span> RESEARCH                    <span class="operator">|</span> <span class="number">1800</span>                      <span class="operator">|</span> <span class="number">2021</span>                       <span class="operator">|</span> <span class="number">06</span>                          <span class="operator">|</span> <span class="number">09</span>                        <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">10</span>                           <span class="operator">|</span> ACCOUNTING                  <span class="operator">|</span> <span class="number">1700</span>                      <span class="operator">|</span> <span class="number">2021</span>                       <span class="operator">|</span> <span class="number">10</span>                          <span class="operator">|</span> <span class="number">26</span>                        <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">20</span>                           <span class="operator">|</span> RESEARCH                    <span class="operator">|</span> <span class="number">1800</span>                      <span class="operator">|</span> <span class="number">2021</span>                       <span class="operator">|</span> <span class="number">10</span>                          <span class="operator">|</span> <span class="number">26</span>                        <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">------------------------------+-----------------------------+---------------------------+----------------------------+-----------------------------+---------------------------+</span></span><br><span class="line"><span class="number">5</span> <span class="keyword">rows</span> selected (<span class="number">0.094</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span></span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>方式三：创建文件夹后load数据到分区<ul>
<li>创建目录  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> dfs <span class="operator">-</span>mkdir <span class="operator">-</span>p <span class="operator">/</span><span class="keyword">user</span><span class="operator">/</span>hive<span class="operator">/</span>warehouse<span class="operator">/</span>mock_data.db<span class="operator">/</span>dept_partition_level<span class="operator">/</span><span class="keyword">year</span><span class="operator">=</span><span class="number">2021</span><span class="operator">/</span><span class="keyword">month</span><span class="operator">=</span><span class="number">12</span><span class="operator">/</span><span class="keyword">day</span><span class="operator">=</span><span class="number">28</span>;</span><br><span class="line"><span class="operator">+</span><span class="comment">-------------+</span></span><br><span class="line"><span class="operator">|</span> DFS Output  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-------------+</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-------------+</span></span><br><span class="line"><span class="keyword">No</span> <span class="keyword">rows</span> selected (<span class="number">0.017</span> seconds)</span><br></pre></td></tr></table></figure></li>
<li>上传数据  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> load data <span class="keyword">local</span> inpath <span class="string">&#x27;/home/atguigu/hive/emp/partitioned/dept_20210609.log&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span> mock_data.dept_partition_level <span class="keyword">partition</span>(<span class="keyword">year</span><span class="operator">=</span><span class="string">&#x27;2021&#x27;</span>, <span class="keyword">month</span><span class="operator">=</span><span class="string">&#x27;12&#x27;</span>,<span class="keyword">day</span><span class="operator">=</span><span class="string">&#x27;28&#x27;</span>);</span><br><span class="line"><span class="keyword">No</span> <span class="keyword">rows</span> affected (<span class="number">0.275</span> seconds)</span><br></pre></td></tr></table></figure></li>
<li>查询数据  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span></span><br><span class="line"> <span class="keyword">from</span> mock_data.dept_partition_level</span><br><span class="line"> <span class="keyword">where</span> <span class="keyword">year</span> <span class="operator">=</span> <span class="number">2021</span>;</span><br><span class="line"><span class="operator">+</span><span class="comment">------------------------------+-----------------------------+---------------------------+----------------------------+-----------------------------+---------------------------+</span></span><br><span class="line"><span class="operator">|</span> dept_partition_level.deptno  <span class="operator">|</span> dept_partition_level.dname  <span class="operator">|</span> dept_partition_level.loc  <span class="operator">|</span> dept_partition_level.year  <span class="operator">|</span> dept_partition_level.month  <span class="operator">|</span> dept_partition_level.day  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">------------------------------+-----------------------------+---------------------------+----------------------------+-----------------------------+---------------------------+</span></span><br><span class="line"><span class="operator">|</span> <span class="number">60</span>                           <span class="operator">|</span> ACCOUNTING                  <span class="operator">|</span> <span class="number">1700</span>                      <span class="operator">|</span> <span class="number">2021</span>                       <span class="operator">|</span> <span class="number">04</span>                          <span class="operator">|</span> <span class="number">01</span>                        <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">10</span>                           <span class="operator">|</span> ACCOUNTING                  <span class="operator">|</span> <span class="number">1700</span>                      <span class="operator">|</span> <span class="number">2021</span>                       <span class="operator">|</span> <span class="number">06</span>                          <span class="operator">|</span> <span class="number">09</span>                        <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">20</span>                           <span class="operator">|</span> RESEARCH                    <span class="operator">|</span> <span class="number">1800</span>                      <span class="operator">|</span> <span class="number">2021</span>                       <span class="operator">|</span> <span class="number">06</span>                          <span class="operator">|</span> <span class="number">09</span>                        <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">10</span>                           <span class="operator">|</span> ACCOUNTING                  <span class="operator">|</span> <span class="number">1700</span>                      <span class="operator">|</span> <span class="number">2021</span>                       <span class="operator">|</span> <span class="number">10</span>                          <span class="operator">|</span> <span class="number">26</span>                        <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">20</span>                           <span class="operator">|</span> RESEARCH                    <span class="operator">|</span> <span class="number">1800</span>                      <span class="operator">|</span> <span class="number">2021</span>                       <span class="operator">|</span> <span class="number">10</span>                          <span class="operator">|</span> <span class="number">26</span>                        <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">10</span>                           <span class="operator">|</span> ACCOUNTING                  <span class="operator">|</span> <span class="number">1700</span>                      <span class="operator">|</span> <span class="number">2021</span>                       <span class="operator">|</span> <span class="number">12</span>                          <span class="operator">|</span> <span class="number">28</span>                        <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">20</span>                           <span class="operator">|</span> RESEARCH                    <span class="operator">|</span> <span class="number">1800</span>                      <span class="operator">|</span> <span class="number">2021</span>                       <span class="operator">|</span> <span class="number">12</span>                          <span class="operator">|</span> <span class="number">28</span>                        <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">------------------------------+-----------------------------+---------------------------+----------------------------+-----------------------------+---------------------------+</span></span><br><span class="line"><span class="number">7</span> <span class="keyword">rows</span> selected (<span class="number">0.107</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span></span><br></pre></td></tr></table></figure>
<h3 id="7-1-3-动态分区"><a href="#7-1-3-动态分区" class="headerlink" title="7.1.3 动态分区"></a>7.1.3 动态分区</h3></li>
</ul>
</li>
</ul>
</li>
</ol>
<ul>
<li>关系型数据库中，对分区表Insert数据时候，数据库自动会根据分区字段的值，将数据插入到相应的分区中，Hive中也提供了类似的机制，即动态分区(Dynamic Partition)，只不过，使用Hive的动态分区，需要进行相应的配置。<ol>
<li>开启动态分区参数设置<ol>
<li>开启动态分区功能（默认true，开启） <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive.exec.dynamic.partition=true</span><br></pre></td></tr></table></figure></li>
<li>设置为非严格模式（动态分区的模式，默认strict，表示必须指定至少一个分区为静态分区，nonstrict模式表示允许所有的分区字段都可以使用动态分区。） <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive.exec.dynamic.partition.mode=nonstrict</span><br></pre></td></tr></table></figure></li>
<li>在所有执行MR的节点上，最大一共可以创建多少个动态分区。默认1000 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive.exec.max.dynamic.partitions=1000</span><br></pre></td></tr></table></figure></li>
<li>在每个执行MR的节点上，最大可以创建多少个动态分区。该参数需要根据实际的数据来设定。比如：源数据中包含了一年的数据，即day字段有365个值，那么该参数就需要设置成大于365，如果使用默认值100，则会报错。 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive.exec.max.dynamic.partitions.pernode=100</span><br></pre></td></tr></table></figure></li>
<li>整个MR Job中，最大可以创建多少个HDFS文件。默认100000 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive.exec.max.created.files=100000</span><br></pre></td></tr></table></figure></li>
<li>当有空分区生成时，是否抛出异常。一般不需要设置。默认false <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hive.error.on.empty.partition=false</span><br></pre></td></tr></table></figure></li>
</ol>
</li>
<li>案例实操</li>
</ol>
<ul>
<li>需求：将dept表中的数据按照地区（loc字段），插入到目标表dept_partition的相应分区中。<ol>
<li>创建目标分区表 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> dept_partition_dynamic (</span><br><span class="line">    deptno <span class="type">int</span>,</span><br><span class="line">    dname  string</span><br><span class="line">)</span><br><span class="line">partitioned <span class="keyword">by</span> (loc string)</span><br><span class="line"><span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span>;</span><br></pre></td></tr></table></figure></li>
<li>设置动态分区，插入数据 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">set</span> hive.exec.dynamic.partition.mode <span class="operator">=</span> nonstrict;</span><br><span class="line"><span class="keyword">No</span> <span class="keyword">rows</span> affected (<span class="number">0.005</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">insert</span> <span class="keyword">into</span> mock_data.dept_partition_dynamic <span class="keyword">partition</span>(loc)</span><br><span class="line"> <span class="keyword">select</span> deptno,dname,loc</span><br><span class="line"> <span class="keyword">from</span> mock_data.dept_partition_level;</span><br><span class="line"><span class="keyword">No</span> <span class="keyword">rows</span> affected (<span class="number">30.709</span> seconds)</span><br></pre></td></tr></table></figure></li>
<li>查看目标分区表的分区情况 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">show</span> partitions mock_data.dept_partition_dynamic;</span><br><span class="line"><span class="operator">+</span><span class="comment">---------------------------------+</span></span><br><span class="line"><span class="operator">|</span>            <span class="keyword">partition</span>            <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">---------------------------------+</span></span><br><span class="line"><span class="operator">|</span> loc<span class="operator">=</span><span class="number">1700</span>                        <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> loc<span class="operator">=</span><span class="number">1800</span>                        <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> loc<span class="operator">=</span><span class="number">1900</span>                        <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> loc<span class="operator">=</span><span class="number">2000</span>                        <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> loc<span class="operator">=</span>__HIVE_DEFAULT_PARTITION__  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">---------------------------------+</span></span><br><span class="line"><span class="number">5</span> <span class="keyword">rows</span> selected (<span class="number">0.096</span> seconds)</span><br></pre></td></tr></table></figure></li>
<li>思考：目标分区表是如何匹配到分区字段的？<ul>
<li>通过元数据获取分区表和分区字段</li>
<li>通过分区字段匹配查询结果中的字段</li>
</ul>
</li>
</ol>
</li>
</ul>
</li>
</ul>
<h2 id="7-2-分桶表"><a href="#7-2-分桶表" class="headerlink" title="7.2 分桶表"></a>7.2 分桶表</h2><ul>
<li>分区提供一个隔离数据和优化查询的便利方式。不过，并非所有的数据集都可形成合理的分区。对于一张表或者分区，Hive 可以进一步组织成桶，也就是更为细粒度的数据范围划分。</li>
<li>分桶是将数据集分解成更容易管理的若干部分的另一个技术。</li>
<li>分区针对的是数据的存储路径；分桶针对的是数据文件。</li>
</ul>
<ol>
<li>先创建分桶表<ol>
<li>数据准备 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">1001	ss1</span><br><span class="line">1002	ss2</span><br><span class="line">1003	ss3</span><br><span class="line">1004	ss4</span><br><span class="line">1005	ss5</span><br><span class="line">1006	ss6</span><br><span class="line">1007	ss7</span><br><span class="line">1008	ss8</span><br><span class="line">1009	ss9</span><br><span class="line">1010	ss10</span><br><span class="line">1011	ss11</span><br><span class="line">1012	ss12</span><br><span class="line">1013	ss13</span><br><span class="line">1014	ss14</span><br><span class="line">1015	ss15</span><br><span class="line">1016	ss16</span><br></pre></td></tr></table></figure></li>
<li>创建分桶表 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> stu_bucket(</span><br><span class="line">    id   <span class="type">int</span>,</span><br><span class="line">    name string</span><br><span class="line">)</span><br><span class="line">clustered <span class="keyword">by</span> (id) <span class="keyword">into</span> <span class="number">4</span> buckets</span><br><span class="line"><span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> &quot;\t&quot;;</span><br></pre></td></tr></table></figure></li>
<li>查看表结构   <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">desc</span> formatted stu_bucket;</span><br><span class="line">Num Buckets:            <span class="number">4</span>   </span><br></pre></td></tr></table></figure></li>
<li>导入数据到分桶表中，load的方式 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">load data <span class="keyword">local</span> inpath <span class="string">&#x27;/home/atguigu/hive/bucket/stu_bucket.txt&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span> stu_bucket;</span><br></pre></td></tr></table></figure></li>
<li>查看创建的分桶表中是否分成4个桶 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> dfs <span class="operator">-</span>ls <span class="operator">/</span><span class="keyword">user</span><span class="operator">/</span>hive<span class="operator">/</span>warehouse<span class="operator">/</span>mock_data.db<span class="operator">/</span>stu_bucket;</span><br><span class="line"><span class="operator">+</span><span class="comment">----------------------------------------------------+</span></span><br><span class="line"><span class="operator">|</span>                     DFS Output                     <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------------------------------------------------+</span></span><br><span class="line"><span class="operator">|</span> Found <span class="number">4</span> items                                      <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="operator">-</span>rw<span class="operator">-</span>r<span class="comment">--r--   3 atguigu supergroup         38 2021-10-26 18:44 /user/hive/warehouse/mock_data.db/stu_bucket/000000_0 |</span></span><br><span class="line"><span class="operator">|</span> <span class="operator">-</span>rw<span class="operator">-</span>r<span class="comment">--r--   3 atguigu supergroup         37 2021-10-26 18:44 /user/hive/warehouse/mock_data.db/stu_bucket/000001_0 |</span></span><br><span class="line"><span class="operator">|</span> <span class="operator">-</span>rw<span class="operator">-</span>r<span class="comment">--r--   3 atguigu supergroup         38 2021-10-26 18:44 /user/hive/warehouse/mock_data.db/stu_bucket/000002_0 |</span></span><br><span class="line"><span class="operator">|</span> <span class="operator">-</span>rw<span class="operator">-</span>r<span class="comment">--r--   3 atguigu supergroup         38 2021-10-26 18:44 /user/hive/warehouse/mock_data.db/stu_bucket/000003_0 |</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------------------------------------------------+</span></span><br><span class="line"><span class="number">5</span> <span class="keyword">rows</span> selected (<span class="number">0.01</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span></span><br></pre></td></tr></table></figure></li>
<li>分桶规则：<ul>
<li>根据结果可知：Hive的分桶采用对分桶字段的值进行哈希，然后除以桶的个数求余的方式决定该条记录存放在哪个桶当中</li>
</ul>
</li>
</ol>
</li>
<li>分桶表操作需要注意的事项:<ol>
<li>reduce的个数设置为-1,让Job自行决定需要用多少个reduce或者将reduce的个数设置为大于等于分桶表的桶数</li>
<li>从hdfs中load数据到分桶表中，避免本地文件找不到问题</li>
<li>不要使用本地模式(考虑到资源问题，防止MR执行失败，yarn会分配任务到不同节点)</li>
</ol>
</li>
<li>insert方式将数据导入分桶表 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> stu_bucket <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> student;</span><br></pre></td></tr></table></figure>
<h2 id="7-3-抽样查询"><a href="#7-3-抽样查询" class="headerlink" title="7.3 抽样查询"></a>7.3 抽样查询</h2></li>
</ol>
<ul>
<li>对于非常大的数据集，有时用户需要使用的是一个具有代表性的查询结果而不是全部结果。Hive可以通过对表进行抽样来满足这个需求。</li>
<li>语法: TABLESAMPLE(BUCKET x OUT OF y ON 分桶字段) </li>
<li>查询表stu_buck中的数据。  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> stu_buck <span class="keyword">tablesample</span>(bucket <span class="number">1</span> <span class="keyword">out</span> <span class="keyword">of</span> <span class="number">4</span> <span class="keyword">on</span> id);</span><br></pre></td></tr></table></figure></li>
<li>注意：x的值必须小于等于y的值，否则</li>
</ul>
<h1 id="八、函数"><a href="#八、函数" class="headerlink" title="八、函数"></a>八、函数</h1><h2 id="8-1-系统内置函数"><a href="#8-1-系统内置函数" class="headerlink" title="8.1 系统内置函数"></a>8.1 系统内置函数</h2><ol>
<li>查看系统自带的函数 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">show</span> functions <span class="keyword">like</span> count;</span><br><span class="line"><span class="operator">+</span><span class="comment">-----------+</span></span><br><span class="line"><span class="operator">|</span> tab_name  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-----------+</span></span><br><span class="line"><span class="operator">|</span> count     <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-----------+</span></span><br><span class="line"><span class="number">1</span> <span class="type">row</span> selected (<span class="number">0.019</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span></span><br></pre></td></tr></table></figure></li>
<li>显示自带的函数的用法 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">desc</span> <span class="keyword">function</span> upper;</span><br><span class="line"><span class="operator">+</span><span class="comment">----------------------------------------------------+</span></span><br><span class="line"><span class="operator">|</span>                      tab_name                      <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------------------------------------------------+</span></span><br><span class="line"><span class="operator">|</span> <span class="built_in">upper</span>(str) <span class="operator">-</span> <span class="keyword">Returns</span> str <span class="keyword">with</span> <span class="keyword">all</span> characters changed <span class="keyword">to</span> uppercase <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------------------------------------------------+</span></span><br><span class="line"><span class="number">1</span> <span class="type">row</span> selected (<span class="number">0.019</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span></span><br></pre></td></tr></table></figure></li>
<li>详细显示自带的函数的用法 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">desc</span> <span class="keyword">function</span> extended upper;</span><br><span class="line"><span class="operator">+</span><span class="comment">----------------------------------------------------+</span></span><br><span class="line"><span class="operator">|</span>                      tab_name                      <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------------------------------------------------+</span></span><br><span class="line"><span class="operator">|</span> <span class="built_in">upper</span>(str) <span class="operator">-</span> <span class="keyword">Returns</span> str <span class="keyword">with</span> <span class="keyword">all</span> characters changed <span class="keyword">to</span> uppercase <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> Synonyms: ucase                                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> Example:                                           <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>   <span class="operator">&gt;</span> <span class="keyword">SELECT</span> <span class="built_in">upper</span>(<span class="string">&#x27;Facebook&#x27;</span>) <span class="keyword">FROM</span> src LIMIT <span class="number">1</span>;     <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>   <span class="string">&#x27;FACEBOOK&#x27;</span>                                       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="keyword">Function</span> class:org.apache.hadoop.hive.ql.udf.generic.GenericUDFUpper <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="keyword">Function</span> type:BUILTIN                              <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------------------------------------------------+</span></span><br><span class="line"><span class="number">7</span> <span class="keyword">rows</span> selected (<span class="number">0.079</span> seconds)</span><br></pre></td></tr></table></figure>
<h2 id="8-2-常用内置函数"><a href="#8-2-常用内置函数" class="headerlink" title="8.2 常用内置函数"></a>8.2 常用内置函数</h2><h3 id="8-2-1-空字段赋值"><a href="#8-2-1-空字段赋值" class="headerlink" title="8.2.1 空字段赋值"></a>8.2.1 空字段赋值</h3></li>
<li>函数说明</li>
</ol>
<ul>
<li>NVL：给值为NULL的数据赋值，它的格式是NVL( value，default_value)。它的功能是如果value为NULL，则NVL函数返回default_value的值，否则返回value的值，如果两个参数都为NULL ，则返回NULL。<ul>
<li>示例  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> dept;</span><br><span class="line"><span class="operator">+</span><span class="comment">---------------+-----------------+-----------+-----------------+</span></span><br><span class="line"><span class="operator">|</span> dept.dept_no  <span class="operator">|</span> dept.dept_name  <span class="operator">|</span> dept.loc  <span class="operator">|</span> dept.dept_desc  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">---------------+-----------------+-----------+-----------------+</span></span><br><span class="line"><span class="operator">|</span> <span class="number">10</span>            <span class="operator">|</span> CEO             <span class="operator">|</span> <span class="number">1700</span>      <span class="operator">|</span> 这是老板            <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">10</span>            <span class="operator">|</span> ACCOUNTING      <span class="operator">|</span> <span class="number">1700</span>      <span class="operator">|</span> <span class="keyword">NULL</span>            <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">20</span>            <span class="operator">|</span> RESEARCH        <span class="operator">|</span> <span class="number">1800</span>      <span class="operator">|</span> <span class="keyword">NULL</span>            <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">30</span>            <span class="operator">|</span> SALES           <span class="operator">|</span> <span class="number">1900</span>      <span class="operator">|</span> <span class="keyword">NULL</span>            <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">40</span>            <span class="operator">|</span> OPERATIONS      <span class="operator">|</span> <span class="number">1700</span>      <span class="operator">|</span> <span class="keyword">NULL</span>            <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">---------------+-----------------+-----------+-----------------+</span></span><br><span class="line"><span class="number">5</span> <span class="keyword">rows</span> selected (<span class="number">0.058</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">select</span> dept_no, dept_name, loc, nvl(dept_desc,&quot;无描述&quot;) <span class="keyword">from</span> dept;</span><br><span class="line"><span class="operator">+</span><span class="comment">----------+-------------+-------+-------+</span></span><br><span class="line"><span class="operator">|</span> dept_no  <span class="operator">|</span>  dept_name  <span class="operator">|</span>  loc  <span class="operator">|</span>  _c3  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------+-------------+-------+-------+</span></span><br><span class="line"><span class="operator">|</span> <span class="number">10</span>       <span class="operator">|</span> CEO         <span class="operator">|</span> <span class="number">1700</span>  <span class="operator">|</span> 这是老板  <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">10</span>       <span class="operator">|</span> ACCOUNTING  <span class="operator">|</span> <span class="number">1700</span>  <span class="operator">|</span> 无描述   <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">20</span>       <span class="operator">|</span> RESEARCH    <span class="operator">|</span> <span class="number">1800</span>  <span class="operator">|</span> 无描述   <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">30</span>       <span class="operator">|</span> SALES       <span class="operator">|</span> <span class="number">1900</span>  <span class="operator">|</span> 无描述   <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">40</span>       <span class="operator">|</span> OPERATIONS  <span class="operator">|</span> <span class="number">1700</span>  <span class="operator">|</span> 无描述   <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------+-------------+-------+-------+</span></span><br><span class="line"><span class="number">5</span> <span class="keyword">rows</span> selected (<span class="number">0.06</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span></span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>
<h3 id="8-2-2-CASE-WHEN-THEN-ELSE-END"><a href="#8-2-2-CASE-WHEN-THEN-ELSE-END" class="headerlink" title="8.2.2 CASE WHEN THEN ELSE END"></a>8.2.2 CASE WHEN THEN ELSE END</h3><ul>
<li>示例1  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">select</span></span><br><span class="line">        emp_name,</span><br><span class="line">         <span class="keyword">case</span> emp_name</span><br><span class="line">             <span class="keyword">when</span> <span class="string">&#x27;SMITH&#x27;</span> <span class="keyword">then</span> <span class="string">&#x27;史密斯&#x27;</span></span><br><span class="line">             <span class="keyword">when</span> <span class="string">&#x27;ALLEN&#x27;</span> <span class="keyword">then</span> <span class="string">&#x27;艾伦&#x27;</span></span><br><span class="line">             <span class="keyword">when</span> <span class="string">&#x27;WARD&#x27;</span> <span class="keyword">then</span> <span class="string">&#x27;沃德&#x27;</span></span><br><span class="line">             <span class="keyword">when</span> <span class="string">&#x27;JONES&#x27;</span> <span class="keyword">then</span> <span class="string">&#x27;琼斯&#x27;</span></span><br><span class="line">             <span class="keyword">when</span> <span class="string">&#x27;MARTIN&#x27;</span> <span class="keyword">then</span> <span class="string">&#x27;马丁&#x27;</span></span><br><span class="line">             <span class="keyword">when</span> <span class="string">&#x27;BLAKE&#x27;</span> <span class="keyword">then</span> <span class="string">&#x27;布莱克&#x27;</span></span><br><span class="line">             <span class="keyword">when</span> <span class="string">&#x27;CLARK&#x27;</span> <span class="keyword">then</span> <span class="string">&#x27;克拉克&#x27;</span></span><br><span class="line">             <span class="keyword">when</span> <span class="string">&#x27;SCOTT&#x27;</span> <span class="keyword">then</span> <span class="string">&#x27;斯科特&#x27;</span></span><br><span class="line">             <span class="keyword">when</span> <span class="string">&#x27;KING&#x27;</span> <span class="keyword">then</span> <span class="string">&#x27;肯&#x27;</span></span><br><span class="line">             <span class="keyword">when</span> <span class="string">&#x27;TURNER&#x27;</span> <span class="keyword">then</span> <span class="string">&#x27;特纳&#x27;</span></span><br><span class="line">             <span class="keyword">else</span> <span class="string">&#x27;爱啥啥&#x27;</span> <span class="keyword">end</span> chinese_name</span><br><span class="line"> <span class="keyword">from</span> emp;</span><br><span class="line"><span class="operator">+</span><span class="comment">-----------+---------------+</span></span><br><span class="line"><span class="operator">|</span> emp_name  <span class="operator">|</span> chinese_name  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-----------+---------------+</span></span><br><span class="line"><span class="operator">|</span> SMITH     <span class="operator">|</span> 史密斯           <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> ALLEN     <span class="operator">|</span> 艾伦            <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> WARD      <span class="operator">|</span> 沃德            <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> JONES     <span class="operator">|</span> 琼斯            <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> MARTIN    <span class="operator">|</span> 马丁            <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> BLAKE     <span class="operator">|</span> 布莱克           <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> CLARK     <span class="operator">|</span> 克拉克           <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> SCOTT     <span class="operator">|</span> 斯科特           <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> KING      <span class="operator">|</span> 肯             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> TURNER    <span class="operator">|</span> 特纳            <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> ADAMS     <span class="operator">|</span> 爱啥啥           <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> JAMES     <span class="operator">|</span> 爱啥啥           <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> FORD      <span class="operator">|</span> 爱啥啥           <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> MILLER    <span class="operator">|</span> 爱啥啥           <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-----------+---------------+</span></span><br><span class="line"><span class="number">14</span> <span class="keyword">rows</span> selected (<span class="number">0.09</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span></span><br></pre></td></tr></table></figure></li>
<li>示例2  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line">    dept_no,</span><br><span class="line">       <span class="built_in">sum</span>(<span class="keyword">case</span> <span class="keyword">when</span> sal <span class="keyword">between</span> <span class="number">3000</span> <span class="keyword">and</span> <span class="number">8000</span> <span class="keyword">then</span> <span class="number">1</span> <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">end</span> ) <span class="keyword">as</span> `<span class="number">3000</span>到<span class="number">8000</span>`,</span><br><span class="line">       <span class="built_in">sum</span>(if(sal <span class="operator">&lt;</span> <span class="number">3000</span>, <span class="number">1</span>, <span class="number">0</span>)) <span class="keyword">as</span> `小于<span class="number">3000</span>`</span><br><span class="line"><span class="keyword">from</span> emp</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> dept_no;</span><br></pre></td></tr></table></figure>
<h3 id="8-2-3-行转列"><a href="#8-2-3-行转列" class="headerlink" title="8.2.3 行转列"></a>8.2.3 行转列</h3></li>
</ul>
<ol>
<li>相关函数说明<ol>
<li>CONCAT(string A/col, string B/col…)：返回输入字符串连接后的结果，支持任意个输入字符串;</li>
<li>CONCAT_WS(separator, str1, str2,…)：它是一个特殊形式的 CONCAT()。第一个参数剩余参数间的分隔符。分隔符可以是与剩余参数一样的字符串。如果分隔符是 NULL，返回值也将为 NULL。这个函数会跳过分隔符参数后的任何 NULL 和空字符串。分隔符将被加到被连接的字符串之间;<ul>
<li>注意: CONCAT_WS must be “string or array<string>”</li>
</ul>
</li>
<li>COLLECT_SET(col)：函数只接受基本数据类型，它的主要作用是将某字段的值进行去重汇总，产生array类型字段。 </li>
</ol>
</li>
<li>数据准备  <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">name	constellation	blood_type</span><br><span class="line">孙悟空	白羊座	A</span><br><span class="line">大海	射手座	A</span><br><span class="line">宋宋	白羊座	B</span><br><span class="line">猪八戒	白羊座	A</span><br><span class="line">凤姐	射手座	A</span><br><span class="line">苍老师	白羊座	B</span><br></pre></td></tr></table></figure></li>
<li>需求：把星座和血型一样的人归类到一起。结果如下： <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">射手座,A            大海|凤姐</span><br><span class="line">白羊座,A            孙悟空|猪八戒</span><br><span class="line">白羊座,B            宋宋|苍老师</span><br></pre></td></tr></table></figure></li>
<li>创建hive表并导入数据 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">0: jdbc:hive2://hadoop001:10000&gt; create table person_constellation_blood</span><br><span class="line"> (</span><br><span class="line">     name          string,</span><br><span class="line">     constellation string,</span><br><span class="line">     blood_type    string</span><br><span class="line"> )</span><br><span class="line"> row format delimited fields terminated by <span class="string">&quot;\t&quot;</span>;</span><br><span class="line">No rows affected (5.123 seconds)</span><br><span class="line">0: jdbc:hive2://hadoop001:10000&gt; load data <span class="built_in">local</span> inpath <span class="string">&quot;/home/atguigu/hive/person_constellation_blood/person_constellation_blood.txt&quot;</span> into table person_constellation_blood;</span><br><span class="line">No rows affected (0.111 seconds)</span><br><span class="line">0: jdbc:hive2://hadoop001:10000&gt;</span><br></pre></td></tr></table></figure></li>
<li>按需求查询数据 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">select</span> t1.cb,</span><br><span class="line">        concat_ws(&quot;|&quot;, collect_set(t1.name))</span><br><span class="line"> <span class="keyword">from</span> (<span class="keyword">select</span> name,</span><br><span class="line">              concat_ws(&quot;,&quot;, constellation, blood_type) cb</span><br><span class="line">       <span class="keyword">from</span> person_constellation_blood) t1</span><br><span class="line"> <span class="keyword">group</span> <span class="keyword">by</span> t1.cb;</span><br><span class="line"><span class="operator">+</span><span class="comment">--------+----------+</span></span><br><span class="line"><span class="operator">|</span> t1.cb  <span class="operator">|</span>   _c1    <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">--------+----------+</span></span><br><span class="line"><span class="operator">|</span> 射手座,A  <span class="operator">|</span> 大海<span class="operator">|</span>凤姐    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> 白羊座,A  <span class="operator">|</span> 孙悟空<span class="operator">|</span>猪八戒  <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> 白羊座,B  <span class="operator">|</span> 宋宋<span class="operator">|</span>苍老师   <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">--------+----------+</span></span><br><span class="line"><span class="number">3</span> <span class="keyword">rows</span> selected (<span class="number">14.797</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span></span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="8-2-4-列转行"><a href="#8-2-4-列转行" class="headerlink" title="8.2.4 列转行"></a>8.2.4 列转行</h3><ol>
<li>函数说明<ul>
<li>EXPLODE(col)：炸裂函数，将hive一列中复杂的array或者map结构拆分成多行。  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">select</span> explode(split(&quot;1,2,3,4,5&quot;,&quot;,&quot;));</span><br><span class="line"><span class="operator">+</span><span class="comment">------+</span></span><br><span class="line"><span class="operator">|</span> col  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">------+</span></span><br><span class="line"><span class="operator">|</span> <span class="number">1</span>    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">2</span>    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">3</span>    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">4</span>    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">5</span>    <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">------+</span></span><br><span class="line"><span class="number">5</span> <span class="keyword">rows</span> selected (<span class="number">0.057</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span></span><br></pre></td></tr></table></figure></li>
<li>LATERAL VIEW<ul>
<li>用法：LATERAL VIEW udtf(expression) tableAlias AS columnAlias</li>
<li>解释：用于和split, explode等UDTF一起使用，它能够将一列数据拆成多行数据，在此基础上可以对拆分后的数据进行聚合。</li>
</ul>
</li>
</ul>
</li>
<li>数据准备<table>
<thead>
<tr>
<th>movie</th>
<th>category</th>
</tr>
</thead>
<tbody><tr>
<td>《疑犯追踪》</td>
<td>悬疑,动作,科幻,剧情</td>
</tr>
<tr>
<td>《Lie to me》</td>
<td>悬疑,警匪,动作,心理,剧情</td>
</tr>
<tr>
<td>《战狼2》</td>
<td>战争,动作,灾难</td>
</tr>
</tbody></table>
</li>
<li>需求：将电影分类中的数组数据展开。结果如下： <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">《疑犯追踪》	悬疑</span><br><span class="line">《疑犯追踪》	动作</span><br><span class="line">《疑犯追踪》	科幻</span><br><span class="line">《疑犯追踪》	剧情</span><br><span class="line">《Lie to me》	悬疑</span><br><span class="line">《Lie to me》	警匪</span><br><span class="line">《Lie to me》	动作</span><br><span class="line">《Lie to me》	心理</span><br><span class="line">《Lie to me》	剧情</span><br><span class="line">《战狼2》	战争</span><br><span class="line">《战狼2》	动作</span><br><span class="line">《战狼2》	灾难</span><br></pre></td></tr></table></figure></li>
<li>创建hive表并导入数据 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">create</span> <span class="keyword">table</span> movie_info(</span><br><span class="line">     movie string,</span><br><span class="line">     category string)</span><br><span class="line"> <span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> &quot;\t&quot;;</span><br><span class="line"><span class="keyword">No</span> <span class="keyword">rows</span> affected (<span class="number">0.065</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> load data <span class="keyword">local</span> inpath <span class="string">&#x27;/home/atguigu/hive/movie_info/movie_info.txt&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span> mock_data.movie_info;</span><br><span class="line"><span class="keyword">No</span> <span class="keyword">rows</span> affected (<span class="number">0.113</span> seconds)</span><br></pre></td></tr></table></figure></li>
<li>按需求查询数据 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">select</span></span><br><span class="line">        movie, category_name</span><br><span class="line"> <span class="keyword">from</span> movie_info</span><br><span class="line"> <span class="keyword">lateral</span> <span class="keyword">view</span></span><br><span class="line">     explode(split(category,<span class="string">&#x27;,&#x27;</span>)) movie_info_tmp <span class="keyword">as</span> category_name;</span><br><span class="line"><span class="operator">+</span><span class="comment">--------------+----------------+</span></span><br><span class="line"><span class="operator">|</span>    movie     <span class="operator">|</span> category_name  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">--------------+----------------+</span></span><br><span class="line"><span class="operator">|</span> 《疑犯追踪》       <span class="operator">|</span> 悬疑             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> 《疑犯追踪》       <span class="operator">|</span> 动作             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> 《疑犯追踪》       <span class="operator">|</span> 科幻             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> 《疑犯追踪》       <span class="operator">|</span> 剧情             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> 《Lie <span class="keyword">to</span> me》  <span class="operator">|</span> 悬疑             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> 《Lie <span class="keyword">to</span> me》  <span class="operator">|</span> 警匪             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> 《Lie <span class="keyword">to</span> me》  <span class="operator">|</span> 动作             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> 《Lie <span class="keyword">to</span> me》  <span class="operator">|</span> 心理             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> 《Lie <span class="keyword">to</span> me》  <span class="operator">|</span> 剧情             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> 《战狼<span class="number">2</span>》        <span class="operator">|</span> 战争             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> 《战狼<span class="number">2</span>》        <span class="operator">|</span> 动作             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> 《战狼<span class="number">2</span>》        <span class="operator">|</span> 灾难             <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">--------------+----------------+</span></span><br><span class="line"><span class="number">12</span> <span class="keyword">rows</span> selected (<span class="number">0.105</span> seconds)</span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="8-2-5-窗口函数（开窗函数）"><a href="#8-2-5-窗口函数（开窗函数）" class="headerlink" title="8.2.5 窗口函数（开窗函数）"></a>8.2.5 窗口函数（开窗函数）</h3><h4 id="8-2-5-1-窗口函数基本使用"><a href="#8-2-5-1-窗口函数基本使用" class="headerlink" title="8.2.5.1 窗口函数基本使用"></a>8.2.5.1 窗口函数基本使用</h4><ol>
<li>介绍：<ul>
<li>普通聚合函数聚合的行集是组，开窗函数聚合的行集是窗口。因此，普通聚合函数每组（Group by）只有一个返回值，而<font color ='red' >开窗函数则可以为窗口中的每行都返回一个值</font>。</li>
</ul>
</li>
<li>基本结构 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">分析函数(如：sum(), max(), row_number()...) 窗口子句over(partition by 列名 order by 列名 rows between 开始位置 and 结束位置)</span><br></pre></td></tr></table></figure></li>
<li>over函数<ul>
<li>语法结构  <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">over(partition by [column_n] order by [column_m])</span><br></pre></td></tr></table></figure>
<ul>
<li>先按照column_n分区，相同的column_n分为一区，每个分区根据column_m排序（默认升序）。</li>
</ul>
</li>
</ul>
</li>
<li>测试数据<ul>
<li>建表并插入数据  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> student_scores <span class="keyword">as</span></span><br><span class="line"><span class="keyword">select</span> stack(<span class="number">20</span>,</span><br><span class="line">   <span class="number">1</span>, <span class="number">111</span>, <span class="number">68</span>, <span class="number">69</span>, <span class="number">90</span>, <span class="string">&#x27;class1&#x27;</span>, <span class="string">&#x27;department1&#x27;</span>,</span><br><span class="line">   <span class="number">2</span>, <span class="number">112</span>, <span class="number">73</span>, <span class="number">80</span>, <span class="number">96</span>, <span class="string">&#x27;class1&#x27;</span>, <span class="string">&#x27;department1&#x27;</span>,</span><br><span class="line">   <span class="number">3</span>, <span class="number">113</span>, <span class="number">90</span>, <span class="number">74</span>, <span class="number">75</span>, <span class="string">&#x27;class1&#x27;</span>, <span class="string">&#x27;department1&#x27;</span>,</span><br><span class="line">   <span class="number">4</span>, <span class="number">114</span>, <span class="number">89</span>, <span class="number">94</span>, <span class="number">93</span>, <span class="string">&#x27;class1&#x27;</span>, <span class="string">&#x27;department1&#x27;</span>,</span><br><span class="line">   <span class="number">5</span>, <span class="number">115</span>, <span class="number">99</span>, <span class="number">93</span>, <span class="number">89</span>, <span class="string">&#x27;class1&#x27;</span>, <span class="string">&#x27;department1&#x27;</span>,</span><br><span class="line">   <span class="number">6</span>, <span class="number">121</span>, <span class="number">96</span>, <span class="number">74</span>, <span class="number">79</span>, <span class="string">&#x27;class2&#x27;</span>, <span class="string">&#x27;department1&#x27;</span>,</span><br><span class="line">   <span class="number">7</span>, <span class="number">122</span>, <span class="number">89</span>, <span class="number">86</span>, <span class="number">85</span>, <span class="string">&#x27;class2&#x27;</span>, <span class="string">&#x27;department1&#x27;</span>,</span><br><span class="line">   <span class="number">8</span>, <span class="number">123</span>, <span class="number">70</span>, <span class="number">78</span>, <span class="number">61</span>, <span class="string">&#x27;class2&#x27;</span>, <span class="string">&#x27;department1&#x27;</span>,</span><br><span class="line">   <span class="number">9</span>, <span class="number">124</span>, <span class="number">76</span>, <span class="number">70</span>, <span class="number">76</span>, <span class="string">&#x27;class2&#x27;</span>, <span class="string">&#x27;department1&#x27;</span>,</span><br><span class="line">   <span class="number">10</span>, <span class="number">211</span>, <span class="number">89</span>, <span class="number">93</span>, <span class="number">60</span>, <span class="string">&#x27;class1&#x27;</span>, <span class="string">&#x27;department2&#x27;</span>,</span><br><span class="line">   <span class="number">11</span>, <span class="number">212</span>, <span class="number">76</span>, <span class="number">83</span>, <span class="number">75</span>, <span class="string">&#x27;class1&#x27;</span>, <span class="string">&#x27;department2&#x27;</span>,</span><br><span class="line">   <span class="number">12</span>, <span class="number">213</span>, <span class="number">71</span>, <span class="number">94</span>, <span class="number">90</span>, <span class="string">&#x27;class1&#x27;</span>, <span class="string">&#x27;department2&#x27;</span>,</span><br><span class="line">   <span class="number">13</span>, <span class="number">214</span>, <span class="number">94</span>, <span class="number">94</span>, <span class="number">66</span>, <span class="string">&#x27;class1&#x27;</span>, <span class="string">&#x27;department2&#x27;</span>,</span><br><span class="line">   <span class="number">14</span>, <span class="number">215</span>, <span class="number">84</span>, <span class="number">82</span>, <span class="number">73</span>, <span class="string">&#x27;class1&#x27;</span>, <span class="string">&#x27;department2&#x27;</span>,</span><br><span class="line">   <span class="number">15</span>, <span class="number">216</span>, <span class="number">85</span>, <span class="number">74</span>, <span class="number">93</span>, <span class="string">&#x27;class1&#x27;</span>, <span class="string">&#x27;department2&#x27;</span>,</span><br><span class="line">   <span class="number">16</span>, <span class="number">221</span>, <span class="number">77</span>, <span class="number">99</span>, <span class="number">61</span>, <span class="string">&#x27;class2&#x27;</span>, <span class="string">&#x27;department2&#x27;</span>,</span><br><span class="line">   <span class="number">17</span>, <span class="number">222</span>, <span class="number">80</span>, <span class="number">78</span>, <span class="number">96</span>, <span class="string">&#x27;class2&#x27;</span>, <span class="string">&#x27;department2&#x27;</span>,</span><br><span class="line">   <span class="number">18</span>, <span class="number">223</span>, <span class="number">79</span>, <span class="number">74</span>, <span class="number">96</span>, <span class="string">&#x27;class2&#x27;</span>, <span class="string">&#x27;department2&#x27;</span>,</span><br><span class="line">   <span class="number">19</span>, <span class="number">224</span>, <span class="number">75</span>, <span class="number">80</span>, <span class="number">78</span>, <span class="string">&#x27;class2&#x27;</span>, <span class="string">&#x27;department2&#x27;</span>,</span><br><span class="line">   <span class="number">20</span>, <span class="number">225</span>, <span class="number">82</span>, <span class="number">85</span>, <span class="number">63</span>, <span class="string">&#x27;class2&#x27;</span>, <span class="string">&#x27;department2&#x27;</span></span><br><span class="line">) <span class="keyword">as</span> (id,studentId,<span class="keyword">language</span>,math,english,classId,departmentId);</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>窗口示例<ul>
<li>运行sql  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> studentId,</span><br><span class="line">       math,</span><br><span class="line">       departmentId,</span><br><span class="line">       classId,</span><br><span class="line">       <span class="comment">-- 符合所有条件的行作为窗口聚合的行集，这里符合department1的有9个</span></span><br><span class="line">       <span class="built_in">count</span>(math) <span class="keyword">over</span> ()                                                                                    <span class="keyword">as</span> count1,</span><br><span class="line">       <span class="comment">-- 符合条件的行，按照classId分组，每组数据作为窗口聚合的行集</span></span><br><span class="line">       <span class="built_in">count</span>(math) <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> classId)                                                                <span class="keyword">as</span> count2,</span><br><span class="line">       <span class="comment">-- 符合条件的行，按照classId分组，每组数据按照math升序排列后作为窗口聚合的行集</span></span><br><span class="line">       <span class="built_in">count</span>(math) <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> classId <span class="keyword">order</span> <span class="keyword">by</span> math)                                                  <span class="keyword">as</span> count3,</span><br><span class="line">       <span class="comment">-- 符合条件的行，按照classId分组，每组数据按照math升序排列后，当前行向前1行，向后2行（n-1,n,n+1,n+2共四行数据）作为窗口聚合的行集</span></span><br><span class="line">       <span class="built_in">count</span>(math) <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> classId <span class="keyword">order</span> <span class="keyword">by</span> math <span class="keyword">rows</span> <span class="keyword">between</span> <span class="number">1</span> preceding <span class="keyword">and</span> <span class="number">2</span> following)         <span class="keyword">as</span> count4,</span><br><span class="line">       <span class="comment">-- 符合条件的行，按照classId分组，每组数据按照math升序排列后，当前行向后到结束行作为窗口聚合的行集</span></span><br><span class="line">       <span class="built_in">count</span>(math) <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> classId <span class="keyword">order</span> <span class="keyword">by</span> math <span class="keyword">rows</span> <span class="keyword">between</span> <span class="keyword">current</span> <span class="type">row</span> <span class="keyword">and</span> unbounded following) <span class="keyword">as</span> count5,</span><br><span class="line">       <span class="comment">-- 符合条件的行，按照classId分组，每组数据按照math升序排列后，从起点行到当前行作为窗口聚合的行集</span></span><br><span class="line">       <span class="built_in">count</span>(math) <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> classId <span class="keyword">order</span> <span class="keyword">by</span> math <span class="keyword">rows</span> <span class="keyword">between</span> unbounded preceding <span class="keyword">and</span> <span class="keyword">current</span> <span class="type">row</span> ) <span class="keyword">as</span> count6</span><br><span class="line"><span class="keyword">from</span> student_scores</span><br><span class="line"><span class="keyword">where</span> departmentId <span class="operator">=</span> <span class="string">&#x27;department1&#x27;</span>;</span><br></pre></td></tr></table></figure></li>
<li>结果  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="operator">+</span><span class="comment">------------+-------+---------------+----------+---------+---------+---------+---------+---------+---------+</span></span><br><span class="line"><span class="operator">|</span> studentid  <span class="operator">|</span> math  <span class="operator">|</span> departmentid  <span class="operator">|</span> classid  <span class="operator">|</span> count1  <span class="operator">|</span> count2  <span class="operator">|</span> count3  <span class="operator">|</span> count4  <span class="operator">|</span> count5  <span class="operator">|</span> count6  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">------------+-------+---------------+----------+---------+---------+---------+---------+---------+---------+</span></span><br><span class="line"><span class="operator">|</span> <span class="number">111</span>        <span class="operator">|</span> <span class="number">69</span>    <span class="operator">|</span> department1   <span class="operator">|</span> class1   <span class="operator">|</span> <span class="number">9</span>       <span class="operator">|</span> <span class="number">5</span>       <span class="operator">|</span> <span class="number">1</span>       <span class="operator">|</span> <span class="number">3</span>       <span class="operator">|</span> <span class="number">5</span>       <span class="operator">|</span> <span class="number">1</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">113</span>        <span class="operator">|</span> <span class="number">74</span>    <span class="operator">|</span> department1   <span class="operator">|</span> class1   <span class="operator">|</span> <span class="number">9</span>       <span class="operator">|</span> <span class="number">5</span>       <span class="operator">|</span> <span class="number">2</span>       <span class="operator">|</span> <span class="number">4</span>       <span class="operator">|</span> <span class="number">4</span>       <span class="operator">|</span> <span class="number">2</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">112</span>        <span class="operator">|</span> <span class="number">80</span>    <span class="operator">|</span> department1   <span class="operator">|</span> class1   <span class="operator">|</span> <span class="number">9</span>       <span class="operator">|</span> <span class="number">5</span>       <span class="operator">|</span> <span class="number">3</span>       <span class="operator">|</span> <span class="number">4</span>       <span class="operator">|</span> <span class="number">3</span>       <span class="operator">|</span> <span class="number">3</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">115</span>        <span class="operator">|</span> <span class="number">93</span>    <span class="operator">|</span> department1   <span class="operator">|</span> class1   <span class="operator">|</span> <span class="number">9</span>       <span class="operator">|</span> <span class="number">5</span>       <span class="operator">|</span> <span class="number">4</span>       <span class="operator">|</span> <span class="number">3</span>       <span class="operator">|</span> <span class="number">2</span>       <span class="operator">|</span> <span class="number">4</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">114</span>        <span class="operator">|</span> <span class="number">94</span>    <span class="operator">|</span> department1   <span class="operator">|</span> class1   <span class="operator">|</span> <span class="number">9</span>       <span class="operator">|</span> <span class="number">5</span>       <span class="operator">|</span> <span class="number">5</span>       <span class="operator">|</span> <span class="number">2</span>       <span class="operator">|</span> <span class="number">1</span>       <span class="operator">|</span> <span class="number">5</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">124</span>        <span class="operator">|</span> <span class="number">70</span>    <span class="operator">|</span> department1   <span class="operator">|</span> class2   <span class="operator">|</span> <span class="number">9</span>       <span class="operator">|</span> <span class="number">4</span>       <span class="operator">|</span> <span class="number">1</span>       <span class="operator">|</span> <span class="number">3</span>       <span class="operator">|</span> <span class="number">4</span>       <span class="operator">|</span> <span class="number">1</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">121</span>        <span class="operator">|</span> <span class="number">74</span>    <span class="operator">|</span> department1   <span class="operator">|</span> class2   <span class="operator">|</span> <span class="number">9</span>       <span class="operator">|</span> <span class="number">4</span>       <span class="operator">|</span> <span class="number">2</span>       <span class="operator">|</span> <span class="number">4</span>       <span class="operator">|</span> <span class="number">3</span>       <span class="operator">|</span> <span class="number">2</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">123</span>        <span class="operator">|</span> <span class="number">78</span>    <span class="operator">|</span> department1   <span class="operator">|</span> class2   <span class="operator">|</span> <span class="number">9</span>       <span class="operator">|</span> <span class="number">4</span>       <span class="operator">|</span> <span class="number">3</span>       <span class="operator">|</span> <span class="number">3</span>       <span class="operator">|</span> <span class="number">2</span>       <span class="operator">|</span> <span class="number">3</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">122</span>        <span class="operator">|</span> <span class="number">86</span>    <span class="operator">|</span> department1   <span class="operator">|</span> class2   <span class="operator">|</span> <span class="number">9</span>       <span class="operator">|</span> <span class="number">4</span>       <span class="operator">|</span> <span class="number">4</span>       <span class="operator">|</span> <span class="number">2</span>       <span class="operator">|</span> <span class="number">1</span>       <span class="operator">|</span> <span class="number">4</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">------------+-------+---------------+----------+---------+---------+---------+---------+---------+---------+</span></span><br></pre></td></tr></table></figure></li>
<li>结果解析：  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">studentid=114 为例</span><br><span class="line">count1: 为departmentId=department1的行数为9，</span><br><span class="line">count2: 为分区class1中的行数5，</span><br><span class="line">count3: 为分区class1中math升序排列到当前行94 match&lt;=94行数5</span><br><span class="line">count4: 为分区class1中math升序排列后取，当前行-1=115，114，当前行+1=不存在，当前行+2=不存在，行数2</span><br><span class="line">count5: 为分区class1中math升序排列后取，当前行到结束行，114为当前分区最后一行，行数1</span><br><span class="line">count6: 为分区class1中math升序排列后取，开始行到当前行，行数5</span><br></pre></td></tr></table></figure>
<ul>
<li>如果不指定ROWS BETWEEN，默认统计窗口是从起点到当前行</li>
<li>ROWS BETWEEN，也叫做window子句。<ul>
<li>PRECEDING：往前<ul>
<li>n PRECEDING：往前n行数据</li>
</ul>
</li>
<li>FOLLOWING：往后<ul>
<li>n FOLLOWING：往后n行数据</li>
</ul>
</li>
<li>CURRENT ROW：当前行</li>
<li>UNBOUNDED：无边界（一般结合PRECEDING，FOLLOWING使用）</li>
<li>UNBOUNDED PRECEDING 表示该窗口最前面的行（起点）</li>
<li>UNBOUNDED FOLLOWING：表示该窗口最后面的行（终点）</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ol>
<h4 id="8-2-5-2-聚合开窗函数"><a href="#8-2-5-2-聚合开窗函数" class="headerlink" title="8.2.5.2 聚合开窗函数"></a>8.2.5.2 聚合开窗函数</h4><ol>
<li>sum()，min()，max()，avg()都与count()类似<ul>
<li>示例  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> studentId,</span><br><span class="line">       math,</span><br><span class="line">       departmentId,</span><br><span class="line">       classId,</span><br><span class="line">       <span class="comment">-- 符合所有条件的行作为窗口聚合的行集，这里符合department1的有9个</span></span><br><span class="line">       <span class="built_in">avg</span>(math) <span class="keyword">over</span> ()                                                                                    <span class="keyword">as</span> avg1,</span><br><span class="line">       <span class="comment">-- 符合条件的行，按照classId分组，每组数据作为窗口聚合的行集</span></span><br><span class="line">       <span class="built_in">avg</span>(math) <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> classId)                                                                <span class="keyword">as</span> avg2,</span><br><span class="line">       <span class="comment">-- 符合条件的行，按照classId分组，每组数据按照math升序排列后到当前行截止，作为窗口聚合的行集</span></span><br><span class="line">       <span class="built_in">avg</span>(math) <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> classId <span class="keyword">order</span> <span class="keyword">by</span> math)                                                  <span class="keyword">as</span> avg3,</span><br><span class="line">       <span class="comment">-- 符合条件的行，按照classId分组，每组数据按照math升序排列后，当前行向前1行，向后2行（n-1,n,n+1,n+2共四行数据）作为窗口聚合的行集</span></span><br><span class="line">       <span class="built_in">avg</span>(math) <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> classId <span class="keyword">order</span> <span class="keyword">by</span> math <span class="keyword">rows</span> <span class="keyword">between</span> <span class="number">1</span> preceding <span class="keyword">and</span> <span class="number">2</span> following)         <span class="keyword">as</span> avg4,</span><br><span class="line">       <span class="comment">-- 符合条件的行，按照classId分组，每组数据按照math升序排列后，当前行向后到结束行作为窗口聚合的行集</span></span><br><span class="line">       <span class="built_in">avg</span>(math) <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> classId <span class="keyword">order</span> <span class="keyword">by</span> math <span class="keyword">rows</span> <span class="keyword">between</span> <span class="keyword">current</span> <span class="type">row</span> <span class="keyword">and</span> unbounded following) <span class="keyword">as</span> avg5,</span><br><span class="line">       <span class="comment">-- 符合条件的行，按照classId分组，每组数据按照math升序排列后，从起点行到当前行作为窗口聚合的行集</span></span><br><span class="line">       <span class="built_in">avg</span>(math) <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> classId <span class="keyword">order</span> <span class="keyword">by</span> math <span class="keyword">rows</span> <span class="keyword">between</span> unbounded preceding <span class="keyword">and</span> <span class="keyword">current</span> <span class="type">row</span> ) <span class="keyword">as</span> avg6</span><br><span class="line"><span class="keyword">from</span> student_scores</span><br><span class="line"><span class="keyword">where</span> departmentId <span class="operator">=</span> <span class="string">&#x27;department1&#x27;</span>;</span><br><span class="line">       </span><br><span class="line"><span class="operator">+</span><span class="comment">------------+-------+---------------+----------+--------------------+-------+--------------------+--------------------+--------------------+--------------------+</span></span><br><span class="line"><span class="operator">|</span> studentid  <span class="operator">|</span> math  <span class="operator">|</span> departmentid  <span class="operator">|</span> classid  <span class="operator">|</span>        avg1        <span class="operator">|</span> avg2  <span class="operator">|</span>        avg3        <span class="operator">|</span>        avg4        <span class="operator">|</span>        avg5        <span class="operator">|</span>        avg6        <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">------------+-------+---------------+----------+--------------------+-------+--------------------+--------------------+--------------------+--------------------+</span></span><br><span class="line"><span class="operator">|</span> <span class="number">111</span>        <span class="operator">|</span> <span class="number">69</span>    <span class="operator">|</span> department1   <span class="operator">|</span> class1   <span class="operator">|</span> <span class="number">79.77777777777777</span>  <span class="operator">|</span> <span class="number">82.0</span>  <span class="operator">|</span> <span class="number">69.0</span>               <span class="operator">|</span> <span class="number">74.33333333333333</span>  <span class="operator">|</span> <span class="number">82.0</span>               <span class="operator">|</span> <span class="number">69.0</span>               <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">113</span>        <span class="operator">|</span> <span class="number">74</span>    <span class="operator">|</span> department1   <span class="operator">|</span> class1   <span class="operator">|</span> <span class="number">79.77777777777777</span>  <span class="operator">|</span> <span class="number">82.0</span>  <span class="operator">|</span> <span class="number">71.5</span>               <span class="operator">|</span> <span class="number">79.0</span>               <span class="operator">|</span> <span class="number">85.25</span>              <span class="operator">|</span> <span class="number">71.5</span>               <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">112</span>        <span class="operator">|</span> <span class="number">80</span>    <span class="operator">|</span> department1   <span class="operator">|</span> class1   <span class="operator">|</span> <span class="number">79.77777777777777</span>  <span class="operator">|</span> <span class="number">82.0</span>  <span class="operator">|</span> <span class="number">74.33333333333333</span>  <span class="operator">|</span> <span class="number">85.25</span>              <span class="operator">|</span> <span class="number">89.0</span>               <span class="operator">|</span> <span class="number">74.33333333333333</span>  <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">115</span>        <span class="operator">|</span> <span class="number">93</span>    <span class="operator">|</span> department1   <span class="operator">|</span> class1   <span class="operator">|</span> <span class="number">79.77777777777777</span>  <span class="operator">|</span> <span class="number">82.0</span>  <span class="operator">|</span> <span class="number">79.0</span>               <span class="operator">|</span> <span class="number">89.0</span>               <span class="operator">|</span> <span class="number">93.5</span>               <span class="operator">|</span> <span class="number">79.0</span>               <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">114</span>        <span class="operator">|</span> <span class="number">94</span>    <span class="operator">|</span> department1   <span class="operator">|</span> class1   <span class="operator">|</span> <span class="number">79.77777777777777</span>  <span class="operator">|</span> <span class="number">82.0</span>  <span class="operator">|</span> <span class="number">82.0</span>               <span class="operator">|</span> <span class="number">93.5</span>               <span class="operator">|</span> <span class="number">94.0</span>               <span class="operator">|</span> <span class="number">82.0</span>               <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">124</span>        <span class="operator">|</span> <span class="number">70</span>    <span class="operator">|</span> department1   <span class="operator">|</span> class2   <span class="operator">|</span> <span class="number">79.77777777777777</span>  <span class="operator">|</span> <span class="number">77.0</span>  <span class="operator">|</span> <span class="number">70.0</span>               <span class="operator">|</span> <span class="number">74.0</span>               <span class="operator">|</span> <span class="number">77.0</span>               <span class="operator">|</span> <span class="number">70.0</span>               <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">121</span>        <span class="operator">|</span> <span class="number">74</span>    <span class="operator">|</span> department1   <span class="operator">|</span> class2   <span class="operator">|</span> <span class="number">79.77777777777777</span>  <span class="operator">|</span> <span class="number">77.0</span>  <span class="operator">|</span> <span class="number">72.0</span>               <span class="operator">|</span> <span class="number">77.0</span>               <span class="operator">|</span> <span class="number">79.33333333333333</span>  <span class="operator">|</span> <span class="number">72.0</span>               <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">123</span>        <span class="operator">|</span> <span class="number">78</span>    <span class="operator">|</span> department1   <span class="operator">|</span> class2   <span class="operator">|</span> <span class="number">79.77777777777777</span>  <span class="operator">|</span> <span class="number">77.0</span>  <span class="operator">|</span> <span class="number">74.0</span>               <span class="operator">|</span> <span class="number">79.33333333333333</span>  <span class="operator">|</span> <span class="number">82.0</span>               <span class="operator">|</span> <span class="number">74.0</span>               <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">122</span>        <span class="operator">|</span> <span class="number">86</span>    <span class="operator">|</span> department1   <span class="operator">|</span> class2   <span class="operator">|</span> <span class="number">79.77777777777777</span>  <span class="operator">|</span> <span class="number">77.0</span>  <span class="operator">|</span> <span class="number">77.0</span>               <span class="operator">|</span> <span class="number">82.0</span>               <span class="operator">|</span> <span class="number">86.0</span>               <span class="operator">|</span> <span class="number">77.0</span>               <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">------------+-------+---------------+----------+--------------------+-------+--------------------+--------------------+--------------------+--------------------+</span></span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>first_value <ul>
<li>作用：返回分区中的第一个值</li>
<li>sql示例  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> studentId,</span><br><span class="line">       math,</span><br><span class="line">       departmentId,</span><br><span class="line">       classId,</span><br><span class="line">       <span class="built_in">first_value</span>(math) <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> classId <span class="keyword">order</span> <span class="keyword">by</span> math) first_value_1,</span><br><span class="line">       <span class="built_in">first_value</span>(math) <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> classId <span class="keyword">order</span> <span class="keyword">by</span> math <span class="keyword">rows</span> <span class="keyword">between</span> <span class="number">1</span> preceding <span class="keyword">and</span> <span class="number">2</span> following) first_value_2</span><br><span class="line"><span class="keyword">from</span> student_scores</span><br><span class="line"><span class="keyword">where</span> departmentId <span class="operator">=</span> <span class="string">&#x27;department1&#x27;</span>;</span><br><span class="line"></span><br><span class="line"><span class="operator">+</span><span class="comment">------------+-------+---------------+----------+----------------+----------------+</span></span><br><span class="line"><span class="operator">|</span> studentid  <span class="operator">|</span> math  <span class="operator">|</span> departmentid  <span class="operator">|</span> classid  <span class="operator">|</span> first_value_1  <span class="operator">|</span> first_value_2  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">------------+-------+---------------+----------+----------------+----------------+</span></span><br><span class="line"><span class="operator">|</span> <span class="number">111</span>        <span class="operator">|</span> <span class="number">69</span>    <span class="operator">|</span> department1   <span class="operator">|</span> class1   <span class="operator">|</span> <span class="number">69</span>             <span class="operator">|</span> <span class="number">69</span>             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">113</span>        <span class="operator">|</span> <span class="number">74</span>    <span class="operator">|</span> department1   <span class="operator">|</span> class1   <span class="operator">|</span> <span class="number">69</span>             <span class="operator">|</span> <span class="number">69</span>             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">112</span>        <span class="operator">|</span> <span class="number">80</span>    <span class="operator">|</span> department1   <span class="operator">|</span> class1   <span class="operator">|</span> <span class="number">69</span>             <span class="operator">|</span> <span class="number">74</span>             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">115</span>        <span class="operator">|</span> <span class="number">93</span>    <span class="operator">|</span> department1   <span class="operator">|</span> class1   <span class="operator">|</span> <span class="number">69</span>             <span class="operator">|</span> <span class="number">80</span>             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">114</span>        <span class="operator">|</span> <span class="number">94</span>    <span class="operator">|</span> department1   <span class="operator">|</span> class1   <span class="operator">|</span> <span class="number">69</span>             <span class="operator">|</span> <span class="number">93</span>             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">124</span>        <span class="operator">|</span> <span class="number">70</span>    <span class="operator">|</span> department1   <span class="operator">|</span> class2   <span class="operator">|</span> <span class="number">70</span>             <span class="operator">|</span> <span class="number">70</span>             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">121</span>        <span class="operator">|</span> <span class="number">74</span>    <span class="operator">|</span> department1   <span class="operator">|</span> class2   <span class="operator">|</span> <span class="number">70</span>             <span class="operator">|</span> <span class="number">70</span>             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">123</span>        <span class="operator">|</span> <span class="number">78</span>    <span class="operator">|</span> department1   <span class="operator">|</span> class2   <span class="operator">|</span> <span class="number">70</span>             <span class="operator">|</span> <span class="number">74</span>             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">122</span>        <span class="operator">|</span> <span class="number">86</span>    <span class="operator">|</span> department1   <span class="operator">|</span> class2   <span class="operator">|</span> <span class="number">70</span>             <span class="operator">|</span> <span class="number">78</span>             <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">------------+-------+---------------+----------+----------------+----------------+</span></span><br></pre></td></tr></table></figure></li>
<li>解析  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">studentId=115为例</span><br><span class="line">first_value1：为分区class1中按照math排序的第一个值69(rows between未指定为第一行到当前行)，</span><br><span class="line">first_value2：为分区class2中按照math排序后当前行向前1行向后2行区间的第一个值80。</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>last_vlaue <ul>
<li>作用：返回分区最后一个值</li>
<li>示例sql  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="operator">+</span><span class="comment">------------+-------+---------------+----------+---------------+---------------+</span></span><br><span class="line"><span class="operator">|</span> studentid  <span class="operator">|</span> math  <span class="operator">|</span> departmentid  <span class="operator">|</span> classid  <span class="operator">|</span> last_value_1  <span class="operator">|</span> last_value_2  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">------------+-------+---------------+----------+---------------+---------------+</span></span><br><span class="line"><span class="operator">|</span> <span class="number">111</span>        <span class="operator">|</span> <span class="number">69</span>    <span class="operator">|</span> department1   <span class="operator">|</span> class1   <span class="operator">|</span> <span class="number">69</span>            <span class="operator">|</span> <span class="number">80</span>            <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">113</span>        <span class="operator">|</span> <span class="number">74</span>    <span class="operator">|</span> department1   <span class="operator">|</span> class1   <span class="operator">|</span> <span class="number">74</span>            <span class="operator">|</span> <span class="number">93</span>            <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">112</span>        <span class="operator">|</span> <span class="number">80</span>    <span class="operator">|</span> department1   <span class="operator">|</span> class1   <span class="operator">|</span> <span class="number">80</span>            <span class="operator">|</span> <span class="number">94</span>            <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">115</span>        <span class="operator">|</span> <span class="number">93</span>    <span class="operator">|</span> department1   <span class="operator">|</span> class1   <span class="operator">|</span> <span class="number">93</span>            <span class="operator">|</span> <span class="number">94</span>            <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">114</span>        <span class="operator">|</span> <span class="number">94</span>    <span class="operator">|</span> department1   <span class="operator">|</span> class1   <span class="operator">|</span> <span class="number">94</span>            <span class="operator">|</span> <span class="number">94</span>            <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">124</span>        <span class="operator">|</span> <span class="number">70</span>    <span class="operator">|</span> department1   <span class="operator">|</span> class2   <span class="operator">|</span> <span class="number">70</span>            <span class="operator">|</span> <span class="number">78</span>            <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">121</span>        <span class="operator">|</span> <span class="number">74</span>    <span class="operator">|</span> department1   <span class="operator">|</span> class2   <span class="operator">|</span> <span class="number">74</span>            <span class="operator">|</span> <span class="number">86</span>            <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">123</span>        <span class="operator">|</span> <span class="number">78</span>    <span class="operator">|</span> department1   <span class="operator">|</span> class2   <span class="operator">|</span> <span class="number">78</span>            <span class="operator">|</span> <span class="number">86</span>            <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">122</span>        <span class="operator">|</span> <span class="number">86</span>    <span class="operator">|</span> department1   <span class="operator">|</span> class2   <span class="operator">|</span> <span class="number">86</span>            <span class="operator">|</span> <span class="number">86</span>            <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">------------+-------+---------------+----------+---------------+---------------+</span></span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>lag <ul>
<li>作用：LAG(col, n, DEFAULT)用于统计窗口内向上第n行的值<ul>
<li>col：列名</li>
<li>n：向上n行，[可选，默认为1]</li>
<li>DEFAULT：当向上n行为NULL时，取默认值；如果不指定，则为NULL</li>
</ul>
</li>
<li>示例sql:  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> studentId,</span><br><span class="line">       math,</span><br><span class="line">       departmentId,</span><br><span class="line">       classId,</span><br><span class="line">       <span class="built_in">lag</span>(math,<span class="number">1</span>,<span class="number">60</span>) <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> classId <span class="keyword">order</span> <span class="keyword">by</span> math) lag_1,</span><br><span class="line">       <span class="built_in">lag</span>(math,<span class="number">2</span>) <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> classId <span class="keyword">order</span> <span class="keyword">by</span> math) lag_1</span><br><span class="line"><span class="keyword">from</span> student_scores</span><br><span class="line"><span class="keyword">where</span> departmentId <span class="operator">=</span> <span class="string">&#x27;department1&#x27;</span>;</span><br><span class="line"></span><br><span class="line"><span class="operator">+</span><span class="comment">------------+-------+---------------+----------+--------+--------+</span></span><br><span class="line"><span class="operator">|</span> studentid  <span class="operator">|</span> math  <span class="operator">|</span> departmentid  <span class="operator">|</span> classid  <span class="operator">|</span> lag_1  <span class="operator">|</span> lag_1  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">------------+-------+---------------+----------+--------+--------+</span></span><br><span class="line"><span class="operator">|</span> <span class="number">111</span>        <span class="operator">|</span> <span class="number">69</span>    <span class="operator">|</span> department1   <span class="operator">|</span> class1   <span class="operator">|</span> <span class="number">60</span>     <span class="operator">|</span> <span class="keyword">NULL</span>   <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">113</span>        <span class="operator">|</span> <span class="number">74</span>    <span class="operator">|</span> department1   <span class="operator">|</span> class1   <span class="operator">|</span> <span class="number">69</span>     <span class="operator">|</span> <span class="keyword">NULL</span>   <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">112</span>        <span class="operator">|</span> <span class="number">80</span>    <span class="operator">|</span> department1   <span class="operator">|</span> class1   <span class="operator">|</span> <span class="number">74</span>     <span class="operator">|</span> <span class="number">69</span>     <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">115</span>        <span class="operator">|</span> <span class="number">93</span>    <span class="operator">|</span> department1   <span class="operator">|</span> class1   <span class="operator">|</span> <span class="number">80</span>     <span class="operator">|</span> <span class="number">74</span>     <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">114</span>        <span class="operator">|</span> <span class="number">94</span>    <span class="operator">|</span> department1   <span class="operator">|</span> class1   <span class="operator">|</span> <span class="number">93</span>     <span class="operator">|</span> <span class="number">80</span>     <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">124</span>        <span class="operator">|</span> <span class="number">70</span>    <span class="operator">|</span> department1   <span class="operator">|</span> class2   <span class="operator">|</span> <span class="number">60</span>     <span class="operator">|</span> <span class="keyword">NULL</span>   <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">121</span>        <span class="operator">|</span> <span class="number">74</span>    <span class="operator">|</span> department1   <span class="operator">|</span> class2   <span class="operator">|</span> <span class="number">70</span>     <span class="operator">|</span> <span class="keyword">NULL</span>   <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">123</span>        <span class="operator">|</span> <span class="number">78</span>    <span class="operator">|</span> department1   <span class="operator">|</span> class2   <span class="operator">|</span> <span class="number">74</span>     <span class="operator">|</span> <span class="number">70</span>     <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">122</span>        <span class="operator">|</span> <span class="number">86</span>    <span class="operator">|</span> department1   <span class="operator">|</span> class2   <span class="operator">|</span> <span class="number">78</span>     <span class="operator">|</span> <span class="number">74</span>     <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">------------+-------+---------------+----------+--------+--------+</span></span><br></pre></td></tr></table></figure></li>
<li>解析  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">studentId=113，</span><br><span class="line">lag1为分区class1按照math排序后当前行向上2行的值NULL，但是设置了DEFUALT，所以为60，</span><br><span class="line">lag2因为没有设置DEFAULT，所以为NULL。</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>lead<ul>
<li>作用：LEAD(col, n, DEFAULT)与LAG相反，用于统计窗口内向下n行的值<ul>
<li>col：列名</li>
<li>n：向下n行，[可选，默认为1]</li>
<li>DEFAULT：当向下n行为NULL时，取默认值；如果不指定，则为NULL</li>
</ul>
</li>
<li>sql示例  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> studentId,</span><br><span class="line">       math,</span><br><span class="line">       departmentId,</span><br><span class="line">       classId,</span><br><span class="line">       <span class="built_in">lead</span>(math,<span class="number">1</span>,<span class="number">60</span>) <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> classId <span class="keyword">order</span> <span class="keyword">by</span> math) lead_1,</span><br><span class="line">       <span class="built_in">lead</span>(math,<span class="number">2</span>) <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> classId <span class="keyword">order</span> <span class="keyword">by</span> math) lead_1</span><br><span class="line"><span class="keyword">from</span> student_scores</span><br><span class="line"><span class="keyword">where</span> departmentId <span class="operator">=</span> <span class="string">&#x27;department1&#x27;</span>;</span><br><span class="line"></span><br><span class="line"><span class="operator">+</span><span class="comment">------------+-------+---------------+----------+--------+--------+</span></span><br><span class="line"><span class="operator">|</span> studentid  <span class="operator">|</span> math  <span class="operator">|</span> departmentid  <span class="operator">|</span> classid  <span class="operator">|</span> lead_1  <span class="operator">|</span> lead_1  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">------------+-------+---------------+----------+--------+--------+</span></span><br><span class="line"><span class="operator">|</span> <span class="number">111</span>        <span class="operator">|</span> <span class="number">69</span>    <span class="operator">|</span> department1   <span class="operator">|</span> class1   <span class="operator">|</span> <span class="number">74</span>     <span class="operator">|</span> <span class="number">80</span>     <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">113</span>        <span class="operator">|</span> <span class="number">74</span>    <span class="operator">|</span> department1   <span class="operator">|</span> class1   <span class="operator">|</span> <span class="number">80</span>     <span class="operator">|</span> <span class="number">93</span>     <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">112</span>        <span class="operator">|</span> <span class="number">80</span>    <span class="operator">|</span> department1   <span class="operator">|</span> class1   <span class="operator">|</span> <span class="number">93</span>     <span class="operator">|</span> <span class="number">94</span>     <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">115</span>        <span class="operator">|</span> <span class="number">93</span>    <span class="operator">|</span> department1   <span class="operator">|</span> class1   <span class="operator">|</span> <span class="number">94</span>     <span class="operator">|</span> <span class="keyword">NULL</span>   <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">114</span>        <span class="operator">|</span> <span class="number">94</span>    <span class="operator">|</span> department1   <span class="operator">|</span> class1   <span class="operator">|</span> <span class="number">60</span>     <span class="operator">|</span> <span class="keyword">NULL</span>   <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">124</span>        <span class="operator">|</span> <span class="number">70</span>    <span class="operator">|</span> department1   <span class="operator">|</span> class2   <span class="operator">|</span> <span class="number">74</span>     <span class="operator">|</span> <span class="number">78</span>     <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">121</span>        <span class="operator">|</span> <span class="number">74</span>    <span class="operator">|</span> department1   <span class="operator">|</span> class2   <span class="operator">|</span> <span class="number">78</span>     <span class="operator">|</span> <span class="number">86</span>     <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">123</span>        <span class="operator">|</span> <span class="number">78</span>    <span class="operator">|</span> department1   <span class="operator">|</span> class2   <span class="operator">|</span> <span class="number">86</span>     <span class="operator">|</span> <span class="keyword">NULL</span>   <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">122</span>        <span class="operator">|</span> <span class="number">86</span>    <span class="operator">|</span> department1   <span class="operator">|</span> class2   <span class="operator">|</span> <span class="number">60</span>     <span class="operator">|</span> <span class="keyword">NULL</span>   <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">------------+-------+---------------+----------+--------+--------+</span></span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>cume_dist<ul>
<li>作用：计算某个窗口或分区中某个值的累积分布。假定升序排序，则使用以下公式确定累积分布：(小于等于当前值的行数) / (分区内总行数)</li>
<li>sql示例  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> studentId,</span><br><span class="line">       math,</span><br><span class="line">       departmentId,</span><br><span class="line">       classId,</span><br><span class="line">       <span class="built_in">cume_dist</span>() <span class="keyword">over</span> (<span class="keyword">order</span> <span class="keyword">by</span> math)                      <span class="keyword">as</span> cume_dist1,</span><br><span class="line">       <span class="built_in">cume_dist</span>() <span class="keyword">over</span> (<span class="keyword">order</span> <span class="keyword">by</span> math <span class="keyword">desc</span>)                 <span class="keyword">as</span> cume_dist2,</span><br><span class="line">       <span class="built_in">cume_dist</span>() <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> classId <span class="keyword">order</span> <span class="keyword">by</span> math) <span class="keyword">as</span> cume_dist3</span><br><span class="line"><span class="keyword">from</span> student_scores</span><br><span class="line"><span class="keyword">where</span> departmentId <span class="operator">=</span> <span class="string">&#x27;department1&#x27;</span>;</span><br><span class="line"></span><br><span class="line"><span class="operator">+</span><span class="comment">------------+-------+---------------+----------+---------------------+---------------------+-------------+</span></span><br><span class="line"><span class="operator">|</span> studentid  <span class="operator">|</span> math  <span class="operator">|</span> departmentid  <span class="operator">|</span> classid  <span class="operator">|</span>     cume_dist1      <span class="operator">|</span>     cume_dist2      <span class="operator">|</span> cume_dist3  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">------------+-------+---------------+----------+---------------------+---------------------+-------------+</span></span><br><span class="line"><span class="operator">|</span> <span class="number">111</span>        <span class="operator">|</span> <span class="number">69</span>    <span class="operator">|</span> department1   <span class="operator">|</span> class1   <span class="operator">|</span> <span class="number">0.1111111111111111</span>  <span class="operator">|</span> <span class="number">1.0</span>                 <span class="operator">|</span> <span class="number">0.2</span>         <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">113</span>        <span class="operator">|</span> <span class="number">74</span>    <span class="operator">|</span> department1   <span class="operator">|</span> class1   <span class="operator">|</span> <span class="number">0.4444444444444444</span>  <span class="operator">|</span> <span class="number">0.7777777777777778</span>  <span class="operator">|</span> <span class="number">0.4</span>         <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">112</span>        <span class="operator">|</span> <span class="number">80</span>    <span class="operator">|</span> department1   <span class="operator">|</span> class1   <span class="operator">|</span> <span class="number">0.6666666666666666</span>  <span class="operator">|</span> <span class="number">0.4444444444444444</span>  <span class="operator">|</span> <span class="number">0.6</span>         <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">115</span>        <span class="operator">|</span> <span class="number">93</span>    <span class="operator">|</span> department1   <span class="operator">|</span> class1   <span class="operator">|</span> <span class="number">0.8888888888888888</span>  <span class="operator">|</span> <span class="number">0.2222222222222222</span>  <span class="operator">|</span> <span class="number">0.8</span>         <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">114</span>        <span class="operator">|</span> <span class="number">94</span>    <span class="operator">|</span> department1   <span class="operator">|</span> class1   <span class="operator">|</span> <span class="number">1.0</span>                 <span class="operator">|</span> <span class="number">0.1111111111111111</span>  <span class="operator">|</span> <span class="number">1.0</span>         <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">124</span>        <span class="operator">|</span> <span class="number">70</span>    <span class="operator">|</span> department1   <span class="operator">|</span> class2   <span class="operator">|</span> <span class="number">0.2222222222222222</span>  <span class="operator">|</span> <span class="number">0.8888888888888888</span>  <span class="operator">|</span> <span class="number">0.25</span>        <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">121</span>        <span class="operator">|</span> <span class="number">74</span>    <span class="operator">|</span> department1   <span class="operator">|</span> class2   <span class="operator">|</span> <span class="number">0.4444444444444444</span>  <span class="operator">|</span> <span class="number">0.7777777777777778</span>  <span class="operator">|</span> <span class="number">0.5</span>         <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">123</span>        <span class="operator">|</span> <span class="number">78</span>    <span class="operator">|</span> department1   <span class="operator">|</span> class2   <span class="operator">|</span> <span class="number">0.5555555555555556</span>  <span class="operator">|</span> <span class="number">0.5555555555555556</span>  <span class="operator">|</span> <span class="number">0.75</span>        <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">122</span>        <span class="operator">|</span> <span class="number">86</span>    <span class="operator">|</span> department1   <span class="operator">|</span> class2   <span class="operator">|</span> <span class="number">0.7777777777777778</span>  <span class="operator">|</span> <span class="number">0.3333333333333333</span>  <span class="operator">|</span> <span class="number">1.0</span>         <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">------------+-------+---------------+----------+---------------------+---------------------+-------------+</span></span><br></pre></td></tr></table></figure></li>
<li>结果解析  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">studentId=115为例</span><br><span class="line">cume_dist1=小于等于93的行数8/总行数9=0.8888888888888888</span><br><span class="line">cume_dist2=大于等于93的行数2/总行数9=0.2222222222222222</span><br><span class="line">cume_dist3=class1分区内小于等于93的行数4/总行数5=0.8</span><br></pre></td></tr></table></figure>
<h4 id="8-2-5-3-排序开窗函数"><a href="#8-2-5-3-排序开窗函数" class="headerlink" title="8.2.5.3 排序开窗函数"></a>8.2.5.3 排序开窗函数</h4></li>
</ul>
</li>
<li>row_number<ul>
<li>作用：row_number() over([partition by col1] [order by col2])开窗函数是基于over子句中order by列的一个排名。在窗口或分区内从1开始排序，即使遇到col2相等时，名次依旧增加。例如：有两条记录相等，但一个是第一，一个是第二</li>
<li>sql示例  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> id,</span><br><span class="line">       studentId,</span><br><span class="line">       <span class="keyword">language</span>,</span><br><span class="line">       math,</span><br><span class="line">       english,</span><br><span class="line">       classId,</span><br><span class="line">       departmentId,</span><br><span class="line">       <span class="built_in">row_number</span>() <span class="keyword">over</span> (<span class="keyword">order</span> <span class="keyword">by</span> math) <span class="keyword">as</span> math_order,</span><br><span class="line">       <span class="built_in">row_number</span>() <span class="keyword">over</span> (<span class="keyword">order</span> <span class="keyword">by</span> english) <span class="keyword">as</span> english_order,</span><br><span class="line">       <span class="built_in">row_number</span>() <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> classId <span class="keyword">order</span> <span class="keyword">by</span> <span class="keyword">language</span>) <span class="keyword">as</span> class_language_order</span><br><span class="line"><span class="keyword">from</span> student_scores;</span><br><span class="line"></span><br><span class="line"><span class="operator">+</span><span class="comment">-----+------------+-----------+-------+----------+----------+---------------+------------+---------------+----------------------+</span></span><br><span class="line"><span class="operator">|</span> id  <span class="operator">|</span> studentid  <span class="operator">|</span> <span class="keyword">language</span>  <span class="operator">|</span> math  <span class="operator">|</span> english  <span class="operator">|</span> classid  <span class="operator">|</span> departmentid  <span class="operator">|</span> math_order  <span class="operator">|</span> english_order  <span class="operator">|</span> class_language_order  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-----+------------+-----------+-------+----------+----------+---------------+------------+---------------+----------------------+</span></span><br><span class="line"><span class="operator">|</span> <span class="number">1</span>   <span class="operator">|</span> <span class="number">111</span>        <span class="operator">|</span> <span class="number">68</span>        <span class="operator">|</span> <span class="number">69</span>    <span class="operator">|</span> <span class="number">90</span>       <span class="operator">|</span> class1   <span class="operator">|</span> department1   <span class="operator">|</span> <span class="number">1</span>          <span class="operator">|</span> <span class="number">14</span>            <span class="operator">|</span> <span class="number">1</span>                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">12</span>  <span class="operator">|</span> <span class="number">213</span>        <span class="operator">|</span> <span class="number">71</span>        <span class="operator">|</span> <span class="number">94</span>    <span class="operator">|</span> <span class="number">90</span>       <span class="operator">|</span> class1   <span class="operator">|</span> department2   <span class="operator">|</span> <span class="number">17</span>         <span class="operator">|</span> <span class="number">15</span>            <span class="operator">|</span> <span class="number">2</span>                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">2</span>   <span class="operator">|</span> <span class="number">112</span>        <span class="operator">|</span> <span class="number">73</span>        <span class="operator">|</span> <span class="number">80</span>    <span class="operator">|</span> <span class="number">96</span>       <span class="operator">|</span> class1   <span class="operator">|</span> department1   <span class="operator">|</span> <span class="number">10</span>         <span class="operator">|</span> <span class="number">20</span>            <span class="operator">|</span> <span class="number">3</span>                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">11</span>  <span class="operator">|</span> <span class="number">212</span>        <span class="operator">|</span> <span class="number">76</span>        <span class="operator">|</span> <span class="number">83</span>    <span class="operator">|</span> <span class="number">75</span>       <span class="operator">|</span> class1   <span class="operator">|</span> department2   <span class="operator">|</span> <span class="number">12</span>         <span class="operator">|</span> <span class="number">8</span>             <span class="operator">|</span> <span class="number">4</span>                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">14</span>  <span class="operator">|</span> <span class="number">215</span>        <span class="operator">|</span> <span class="number">84</span>        <span class="operator">|</span> <span class="number">82</span>    <span class="operator">|</span> <span class="number">73</span>       <span class="operator">|</span> class1   <span class="operator">|</span> department2   <span class="operator">|</span> <span class="number">11</span>         <span class="operator">|</span> <span class="number">6</span>             <span class="operator">|</span> <span class="number">5</span>                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">15</span>  <span class="operator">|</span> <span class="number">216</span>        <span class="operator">|</span> <span class="number">85</span>        <span class="operator">|</span> <span class="number">74</span>    <span class="operator">|</span> <span class="number">93</span>       <span class="operator">|</span> class1   <span class="operator">|</span> department2   <span class="operator">|</span> <span class="number">4</span>          <span class="operator">|</span> <span class="number">16</span>            <span class="operator">|</span> <span class="number">6</span>                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">10</span>  <span class="operator">|</span> <span class="number">211</span>        <span class="operator">|</span> <span class="number">89</span>        <span class="operator">|</span> <span class="number">93</span>    <span class="operator">|</span> <span class="number">60</span>       <span class="operator">|</span> class1   <span class="operator">|</span> department2   <span class="operator">|</span> <span class="number">16</span>         <span class="operator">|</span> <span class="number">1</span>             <span class="operator">|</span> <span class="number">7</span>                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">4</span>   <span class="operator">|</span> <span class="number">114</span>        <span class="operator">|</span> <span class="number">89</span>        <span class="operator">|</span> <span class="number">94</span>    <span class="operator">|</span> <span class="number">93</span>       <span class="operator">|</span> class1   <span class="operator">|</span> department1   <span class="operator">|</span> <span class="number">18</span>         <span class="operator">|</span> <span class="number">17</span>            <span class="operator">|</span> <span class="number">8</span>                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">3</span>   <span class="operator">|</span> <span class="number">113</span>        <span class="operator">|</span> <span class="number">90</span>        <span class="operator">|</span> <span class="number">74</span>    <span class="operator">|</span> <span class="number">75</span>       <span class="operator">|</span> class1   <span class="operator">|</span> department1   <span class="operator">|</span> <span class="number">5</span>          <span class="operator">|</span> <span class="number">7</span>             <span class="operator">|</span> <span class="number">9</span>                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">13</span>  <span class="operator">|</span> <span class="number">214</span>        <span class="operator">|</span> <span class="number">94</span>        <span class="operator">|</span> <span class="number">94</span>    <span class="operator">|</span> <span class="number">66</span>       <span class="operator">|</span> class1   <span class="operator">|</span> department2   <span class="operator">|</span> <span class="number">19</span>         <span class="operator">|</span> <span class="number">5</span>             <span class="operator">|</span> <span class="number">10</span>                   <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">5</span>   <span class="operator">|</span> <span class="number">115</span>        <span class="operator">|</span> <span class="number">99</span>        <span class="operator">|</span> <span class="number">93</span>    <span class="operator">|</span> <span class="number">89</span>       <span class="operator">|</span> class1   <span class="operator">|</span> department1   <span class="operator">|</span> <span class="number">15</span>         <span class="operator">|</span> <span class="number">13</span>            <span class="operator">|</span> <span class="number">11</span>                   <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">8</span>   <span class="operator">|</span> <span class="number">123</span>        <span class="operator">|</span> <span class="number">70</span>        <span class="operator">|</span> <span class="number">78</span>    <span class="operator">|</span> <span class="number">61</span>       <span class="operator">|</span> class2   <span class="operator">|</span> department1   <span class="operator">|</span> <span class="number">7</span>          <span class="operator">|</span> <span class="number">2</span>             <span class="operator">|</span> <span class="number">1</span>                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">19</span>  <span class="operator">|</span> <span class="number">224</span>        <span class="operator">|</span> <span class="number">75</span>        <span class="operator">|</span> <span class="number">80</span>    <span class="operator">|</span> <span class="number">78</span>       <span class="operator">|</span> class2   <span class="operator">|</span> department2   <span class="operator">|</span> <span class="number">9</span>          <span class="operator">|</span> <span class="number">10</span>            <span class="operator">|</span> <span class="number">2</span>                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">9</span>   <span class="operator">|</span> <span class="number">124</span>        <span class="operator">|</span> <span class="number">76</span>        <span class="operator">|</span> <span class="number">70</span>    <span class="operator">|</span> <span class="number">76</span>       <span class="operator">|</span> class2   <span class="operator">|</span> department1   <span class="operator">|</span> <span class="number">2</span>          <span class="operator">|</span> <span class="number">9</span>             <span class="operator">|</span> <span class="number">3</span>                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">16</span>  <span class="operator">|</span> <span class="number">221</span>        <span class="operator">|</span> <span class="number">77</span>        <span class="operator">|</span> <span class="number">99</span>    <span class="operator">|</span> <span class="number">61</span>       <span class="operator">|</span> class2   <span class="operator">|</span> department2   <span class="operator">|</span> <span class="number">20</span>         <span class="operator">|</span> <span class="number">3</span>             <span class="operator">|</span> <span class="number">4</span>                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">18</span>  <span class="operator">|</span> <span class="number">223</span>        <span class="operator">|</span> <span class="number">79</span>        <span class="operator">|</span> <span class="number">74</span>    <span class="operator">|</span> <span class="number">96</span>       <span class="operator">|</span> class2   <span class="operator">|</span> department2   <span class="operator">|</span> <span class="number">3</span>          <span class="operator">|</span> <span class="number">19</span>            <span class="operator">|</span> <span class="number">5</span>                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">17</span>  <span class="operator">|</span> <span class="number">222</span>        <span class="operator">|</span> <span class="number">80</span>        <span class="operator">|</span> <span class="number">78</span>    <span class="operator">|</span> <span class="number">96</span>       <span class="operator">|</span> class2   <span class="operator">|</span> department2   <span class="operator">|</span> <span class="number">8</span>          <span class="operator">|</span> <span class="number">18</span>            <span class="operator">|</span> <span class="number">6</span>                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">20</span>  <span class="operator">|</span> <span class="number">225</span>        <span class="operator">|</span> <span class="number">82</span>        <span class="operator">|</span> <span class="number">85</span>    <span class="operator">|</span> <span class="number">63</span>       <span class="operator">|</span> class2   <span class="operator">|</span> department2   <span class="operator">|</span> <span class="number">13</span>         <span class="operator">|</span> <span class="number">4</span>             <span class="operator">|</span> <span class="number">7</span>                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">7</span>   <span class="operator">|</span> <span class="number">122</span>        <span class="operator">|</span> <span class="number">89</span>        <span class="operator">|</span> <span class="number">86</span>    <span class="operator">|</span> <span class="number">85</span>       <span class="operator">|</span> class2   <span class="operator">|</span> department1   <span class="operator">|</span> <span class="number">14</span>         <span class="operator">|</span> <span class="number">12</span>            <span class="operator">|</span> <span class="number">8</span>                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">6</span>   <span class="operator">|</span> <span class="number">121</span>        <span class="operator">|</span> <span class="number">96</span>        <span class="operator">|</span> <span class="number">74</span>    <span class="operator">|</span> <span class="number">79</span>       <span class="operator">|</span> class2   <span class="operator">|</span> department1   <span class="operator">|</span> <span class="number">6</span>          <span class="operator">|</span> <span class="number">11</span>            <span class="operator">|</span> <span class="number">9</span>                    <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-----+------------+-----------+-------+----------+----------+---------------+------------+---------------+----------------------+</span></span><br></pre></td></tr></table></figure></li>
<li>结果解析   <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">math_order: 以match排序</span><br><span class="line">english_order: 以english排序</span><br><span class="line">class_order: 以classId,departmentId分组后组内排序</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>rank<ul>
<li>作用：rank() over([partition by col1] [order by col2])，当遇到col2相等时，名次相同，但是下一个col2值的名次递增N（N是重复的次数）。例如：有两条记录是并列第一，下一个是第三，没有第二。</li>
<li>sql示例  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> id,</span><br><span class="line">       studentId,</span><br><span class="line">       <span class="keyword">language</span>,</span><br><span class="line">       math,</span><br><span class="line">       english,</span><br><span class="line">       classId,</span><br><span class="line">       departmentId,</span><br><span class="line">       <span class="built_in">rank</span>() <span class="keyword">over</span> (<span class="keyword">order</span> <span class="keyword">by</span> math) <span class="keyword">as</span> math_rank,</span><br><span class="line">       <span class="built_in">rank</span>() <span class="keyword">over</span> (<span class="keyword">order</span> <span class="keyword">by</span> english) <span class="keyword">as</span> english_rank,</span><br><span class="line">       <span class="built_in">rank</span>() <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> classId,departmentId <span class="keyword">order</span> <span class="keyword">by</span> <span class="keyword">language</span> ) <span class="keyword">as</span> class_language_rank</span><br><span class="line"><span class="keyword">from</span> student_scores;</span><br><span class="line"></span><br><span class="line"><span class="operator">+</span><span class="comment">-----+------------+-----------+-------+----------+----------+---------------+------------+---------------+----------------------+</span></span><br><span class="line"><span class="operator">|</span> id  <span class="operator">|</span> studentid  <span class="operator">|</span> <span class="keyword">language</span>  <span class="operator">|</span> math  <span class="operator">|</span> english  <span class="operator">|</span> classid  <span class="operator">|</span> departmentid  <span class="operator">|</span> math_rank  <span class="operator">|</span> english_rank  <span class="operator">|</span> class_language_rank  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-----+------------+-----------+-------+----------+----------+---------------+------------+---------------+----------------------+</span></span><br><span class="line"><span class="operator">|</span> <span class="number">1</span>   <span class="operator">|</span> <span class="number">111</span>        <span class="operator">|</span> <span class="number">68</span>        <span class="operator">|</span> <span class="number">69</span>    <span class="operator">|</span> <span class="number">90</span>       <span class="operator">|</span> class1   <span class="operator">|</span> department1   <span class="operator">|</span> <span class="number">1</span>          <span class="operator">|</span> <span class="number">14</span>            <span class="operator">|</span> <span class="number">1</span>                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">2</span>   <span class="operator">|</span> <span class="number">112</span>        <span class="operator">|</span> <span class="number">73</span>        <span class="operator">|</span> <span class="number">80</span>    <span class="operator">|</span> <span class="number">96</span>       <span class="operator">|</span> class1   <span class="operator">|</span> department1   <span class="operator">|</span> <span class="number">9</span>          <span class="operator">|</span> <span class="number">18</span>            <span class="operator">|</span> <span class="number">2</span>                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">4</span>   <span class="operator">|</span> <span class="number">114</span>        <span class="operator">|</span> <span class="number">89</span>        <span class="operator">|</span> <span class="number">94</span>    <span class="operator">|</span> <span class="number">93</span>       <span class="operator">|</span> class1   <span class="operator">|</span> department1   <span class="operator">|</span> <span class="number">17</span>         <span class="operator">|</span> <span class="number">16</span>            <span class="operator">|</span> <span class="number">3</span>                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">3</span>   <span class="operator">|</span> <span class="number">113</span>        <span class="operator">|</span> <span class="number">90</span>        <span class="operator">|</span> <span class="number">74</span>    <span class="operator">|</span> <span class="number">75</span>       <span class="operator">|</span> class1   <span class="operator">|</span> department1   <span class="operator">|</span> <span class="number">3</span>          <span class="operator">|</span> <span class="number">7</span>             <span class="operator">|</span> <span class="number">4</span>                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">5</span>   <span class="operator">|</span> <span class="number">115</span>        <span class="operator">|</span> <span class="number">99</span>        <span class="operator">|</span> <span class="number">93</span>    <span class="operator">|</span> <span class="number">89</span>       <span class="operator">|</span> class1   <span class="operator">|</span> department1   <span class="operator">|</span> <span class="number">15</span>         <span class="operator">|</span> <span class="number">13</span>            <span class="operator">|</span> <span class="number">5</span>                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">12</span>  <span class="operator">|</span> <span class="number">213</span>        <span class="operator">|</span> <span class="number">71</span>        <span class="operator">|</span> <span class="number">94</span>    <span class="operator">|</span> <span class="number">90</span>       <span class="operator">|</span> class1   <span class="operator">|</span> department2   <span class="operator">|</span> <span class="number">17</span>         <span class="operator">|</span> <span class="number">14</span>            <span class="operator">|</span> <span class="number">1</span>                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">11</span>  <span class="operator">|</span> <span class="number">212</span>        <span class="operator">|</span> <span class="number">76</span>        <span class="operator">|</span> <span class="number">83</span>    <span class="operator">|</span> <span class="number">75</span>       <span class="operator">|</span> class1   <span class="operator">|</span> department2   <span class="operator">|</span> <span class="number">12</span>         <span class="operator">|</span> <span class="number">7</span>             <span class="operator">|</span> <span class="number">2</span>                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">14</span>  <span class="operator">|</span> <span class="number">215</span>        <span class="operator">|</span> <span class="number">84</span>        <span class="operator">|</span> <span class="number">82</span>    <span class="operator">|</span> <span class="number">73</span>       <span class="operator">|</span> class1   <span class="operator">|</span> department2   <span class="operator">|</span> <span class="number">11</span>         <span class="operator">|</span> <span class="number">6</span>             <span class="operator">|</span> <span class="number">3</span>                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">15</span>  <span class="operator">|</span> <span class="number">216</span>        <span class="operator">|</span> <span class="number">85</span>        <span class="operator">|</span> <span class="number">74</span>    <span class="operator">|</span> <span class="number">93</span>       <span class="operator">|</span> class1   <span class="operator">|</span> department2   <span class="operator">|</span> <span class="number">3</span>          <span class="operator">|</span> <span class="number">16</span>            <span class="operator">|</span> <span class="number">4</span>                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">10</span>  <span class="operator">|</span> <span class="number">211</span>        <span class="operator">|</span> <span class="number">89</span>        <span class="operator">|</span> <span class="number">93</span>    <span class="operator">|</span> <span class="number">60</span>       <span class="operator">|</span> class1   <span class="operator">|</span> department2   <span class="operator">|</span> <span class="number">15</span>         <span class="operator">|</span> <span class="number">1</span>             <span class="operator">|</span> <span class="number">5</span>                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">13</span>  <span class="operator">|</span> <span class="number">214</span>        <span class="operator">|</span> <span class="number">94</span>        <span class="operator">|</span> <span class="number">94</span>    <span class="operator">|</span> <span class="number">66</span>       <span class="operator">|</span> class1   <span class="operator">|</span> department2   <span class="operator">|</span> <span class="number">17</span>         <span class="operator">|</span> <span class="number">5</span>             <span class="operator">|</span> <span class="number">6</span>                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">8</span>   <span class="operator">|</span> <span class="number">123</span>        <span class="operator">|</span> <span class="number">70</span>        <span class="operator">|</span> <span class="number">78</span>    <span class="operator">|</span> <span class="number">61</span>       <span class="operator">|</span> class2   <span class="operator">|</span> department1   <span class="operator">|</span> <span class="number">7</span>          <span class="operator">|</span> <span class="number">2</span>             <span class="operator">|</span> <span class="number">1</span>                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">9</span>   <span class="operator">|</span> <span class="number">124</span>        <span class="operator">|</span> <span class="number">76</span>        <span class="operator">|</span> <span class="number">70</span>    <span class="operator">|</span> <span class="number">76</span>       <span class="operator">|</span> class2   <span class="operator">|</span> department1   <span class="operator">|</span> <span class="number">2</span>          <span class="operator">|</span> <span class="number">9</span>             <span class="operator">|</span> <span class="number">2</span>                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">7</span>   <span class="operator">|</span> <span class="number">122</span>        <span class="operator">|</span> <span class="number">89</span>        <span class="operator">|</span> <span class="number">86</span>    <span class="operator">|</span> <span class="number">85</span>       <span class="operator">|</span> class2   <span class="operator">|</span> department1   <span class="operator">|</span> <span class="number">14</span>         <span class="operator">|</span> <span class="number">12</span>            <span class="operator">|</span> <span class="number">3</span>                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">6</span>   <span class="operator">|</span> <span class="number">121</span>        <span class="operator">|</span> <span class="number">96</span>        <span class="operator">|</span> <span class="number">74</span>    <span class="operator">|</span> <span class="number">79</span>       <span class="operator">|</span> class2   <span class="operator">|</span> department1   <span class="operator">|</span> <span class="number">3</span>          <span class="operator">|</span> <span class="number">11</span>            <span class="operator">|</span> <span class="number">4</span>                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">19</span>  <span class="operator">|</span> <span class="number">224</span>        <span class="operator">|</span> <span class="number">75</span>        <span class="operator">|</span> <span class="number">80</span>    <span class="operator">|</span> <span class="number">78</span>       <span class="operator">|</span> class2   <span class="operator">|</span> department2   <span class="operator">|</span> <span class="number">9</span>          <span class="operator">|</span> <span class="number">10</span>            <span class="operator">|</span> <span class="number">1</span>                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">16</span>  <span class="operator">|</span> <span class="number">221</span>        <span class="operator">|</span> <span class="number">77</span>        <span class="operator">|</span> <span class="number">99</span>    <span class="operator">|</span> <span class="number">61</span>       <span class="operator">|</span> class2   <span class="operator">|</span> department2   <span class="operator">|</span> <span class="number">20</span>         <span class="operator">|</span> <span class="number">2</span>             <span class="operator">|</span> <span class="number">2</span>                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">18</span>  <span class="operator">|</span> <span class="number">223</span>        <span class="operator">|</span> <span class="number">79</span>        <span class="operator">|</span> <span class="number">74</span>    <span class="operator">|</span> <span class="number">96</span>       <span class="operator">|</span> class2   <span class="operator">|</span> department2   <span class="operator">|</span> <span class="number">3</span>          <span class="operator">|</span> <span class="number">18</span>            <span class="operator">|</span> <span class="number">3</span>                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">17</span>  <span class="operator">|</span> <span class="number">222</span>        <span class="operator">|</span> <span class="number">80</span>        <span class="operator">|</span> <span class="number">78</span>    <span class="operator">|</span> <span class="number">96</span>       <span class="operator">|</span> class2   <span class="operator">|</span> department2   <span class="operator">|</span> <span class="number">7</span>          <span class="operator">|</span> <span class="number">18</span>            <span class="operator">|</span> <span class="number">4</span>                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">20</span>  <span class="operator">|</span> <span class="number">225</span>        <span class="operator">|</span> <span class="number">82</span>        <span class="operator">|</span> <span class="number">85</span>    <span class="operator">|</span> <span class="number">63</span>       <span class="operator">|</span> class2   <span class="operator">|</span> department2   <span class="operator">|</span> <span class="number">13</span>         <span class="operator">|</span> <span class="number">4</span>             <span class="operator">|</span> <span class="number">5</span>                    <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-----+------------+-----------+-------+----------+----------+---------------+------------+---------------+----------------------+</span></span><br></pre></td></tr></table></figure></li>
<li>结果解析  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">math_rank: 以match排序 ,student_id：113、216、121、223 math相同，排名都为3，123序号=7</span><br><span class="line">class_language_rank: 以classId,departmentId分组后组内排序</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>dense_rank<ul>
<li>作用：dense_rank() over([partition by col1] [order by col2])与rank类似，当遇到col2相等时，名次同样相等，不同的是，下一个col2值的名次+1，而不是+N。</li>
<li>sql示例  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> id,</span><br><span class="line">       studentId,</span><br><span class="line">       <span class="keyword">language</span>,</span><br><span class="line">       math,</span><br><span class="line">       english,</span><br><span class="line">       classId,</span><br><span class="line">       departmentId,</span><br><span class="line">       <span class="built_in">dense_rank</span>() <span class="keyword">over</span> (<span class="keyword">order</span> <span class="keyword">by</span> math) <span class="keyword">as</span> math_dense_rank,</span><br><span class="line">       <span class="built_in">dense_rank</span>() <span class="keyword">over</span> (<span class="keyword">order</span> <span class="keyword">by</span> english) <span class="keyword">as</span> english_dense_rank,</span><br><span class="line">       <span class="built_in">dense_rank</span>() <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> classId,departmentId <span class="keyword">order</span> <span class="keyword">by</span> <span class="keyword">language</span> ) <span class="keyword">as</span> class_language_dense_rank</span><br><span class="line"><span class="keyword">from</span> student_scores;</span><br><span class="line"></span><br><span class="line"><span class="operator">+</span><span class="comment">-----+------------+-----------+-------+----------+----------+---------------+------------------+---------------------+----------------------------+</span></span><br><span class="line"><span class="operator">|</span> id  <span class="operator">|</span> studentid  <span class="operator">|</span> <span class="keyword">language</span>  <span class="operator">|</span> math  <span class="operator">|</span> english  <span class="operator">|</span> classid  <span class="operator">|</span> departmentid  <span class="operator">|</span> math_dense_rank  <span class="operator">|</span> english_dense_rank  <span class="operator">|</span> class_language_dense_rank  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-----+------------+-----------+-------+----------+----------+---------------+------------------+---------------------+----------------------------+</span></span><br><span class="line"><span class="operator">|</span> <span class="number">1</span>   <span class="operator">|</span> <span class="number">111</span>        <span class="operator">|</span> <span class="number">68</span>        <span class="operator">|</span> <span class="number">69</span>    <span class="operator">|</span> <span class="number">90</span>       <span class="operator">|</span> class1   <span class="operator">|</span> department1   <span class="operator">|</span> <span class="number">1</span>                <span class="operator">|</span> <span class="number">12</span>                  <span class="operator">|</span> <span class="number">1</span>                          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">2</span>   <span class="operator">|</span> <span class="number">112</span>        <span class="operator">|</span> <span class="number">73</span>        <span class="operator">|</span> <span class="number">80</span>    <span class="operator">|</span> <span class="number">96</span>       <span class="operator">|</span> class1   <span class="operator">|</span> department1   <span class="operator">|</span> <span class="number">5</span>                <span class="operator">|</span> <span class="number">14</span>                  <span class="operator">|</span> <span class="number">2</span>                          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">4</span>   <span class="operator">|</span> <span class="number">114</span>        <span class="operator">|</span> <span class="number">89</span>        <span class="operator">|</span> <span class="number">94</span>    <span class="operator">|</span> <span class="number">93</span>       <span class="operator">|</span> class1   <span class="operator">|</span> department1   <span class="operator">|</span> <span class="number">11</span>               <span class="operator">|</span> <span class="number">13</span>                  <span class="operator">|</span> <span class="number">3</span>                          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">3</span>   <span class="operator">|</span> <span class="number">113</span>        <span class="operator">|</span> <span class="number">90</span>        <span class="operator">|</span> <span class="number">74</span>    <span class="operator">|</span> <span class="number">75</span>       <span class="operator">|</span> class1   <span class="operator">|</span> department1   <span class="operator">|</span> <span class="number">3</span>                <span class="operator">|</span> <span class="number">6</span>                   <span class="operator">|</span> <span class="number">4</span>                          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">5</span>   <span class="operator">|</span> <span class="number">115</span>        <span class="operator">|</span> <span class="number">99</span>        <span class="operator">|</span> <span class="number">93</span>    <span class="operator">|</span> <span class="number">89</span>       <span class="operator">|</span> class1   <span class="operator">|</span> department1   <span class="operator">|</span> <span class="number">10</span>               <span class="operator">|</span> <span class="number">11</span>                  <span class="operator">|</span> <span class="number">5</span>                          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">12</span>  <span class="operator">|</span> <span class="number">213</span>        <span class="operator">|</span> <span class="number">71</span>        <span class="operator">|</span> <span class="number">94</span>    <span class="operator">|</span> <span class="number">90</span>       <span class="operator">|</span> class1   <span class="operator">|</span> department2   <span class="operator">|</span> <span class="number">11</span>               <span class="operator">|</span> <span class="number">12</span>                  <span class="operator">|</span> <span class="number">1</span>                          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">11</span>  <span class="operator">|</span> <span class="number">212</span>        <span class="operator">|</span> <span class="number">76</span>        <span class="operator">|</span> <span class="number">83</span>    <span class="operator">|</span> <span class="number">75</span>       <span class="operator">|</span> class1   <span class="operator">|</span> department2   <span class="operator">|</span> <span class="number">7</span>                <span class="operator">|</span> <span class="number">6</span>                   <span class="operator">|</span> <span class="number">2</span>                          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">14</span>  <span class="operator">|</span> <span class="number">215</span>        <span class="operator">|</span> <span class="number">84</span>        <span class="operator">|</span> <span class="number">82</span>    <span class="operator">|</span> <span class="number">73</span>       <span class="operator">|</span> class1   <span class="operator">|</span> department2   <span class="operator">|</span> <span class="number">6</span>                <span class="operator">|</span> <span class="number">5</span>                   <span class="operator">|</span> <span class="number">3</span>                          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">15</span>  <span class="operator">|</span> <span class="number">216</span>        <span class="operator">|</span> <span class="number">85</span>        <span class="operator">|</span> <span class="number">74</span>    <span class="operator">|</span> <span class="number">93</span>       <span class="operator">|</span> class1   <span class="operator">|</span> department2   <span class="operator">|</span> <span class="number">3</span>                <span class="operator">|</span> <span class="number">13</span>                  <span class="operator">|</span> <span class="number">4</span>                          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">10</span>  <span class="operator">|</span> <span class="number">211</span>        <span class="operator">|</span> <span class="number">89</span>        <span class="operator">|</span> <span class="number">93</span>    <span class="operator">|</span> <span class="number">60</span>       <span class="operator">|</span> class1   <span class="operator">|</span> department2   <span class="operator">|</span> <span class="number">10</span>               <span class="operator">|</span> <span class="number">1</span>                   <span class="operator">|</span> <span class="number">5</span>                          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">13</span>  <span class="operator">|</span> <span class="number">214</span>        <span class="operator">|</span> <span class="number">94</span>        <span class="operator">|</span> <span class="number">94</span>    <span class="operator">|</span> <span class="number">66</span>       <span class="operator">|</span> class1   <span class="operator">|</span> department2   <span class="operator">|</span> <span class="number">11</span>               <span class="operator">|</span> <span class="number">4</span>                   <span class="operator">|</span> <span class="number">6</span>                          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">8</span>   <span class="operator">|</span> <span class="number">123</span>        <span class="operator">|</span> <span class="number">70</span>        <span class="operator">|</span> <span class="number">78</span>    <span class="operator">|</span> <span class="number">61</span>       <span class="operator">|</span> class2   <span class="operator">|</span> department1   <span class="operator">|</span> <span class="number">4</span>                <span class="operator">|</span> <span class="number">2</span>                   <span class="operator">|</span> <span class="number">1</span>                          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">9</span>   <span class="operator">|</span> <span class="number">124</span>        <span class="operator">|</span> <span class="number">76</span>        <span class="operator">|</span> <span class="number">70</span>    <span class="operator">|</span> <span class="number">76</span>       <span class="operator">|</span> class2   <span class="operator">|</span> department1   <span class="operator">|</span> <span class="number">2</span>                <span class="operator">|</span> <span class="number">7</span>                   <span class="operator">|</span> <span class="number">2</span>                          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">7</span>   <span class="operator">|</span> <span class="number">122</span>        <span class="operator">|</span> <span class="number">89</span>        <span class="operator">|</span> <span class="number">86</span>    <span class="operator">|</span> <span class="number">85</span>       <span class="operator">|</span> class2   <span class="operator">|</span> department1   <span class="operator">|</span> <span class="number">9</span>                <span class="operator">|</span> <span class="number">10</span>                  <span class="operator">|</span> <span class="number">3</span>                          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">6</span>   <span class="operator">|</span> <span class="number">121</span>        <span class="operator">|</span> <span class="number">96</span>        <span class="operator">|</span> <span class="number">74</span>    <span class="operator">|</span> <span class="number">79</span>       <span class="operator">|</span> class2   <span class="operator">|</span> department1   <span class="operator">|</span> <span class="number">3</span>                <span class="operator">|</span> <span class="number">9</span>                   <span class="operator">|</span> <span class="number">4</span>                          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">19</span>  <span class="operator">|</span> <span class="number">224</span>        <span class="operator">|</span> <span class="number">75</span>        <span class="operator">|</span> <span class="number">80</span>    <span class="operator">|</span> <span class="number">78</span>       <span class="operator">|</span> class2   <span class="operator">|</span> department2   <span class="operator">|</span> <span class="number">5</span>                <span class="operator">|</span> <span class="number">8</span>                   <span class="operator">|</span> <span class="number">1</span>                          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">16</span>  <span class="operator">|</span> <span class="number">221</span>        <span class="operator">|</span> <span class="number">77</span>        <span class="operator">|</span> <span class="number">99</span>    <span class="operator">|</span> <span class="number">61</span>       <span class="operator">|</span> class2   <span class="operator">|</span> department2   <span class="operator">|</span> <span class="number">12</span>               <span class="operator">|</span> <span class="number">2</span>                   <span class="operator">|</span> <span class="number">2</span>                          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">18</span>  <span class="operator">|</span> <span class="number">223</span>        <span class="operator">|</span> <span class="number">79</span>        <span class="operator">|</span> <span class="number">74</span>    <span class="operator">|</span> <span class="number">96</span>       <span class="operator">|</span> class2   <span class="operator">|</span> department2   <span class="operator">|</span> <span class="number">3</span>                <span class="operator">|</span> <span class="number">14</span>                  <span class="operator">|</span> <span class="number">3</span>                          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">17</span>  <span class="operator">|</span> <span class="number">222</span>        <span class="operator">|</span> <span class="number">80</span>        <span class="operator">|</span> <span class="number">78</span>    <span class="operator">|</span> <span class="number">96</span>       <span class="operator">|</span> class2   <span class="operator">|</span> department2   <span class="operator">|</span> <span class="number">4</span>                <span class="operator">|</span> <span class="number">14</span>                  <span class="operator">|</span> <span class="number">4</span>                          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">20</span>  <span class="operator">|</span> <span class="number">225</span>        <span class="operator">|</span> <span class="number">82</span>        <span class="operator">|</span> <span class="number">85</span>    <span class="operator">|</span> <span class="number">63</span>       <span class="operator">|</span> class2   <span class="operator">|</span> department2   <span class="operator">|</span> <span class="number">8</span>                <span class="operator">|</span> <span class="number">3</span>                   <span class="operator">|</span> <span class="number">5</span>                          <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-----+------------+-----------+-------+----------+----------+---------------+------------------+---------------------+----------------------------+</span></span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>ntile<ul>
<li>作用：ntile(N) over([partition by col1] [order by col2])，将分区中的数据按照顺序划分为N片，返回当前片的值。<ul>
<li>注1：如果切片分布不均匀，默认增加第一个切片的分布</li>
<li>注2：ntile不支持ROWS BETWEEN</li>
</ul>
</li>
<li>sql示例  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> id,</span><br><span class="line">       studentId,</span><br><span class="line">       <span class="keyword">language</span>,</span><br><span class="line">       math,</span><br><span class="line">       english,</span><br><span class="line">       classId,</span><br><span class="line">       departmentId,</span><br><span class="line">       <span class="built_in">ntile</span>(<span class="number">2</span>) <span class="keyword">over</span> (<span class="keyword">order</span> <span class="keyword">by</span> math) <span class="keyword">as</span> math_ntile,</span><br><span class="line">       <span class="built_in">ntile</span>(<span class="number">2</span>) <span class="keyword">over</span> (<span class="keyword">order</span> <span class="keyword">by</span> english) <span class="keyword">as</span> english_ntile,</span><br><span class="line">       <span class="built_in">ntile</span>(<span class="number">2</span>) <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> classId,departmentId <span class="keyword">order</span> <span class="keyword">by</span> <span class="keyword">language</span> ) <span class="keyword">as</span> class_language_ntile</span><br><span class="line"><span class="keyword">from</span> student_scores;</span><br><span class="line"></span><br><span class="line"><span class="operator">+</span><span class="comment">-----+------------+-----------+-------+----------+----------+---------------+-------------+----------------+-----------------------+</span></span><br><span class="line"><span class="operator">|</span> id  <span class="operator">|</span> studentid  <span class="operator">|</span> <span class="keyword">language</span>  <span class="operator">|</span> math  <span class="operator">|</span> english  <span class="operator">|</span> classid  <span class="operator">|</span> departmentid  <span class="operator">|</span> math_ntile  <span class="operator">|</span> english_ntile  <span class="operator">|</span> class_language_ntile  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-----+------------+-----------+-------+----------+----------+---------------+-------------+----------------+-----------------------+</span></span><br><span class="line"><span class="operator">|</span> <span class="number">1</span>   <span class="operator">|</span> <span class="number">111</span>        <span class="operator">|</span> <span class="number">68</span>        <span class="operator">|</span> <span class="number">69</span>    <span class="operator">|</span> <span class="number">90</span>       <span class="operator">|</span> class1   <span class="operator">|</span> department1   <span class="operator">|</span> <span class="number">1</span>           <span class="operator">|</span> <span class="number">2</span>              <span class="operator">|</span> <span class="number">1</span>                     <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">2</span>   <span class="operator">|</span> <span class="number">112</span>        <span class="operator">|</span> <span class="number">73</span>        <span class="operator">|</span> <span class="number">80</span>    <span class="operator">|</span> <span class="number">96</span>       <span class="operator">|</span> class1   <span class="operator">|</span> department1   <span class="operator">|</span> <span class="number">1</span>           <span class="operator">|</span> <span class="number">2</span>              <span class="operator">|</span> <span class="number">1</span>                     <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">4</span>   <span class="operator">|</span> <span class="number">114</span>        <span class="operator">|</span> <span class="number">89</span>        <span class="operator">|</span> <span class="number">94</span>    <span class="operator">|</span> <span class="number">93</span>       <span class="operator">|</span> class1   <span class="operator">|</span> department1   <span class="operator">|</span> <span class="number">2</span>           <span class="operator">|</span> <span class="number">2</span>              <span class="operator">|</span> <span class="number">1</span>                     <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">3</span>   <span class="operator">|</span> <span class="number">113</span>        <span class="operator">|</span> <span class="number">90</span>        <span class="operator">|</span> <span class="number">74</span>    <span class="operator">|</span> <span class="number">75</span>       <span class="operator">|</span> class1   <span class="operator">|</span> department1   <span class="operator">|</span> <span class="number">1</span>           <span class="operator">|</span> <span class="number">1</span>              <span class="operator">|</span> <span class="number">2</span>                     <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">5</span>   <span class="operator">|</span> <span class="number">115</span>        <span class="operator">|</span> <span class="number">99</span>        <span class="operator">|</span> <span class="number">93</span>    <span class="operator">|</span> <span class="number">89</span>       <span class="operator">|</span> class1   <span class="operator">|</span> department1   <span class="operator">|</span> <span class="number">2</span>           <span class="operator">|</span> <span class="number">2</span>              <span class="operator">|</span> <span class="number">2</span>                     <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">12</span>  <span class="operator">|</span> <span class="number">213</span>        <span class="operator">|</span> <span class="number">71</span>        <span class="operator">|</span> <span class="number">94</span>    <span class="operator">|</span> <span class="number">90</span>       <span class="operator">|</span> class1   <span class="operator">|</span> department2   <span class="operator">|</span> <span class="number">2</span>           <span class="operator">|</span> <span class="number">2</span>              <span class="operator">|</span> <span class="number">1</span>                     <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">11</span>  <span class="operator">|</span> <span class="number">212</span>        <span class="operator">|</span> <span class="number">76</span>        <span class="operator">|</span> <span class="number">83</span>    <span class="operator">|</span> <span class="number">75</span>       <span class="operator">|</span> class1   <span class="operator">|</span> department2   <span class="operator">|</span> <span class="number">2</span>           <span class="operator">|</span> <span class="number">1</span>              <span class="operator">|</span> <span class="number">1</span>                     <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">14</span>  <span class="operator">|</span> <span class="number">215</span>        <span class="operator">|</span> <span class="number">84</span>        <span class="operator">|</span> <span class="number">82</span>    <span class="operator">|</span> <span class="number">73</span>       <span class="operator">|</span> class1   <span class="operator">|</span> department2   <span class="operator">|</span> <span class="number">2</span>           <span class="operator">|</span> <span class="number">1</span>              <span class="operator">|</span> <span class="number">1</span>                     <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">15</span>  <span class="operator">|</span> <span class="number">216</span>        <span class="operator">|</span> <span class="number">85</span>        <span class="operator">|</span> <span class="number">74</span>    <span class="operator">|</span> <span class="number">93</span>       <span class="operator">|</span> class1   <span class="operator">|</span> department2   <span class="operator">|</span> <span class="number">1</span>           <span class="operator">|</span> <span class="number">2</span>              <span class="operator">|</span> <span class="number">2</span>                     <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">10</span>  <span class="operator">|</span> <span class="number">211</span>        <span class="operator">|</span> <span class="number">89</span>        <span class="operator">|</span> <span class="number">93</span>    <span class="operator">|</span> <span class="number">60</span>       <span class="operator">|</span> class1   <span class="operator">|</span> department2   <span class="operator">|</span> <span class="number">2</span>           <span class="operator">|</span> <span class="number">1</span>              <span class="operator">|</span> <span class="number">2</span>                     <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">13</span>  <span class="operator">|</span> <span class="number">214</span>        <span class="operator">|</span> <span class="number">94</span>        <span class="operator">|</span> <span class="number">94</span>    <span class="operator">|</span> <span class="number">66</span>       <span class="operator">|</span> class1   <span class="operator">|</span> department2   <span class="operator">|</span> <span class="number">2</span>           <span class="operator">|</span> <span class="number">1</span>              <span class="operator">|</span> <span class="number">2</span>                     <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">8</span>   <span class="operator">|</span> <span class="number">123</span>        <span class="operator">|</span> <span class="number">70</span>        <span class="operator">|</span> <span class="number">78</span>    <span class="operator">|</span> <span class="number">61</span>       <span class="operator">|</span> class2   <span class="operator">|</span> department1   <span class="operator">|</span> <span class="number">1</span>           <span class="operator">|</span> <span class="number">1</span>              <span class="operator">|</span> <span class="number">1</span>                     <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">9</span>   <span class="operator">|</span> <span class="number">124</span>        <span class="operator">|</span> <span class="number">76</span>        <span class="operator">|</span> <span class="number">70</span>    <span class="operator">|</span> <span class="number">76</span>       <span class="operator">|</span> class2   <span class="operator">|</span> department1   <span class="operator">|</span> <span class="number">1</span>           <span class="operator">|</span> <span class="number">1</span>              <span class="operator">|</span> <span class="number">1</span>                     <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">7</span>   <span class="operator">|</span> <span class="number">122</span>        <span class="operator">|</span> <span class="number">89</span>        <span class="operator">|</span> <span class="number">86</span>    <span class="operator">|</span> <span class="number">85</span>       <span class="operator">|</span> class2   <span class="operator">|</span> department1   <span class="operator">|</span> <span class="number">2</span>           <span class="operator">|</span> <span class="number">2</span>              <span class="operator">|</span> <span class="number">2</span>                     <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">6</span>   <span class="operator">|</span> <span class="number">121</span>        <span class="operator">|</span> <span class="number">96</span>        <span class="operator">|</span> <span class="number">74</span>    <span class="operator">|</span> <span class="number">79</span>       <span class="operator">|</span> class2   <span class="operator">|</span> department1   <span class="operator">|</span> <span class="number">1</span>           <span class="operator">|</span> <span class="number">2</span>              <span class="operator">|</span> <span class="number">2</span>                     <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">19</span>  <span class="operator">|</span> <span class="number">224</span>        <span class="operator">|</span> <span class="number">75</span>        <span class="operator">|</span> <span class="number">80</span>    <span class="operator">|</span> <span class="number">78</span>       <span class="operator">|</span> class2   <span class="operator">|</span> department2   <span class="operator">|</span> <span class="number">1</span>           <span class="operator">|</span> <span class="number">1</span>              <span class="operator">|</span> <span class="number">1</span>                     <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">16</span>  <span class="operator">|</span> <span class="number">221</span>        <span class="operator">|</span> <span class="number">77</span>        <span class="operator">|</span> <span class="number">99</span>    <span class="operator">|</span> <span class="number">61</span>       <span class="operator">|</span> class2   <span class="operator">|</span> department2   <span class="operator">|</span> <span class="number">2</span>           <span class="operator">|</span> <span class="number">1</span>              <span class="operator">|</span> <span class="number">1</span>                     <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">18</span>  <span class="operator">|</span> <span class="number">223</span>        <span class="operator">|</span> <span class="number">79</span>        <span class="operator">|</span> <span class="number">74</span>    <span class="operator">|</span> <span class="number">96</span>       <span class="operator">|</span> class2   <span class="operator">|</span> department2   <span class="operator">|</span> <span class="number">1</span>           <span class="operator">|</span> <span class="number">2</span>              <span class="operator">|</span> <span class="number">1</span>                     <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">17</span>  <span class="operator">|</span> <span class="number">222</span>        <span class="operator">|</span> <span class="number">80</span>        <span class="operator">|</span> <span class="number">78</span>    <span class="operator">|</span> <span class="number">96</span>       <span class="operator">|</span> class2   <span class="operator">|</span> department2   <span class="operator">|</span> <span class="number">1</span>           <span class="operator">|</span> <span class="number">2</span>              <span class="operator">|</span> <span class="number">2</span>                     <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">20</span>  <span class="operator">|</span> <span class="number">225</span>        <span class="operator">|</span> <span class="number">82</span>        <span class="operator">|</span> <span class="number">85</span>    <span class="operator">|</span> <span class="number">63</span>       <span class="operator">|</span> class2   <span class="operator">|</span> department2   <span class="operator">|</span> <span class="number">2</span>           <span class="operator">|</span> <span class="number">1</span>              <span class="operator">|</span> <span class="number">2</span>                     <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-----+------------+-----------+-------+----------+----------+---------------+-------------+----------------+-----------------------+</span></span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>percent_rank<ul>
<li>作用：percent_rank() over([partition by col1] [order by col2])，计算给定行的百分比排名。分组内当前行的RANK值-1/分组内总行数-1，可以用来计算超过了百分之多少的人。</li>
<li>sql示例  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> id,</span><br><span class="line">       studentid,</span><br><span class="line">       <span class="keyword">language</span>,</span><br><span class="line">       math,</span><br><span class="line">       english,</span><br><span class="line">       classid,</span><br><span class="line">       departmentid,</span><br><span class="line">       <span class="built_in">percent_rank</span>() <span class="keyword">over</span> (<span class="keyword">order</span> <span class="keyword">by</span> math)                                        <span class="keyword">as</span> math_percent_rank,</span><br><span class="line">       <span class="built_in">percent_rank</span>() <span class="keyword">over</span> (<span class="keyword">order</span> <span class="keyword">by</span> english)                                     <span class="keyword">as</span> english_percent_rank,</span><br><span class="line">       <span class="built_in">percent_rank</span>() <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> classId,departmentId <span class="keyword">order</span> <span class="keyword">by</span> <span class="keyword">language</span> ) <span class="keyword">as</span> class_language_percent_rank</span><br><span class="line"><span class="keyword">from</span> student_scores;</span><br><span class="line"></span><br><span class="line"><span class="operator">+</span><span class="comment">-----+------------+-----------+-------+----------+----------+---------------+----------------------+-----------------------+------------------------------+</span></span><br><span class="line"><span class="operator">|</span> id  <span class="operator">|</span> studentid  <span class="operator">|</span> <span class="keyword">language</span>  <span class="operator">|</span> math  <span class="operator">|</span> english  <span class="operator">|</span> classid  <span class="operator">|</span> departmentid  <span class="operator">|</span>  math_percent_rank   <span class="operator">|</span> english_percent_rank  <span class="operator">|</span> class_language_percent_rank  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-----+------------+-----------+-------+----------+----------+---------------+----------------------+-----------------------+------------------------------+</span></span><br><span class="line"><span class="operator">|</span> <span class="number">1</span>   <span class="operator">|</span> <span class="number">111</span>        <span class="operator">|</span> <span class="number">68</span>        <span class="operator">|</span> <span class="number">69</span>    <span class="operator">|</span> <span class="number">90</span>       <span class="operator">|</span> class1   <span class="operator">|</span> department1   <span class="operator">|</span> <span class="number">0.0</span>                  <span class="operator">|</span> <span class="number">0.6842105263157895</span>    <span class="operator">|</span> <span class="number">0.0</span>                          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">2</span>   <span class="operator">|</span> <span class="number">112</span>        <span class="operator">|</span> <span class="number">73</span>        <span class="operator">|</span> <span class="number">80</span>    <span class="operator">|</span> <span class="number">96</span>       <span class="operator">|</span> class1   <span class="operator">|</span> department1   <span class="operator">|</span> <span class="number">0.42105263157894735</span>  <span class="operator">|</span> <span class="number">0.8947368421052632</span>    <span class="operator">|</span> <span class="number">0.25</span>                         <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">4</span>   <span class="operator">|</span> <span class="number">114</span>        <span class="operator">|</span> <span class="number">89</span>        <span class="operator">|</span> <span class="number">94</span>    <span class="operator">|</span> <span class="number">93</span>       <span class="operator">|</span> class1   <span class="operator">|</span> department1   <span class="operator">|</span> <span class="number">0.8421052631578947</span>   <span class="operator">|</span> <span class="number">0.7894736842105263</span>    <span class="operator">|</span> <span class="number">0.5</span>                          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">3</span>   <span class="operator">|</span> <span class="number">113</span>        <span class="operator">|</span> <span class="number">90</span>        <span class="operator">|</span> <span class="number">74</span>    <span class="operator">|</span> <span class="number">75</span>       <span class="operator">|</span> class1   <span class="operator">|</span> department1   <span class="operator">|</span> <span class="number">0.10526315789473684</span>  <span class="operator">|</span> <span class="number">0.3157894736842105</span>    <span class="operator">|</span> <span class="number">0.75</span>                         <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">5</span>   <span class="operator">|</span> <span class="number">115</span>        <span class="operator">|</span> <span class="number">99</span>        <span class="operator">|</span> <span class="number">93</span>    <span class="operator">|</span> <span class="number">89</span>       <span class="operator">|</span> class1   <span class="operator">|</span> department1   <span class="operator">|</span> <span class="number">0.7368421052631579</span>   <span class="operator">|</span> <span class="number">0.631578947368421</span>     <span class="operator">|</span> <span class="number">1.0</span>                          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">12</span>  <span class="operator">|</span> <span class="number">213</span>        <span class="operator">|</span> <span class="number">71</span>        <span class="operator">|</span> <span class="number">94</span>    <span class="operator">|</span> <span class="number">90</span>       <span class="operator">|</span> class1   <span class="operator">|</span> department2   <span class="operator">|</span> <span class="number">0.8421052631578947</span>   <span class="operator">|</span> <span class="number">0.6842105263157895</span>    <span class="operator">|</span> <span class="number">0.0</span>                          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">11</span>  <span class="operator">|</span> <span class="number">212</span>        <span class="operator">|</span> <span class="number">76</span>        <span class="operator">|</span> <span class="number">83</span>    <span class="operator">|</span> <span class="number">75</span>       <span class="operator">|</span> class1   <span class="operator">|</span> department2   <span class="operator">|</span> <span class="number">0.5789473684210527</span>   <span class="operator">|</span> <span class="number">0.3157894736842105</span>    <span class="operator">|</span> <span class="number">0.2</span>                          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">14</span>  <span class="operator">|</span> <span class="number">215</span>        <span class="operator">|</span> <span class="number">84</span>        <span class="operator">|</span> <span class="number">82</span>    <span class="operator">|</span> <span class="number">73</span>       <span class="operator">|</span> class1   <span class="operator">|</span> department2   <span class="operator">|</span> <span class="number">0.5263157894736842</span>   <span class="operator">|</span> <span class="number">0.2631578947368421</span>    <span class="operator">|</span> <span class="number">0.4</span>                          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">15</span>  <span class="operator">|</span> <span class="number">216</span>        <span class="operator">|</span> <span class="number">85</span>        <span class="operator">|</span> <span class="number">74</span>    <span class="operator">|</span> <span class="number">93</span>       <span class="operator">|</span> class1   <span class="operator">|</span> department2   <span class="operator">|</span> <span class="number">0.10526315789473684</span>  <span class="operator">|</span> <span class="number">0.7894736842105263</span>    <span class="operator">|</span> <span class="number">0.6</span>                          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">10</span>  <span class="operator">|</span> <span class="number">211</span>        <span class="operator">|</span> <span class="number">89</span>        <span class="operator">|</span> <span class="number">93</span>    <span class="operator">|</span> <span class="number">60</span>       <span class="operator">|</span> class1   <span class="operator">|</span> department2   <span class="operator">|</span> <span class="number">0.7368421052631579</span>   <span class="operator">|</span> <span class="number">0.0</span>                   <span class="operator">|</span> <span class="number">0.8</span>                          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">13</span>  <span class="operator">|</span> <span class="number">214</span>        <span class="operator">|</span> <span class="number">94</span>        <span class="operator">|</span> <span class="number">94</span>    <span class="operator">|</span> <span class="number">66</span>       <span class="operator">|</span> class1   <span class="operator">|</span> department2   <span class="operator">|</span> <span class="number">0.8421052631578947</span>   <span class="operator">|</span> <span class="number">0.21052631578947367</span>   <span class="operator">|</span> <span class="number">1.0</span>                          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">8</span>   <span class="operator">|</span> <span class="number">123</span>        <span class="operator">|</span> <span class="number">70</span>        <span class="operator">|</span> <span class="number">78</span>    <span class="operator">|</span> <span class="number">61</span>       <span class="operator">|</span> class2   <span class="operator">|</span> department1   <span class="operator">|</span> <span class="number">0.3157894736842105</span>   <span class="operator">|</span> <span class="number">0.05263157894736842</span>   <span class="operator">|</span> <span class="number">0.0</span>                          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">9</span>   <span class="operator">|</span> <span class="number">124</span>        <span class="operator">|</span> <span class="number">76</span>        <span class="operator">|</span> <span class="number">70</span>    <span class="operator">|</span> <span class="number">76</span>       <span class="operator">|</span> class2   <span class="operator">|</span> department1   <span class="operator">|</span> <span class="number">0.05263157894736842</span>  <span class="operator">|</span> <span class="number">0.42105263157894735</span>   <span class="operator">|</span> <span class="number">0.3333333333333333</span>           <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">7</span>   <span class="operator">|</span> <span class="number">122</span>        <span class="operator">|</span> <span class="number">89</span>        <span class="operator">|</span> <span class="number">86</span>    <span class="operator">|</span> <span class="number">85</span>       <span class="operator">|</span> class2   <span class="operator">|</span> department1   <span class="operator">|</span> <span class="number">0.6842105263157895</span>   <span class="operator">|</span> <span class="number">0.5789473684210527</span>    <span class="operator">|</span> <span class="number">0.6666666666666666</span>           <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">6</span>   <span class="operator">|</span> <span class="number">121</span>        <span class="operator">|</span> <span class="number">96</span>        <span class="operator">|</span> <span class="number">74</span>    <span class="operator">|</span> <span class="number">79</span>       <span class="operator">|</span> class2   <span class="operator">|</span> department1   <span class="operator">|</span> <span class="number">0.10526315789473684</span>  <span class="operator">|</span> <span class="number">0.5263157894736842</span>    <span class="operator">|</span> <span class="number">1.0</span>                          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">19</span>  <span class="operator">|</span> <span class="number">224</span>        <span class="operator">|</span> <span class="number">75</span>        <span class="operator">|</span> <span class="number">80</span>    <span class="operator">|</span> <span class="number">78</span>       <span class="operator">|</span> class2   <span class="operator">|</span> department2   <span class="operator">|</span> <span class="number">0.42105263157894735</span>  <span class="operator">|</span> <span class="number">0.47368421052631576</span>   <span class="operator">|</span> <span class="number">0.0</span>                          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">16</span>  <span class="operator">|</span> <span class="number">221</span>        <span class="operator">|</span> <span class="number">77</span>        <span class="operator">|</span> <span class="number">99</span>    <span class="operator">|</span> <span class="number">61</span>       <span class="operator">|</span> class2   <span class="operator">|</span> department2   <span class="operator">|</span> <span class="number">1.0</span>                  <span class="operator">|</span> <span class="number">0.05263157894736842</span>   <span class="operator">|</span> <span class="number">0.25</span>                         <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">18</span>  <span class="operator">|</span> <span class="number">223</span>        <span class="operator">|</span> <span class="number">79</span>        <span class="operator">|</span> <span class="number">74</span>    <span class="operator">|</span> <span class="number">96</span>       <span class="operator">|</span> class2   <span class="operator">|</span> department2   <span class="operator">|</span> <span class="number">0.10526315789473684</span>  <span class="operator">|</span> <span class="number">0.8947368421052632</span>    <span class="operator">|</span> <span class="number">0.5</span>                          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">17</span>  <span class="operator">|</span> <span class="number">222</span>        <span class="operator">|</span> <span class="number">80</span>        <span class="operator">|</span> <span class="number">78</span>    <span class="operator">|</span> <span class="number">96</span>       <span class="operator">|</span> class2   <span class="operator">|</span> department2   <span class="operator">|</span> <span class="number">0.3157894736842105</span>   <span class="operator">|</span> <span class="number">0.8947368421052632</span>    <span class="operator">|</span> <span class="number">0.75</span>                         <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">20</span>  <span class="operator">|</span> <span class="number">225</span>        <span class="operator">|</span> <span class="number">82</span>        <span class="operator">|</span> <span class="number">85</span>    <span class="operator">|</span> <span class="number">63</span>       <span class="operator">|</span> class2   <span class="operator">|</span> department2   <span class="operator">|</span> <span class="number">0.631578947368421</span>    <span class="operator">|</span> <span class="number">0.15789473684210525</span>   <span class="operator">|</span> <span class="number">1.0</span>                          <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-----+------------+-----------+-------+----------+----------+---------------+----------------------+-----------------------+------------------------------+</span></span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ol>
<h4 id="8-2-5-4-多维度分析"><a href="#8-2-5-4-多维度分析" class="headerlink" title="8.2.5.4 多维度分析"></a>8.2.5.4 多维度分析</h4><ol>
<li>GROUPING SETS<ul>
<li>作用：在一个GROUP BY查询中，根据不同的维度组合进行聚合，等价于将不同维度的GROUP BY结果集进行UNION ALL</li>
<li>示例  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="keyword">month</span>,</span><br><span class="line">       <span class="keyword">day</span>,</span><br><span class="line">       <span class="built_in">COUNT</span>(<span class="keyword">DISTINCT</span> cookieid) <span class="keyword">AS</span> uv</span><br><span class="line"><span class="keyword">FROM</span> visit_log</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> <span class="keyword">month</span>, <span class="keyword">day</span></span><br><span class="line">    <span class="keyword">GROUPING</span> SETS ( <span class="keyword">month</span>, <span class="keyword">day</span>);</span><br><span class="line">    </span><br><span class="line"><span class="operator">+</span><span class="comment">----------+-------------+-----+</span></span><br><span class="line"><span class="operator">|</span>  <span class="keyword">month</span>   <span class="operator">|</span>     <span class="keyword">day</span>     <span class="operator">|</span> uv  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------+-------------+-----+</span></span><br><span class="line"><span class="operator">|</span> <span class="keyword">NULL</span>     <span class="operator">|</span> <span class="number">2015</span><span class="number">-03</span><span class="number">-10</span>  <span class="operator">|</span> <span class="number">4</span>   <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="keyword">NULL</span>     <span class="operator">|</span> <span class="number">2015</span><span class="number">-03</span><span class="number">-12</span>  <span class="operator">|</span> <span class="number">1</span>   <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="keyword">NULL</span>     <span class="operator">|</span> <span class="number">2015</span><span class="number">-04</span><span class="number">-12</span>  <span class="operator">|</span> <span class="number">2</span>   <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="keyword">NULL</span>     <span class="operator">|</span> <span class="number">2015</span><span class="number">-04</span><span class="number">-13</span>  <span class="operator">|</span> <span class="number">3</span>   <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="keyword">NULL</span>     <span class="operator">|</span> <span class="number">2015</span><span class="number">-04</span><span class="number">-15</span>  <span class="operator">|</span> <span class="number">2</span>   <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="keyword">NULL</span>     <span class="operator">|</span> <span class="number">2015</span><span class="number">-04</span><span class="number">-16</span>  <span class="operator">|</span> <span class="number">2</span>   <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">2015</span><span class="number">-03</span>  <span class="operator">|</span> <span class="keyword">NULL</span>        <span class="operator">|</span> <span class="number">5</span>   <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">2015</span><span class="number">-04</span>  <span class="operator">|</span> <span class="keyword">NULL</span>        <span class="operator">|</span> <span class="number">6</span>   <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------+-------------+-----+</span></span><br></pre></td></tr></table></figure></li>
<li>分析  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"># 等价于</span><br><span class="line"><span class="keyword">SELECT</span> <span class="keyword">month</span>,<span class="keyword">NULL</span> <span class="keyword">as</span> <span class="keyword">day</span>,<span class="built_in">COUNT</span>(<span class="keyword">DISTINCT</span> cookieid) <span class="keyword">AS</span> uv <span class="keyword">FROM</span> test2 <span class="keyword">GROUP</span> <span class="keyword">BY</span> <span class="keyword">month</span> </span><br><span class="line"><span class="keyword">UNION</span> <span class="keyword">ALL</span> </span><br><span class="line"><span class="keyword">SELECT</span> <span class="keyword">NULL</span> <span class="keyword">as</span> <span class="keyword">month</span>,<span class="keyword">day</span>,<span class="built_in">COUNT</span>(<span class="keyword">DISTINCT</span> cookieid) <span class="keyword">AS</span> uv <span class="keyword">FROM</span> test2 <span class="keyword">GROUP</span> <span class="keyword">BY</span> <span class="keyword">day</span>;</span><br></pre></td></tr></table></figure></li>
<li>升级版 GROUPING__ID，表示结果属于哪一个分组集合。（<font color ='red' >GROUPING__ID</font>两个下划线__）  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">    <span class="keyword">month</span>,</span><br><span class="line">    <span class="keyword">day</span>,</span><br><span class="line">    <span class="built_in">COUNT</span>(<span class="keyword">DISTINCT</span> cookieid) <span class="keyword">AS</span> uv,</span><br><span class="line">    GROUPING__ID</span><br><span class="line"><span class="keyword">FROM</span> visit_log</span><br><span class="line">    <span class="keyword">GROUP</span> <span class="keyword">BY</span> <span class="keyword">month</span>,<span class="keyword">day</span></span><br><span class="line">    <span class="keyword">GROUPING</span> SETS (<span class="keyword">month</span>,<span class="keyword">day</span>,(<span class="keyword">month</span>,<span class="keyword">day</span>))</span><br><span class="line">    <span class="keyword">ORDER</span> <span class="keyword">BY</span> GROUPING__ID;</span><br><span class="line"></span><br><span class="line"><span class="operator">+</span><span class="comment">----------+-------------+-----+---------------+</span></span><br><span class="line"><span class="operator">|</span>  <span class="keyword">month</span>   <span class="operator">|</span>     <span class="keyword">day</span>     <span class="operator">|</span> uv  <span class="operator">|</span> grouping__id  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------+-------------+-----+---------------+</span></span><br><span class="line"><span class="operator">|</span> <span class="number">2015</span><span class="number">-03</span>  <span class="operator">|</span> <span class="number">2015</span><span class="number">-03</span><span class="number">-10</span>  <span class="operator">|</span> <span class="number">4</span>   <span class="operator">|</span> <span class="number">0</span>             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">2015</span><span class="number">-04</span>  <span class="operator">|</span> <span class="number">2015</span><span class="number">-04</span><span class="number">-16</span>  <span class="operator">|</span> <span class="number">2</span>   <span class="operator">|</span> <span class="number">0</span>             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">2015</span><span class="number">-04</span>  <span class="operator">|</span> <span class="number">2015</span><span class="number">-04</span><span class="number">-13</span>  <span class="operator">|</span> <span class="number">3</span>   <span class="operator">|</span> <span class="number">0</span>             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">2015</span><span class="number">-04</span>  <span class="operator">|</span> <span class="number">2015</span><span class="number">-04</span><span class="number">-12</span>  <span class="operator">|</span> <span class="number">2</span>   <span class="operator">|</span> <span class="number">0</span>             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">2015</span><span class="number">-04</span>  <span class="operator">|</span> <span class="number">2015</span><span class="number">-04</span><span class="number">-15</span>  <span class="operator">|</span> <span class="number">2</span>   <span class="operator">|</span> <span class="number">0</span>             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">2015</span><span class="number">-03</span>  <span class="operator">|</span> <span class="number">2015</span><span class="number">-03</span><span class="number">-12</span>  <span class="operator">|</span> <span class="number">1</span>   <span class="operator">|</span> <span class="number">0</span>             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">2015</span><span class="number">-03</span>  <span class="operator">|</span> <span class="keyword">NULL</span>        <span class="operator">|</span> <span class="number">5</span>   <span class="operator">|</span> <span class="number">1</span>             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">2015</span><span class="number">-04</span>  <span class="operator">|</span> <span class="keyword">NULL</span>        <span class="operator">|</span> <span class="number">6</span>   <span class="operator">|</span> <span class="number">1</span>             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="keyword">NULL</span>     <span class="operator">|</span> <span class="number">2015</span><span class="number">-04</span><span class="number">-16</span>  <span class="operator">|</span> <span class="number">2</span>   <span class="operator">|</span> <span class="number">2</span>             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="keyword">NULL</span>     <span class="operator">|</span> <span class="number">2015</span><span class="number">-04</span><span class="number">-15</span>  <span class="operator">|</span> <span class="number">2</span>   <span class="operator">|</span> <span class="number">2</span>             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="keyword">NULL</span>     <span class="operator">|</span> <span class="number">2015</span><span class="number">-04</span><span class="number">-13</span>  <span class="operator">|</span> <span class="number">3</span>   <span class="operator">|</span> <span class="number">2</span>             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="keyword">NULL</span>     <span class="operator">|</span> <span class="number">2015</span><span class="number">-04</span><span class="number">-12</span>  <span class="operator">|</span> <span class="number">2</span>   <span class="operator">|</span> <span class="number">2</span>             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="keyword">NULL</span>     <span class="operator">|</span> <span class="number">2015</span><span class="number">-03</span><span class="number">-12</span>  <span class="operator">|</span> <span class="number">1</span>   <span class="operator">|</span> <span class="number">2</span>             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="keyword">NULL</span>     <span class="operator">|</span> <span class="number">2015</span><span class="number">-03</span><span class="number">-10</span>  <span class="operator">|</span> <span class="number">4</span>   <span class="operator">|</span> <span class="number">2</span>             <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------+-------------+-----+---------------+</span></span><br></pre></td></tr></table></figure></li>
<li>分析  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 等价</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="keyword">month</span>,<span class="keyword">NULL</span> <span class="keyword">as</span> <span class="keyword">day</span>,<span class="built_in">COUNT</span>(<span class="keyword">DISTINCT</span> cookieid) <span class="keyword">AS</span> uv,<span class="number">1</span> <span class="keyword">AS</span> GROUPING__ID <span class="keyword">FROM</span> test2 <span class="keyword">GROUP</span> <span class="keyword">BY</span> <span class="keyword">month</span> </span><br><span class="line"><span class="keyword">UNION</span> <span class="keyword">ALL</span> </span><br><span class="line"><span class="keyword">SELECT</span> <span class="keyword">NULL</span> <span class="keyword">as</span> <span class="keyword">month</span>,<span class="keyword">day</span>,<span class="built_in">COUNT</span>(<span class="keyword">DISTINCT</span> cookieid) <span class="keyword">AS</span> uv,<span class="number">2</span> <span class="keyword">AS</span> GROUPING__ID <span class="keyword">FROM</span> test2 <span class="keyword">GROUP</span> <span class="keyword">BY</span> <span class="keyword">day</span></span><br><span class="line"><span class="keyword">UNION</span> <span class="keyword">ALL</span> </span><br><span class="line"><span class="keyword">SELECT</span> <span class="keyword">month</span>,<span class="keyword">day</span>,<span class="built_in">COUNT</span>(<span class="keyword">DISTINCT</span> cookieid) <span class="keyword">AS</span> uv,<span class="number">3</span> <span class="keyword">AS</span> GROUPING__ID <span class="keyword">FROM</span> test2 <span class="keyword">GROUP</span> <span class="keyword">BY</span> <span class="keyword">month</span>,<span class="keyword">day</span>;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>CUBE<ul>
<li>作用：根据GROUP BY的维度的所有组合进行聚合。</li>
<li>示例  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">    <span class="keyword">month</span>,</span><br><span class="line">    <span class="keyword">day</span>,</span><br><span class="line">    <span class="built_in">COUNT</span>(<span class="keyword">DISTINCT</span> cookieid) <span class="keyword">AS</span> uv,</span><br><span class="line">    GROUPING__ID</span><br><span class="line"><span class="keyword">FROM</span> visit_log</span><br><span class="line">    <span class="keyword">GROUP</span> <span class="keyword">BY</span> <span class="keyword">month</span>,<span class="keyword">day</span></span><br><span class="line">    <span class="keyword">WITH</span> <span class="keyword">CUBE</span></span><br><span class="line">    <span class="keyword">ORDER</span> <span class="keyword">BY</span> GROUPING__ID;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="operator">+</span><span class="comment">----------+-------------+-----+---------------+</span></span><br><span class="line"><span class="operator">|</span>  <span class="keyword">month</span>   <span class="operator">|</span>     <span class="keyword">day</span>     <span class="operator">|</span> uv  <span class="operator">|</span> grouping__id  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------+-------------+-----+---------------+</span></span><br><span class="line"><span class="operator">|</span> <span class="number">2015</span><span class="number">-03</span>  <span class="operator">|</span> <span class="number">2015</span><span class="number">-03</span><span class="number">-10</span>  <span class="operator">|</span> <span class="number">4</span>   <span class="operator">|</span> <span class="number">0</span>             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">2015</span><span class="number">-04</span>  <span class="operator">|</span> <span class="number">2015</span><span class="number">-04</span><span class="number">-16</span>  <span class="operator">|</span> <span class="number">2</span>   <span class="operator">|</span> <span class="number">0</span>             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">2015</span><span class="number">-04</span>  <span class="operator">|</span> <span class="number">2015</span><span class="number">-04</span><span class="number">-13</span>  <span class="operator">|</span> <span class="number">3</span>   <span class="operator">|</span> <span class="number">0</span>             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">2015</span><span class="number">-04</span>  <span class="operator">|</span> <span class="number">2015</span><span class="number">-04</span><span class="number">-12</span>  <span class="operator">|</span> <span class="number">2</span>   <span class="operator">|</span> <span class="number">0</span>             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">2015</span><span class="number">-04</span>  <span class="operator">|</span> <span class="number">2015</span><span class="number">-04</span><span class="number">-15</span>  <span class="operator">|</span> <span class="number">2</span>   <span class="operator">|</span> <span class="number">0</span>             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">2015</span><span class="number">-03</span>  <span class="operator">|</span> <span class="number">2015</span><span class="number">-03</span><span class="number">-12</span>  <span class="operator">|</span> <span class="number">1</span>   <span class="operator">|</span> <span class="number">0</span>             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">2015</span><span class="number">-03</span>  <span class="operator">|</span> <span class="keyword">NULL</span>        <span class="operator">|</span> <span class="number">5</span>   <span class="operator">|</span> <span class="number">1</span>             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">2015</span><span class="number">-04</span>  <span class="operator">|</span> <span class="keyword">NULL</span>        <span class="operator">|</span> <span class="number">6</span>   <span class="operator">|</span> <span class="number">1</span>             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="keyword">NULL</span>     <span class="operator">|</span> <span class="number">2015</span><span class="number">-04</span><span class="number">-16</span>  <span class="operator">|</span> <span class="number">2</span>   <span class="operator">|</span> <span class="number">2</span>             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="keyword">NULL</span>     <span class="operator">|</span> <span class="number">2015</span><span class="number">-04</span><span class="number">-15</span>  <span class="operator">|</span> <span class="number">2</span>   <span class="operator">|</span> <span class="number">2</span>             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="keyword">NULL</span>     <span class="operator">|</span> <span class="number">2015</span><span class="number">-04</span><span class="number">-13</span>  <span class="operator">|</span> <span class="number">3</span>   <span class="operator">|</span> <span class="number">2</span>             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="keyword">NULL</span>     <span class="operator">|</span> <span class="number">2015</span><span class="number">-04</span><span class="number">-12</span>  <span class="operator">|</span> <span class="number">2</span>   <span class="operator">|</span> <span class="number">2</span>             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="keyword">NULL</span>     <span class="operator">|</span> <span class="number">2015</span><span class="number">-03</span><span class="number">-12</span>  <span class="operator">|</span> <span class="number">1</span>   <span class="operator">|</span> <span class="number">2</span>             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="keyword">NULL</span>     <span class="operator">|</span> <span class="number">2015</span><span class="number">-03</span><span class="number">-10</span>  <span class="operator">|</span> <span class="number">4</span>   <span class="operator">|</span> <span class="number">2</span>             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="keyword">NULL</span>     <span class="operator">|</span> <span class="keyword">NULL</span>        <span class="operator">|</span> <span class="number">7</span>   <span class="operator">|</span> <span class="number">3</span>             <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------+-------------+-----+---------------+</span></span><br></pre></td></tr></table></figure></li>
<li>解析  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">等价于</span><br><span class="line"><span class="keyword">SELECT</span> <span class="keyword">NULL</span> <span class="keyword">as</span> <span class="keyword">month</span>,<span class="keyword">NULL</span> <span class="keyword">as</span> <span class="keyword">day</span>,<span class="built_in">COUNT</span>(<span class="keyword">DISTINCT</span> cookieid) <span class="keyword">AS</span> uv,<span class="number">0</span> <span class="keyword">AS</span> GROUPING__ID <span class="keyword">FROM</span> test2</span><br><span class="line"><span class="keyword">UNION</span> <span class="keyword">ALL</span> </span><br><span class="line"><span class="keyword">SELECT</span> <span class="keyword">month</span>,<span class="keyword">NULL</span> <span class="keyword">as</span> <span class="keyword">day</span>,<span class="built_in">COUNT</span>(<span class="keyword">DISTINCT</span> cookieid) <span class="keyword">AS</span> uv,<span class="number">1</span> <span class="keyword">AS</span> GROUPING__ID <span class="keyword">FROM</span> test2 <span class="keyword">GROUP</span> <span class="keyword">BY</span> <span class="keyword">month</span> </span><br><span class="line"><span class="keyword">UNION</span> <span class="keyword">ALL</span> </span><br><span class="line"><span class="keyword">SELECT</span> <span class="keyword">NULL</span> <span class="keyword">as</span> <span class="keyword">month</span>,<span class="keyword">day</span>,<span class="built_in">COUNT</span>(<span class="keyword">DISTINCT</span> cookieid) <span class="keyword">AS</span> uv,<span class="number">2</span> <span class="keyword">AS</span> GROUPING__ID <span class="keyword">FROM</span> test2 <span class="keyword">GROUP</span> <span class="keyword">BY</span> <span class="keyword">day</span></span><br><span class="line"><span class="keyword">UNION</span> <span class="keyword">ALL</span> </span><br><span class="line"><span class="keyword">SELECT</span> <span class="keyword">month</span>,<span class="keyword">day</span>,<span class="built_in">COUNT</span>(<span class="keyword">DISTINCT</span> cookieid) <span class="keyword">AS</span> uv,<span class="number">3</span> <span class="keyword">AS</span> GROUPING__ID <span class="keyword">FROM</span> test2 <span class="keyword">GROUP</span> <span class="keyword">BY</span> <span class="keyword">month</span>,<span class="keyword">day</span>;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>ROLLUP<ul>
<li>作用：是CUBE的子集，以最左侧的维度为主，从该维度进行层级聚合。</li>
<li>比如，以month维度进行层级聚合：实现的上钻过程：月天的UV-&gt;月的UV-&gt;总UV  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">    <span class="keyword">month</span>,</span><br><span class="line">    <span class="keyword">day</span>,</span><br><span class="line">    <span class="built_in">COUNT</span>(<span class="keyword">DISTINCT</span> cookieid) <span class="keyword">AS</span> uv,</span><br><span class="line">    GROUPING__ID</span><br><span class="line"><span class="keyword">FROM</span> visit_log</span><br><span class="line">    <span class="keyword">GROUP</span> <span class="keyword">BY</span> <span class="keyword">month</span>,<span class="keyword">day</span></span><br><span class="line">    <span class="keyword">WITH</span> <span class="keyword">ROLLUP</span></span><br><span class="line">    <span class="keyword">ORDER</span> <span class="keyword">BY</span> GROUPING__ID;</span><br><span class="line">    </span><br><span class="line"><span class="operator">+</span><span class="comment">----------+-------------+-----+---------------+</span></span><br><span class="line"><span class="operator">|</span>  <span class="keyword">month</span>   <span class="operator">|</span>     <span class="keyword">day</span>     <span class="operator">|</span> uv  <span class="operator">|</span> grouping__id  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------+-------------+-----+---------------+</span></span><br><span class="line"><span class="operator">|</span> <span class="number">2015</span><span class="number">-04</span>  <span class="operator">|</span> <span class="number">2015</span><span class="number">-04</span><span class="number">-16</span>  <span class="operator">|</span> <span class="number">2</span>   <span class="operator">|</span> <span class="number">0</span>             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">2015</span><span class="number">-04</span>  <span class="operator">|</span> <span class="number">2015</span><span class="number">-04</span><span class="number">-15</span>  <span class="operator">|</span> <span class="number">2</span>   <span class="operator">|</span> <span class="number">0</span>             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">2015</span><span class="number">-04</span>  <span class="operator">|</span> <span class="number">2015</span><span class="number">-04</span><span class="number">-13</span>  <span class="operator">|</span> <span class="number">3</span>   <span class="operator">|</span> <span class="number">0</span>             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">2015</span><span class="number">-04</span>  <span class="operator">|</span> <span class="number">2015</span><span class="number">-04</span><span class="number">-12</span>  <span class="operator">|</span> <span class="number">2</span>   <span class="operator">|</span> <span class="number">0</span>             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">2015</span><span class="number">-03</span>  <span class="operator">|</span> <span class="number">2015</span><span class="number">-03</span><span class="number">-12</span>  <span class="operator">|</span> <span class="number">1</span>   <span class="operator">|</span> <span class="number">0</span>             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">2015</span><span class="number">-03</span>  <span class="operator">|</span> <span class="number">2015</span><span class="number">-03</span><span class="number">-10</span>  <span class="operator">|</span> <span class="number">4</span>   <span class="operator">|</span> <span class="number">0</span>             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">2015</span><span class="number">-04</span>  <span class="operator">|</span> <span class="keyword">NULL</span>        <span class="operator">|</span> <span class="number">6</span>   <span class="operator">|</span> <span class="number">1</span>             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">2015</span><span class="number">-03</span>  <span class="operator">|</span> <span class="keyword">NULL</span>        <span class="operator">|</span> <span class="number">5</span>   <span class="operator">|</span> <span class="number">1</span>             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="keyword">NULL</span>     <span class="operator">|</span> <span class="keyword">NULL</span>        <span class="operator">|</span> <span class="number">7</span>   <span class="operator">|</span> <span class="number">3</span>             <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------+-------------+-----+---------------+</span></span><br></pre></td></tr></table></figure></li>
<li>如果把month和day调换顺序，则以day维度进行层级聚合：实现的上钻过程：天月的UV-&gt;天的UV-&gt;总UV  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">    <span class="keyword">day</span>,</span><br><span class="line">    <span class="keyword">month</span>,</span><br><span class="line">    <span class="built_in">COUNT</span>(<span class="keyword">DISTINCT</span> cookieid) <span class="keyword">AS</span> uv,</span><br><span class="line">    GROUPING__ID</span><br><span class="line"><span class="keyword">FROM</span> visit_log</span><br><span class="line">    <span class="keyword">GROUP</span> <span class="keyword">BY</span> <span class="keyword">day</span>,<span class="keyword">month</span></span><br><span class="line">    <span class="keyword">WITH</span> <span class="keyword">ROLLUP</span></span><br><span class="line">    <span class="keyword">ORDER</span> <span class="keyword">BY</span> GROUPING__ID;</span><br><span class="line">    </span><br><span class="line"><span class="operator">+</span><span class="comment">-------------+----------+-----+---------------+</span></span><br><span class="line"><span class="operator">|</span>     <span class="keyword">day</span>     <span class="operator">|</span>  <span class="keyword">month</span>   <span class="operator">|</span> uv  <span class="operator">|</span> grouping__id  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-------------+----------+-----+---------------+</span></span><br><span class="line"><span class="operator">|</span> <span class="number">2015</span><span class="number">-04</span><span class="number">-16</span>  <span class="operator">|</span> <span class="number">2015</span><span class="number">-04</span>  <span class="operator">|</span> <span class="number">2</span>   <span class="operator">|</span> <span class="number">0</span>             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">2015</span><span class="number">-04</span><span class="number">-15</span>  <span class="operator">|</span> <span class="number">2015</span><span class="number">-04</span>  <span class="operator">|</span> <span class="number">2</span>   <span class="operator">|</span> <span class="number">0</span>             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">2015</span><span class="number">-04</span><span class="number">-13</span>  <span class="operator">|</span> <span class="number">2015</span><span class="number">-04</span>  <span class="operator">|</span> <span class="number">3</span>   <span class="operator">|</span> <span class="number">0</span>             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">2015</span><span class="number">-04</span><span class="number">-12</span>  <span class="operator">|</span> <span class="number">2015</span><span class="number">-04</span>  <span class="operator">|</span> <span class="number">2</span>   <span class="operator">|</span> <span class="number">0</span>             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">2015</span><span class="number">-03</span><span class="number">-12</span>  <span class="operator">|</span> <span class="number">2015</span><span class="number">-03</span>  <span class="operator">|</span> <span class="number">1</span>   <span class="operator">|</span> <span class="number">0</span>             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">2015</span><span class="number">-03</span><span class="number">-10</span>  <span class="operator">|</span> <span class="number">2015</span><span class="number">-03</span>  <span class="operator">|</span> <span class="number">4</span>   <span class="operator">|</span> <span class="number">0</span>             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">2015</span><span class="number">-04</span><span class="number">-12</span>  <span class="operator">|</span> <span class="keyword">NULL</span>     <span class="operator">|</span> <span class="number">2</span>   <span class="operator">|</span> <span class="number">1</span>             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">2015</span><span class="number">-04</span><span class="number">-16</span>  <span class="operator">|</span> <span class="keyword">NULL</span>     <span class="operator">|</span> <span class="number">2</span>   <span class="operator">|</span> <span class="number">1</span>             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">2015</span><span class="number">-03</span><span class="number">-10</span>  <span class="operator">|</span> <span class="keyword">NULL</span>     <span class="operator">|</span> <span class="number">4</span>   <span class="operator">|</span> <span class="number">1</span>             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">2015</span><span class="number">-04</span><span class="number">-15</span>  <span class="operator">|</span> <span class="keyword">NULL</span>     <span class="operator">|</span> <span class="number">2</span>   <span class="operator">|</span> <span class="number">1</span>             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">2015</span><span class="number">-03</span><span class="number">-12</span>  <span class="operator">|</span> <span class="keyword">NULL</span>     <span class="operator">|</span> <span class="number">1</span>   <span class="operator">|</span> <span class="number">1</span>             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">2015</span><span class="number">-04</span><span class="number">-13</span>  <span class="operator">|</span> <span class="keyword">NULL</span>     <span class="operator">|</span> <span class="number">3</span>   <span class="operator">|</span> <span class="number">1</span>             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="keyword">NULL</span>        <span class="operator">|</span> <span class="keyword">NULL</span>     <span class="operator">|</span> <span class="number">7</span>   <span class="operator">|</span> <span class="number">3</span>             <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-------------+----------+-----+---------------+</span></span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ol>
<h4 id="8-2-5-5-开窗函数练习"><a href="#8-2-5-5-开窗函数练习" class="headerlink" title="8.2.5.5 开窗函数练习"></a>8.2.5.5 开窗函数练习</h4><h5 id="8-2-5-5-1-聚合开窗练习"><a href="#8-2-5-5-1-聚合开窗练习" class="headerlink" title="8.2.5.5.1 聚合开窗练习"></a>8.2.5.5.1 聚合开窗练习</h5><ol>
<li>建表导入数据 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> business <span class="keyword">as</span> </span><br><span class="line"><span class="keyword">select</span> stack(<span class="number">14</span>,</span><br><span class="line">    <span class="string">&#x27;jack&#x27;</span>,<span class="string">&#x27;2017-01-01&#x27;</span>,<span class="string">&#x27;10&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;tony&#x27;</span>,<span class="string">&#x27;2017-01-02&#x27;</span>,<span class="string">&#x27;15&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;jack&#x27;</span>,<span class="string">&#x27;2017-02-03&#x27;</span>,<span class="string">&#x27;23&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;tony&#x27;</span>,<span class="string">&#x27;2017-01-04&#x27;</span>,<span class="string">&#x27;29&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;jack&#x27;</span>,<span class="string">&#x27;2017-01-05&#x27;</span>,<span class="string">&#x27;46&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;jack&#x27;</span>,<span class="string">&#x27;2017-04-06&#x27;</span>,<span class="string">&#x27;42&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;tony&#x27;</span>,<span class="string">&#x27;2017-01-07&#x27;</span>,<span class="string">&#x27;50&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;jack&#x27;</span>,<span class="string">&#x27;2017-01-08&#x27;</span>,<span class="string">&#x27;55&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;mart&#x27;</span>,<span class="string">&#x27;2017-04-08&#x27;</span>,<span class="string">&#x27;62&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;mart&#x27;</span>,<span class="string">&#x27;2017-04-09&#x27;</span>,<span class="string">&#x27;68&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;neil&#x27;</span>,<span class="string">&#x27;2017-05-10&#x27;</span>,<span class="string">&#x27;12&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;mart&#x27;</span>,<span class="string">&#x27;2017-04-11&#x27;</span>,<span class="string">&#x27;75&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;neil&#x27;</span>,<span class="string">&#x27;2017-06-12&#x27;</span>,<span class="string">&#x27;80&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;mart&#x27;</span>,<span class="string">&#x27;2017-04-13&#x27;</span>,<span class="string">&#x27;94&#x27;</span>)</span><br><span class="line"><span class="keyword">as</span> (name, orderdate, cost)</span><br></pre></td></tr></table></figure></li>
<li>需求一： 查询在2017年4月份购买过的顾客及总人数 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line">       name,</span><br><span class="line">       <span class="built_in">count</span>(<span class="operator">*</span>) <span class="keyword">over</span> ()</span><br><span class="line"><span class="keyword">from</span> business</span><br><span class="line"><span class="keyword">where</span> substr(orderdate, <span class="number">0</span>, <span class="number">7</span>) <span class="operator">=</span> <span class="string">&#x27;2017-04&#x27;</span></span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> name;</span><br><span class="line"></span><br><span class="line"><span class="operator">+</span><span class="comment">-------+-----------------+</span></span><br><span class="line"><span class="operator">|</span> name  <span class="operator">|</span> count_window_0  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-------+-----------------+</span></span><br><span class="line"><span class="operator">|</span> mart  <span class="operator">|</span> <span class="number">2</span>               <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> jack  <span class="operator">|</span> <span class="number">2</span>               <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-------+-----------------+</span></span><br></pre></td></tr></table></figure></li>
<li>需求二：查询顾客的购买明细及月购买总额 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line">       name,</span><br><span class="line">       cost,</span><br><span class="line">       orderdate,</span><br><span class="line">       substr(orderdate, <span class="number">0</span>, <span class="number">7</span>),</span><br><span class="line">       <span class="built_in">sum</span>(cost) <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> substr(orderdate, <span class="number">0</span>, <span class="number">7</span>))</span><br><span class="line"><span class="keyword">from</span> business;</span><br><span class="line"></span><br><span class="line"><span class="operator">+</span><span class="comment">-------+-------+-------------+----------+---------------+</span></span><br><span class="line"><span class="operator">|</span> name  <span class="operator">|</span> cost  <span class="operator">|</span>  orderdate  <span class="operator">|</span>   _c3    <span class="operator">|</span> sum_window_0  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-------+-------+-------------+----------+---------------+</span></span><br><span class="line"><span class="operator">|</span> jack  <span class="operator">|</span> <span class="number">10</span>    <span class="operator">|</span> <span class="number">2017</span><span class="number">-01</span><span class="number">-01</span>  <span class="operator">|</span> <span class="number">2017</span><span class="number">-01</span>  <span class="operator">|</span> <span class="number">205.0</span>         <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> jack  <span class="operator">|</span> <span class="number">55</span>    <span class="operator">|</span> <span class="number">2017</span><span class="number">-01</span><span class="number">-08</span>  <span class="operator">|</span> <span class="number">2017</span><span class="number">-01</span>  <span class="operator">|</span> <span class="number">205.0</span>         <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> tony  <span class="operator">|</span> <span class="number">50</span>    <span class="operator">|</span> <span class="number">2017</span><span class="number">-01</span><span class="number">-07</span>  <span class="operator">|</span> <span class="number">2017</span><span class="number">-01</span>  <span class="operator">|</span> <span class="number">205.0</span>         <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> jack  <span class="operator">|</span> <span class="number">46</span>    <span class="operator">|</span> <span class="number">2017</span><span class="number">-01</span><span class="number">-05</span>  <span class="operator">|</span> <span class="number">2017</span><span class="number">-01</span>  <span class="operator">|</span> <span class="number">205.0</span>         <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> tony  <span class="operator">|</span> <span class="number">29</span>    <span class="operator">|</span> <span class="number">2017</span><span class="number">-01</span><span class="number">-04</span>  <span class="operator">|</span> <span class="number">2017</span><span class="number">-01</span>  <span class="operator">|</span> <span class="number">205.0</span>         <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> tony  <span class="operator">|</span> <span class="number">15</span>    <span class="operator">|</span> <span class="number">2017</span><span class="number">-01</span><span class="number">-02</span>  <span class="operator">|</span> <span class="number">2017</span><span class="number">-01</span>  <span class="operator">|</span> <span class="number">205.0</span>         <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> jack  <span class="operator">|</span> <span class="number">23</span>    <span class="operator">|</span> <span class="number">2017</span><span class="number">-02</span><span class="number">-03</span>  <span class="operator">|</span> <span class="number">2017</span><span class="number">-02</span>  <span class="operator">|</span> <span class="number">23.0</span>          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> mart  <span class="operator">|</span> <span class="number">94</span>    <span class="operator">|</span> <span class="number">2017</span><span class="number">-04</span><span class="number">-13</span>  <span class="operator">|</span> <span class="number">2017</span><span class="number">-04</span>  <span class="operator">|</span> <span class="number">341.0</span>         <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> jack  <span class="operator">|</span> <span class="number">42</span>    <span class="operator">|</span> <span class="number">2017</span><span class="number">-04</span><span class="number">-06</span>  <span class="operator">|</span> <span class="number">2017</span><span class="number">-04</span>  <span class="operator">|</span> <span class="number">341.0</span>         <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> mart  <span class="operator">|</span> <span class="number">75</span>    <span class="operator">|</span> <span class="number">2017</span><span class="number">-04</span><span class="number">-11</span>  <span class="operator">|</span> <span class="number">2017</span><span class="number">-04</span>  <span class="operator">|</span> <span class="number">341.0</span>         <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> mart  <span class="operator">|</span> <span class="number">68</span>    <span class="operator">|</span> <span class="number">2017</span><span class="number">-04</span><span class="number">-09</span>  <span class="operator">|</span> <span class="number">2017</span><span class="number">-04</span>  <span class="operator">|</span> <span class="number">341.0</span>         <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> mart  <span class="operator">|</span> <span class="number">62</span>    <span class="operator">|</span> <span class="number">2017</span><span class="number">-04</span><span class="number">-08</span>  <span class="operator">|</span> <span class="number">2017</span><span class="number">-04</span>  <span class="operator">|</span> <span class="number">341.0</span>         <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> neil  <span class="operator">|</span> <span class="number">12</span>    <span class="operator">|</span> <span class="number">2017</span><span class="number">-05</span><span class="number">-10</span>  <span class="operator">|</span> <span class="number">2017</span><span class="number">-05</span>  <span class="operator">|</span> <span class="number">12.0</span>          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> neil  <span class="operator">|</span> <span class="number">80</span>    <span class="operator">|</span> <span class="number">2017</span><span class="number">-06</span><span class="number">-12</span>  <span class="operator">|</span> <span class="number">2017</span><span class="number">-06</span>  <span class="operator">|</span> <span class="number">80.0</span>          <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-------+-------+-------------+----------+---------------+</span></span><br></pre></td></tr></table></figure></li>
<li>需求三：查询每个顾客的购买明细及月购买总额 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line">       name,</span><br><span class="line">       cost,</span><br><span class="line">       orderdate,</span><br><span class="line">       substr(orderdate, <span class="number">0</span>, <span class="number">7</span>),</span><br><span class="line">       <span class="built_in">sum</span>(cost) <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> substr(orderdate, <span class="number">0</span>, <span class="number">7</span>),name)</span><br><span class="line"><span class="keyword">from</span> business;</span><br><span class="line"></span><br><span class="line"><span class="operator">+</span><span class="comment">-------+-------+-------------+----------+---------------+</span></span><br><span class="line"><span class="operator">|</span> name  <span class="operator">|</span> cost  <span class="operator">|</span>  orderdate  <span class="operator">|</span>   _c3    <span class="operator">|</span> sum_window_0  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-------+-------+-------------+----------+---------------+</span></span><br><span class="line"><span class="operator">|</span> jack  <span class="operator">|</span> <span class="number">55</span>    <span class="operator">|</span> <span class="number">2017</span><span class="number">-01</span><span class="number">-08</span>  <span class="operator">|</span> <span class="number">2017</span><span class="number">-01</span>  <span class="operator">|</span> <span class="number">111.0</span>         <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> jack  <span class="operator">|</span> <span class="number">46</span>    <span class="operator">|</span> <span class="number">2017</span><span class="number">-01</span><span class="number">-05</span>  <span class="operator">|</span> <span class="number">2017</span><span class="number">-01</span>  <span class="operator">|</span> <span class="number">111.0</span>         <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> jack  <span class="operator">|</span> <span class="number">10</span>    <span class="operator">|</span> <span class="number">2017</span><span class="number">-01</span><span class="number">-01</span>  <span class="operator">|</span> <span class="number">2017</span><span class="number">-01</span>  <span class="operator">|</span> <span class="number">111.0</span>         <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> tony  <span class="operator">|</span> <span class="number">50</span>    <span class="operator">|</span> <span class="number">2017</span><span class="number">-01</span><span class="number">-07</span>  <span class="operator">|</span> <span class="number">2017</span><span class="number">-01</span>  <span class="operator">|</span> <span class="number">94.0</span>          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> tony  <span class="operator">|</span> <span class="number">29</span>    <span class="operator">|</span> <span class="number">2017</span><span class="number">-01</span><span class="number">-04</span>  <span class="operator">|</span> <span class="number">2017</span><span class="number">-01</span>  <span class="operator">|</span> <span class="number">94.0</span>          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> tony  <span class="operator">|</span> <span class="number">15</span>    <span class="operator">|</span> <span class="number">2017</span><span class="number">-01</span><span class="number">-02</span>  <span class="operator">|</span> <span class="number">2017</span><span class="number">-01</span>  <span class="operator">|</span> <span class="number">94.0</span>          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> jack  <span class="operator">|</span> <span class="number">23</span>    <span class="operator">|</span> <span class="number">2017</span><span class="number">-02</span><span class="number">-03</span>  <span class="operator">|</span> <span class="number">2017</span><span class="number">-02</span>  <span class="operator">|</span> <span class="number">23.0</span>          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> jack  <span class="operator">|</span> <span class="number">42</span>    <span class="operator">|</span> <span class="number">2017</span><span class="number">-04</span><span class="number">-06</span>  <span class="operator">|</span> <span class="number">2017</span><span class="number">-04</span>  <span class="operator">|</span> <span class="number">42.0</span>          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> mart  <span class="operator">|</span> <span class="number">75</span>    <span class="operator">|</span> <span class="number">2017</span><span class="number">-04</span><span class="number">-11</span>  <span class="operator">|</span> <span class="number">2017</span><span class="number">-04</span>  <span class="operator">|</span> <span class="number">299.0</span>         <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> mart  <span class="operator">|</span> <span class="number">94</span>    <span class="operator">|</span> <span class="number">2017</span><span class="number">-04</span><span class="number">-13</span>  <span class="operator">|</span> <span class="number">2017</span><span class="number">-04</span>  <span class="operator">|</span> <span class="number">299.0</span>         <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> mart  <span class="operator">|</span> <span class="number">68</span>    <span class="operator">|</span> <span class="number">2017</span><span class="number">-04</span><span class="number">-09</span>  <span class="operator">|</span> <span class="number">2017</span><span class="number">-04</span>  <span class="operator">|</span> <span class="number">299.0</span>         <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> mart  <span class="operator">|</span> <span class="number">62</span>    <span class="operator">|</span> <span class="number">2017</span><span class="number">-04</span><span class="number">-08</span>  <span class="operator">|</span> <span class="number">2017</span><span class="number">-04</span>  <span class="operator">|</span> <span class="number">299.0</span>         <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> neil  <span class="operator">|</span> <span class="number">12</span>    <span class="operator">|</span> <span class="number">2017</span><span class="number">-05</span><span class="number">-10</span>  <span class="operator">|</span> <span class="number">2017</span><span class="number">-05</span>  <span class="operator">|</span> <span class="number">12.0</span>          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> neil  <span class="operator">|</span> <span class="number">80</span>    <span class="operator">|</span> <span class="number">2017</span><span class="number">-06</span><span class="number">-12</span>  <span class="operator">|</span> <span class="number">2017</span><span class="number">-06</span>  <span class="operator">|</span> <span class="number">80.0</span>          <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-------+-------+-------------+----------+---------------+</span></span><br></pre></td></tr></table></figure></li>
<li>需求四：查询每个顾客的购买明细及月购买总额, 将每个顾客的cost按照日期进行累加 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line">       name,</span><br><span class="line">       cost,</span><br><span class="line">       orderdate,</span><br><span class="line">       substr(orderdate, <span class="number">0</span>, <span class="number">7</span>),</span><br><span class="line">       <span class="built_in">sum</span>(cost) <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> name <span class="keyword">order</span> <span class="keyword">by</span> orderdate),</span><br><span class="line">       <span class="built_in">sum</span>(cost) <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> substr(orderdate, <span class="number">0</span>, <span class="number">7</span>),name)</span><br><span class="line"><span class="keyword">from</span> business</span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> name,orderdate;</span><br><span class="line"></span><br><span class="line"><span class="operator">+</span><span class="comment">-------+-------+-------------+----------+--------+---------------+</span></span><br><span class="line"><span class="operator">|</span> name  <span class="operator">|</span> cost  <span class="operator">|</span>  orderdate  <span class="operator">|</span>   _c3    <span class="operator">|</span>  _c4   <span class="operator">|</span> sum_window_1  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-------+-------+-------------+----------+--------+---------------+</span></span><br><span class="line"><span class="operator">|</span> jack  <span class="operator">|</span> <span class="number">10</span>    <span class="operator">|</span> <span class="number">2017</span><span class="number">-01</span><span class="number">-01</span>  <span class="operator">|</span> <span class="number">2017</span><span class="number">-01</span>  <span class="operator">|</span> <span class="number">10.0</span>   <span class="operator">|</span> <span class="number">111.0</span>         <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> jack  <span class="operator">|</span> <span class="number">46</span>    <span class="operator">|</span> <span class="number">2017</span><span class="number">-01</span><span class="number">-05</span>  <span class="operator">|</span> <span class="number">2017</span><span class="number">-01</span>  <span class="operator">|</span> <span class="number">56.0</span>   <span class="operator">|</span> <span class="number">111.0</span>         <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> jack  <span class="operator">|</span> <span class="number">55</span>    <span class="operator">|</span> <span class="number">2017</span><span class="number">-01</span><span class="number">-08</span>  <span class="operator">|</span> <span class="number">2017</span><span class="number">-01</span>  <span class="operator">|</span> <span class="number">111.0</span>  <span class="operator">|</span> <span class="number">111.0</span>         <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> jack  <span class="operator">|</span> <span class="number">23</span>    <span class="operator">|</span> <span class="number">2017</span><span class="number">-02</span><span class="number">-03</span>  <span class="operator">|</span> <span class="number">2017</span><span class="number">-02</span>  <span class="operator">|</span> <span class="number">134.0</span>  <span class="operator">|</span> <span class="number">23.0</span>          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> jack  <span class="operator">|</span> <span class="number">42</span>    <span class="operator">|</span> <span class="number">2017</span><span class="number">-04</span><span class="number">-06</span>  <span class="operator">|</span> <span class="number">2017</span><span class="number">-04</span>  <span class="operator">|</span> <span class="number">176.0</span>  <span class="operator">|</span> <span class="number">42.0</span>          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> mart  <span class="operator">|</span> <span class="number">62</span>    <span class="operator">|</span> <span class="number">2017</span><span class="number">-04</span><span class="number">-08</span>  <span class="operator">|</span> <span class="number">2017</span><span class="number">-04</span>  <span class="operator">|</span> <span class="number">62.0</span>   <span class="operator">|</span> <span class="number">299.0</span>         <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> mart  <span class="operator">|</span> <span class="number">68</span>    <span class="operator">|</span> <span class="number">2017</span><span class="number">-04</span><span class="number">-09</span>  <span class="operator">|</span> <span class="number">2017</span><span class="number">-04</span>  <span class="operator">|</span> <span class="number">130.0</span>  <span class="operator">|</span> <span class="number">299.0</span>         <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> mart  <span class="operator">|</span> <span class="number">75</span>    <span class="operator">|</span> <span class="number">2017</span><span class="number">-04</span><span class="number">-11</span>  <span class="operator">|</span> <span class="number">2017</span><span class="number">-04</span>  <span class="operator">|</span> <span class="number">205.0</span>  <span class="operator">|</span> <span class="number">299.0</span>         <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> mart  <span class="operator">|</span> <span class="number">94</span>    <span class="operator">|</span> <span class="number">2017</span><span class="number">-04</span><span class="number">-13</span>  <span class="operator">|</span> <span class="number">2017</span><span class="number">-04</span>  <span class="operator">|</span> <span class="number">299.0</span>  <span class="operator">|</span> <span class="number">299.0</span>         <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> neil  <span class="operator">|</span> <span class="number">12</span>    <span class="operator">|</span> <span class="number">2017</span><span class="number">-05</span><span class="number">-10</span>  <span class="operator">|</span> <span class="number">2017</span><span class="number">-05</span>  <span class="operator">|</span> <span class="number">12.0</span>   <span class="operator">|</span> <span class="number">12.0</span>          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> neil  <span class="operator">|</span> <span class="number">80</span>    <span class="operator">|</span> <span class="number">2017</span><span class="number">-06</span><span class="number">-12</span>  <span class="operator">|</span> <span class="number">2017</span><span class="number">-06</span>  <span class="operator">|</span> <span class="number">92.0</span>   <span class="operator">|</span> <span class="number">80.0</span>          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> tony  <span class="operator">|</span> <span class="number">15</span>    <span class="operator">|</span> <span class="number">2017</span><span class="number">-01</span><span class="number">-02</span>  <span class="operator">|</span> <span class="number">2017</span><span class="number">-01</span>  <span class="operator">|</span> <span class="number">15.0</span>   <span class="operator">|</span> <span class="number">94.0</span>          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> tony  <span class="operator">|</span> <span class="number">29</span>    <span class="operator">|</span> <span class="number">2017</span><span class="number">-01</span><span class="number">-04</span>  <span class="operator">|</span> <span class="number">2017</span><span class="number">-01</span>  <span class="operator">|</span> <span class="number">44.0</span>   <span class="operator">|</span> <span class="number">94.0</span>          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> tony  <span class="operator">|</span> <span class="number">50</span>    <span class="operator">|</span> <span class="number">2017</span><span class="number">-01</span><span class="number">-07</span>  <span class="operator">|</span> <span class="number">2017</span><span class="number">-01</span>  <span class="operator">|</span> <span class="number">94.0</span>   <span class="operator">|</span> <span class="number">94.0</span>          <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-------+-------+-------------+----------+--------+---------------+</span></span><br></pre></td></tr></table></figure></li>
<li>需求五：查询每个顾客上次的购买时间以及下一次的购买时间 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line">       name,</span><br><span class="line">       cost,</span><br><span class="line">       orderdate,</span><br><span class="line">       <span class="built_in">lag</span>(orderdate, <span class="number">1</span>, &quot;first&quot;) <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> name <span class="keyword">order</span> <span class="keyword">by</span> orderdate),</span><br><span class="line">       <span class="built_in">lead</span>(orderdate, <span class="number">1</span>, &quot;last&quot;) <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> name <span class="keyword">order</span> <span class="keyword">by</span> orderdate)</span><br><span class="line"><span class="keyword">from</span> business</span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> name, orderdate;</span><br><span class="line"></span><br><span class="line"><span class="operator">+</span><span class="comment">-------+-------+-------------+---------------+----------------+</span></span><br><span class="line"><span class="operator">|</span> name  <span class="operator">|</span> cost  <span class="operator">|</span>  orderdate  <span class="operator">|</span> lag_window_0  <span class="operator">|</span> lead_window_1  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-------+-------+-------------+---------------+----------------+</span></span><br><span class="line"><span class="operator">|</span> jack  <span class="operator">|</span> <span class="number">10</span>    <span class="operator">|</span> <span class="number">2017</span><span class="number">-01</span><span class="number">-01</span>  <span class="operator">|</span> <span class="keyword">first</span>         <span class="operator">|</span> <span class="number">2017</span><span class="number">-01</span><span class="number">-05</span>     <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> jack  <span class="operator">|</span> <span class="number">46</span>    <span class="operator">|</span> <span class="number">2017</span><span class="number">-01</span><span class="number">-05</span>  <span class="operator">|</span> <span class="number">2017</span><span class="number">-01</span><span class="number">-01</span>    <span class="operator">|</span> <span class="number">2017</span><span class="number">-01</span><span class="number">-08</span>     <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> jack  <span class="operator">|</span> <span class="number">55</span>    <span class="operator">|</span> <span class="number">2017</span><span class="number">-01</span><span class="number">-08</span>  <span class="operator">|</span> <span class="number">2017</span><span class="number">-01</span><span class="number">-05</span>    <span class="operator">|</span> <span class="number">2017</span><span class="number">-02</span><span class="number">-03</span>     <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> jack  <span class="operator">|</span> <span class="number">23</span>    <span class="operator">|</span> <span class="number">2017</span><span class="number">-02</span><span class="number">-03</span>  <span class="operator">|</span> <span class="number">2017</span><span class="number">-01</span><span class="number">-08</span>    <span class="operator">|</span> <span class="number">2017</span><span class="number">-04</span><span class="number">-06</span>     <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> jack  <span class="operator">|</span> <span class="number">42</span>    <span class="operator">|</span> <span class="number">2017</span><span class="number">-04</span><span class="number">-06</span>  <span class="operator">|</span> <span class="number">2017</span><span class="number">-02</span><span class="number">-03</span>    <span class="operator">|</span> <span class="keyword">last</span>           <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> mart  <span class="operator">|</span> <span class="number">62</span>    <span class="operator">|</span> <span class="number">2017</span><span class="number">-04</span><span class="number">-08</span>  <span class="operator">|</span> <span class="keyword">first</span>         <span class="operator">|</span> <span class="number">2017</span><span class="number">-04</span><span class="number">-09</span>     <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> mart  <span class="operator">|</span> <span class="number">68</span>    <span class="operator">|</span> <span class="number">2017</span><span class="number">-04</span><span class="number">-09</span>  <span class="operator">|</span> <span class="number">2017</span><span class="number">-04</span><span class="number">-08</span>    <span class="operator">|</span> <span class="number">2017</span><span class="number">-04</span><span class="number">-11</span>     <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> mart  <span class="operator">|</span> <span class="number">75</span>    <span class="operator">|</span> <span class="number">2017</span><span class="number">-04</span><span class="number">-11</span>  <span class="operator">|</span> <span class="number">2017</span><span class="number">-04</span><span class="number">-09</span>    <span class="operator">|</span> <span class="number">2017</span><span class="number">-04</span><span class="number">-13</span>     <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> mart  <span class="operator">|</span> <span class="number">94</span>    <span class="operator">|</span> <span class="number">2017</span><span class="number">-04</span><span class="number">-13</span>  <span class="operator">|</span> <span class="number">2017</span><span class="number">-04</span><span class="number">-11</span>    <span class="operator">|</span> <span class="keyword">last</span>           <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> neil  <span class="operator">|</span> <span class="number">12</span>    <span class="operator">|</span> <span class="number">2017</span><span class="number">-05</span><span class="number">-10</span>  <span class="operator">|</span> <span class="keyword">first</span>         <span class="operator">|</span> <span class="number">2017</span><span class="number">-06</span><span class="number">-12</span>     <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> neil  <span class="operator">|</span> <span class="number">80</span>    <span class="operator">|</span> <span class="number">2017</span><span class="number">-06</span><span class="number">-12</span>  <span class="operator">|</span> <span class="number">2017</span><span class="number">-05</span><span class="number">-10</span>    <span class="operator">|</span> <span class="keyword">last</span>           <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> tony  <span class="operator">|</span> <span class="number">15</span>    <span class="operator">|</span> <span class="number">2017</span><span class="number">-01</span><span class="number">-02</span>  <span class="operator">|</span> <span class="keyword">first</span>         <span class="operator">|</span> <span class="number">2017</span><span class="number">-01</span><span class="number">-04</span>     <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> tony  <span class="operator">|</span> <span class="number">29</span>    <span class="operator">|</span> <span class="number">2017</span><span class="number">-01</span><span class="number">-04</span>  <span class="operator">|</span> <span class="number">2017</span><span class="number">-01</span><span class="number">-02</span>    <span class="operator">|</span> <span class="number">2017</span><span class="number">-01</span><span class="number">-07</span>     <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> tony  <span class="operator">|</span> <span class="number">50</span>    <span class="operator">|</span> <span class="number">2017</span><span class="number">-01</span><span class="number">-07</span>  <span class="operator">|</span> <span class="number">2017</span><span class="number">-01</span><span class="number">-04</span>    <span class="operator">|</span> <span class="keyword">last</span>           <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-------+-------+-------------+---------------+----------------+</span></span><br></pre></td></tr></table></figure></li>
<li>需求六：查询前20%时间的订单信息 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="operator">*</span></span><br><span class="line"><span class="keyword">from</span> (</span><br><span class="line">         <span class="keyword">select</span> name,</span><br><span class="line">                cost,</span><br><span class="line">                orderdate,</span><br><span class="line">                <span class="built_in">ntile</span>(<span class="number">5</span>) <span class="keyword">over</span> (<span class="keyword">order</span> <span class="keyword">by</span> orderdate) num</span><br><span class="line">         <span class="keyword">from</span> business</span><br><span class="line">     ) t1</span><br><span class="line"><span class="keyword">where</span> t1.num <span class="operator">=</span> <span class="number">1</span></span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> name, orderdate;</span><br><span class="line"></span><br><span class="line"><span class="operator">+</span><span class="comment">----------+----------+---------------+---------+</span></span><br><span class="line"><span class="operator">|</span> t1.name  <span class="operator">|</span> t1.cost  <span class="operator">|</span> t1.orderdate  <span class="operator">|</span> t1.num  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------+----------+---------------+---------+</span></span><br><span class="line"><span class="operator">|</span> jack     <span class="operator">|</span> <span class="number">10</span>       <span class="operator">|</span> <span class="number">2017</span><span class="number">-01</span><span class="number">-01</span>    <span class="operator">|</span> <span class="number">1</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> tony     <span class="operator">|</span> <span class="number">15</span>       <span class="operator">|</span> <span class="number">2017</span><span class="number">-01</span><span class="number">-02</span>    <span class="operator">|</span> <span class="number">1</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> tony     <span class="operator">|</span> <span class="number">29</span>       <span class="operator">|</span> <span class="number">2017</span><span class="number">-01</span><span class="number">-04</span>    <span class="operator">|</span> <span class="number">1</span>       <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------+----------+---------------+---------+</span></span><br></pre></td></tr></table></figure></li>
</ol>
<h5 id="8-2-5-5-1-排序开窗练习"><a href="#8-2-5-5-1-排序开窗练习" class="headerlink" title="8.2.5.5.1 排序开窗练习"></a>8.2.5.5.1 排序开窗练习</h5><ol>
<li>建表导数据 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> score <span class="keyword">as</span></span><br><span class="line"><span class="keyword">select</span> stack(<span class="number">12</span>,</span><br><span class="line"> &quot;孙悟空&quot;, &quot;语文&quot;, &quot;87&quot;,</span><br><span class="line"> &quot;孙悟空&quot;, &quot;数学&quot;, &quot;95&quot;,</span><br><span class="line"> &quot;孙悟空&quot;, &quot;英语&quot;, &quot;68&quot;,</span><br><span class="line"> &quot;大海&quot;, &quot;语文&quot;, &quot;94&quot;,</span><br><span class="line"> &quot;大海&quot;, &quot;数学&quot;, &quot;56&quot;,</span><br><span class="line"> &quot;大海&quot;, &quot;英语&quot;, &quot;84&quot;,</span><br><span class="line"> &quot;宋宋&quot;, &quot;语文&quot;, &quot;64&quot;,</span><br><span class="line"> &quot;宋宋&quot;, &quot;数学&quot;, &quot;86&quot;,</span><br><span class="line"> &quot;宋宋&quot;, &quot;英语&quot;, &quot;84&quot;,</span><br><span class="line"> &quot;婷婷&quot;, &quot;语文&quot;, &quot;65&quot;,</span><br><span class="line"> &quot;婷婷&quot;, &quot;数学&quot;, &quot;85&quot;,</span><br><span class="line"> &quot;婷婷&quot;, &quot;英语&quot;, &quot;78&quot;)</span><br><span class="line"><span class="keyword">as</span> (name, subject, score);</span><br></pre></td></tr></table></figure></li>
<li>根据学科进行分区操作 并且按照成绩字段进行倒序，最后利用排名函数 完成排名  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> name,</span><br><span class="line">       subject,</span><br><span class="line">       score,</span><br><span class="line">       <span class="built_in">rank</span>() <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> subject <span class="keyword">order</span> <span class="keyword">by</span> score <span class="keyword">desc</span> ),</span><br><span class="line">       <span class="built_in">dense_rank</span>() <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> subject <span class="keyword">order</span> <span class="keyword">by</span> score <span class="keyword">desc</span> ),</span><br><span class="line">       <span class="built_in">row_number</span>() <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> subject <span class="keyword">order</span> <span class="keyword">by</span> score <span class="keyword">desc</span> )</span><br><span class="line"><span class="keyword">from</span> score;</span><br><span class="line"></span><br><span class="line"><span class="operator">+</span><span class="comment">-------+----------+--------+----------------+----------------------+----------------------+</span></span><br><span class="line"><span class="operator">|</span> name  <span class="operator">|</span> subject  <span class="operator">|</span> score  <span class="operator">|</span> rank_window_0  <span class="operator">|</span> dense_rank_window_1  <span class="operator">|</span> row_number_window_2  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-------+----------+--------+----------------+----------------------+----------------------+</span></span><br><span class="line"><span class="operator">|</span> 孙悟空   <span class="operator">|</span> 数学       <span class="operator">|</span> <span class="number">95</span>     <span class="operator">|</span> <span class="number">1</span>              <span class="operator">|</span> <span class="number">1</span>                    <span class="operator">|</span> <span class="number">1</span>                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> 宋宋    <span class="operator">|</span> 数学       <span class="operator">|</span> <span class="number">86</span>     <span class="operator">|</span> <span class="number">2</span>              <span class="operator">|</span> <span class="number">2</span>                    <span class="operator">|</span> <span class="number">2</span>                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> 婷婷    <span class="operator">|</span> 数学       <span class="operator">|</span> <span class="number">85</span>     <span class="operator">|</span> <span class="number">3</span>              <span class="operator">|</span> <span class="number">3</span>                    <span class="operator">|</span> <span class="number">3</span>                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> 大海    <span class="operator">|</span> 数学       <span class="operator">|</span> <span class="number">56</span>     <span class="operator">|</span> <span class="number">4</span>              <span class="operator">|</span> <span class="number">4</span>                    <span class="operator">|</span> <span class="number">4</span>                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> 宋宋    <span class="operator">|</span> 英语       <span class="operator">|</span> <span class="number">84</span>     <span class="operator">|</span> <span class="number">1</span>              <span class="operator">|</span> <span class="number">1</span>                    <span class="operator">|</span> <span class="number">1</span>                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> 大海    <span class="operator">|</span> 英语       <span class="operator">|</span> <span class="number">84</span>     <span class="operator">|</span> <span class="number">1</span>              <span class="operator">|</span> <span class="number">1</span>                    <span class="operator">|</span> <span class="number">2</span>                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> 婷婷    <span class="operator">|</span> 英语       <span class="operator">|</span> <span class="number">78</span>     <span class="operator">|</span> <span class="number">3</span>              <span class="operator">|</span> <span class="number">2</span>                    <span class="operator">|</span> <span class="number">3</span>                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> 孙悟空   <span class="operator">|</span> 英语       <span class="operator">|</span> <span class="number">68</span>     <span class="operator">|</span> <span class="number">4</span>              <span class="operator">|</span> <span class="number">3</span>                    <span class="operator">|</span> <span class="number">4</span>                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> 大海    <span class="operator">|</span> 语文       <span class="operator">|</span> <span class="number">94</span>     <span class="operator">|</span> <span class="number">1</span>              <span class="operator">|</span> <span class="number">1</span>                    <span class="operator">|</span> <span class="number">1</span>                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> 孙悟空   <span class="operator">|</span> 语文       <span class="operator">|</span> <span class="number">87</span>     <span class="operator">|</span> <span class="number">2</span>              <span class="operator">|</span> <span class="number">2</span>                    <span class="operator">|</span> <span class="number">2</span>                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> 婷婷    <span class="operator">|</span> 语文       <span class="operator">|</span> <span class="number">65</span>     <span class="operator">|</span> <span class="number">3</span>              <span class="operator">|</span> <span class="number">3</span>                    <span class="operator">|</span> <span class="number">3</span>                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> 宋宋    <span class="operator">|</span> 语文       <span class="operator">|</span> <span class="number">64</span>     <span class="operator">|</span> <span class="number">4</span>              <span class="operator">|</span> <span class="number">4</span>                    <span class="operator">|</span> <span class="number">4</span>                    <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-------+----------+--------+----------------+----------------------+----------------------+</span></span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="8-2-6-其他常用函数"><a href="#8-2-6-其他常用函数" class="headerlink" title="8.2.6 其他常用函数"></a>8.2.6 其他常用函数</h3><table>
<thead>
<tr>
<th>类别</th>
<th>函数</th>
<th>描述</th>
<th>示例</th>
</tr>
</thead>
<tbody><tr>
<td>常用日期函数</td>
<td>unix_timestamp</td>
<td>返回当前或指定时间的时间戳</td>
<td>select unix_timestamp();select unix_timestamp(“2020-10-28”,’yyyy-MM-dd’);</td>
</tr>
<tr>
<td></td>
<td>from_unixtime</td>
<td>将时间戳转为日期格式</td>
<td>select from_unixtime(1603843200);</td>
</tr>
<tr>
<td></td>
<td>current_date</td>
<td>当前日期</td>
<td>select current_date;</td>
</tr>
<tr>
<td></td>
<td>current_timestamp</td>
<td>当前的日期加时间</td>
<td>select current_timestamp;</td>
</tr>
<tr>
<td></td>
<td>to_date</td>
<td>抽取日期部分</td>
<td>select to_date(‘2020-10-28 12:12:12’);</td>
</tr>
<tr>
<td></td>
<td>year</td>
<td>获取年</td>
<td>select year(‘2020-10-28 12:12:12’);</td>
</tr>
<tr>
<td></td>
<td>month</td>
<td>获取月</td>
<td>select month(‘2020-10-28 12:12:12’);</td>
</tr>
<tr>
<td></td>
<td>day</td>
<td>获取日</td>
<td>select day(‘2020-10-28 12:12:12’);</td>
</tr>
<tr>
<td></td>
<td>hour</td>
<td>获取时</td>
<td>select hour(‘2020-10-28 12:12:12’);</td>
</tr>
<tr>
<td></td>
<td>minute</td>
<td>获取分</td>
<td>select minute(‘2020-10-28 12:12:12’);</td>
</tr>
<tr>
<td></td>
<td>second</td>
<td>获取秒</td>
<td>select second(‘2020-10-28 12:12:12’);</td>
</tr>
<tr>
<td></td>
<td>weekofyear</td>
<td>当前时间是一年中的第几周</td>
<td>select weekofyear(‘2020-10-28 12:12:12’);</td>
</tr>
<tr>
<td></td>
<td>dayofmonth</td>
<td>当前时间是一个月中的第几天</td>
<td>select dayofmonth(‘2020-10-28 12:12:12’);</td>
</tr>
<tr>
<td></td>
<td>months_between</td>
<td>两个日期间的月份</td>
<td>select months_between(‘2020-04-01’,’2020-10-28’);</td>
</tr>
<tr>
<td></td>
<td>add_months</td>
<td>日期加减月</td>
<td>select add_months(‘2020-10-28’,-3);</td>
</tr>
<tr>
<td></td>
<td>datediff</td>
<td>两个日期相差的天数</td>
<td>select datediff(‘2020-11-04’,’2020-10-28’);</td>
</tr>
<tr>
<td></td>
<td>date_add</td>
<td>日期加天数</td>
<td>select date_add(‘2020-10-28’,4);</td>
</tr>
<tr>
<td></td>
<td>date_sub</td>
<td>日期减天数</td>
<td>select date_sub(‘2020-10-28’,-4);</td>
</tr>
<tr>
<td></td>
<td>last_day</td>
<td>日期的当月的最后一天</td>
<td>select last_day(‘2020-02-30’);</td>
</tr>
<tr>
<td></td>
<td>date_format</td>
<td>格式化日期</td>
<td>select date_format(‘2020-10-28 12:12:12’,’yyyy/MM/dd HH:mm:ss’);</td>
</tr>
<tr>
<td>常用取整函数</td>
<td>round</td>
<td>四舍五入</td>
<td>select round(3.14);select round(3.54);</td>
</tr>
<tr>
<td></td>
<td>ceil</td>
<td>向上取整</td>
<td>select ceil(3.14);select ceil(3.54);</td>
</tr>
<tr>
<td></td>
<td>floor</td>
<td>向下取整</td>
<td>select floor(3.14);select floor(3.54);</td>
</tr>
<tr>
<td>常用字符串操作函数</td>
<td>upper</td>
<td>转大写</td>
<td>select upper(‘low’);</td>
</tr>
<tr>
<td></td>
<td>lower</td>
<td>转小写</td>
<td>select lower(‘low’);</td>
</tr>
<tr>
<td></td>
<td>length</td>
<td>长度</td>
<td>select length(“atguigu”);</td>
</tr>
<tr>
<td></td>
<td>trim</td>
<td>前后去空格</td>
<td>select trim(“ atguigu “);</td>
</tr>
<tr>
<td></td>
<td>lpad</td>
<td>向左补齐，到指定长度</td>
<td>select lpad(‘atguigu’,9,’g’);</td>
</tr>
<tr>
<td></td>
<td>rpad</td>
<td>向右补齐，到指定长度</td>
<td>select rpad(‘atguigu’,9,’g’);</td>
</tr>
<tr>
<td></td>
<td>regexp_replace</td>
<td>使用正则表达式匹配目标字符串，匹配成功后替换！</td>
<td>SELECT regexp_replace(‘2020/10/25’, ‘/‘, ‘-‘);</td>
</tr>
<tr>
<td>集合操作</td>
<td>size</td>
<td>集合中元素的个数</td>
<td>select size(friends) from person_info;</td>
</tr>
<tr>
<td></td>
<td>map_keys</td>
<td>返回map中的key</td>
<td>select map_keys(children) from person_info;</td>
</tr>
<tr>
<td></td>
<td>map_value</td>
<td>返回map中的value</td>
<td>select map_values(children) from person_info;</td>
</tr>
<tr>
<td></td>
<td>array_contains</td>
<td>判断array中是否包含某个元素</td>
<td>select array_contains(friends,’bingbing’) from person_info;</td>
</tr>
<tr>
<td></td>
<td>sort_array</td>
<td>将array中的元素排序</td>
<td>select sort_array(friends) from person_info;</td>
</tr>
<tr>
<td></td>
<td>grouping_set</td>
<td>多维分析</td>
<td></td>
</tr>
</tbody></table>
<h2 id="8-3-自定义函数"><a href="#8-3-自定义函数" class="headerlink" title="8.3 自定义函数"></a>8.3 自定义函数</h2><ol>
<li>Hive 自带了一些函数，比如：max/min等，但是数量有限，自己可以通过自定义UDF来方便的扩展。</li>
<li>当Hive提供的内置函数无法满足你的业务处理需要时，此时就可以考虑使用用户自定义函数（UDF：user-defined function）。</li>
<li>根据用户自定义函数类别分为以下三种：<ul>
<li>UDF（User-Defined-Function）：一进一出</li>
<li>UDAF（User-Defined Aggregation Function）：聚集函数，多进一出，类似于：count/max/min</li>
<li>UDTF（User-Defined Table-Generating Functions）一进多出，如lateral view explode()</li>
</ul>
</li>
<li>官方文档地址<ul>
<li><a href="https://cwiki.apache.org/confluence/display/Hive/HivePlugins">https://cwiki.apache.org/confluence/display/Hive/HivePlugins</a></li>
</ul>
</li>
<li>编程步骤：<ol>
<li>继承Hive提供的类<ul>
<li>org.apache.hadoop.hive.ql.udf.generic.GenericUDF  </li>
<li>org.apache.hadoop.hive.ql.udf.generic.GenericUDTF;</li>
</ul>
</li>
<li>实现类中的抽象方法</li>
<li>在hive的命令行窗口创建函数 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 添加jar</span></span><br><span class="line"><span class="keyword">add</span> jar linux_jar_path</span><br><span class="line"><span class="comment">-- 创建function</span></span><br><span class="line"><span class="keyword">create</span> [temporary] <span class="keyword">function</span> [dbname.]function_name <span class="keyword">AS</span> class_name;</span><br></pre></td></tr></table></figure></li>
<li>在hive的命令行窗口删除函数 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">drop</span> [temporary] <span class="keyword">function</span> [if <span class="keyword">exists</span>] [dbname.]function_name;</span><br></pre></td></tr></table></figure>
<h2 id="8-4-自定义UDF函数"><a href="#8-4-自定义UDF函数" class="headerlink" title="8.4 自定义UDF函数"></a>8.4 自定义UDF函数</h2></li>
</ol>
</li>
<li>创建函数类</li>
<li>继承org.apache.hadoop.hive.ql.udf.generic.GenericUDF</li>
<li>重写initialize，evaluate，getDisplayString</li>
<li>执行hive add jar</li>
<li>创建函数</li>
<li>使用函数</li>
<li>示例 <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 一进一出</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">UDFTest</span> <span class="keyword">extends</span> <span class="title">GenericUDF</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> objectInspectors</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> UDFArgumentException</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> ObjectInspector <span class="title">initialize</span><span class="params">(ObjectInspector[] objectInspectors)</span> <span class="keyword">throws</span> UDFArgumentException </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (objectInspectors == <span class="keyword">null</span> || objectInspectors.length != <span class="number">1</span>) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> UDFArgumentLengthException(<span class="string">&quot;参数数量有误&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        ObjectInspector objectInspector = objectInspectors[<span class="number">0</span>];</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (objectInspector.getCategory() != ObjectInspector.Category.PRIMITIVE) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> UDFArgumentTypeException(<span class="number">0</span>, <span class="string">&quot;参数类型错误&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> PrimitiveObjectInspectorFactory.javaIntObjectInspector;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 函数体的核心逻辑</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> deferredObjects</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> HiveException</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Object <span class="title">evaluate</span><span class="params">(DeferredObject[] deferredObjects)</span> <span class="keyword">throws</span> HiveException </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (deferredObjects == <span class="keyword">null</span> || deferredObjects.length != <span class="number">1</span> || deferredObjects[<span class="number">0</span>].get() == <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> deferredObjects[<span class="number">0</span>].get().toString().length();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 针对当前函数的说明</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> strings</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getDisplayString</span><span class="params">(String[] strings)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&quot;针对当前函数的说明&quot;</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">add</span> jar <span class="operator">/</span>home<span class="operator">/</span>atguigu<span class="operator">/</span>hive<span class="operator">/</span>custom_function<span class="operator">/</span>sgg<span class="operator">-</span>hive<span class="operator">-</span>custom<span class="operator">-</span><span class="keyword">function</span><span class="number">-0.0</span><span class="number">.1</span><span class="operator">-</span>SNAPSHOT.jar;</span><br><span class="line"><span class="keyword">No</span> <span class="keyword">rows</span> affected (<span class="number">0.037</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">create</span> temporary <span class="keyword">function</span> str_split <span class="keyword">as</span> <span class="string">&#x27;tech.anzhen.sgg.hive.custom.function.udf.UDTFTest&#x27;</span>;</span><br><span class="line"><span class="keyword">No</span> <span class="keyword">rows</span> affected (<span class="number">0.028</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">select</span> str_split(<span class="string">&#x27;a,d,d,w&#x27;</span>,<span class="string">&#x27;,&#x27;</span>,<span class="string">&#x27;12&#x27;</span>);</span><br><span class="line">Error: Error while compiling statement: FAILED: UDFArgumentLengthException 参数数量有误 (state<span class="operator">=</span><span class="number">42000</span>,code<span class="operator">=</span><span class="number">40000</span>)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">select</span> str_split(<span class="string">&#x27;a,d,d,w&#x27;</span>,<span class="string">&#x27;,&#x27;</span>);</span><br><span class="line"><span class="operator">+</span><span class="comment">-------+</span></span><br><span class="line"><span class="operator">|</span> word  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-------+</span></span><br><span class="line"><span class="operator">|</span> a     <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> d     <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> d     <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> w     <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-------+</span></span><br></pre></td></tr></table></figure>
<h2 id="8-5-自定义UDTF函数"><a href="#8-5-自定义UDTF函数" class="headerlink" title="8.5 自定义UDTF函数"></a>8.5 自定义UDTF函数</h2></li>
<li>创建函数类</li>
<li>继承org.apache.hadoop.hive.ql.udf.generic.GenericUDTF</li>
<li>重写initialize，process，close</li>
<li>执行hive add jar</li>
<li>创建函数</li>
<li>使用函数</li>
<li>示例 <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">UDTFMultiColumnTest</span> <span class="keyword">extends</span> <span class="title">GenericUDTF</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> StructObjectInspector <span class="title">initialize</span><span class="params">(StructObjectInspector argOIs)</span> <span class="keyword">throws</span> UDFArgumentException </span>&#123;</span><br><span class="line">        <span class="comment">//参数数量</span></span><br><span class="line">        List&lt;? extends StructField&gt; allStructFieldRefs = argOIs.getAllStructFieldRefs();</span><br><span class="line">        <span class="keyword">if</span> (allStructFieldRefs == <span class="keyword">null</span> || allStructFieldRefs.size() != <span class="number">3</span>) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> UDFArgumentLengthException(<span class="string">&quot;参数数量有误&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">//参数数据类型</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; allStructFieldRefs.size(); i++) &#123;</span><br><span class="line">            StructField structField = allStructFieldRefs.get(i);</span><br><span class="line">            <span class="keyword">if</span> (structField.getFieldObjectInspector().getCategory() != ObjectInspector.Category.PRIMITIVE) &#123;</span><br><span class="line">                <span class="keyword">throw</span> <span class="keyword">new</span> UDFArgumentTypeException(i, <span class="string">&quot;参数类型有误&quot;</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        ArrayList&lt;String&gt; fieldNames = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">        fieldNames.add(<span class="string">&quot;word1&quot;</span>);</span><br><span class="line">        fieldNames.add(<span class="string">&quot;word2&quot;</span>);</span><br><span class="line">        ArrayList&lt;ObjectInspector&gt; fieldOIs = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">        fieldOIs.add(PrimitiveObjectInspectorFactory.javaStringObjectInspector);</span><br><span class="line">        fieldOIs.add(PrimitiveObjectInspectorFactory.javaStringObjectInspector);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> ObjectInspectorFactory.getStandardStructObjectInspector(fieldNames,</span><br><span class="line">                fieldOIs);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">process</span><span class="params">(Object[] args)</span> <span class="keyword">throws</span> HiveException </span>&#123;</span><br><span class="line">        String str = args[<span class="number">0</span>].toString();</span><br><span class="line">        String split1 = args[<span class="number">1</span>].toString();</span><br><span class="line">        String split2 = args[<span class="number">2</span>].toString();</span><br><span class="line">        String[] split1Array = str.split(split1);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; split1Array.length; i++) &#123;</span><br><span class="line">            String split2Str = split1Array[i];</span><br><span class="line">            String[] split2Array = split2Str.split(split2);</span><br><span class="line">            forward(split2Array);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 收尾</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> HiveException</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> <span class="keyword">throws</span> HiveException </span>&#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">add</span> jar <span class="operator">/</span>home<span class="operator">/</span>atguigu<span class="operator">/</span>hive<span class="operator">/</span>custom_function<span class="operator">/</span>sgg<span class="operator">-</span>hive<span class="operator">-</span>custom<span class="operator">-</span><span class="keyword">function</span><span class="number">-0.0</span><span class="number">.1</span><span class="operator">-</span>SNAPSHOT.jar;</span><br><span class="line"><span class="keyword">No</span> <span class="keyword">rows</span> affected (<span class="number">0.037</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">create</span> temporary <span class="keyword">function</span> str_splits <span class="keyword">as</span> <span class="string">&#x27;tech.anzhen.sgg.hive.custom.function.udf.UDTFMultiColumnTest&#x27;</span>;</span><br><span class="line"><span class="keyword">No</span> <span class="keyword">rows</span> affected (<span class="number">0.037</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">select</span> str_splits(<span class="string">&#x27;a,d,d,w&#x27;</span>,<span class="string">&#x27;,&#x27;</span>);</span><br><span class="line">Error: Error while compiling statement: FAILED: UDFArgumentLengthException 参数数量有误 (state<span class="operator">=</span><span class="number">42000</span>,code<span class="operator">=</span><span class="number">40000</span>)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">select</span> str_splits(<span class="string">&#x27;a-1,b-2,c-3&#x27;</span>,<span class="string">&#x27;,&#x27;</span>,<span class="string">&#x27;-&#x27;</span>);</span><br><span class="line"><span class="operator">+</span><span class="comment">--------+--------+</span></span><br><span class="line"><span class="operator">|</span> word1  <span class="operator">|</span> word2  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">--------+--------+</span></span><br><span class="line"><span class="operator">|</span> a      <span class="operator">|</span> <span class="number">1</span>      <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> b      <span class="operator">|</span> <span class="number">2</span>      <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> c      <span class="operator">|</span> <span class="number">3</span>      <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">--------+--------+</span></span><br><span class="line"><span class="number">3</span> <span class="keyword">rows</span> selected (<span class="number">0.062</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span></span><br></pre></td></tr></table></figure></li>
</ol>
<h1 id="九、压缩和存储"><a href="#九、压缩和存储" class="headerlink" title="九、压缩和存储"></a>九、压缩和存储</h1><h2 id="9-1-Hadoop压缩配置"><a href="#9-1-Hadoop压缩配置" class="headerlink" title="9.1 Hadoop压缩配置"></a>9.1 Hadoop压缩配置</h2><h3 id="9-1-1-MR支持的压缩编码"><a href="#9-1-1-MR支持的压缩编码" class="headerlink" title="9.1.1 MR支持的压缩编码"></a>9.1.1 MR支持的压缩编码</h3><ul>
<li><p>MR支持的压缩编码</p>
<table>
<thead>
<tr>
<th>压缩格式</th>
<th>算法</th>
<th>文件扩展名</th>
<th>是否可切分</th>
</tr>
</thead>
<tbody><tr>
<td>DEFLATE</td>
<td>DEFLATE</td>
<td>.deflate</td>
<td>否</td>
</tr>
<tr>
<td>Gzip</td>
<td>DEFLATE</td>
<td>.gz</td>
<td>否</td>
</tr>
<tr>
<td>bzip2</td>
<td>bzip2</td>
<td>.bz2</td>
<td>是</td>
</tr>
<tr>
<td>LZO</td>
<td>LZO</td>
<td>.lzo</td>
<td>是</td>
</tr>
<tr>
<td>Snappy</td>
<td>Snappy</td>
<td>.snappy</td>
<td>否</td>
</tr>
</tbody></table>
</li>
<li><p>为了支持多种压缩/解压缩算法，Hadoop引入了编码/解码器，如下表所示：</p>
<table>
<thead>
<tr>
<th>压缩格式</th>
<th>对应的编码/解码器</th>
</tr>
</thead>
<tbody><tr>
<td>DEFLATE</td>
<td>org.apache.hadoop.io.compress.DefaultCodec</td>
</tr>
<tr>
<td>gzip</td>
<td>org.apache.hadoop.io.compress.GzipCodec</td>
</tr>
<tr>
<td>bzip2</td>
<td>org.apache.hadoop.io.compress.BZip2Codec</td>
</tr>
<tr>
<td>LZO</td>
<td>com.hadoop.compression.lzo.LzopCodec</td>
</tr>
<tr>
<td>Snappy</td>
<td>org.apache.hadoop.io.compress.SnappyCodec</td>
</tr>
</tbody></table>
</li>
<li><p>压缩性能的比较：</p>
<table>
<thead>
<tr>
<th>压缩算法</th>
<th>原始文件大小</th>
<th>压缩文件大小</th>
<th>压缩速度</th>
<th>解压速度</th>
</tr>
</thead>
<tbody><tr>
<td>gzip</td>
<td>8.3GB</td>
<td>1.8GB</td>
<td>17.5MB/s</td>
<td>58MB/s</td>
</tr>
<tr>
<td>bzip2</td>
<td>8.3GB</td>
<td>1.1GB</td>
<td>2.4MB/s</td>
<td>9.5MB/s</td>
</tr>
<tr>
<td>LZO</td>
<td>8.3GB</td>
<td>2.9GB</td>
<td>49.3MB/s</td>
<td>74.6MB/s</td>
</tr>
</tbody></table>
<blockquote>
<p><a href="http://google.github.io/snappy/">http://google.github.io/snappy/</a><br>On a single core of a Core i7 processor in 64-bit mode, Snappy compresses at about 250 MB/sec or more and decompresses at about 500 MB/sec or more.</p>
</blockquote>
</li>
</ul>
<h3 id="9-1-2-压缩参数配置"><a href="#9-1-2-压缩参数配置" class="headerlink" title="9.1.2 压缩参数配置"></a>9.1.2 压缩参数配置</h3><ul>
<li>要在Hadoop中启用压缩，可以配置如下参数（mapred-site.xml文件中）：<table>
<thead>
<tr>
<th>参数</th>
<th>默认值</th>
<th>阶段</th>
<th>建议</th>
</tr>
</thead>
<tbody><tr>
<td>io.compression.codecs（在core-site.xml中配置）</td>
<td>org.apache.hadoop.io.compress.DefaultCodec, org.apache.hadoop.io.compress.GzipCodec, org.apache.hadoop.io.compress.BZip2Codec,</td>
<td></td>
<td></td>
</tr>
<tr>
<td>org.apache.hadoop.io.compress.Lz4Codec</td>
<td>输入压缩</td>
<td>Hadoop使用文件扩展名判断是否支持某种编解码器</td>
<td></td>
</tr>
<tr>
<td>mapreduce.map.output.compress</td>
<td>false</td>
<td>mapper输出</td>
<td>这个参数设为true启用压缩</td>
</tr>
<tr>
<td>mapreduce.map.output.compress.codec</td>
<td>org.apache.hadoop.io.compress.DefaultCodec</td>
<td>mapper输出</td>
<td>使用LZO、LZ4或snappy编解码器在此阶段压缩数据</td>
</tr>
<tr>
<td>mapreduce.output.fileoutputformat.compress</td>
<td>false</td>
<td>reducer输出</td>
<td>这个参数设为true启用压缩</td>
</tr>
<tr>
<td>mapreduce.output.fileoutputformat.compress.codec</td>
<td>org.apache.hadoop.io.compress. DefaultCodec</td>
<td>reducer输出</td>
<td>使用标准工具或者编解码器，如gzip和bzip2</td>
</tr>
<tr>
<td>mapreduce.output.fileoutputformat.compress.type</td>
<td>RECORD</td>
<td>reducer输出</td>
<td>SequenceFile输出使用的压缩类型：NONE和BLOCK</td>
</tr>
</tbody></table>
</li>
</ul>
<h2 id="9-2-开启Map输出阶段压缩"><a href="#9-2-开启Map输出阶段压缩" class="headerlink" title="9.2 开启Map输出阶段压缩"></a>9.2 开启Map输出阶段压缩</h2><p>开启map输出阶段压缩可以减少job中map和Reduce task间数据传输量。具体配置如下：</p>
<ol>
<li>开启hive中间传输数据压缩功能 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span><span class="keyword">set</span> hive.exec.compress.intermediate<span class="operator">=</span><span class="literal">true</span>;</span><br></pre></td></tr></table></figure></li>
<li>开启mapreduce中map输出压缩功能 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span><span class="keyword">set</span> mapreduce.map.output.compress<span class="operator">=</span><span class="literal">true</span>;</span><br></pre></td></tr></table></figure></li>
<li>设置mapreduce中map输出数据的压缩方式 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span><span class="keyword">set</span> mapreduce.map.output.compress.codec <span class="operator">=</span> org.apache.hadoop.io.compress.SnappyCodec;</span><br></pre></td></tr></table></figure></li>
<li>执行查询语句 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="built_in">count</span>(ename) name <span class="keyword">from</span> emp;</span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="9-3-开启Reduce输出阶段压缩"><a href="#9-3-开启Reduce输出阶段压缩" class="headerlink" title="9.3 开启Reduce输出阶段压缩"></a>9.3 开启Reduce输出阶段压缩</h2><p>当Hive将输出写入到表中时，输出内容同样可以进行压缩。属性hive.exec.compress.output控制着这个功能。用户可能需要保持默认设置文件中的默认值false，这样默认的输出就是非压缩的纯文本文件了。用户可以通过在查询语句或执行脚本中设置这个值为true，来开启输出结果压缩功能。</p>
<ol>
<li>开启hive最终输出数据压缩功能 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span><span class="keyword">set</span> hive.exec.compress.output<span class="operator">=</span><span class="literal">true</span>;</span><br></pre></td></tr></table></figure></li>
<li>开启mapreduce最终输出数据压缩 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span><span class="keyword">set</span> mapreduce.output.fileoutputformat.compress<span class="operator">=</span><span class="literal">true</span>;</span><br></pre></td></tr></table></figure></li>
<li>设置mapreduce最终数据输出压缩方式 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">set</span> mapreduce.output.fileoutputformat.compress.codec <span class="operator">=</span> org.apache.hadoop.io.compress.SnappyCodec;</span><br></pre></td></tr></table></figure></li>
<li>设置mapreduce最终数据输出压缩为块压缩 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">set</span> mapreduce.output.fileoutputformat.compress.type<span class="operator">=</span>BLOCK;</span><br></pre></td></tr></table></figure></li>
<li>测试一下输出结果是否是压缩文件 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">insert</span> overwrite <span class="keyword">local</span> directory <span class="string">&#x27;/opt/module/hive-3.1.2/datas/distribute-result&#x27;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> emp distribute <span class="keyword">by</span> deptno sort <span class="keyword">by</span> empno <span class="keyword">desc</span>;</span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="9-4-文件存储格式"><a href="#9-4-文件存储格式" class="headerlink" title="9.4 文件存储格式"></a>9.4 文件存储格式</h2><ul>
<li>建表语法中以下规则就是确定Hive的存储格式  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">[STORED <span class="keyword">AS</span> file_format] </span><br></pre></td></tr></table></figure></li>
<li>Hive中的存储格式<ul>
<li>textfile   (默认格式) — 行存</li>
<li>SEQUENCEFILE  二进制的序列文件 — 行存</li>
<li>ORC (Hive中最常用的一种存储格式) — 列存</li>
<li>PARQUET (和ORC接近的一种存储格式，hive中支持比较早的一种存储格式)– 列存</li>
</ul>
</li>
</ul>
<h3 id="9-4-1-列式存储和行式存储"><a href="#9-4-1-列式存储和行式存储" class="headerlink" title="9.4.1 列式存储和行式存储"></a>9.4.1 列式存储和行式存储</h3><p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16355110552074.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"><br>如图所示左边为逻辑表，右边第一个为行式存储，第二个为列式存储。</p>
<ol>
<li>行存储的特点: 查询满足条件的一整行数据的时候，列存储则需要去每个聚集的字段找到对应的每个列的值，行存储只需要找到其中一个值，其余的值都在相邻地方，所以此时行存储查询的速度更快。</li>
<li>列存储的特点: 因为每个字段的数据聚集存储，在查询只需要少数几个字段的时候，能大大减少读取的数据量；每个字段的数据类型一定是相同的，列式存储可以针对性的设计更好的设计压缩算法。</li>
</ol>
<ul>
<li>TEXTFILE和SEQUENCEFILE的存储格式都是基于行存储的；</li>
<li>ORC和PARQUET是基于列式存储的。</li>
</ul>
<h3 id="9-4-2-TextFile格式"><a href="#9-4-2-TextFile格式" class="headerlink" title="9.4.2 TextFile格式"></a>9.4.2 TextFile格式</h3><p>默认格式，数据不做压缩，磁盘开销大，数据解析开销大。可结合Gzip、Bzip2使用，但使用Gzip这种方式，hive不会对数据进行切分，从而无法对数据进行并行操作</p>
<h3 id="9-4-3-Orc格式"><a href="#9-4-3-Orc格式" class="headerlink" title="9.4.3 Orc格式"></a>9.4.3 Orc格式</h3><p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16355111657327.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
<p>Orc (Optimized Row Columnar)是Hive 0.11版里引入的新的存储格式。<br>如图所示可以看到每个Orc文件由1个或多个stripe组成，每个stripe一般为HDFS的块大小，每一个stripe包含多条记录，这些记录按照列进行独立存储，对应到Parquet中的row group的概念。每个Stripe里有三部分组成，分别是Index Data，Row Data，Stripe Footer：</p>
<ol>
<li>Index Data：一个轻量级的index，默认是每隔1W行做一个索引。这里做的索引应该只是记录某行的各字段在Row Data中的offset。</li>
<li>Row Data：存的是具体的数据，先取部分行，然后对这些行按列进行存储。对每个列进行了编码，分成多个Stream来存储。</li>
<li>Stripe Footer：存的是各个Stream的类型，长度等信息。</li>
<li>每个文件有一个File Footer，这里面存的是每个Stripe的行数，每个Column的数据类型信息等；每个文件的尾部是一个PostScript，这里面记录了整个文件的压缩类型以及FileFooter的长度信息等。在读取文件时，会seek到文件尾部读PostScript，从里面解析到File Footer长度，再读FileFooter，从里面解析到各个Stripe信息，再读各个Stripe，即从后往前读。</li>
<li>以json方式查看orc文件 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hive --orcfiledump -j -p /user/hive/warehouse/dim.db/dim_province/000000_0</span><br></pre></td></tr></table></figure>
<ul>
<li>文件示例：<a href="media/16346959668071/log_orc.json">log_orc.json</a><br>  <img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16356457996704.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"><ul>
<li>schema: 为每一个字段做了编号，从1开始，编号为0的columnId中描述了整个表的字段定义。<br>  <img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16356459007816.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></li>
<li>stripeStatistics:是ORC文件中所有stripes的统计信息，其中有每个stripe中每个字段的min/max值，是否有空值等等<br>  <img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16356459954433.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></li>
<li>fileStatistics:整个文件中每个字段的统计信息，该表只有一个文件，也只有一个stripe<br>  <img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16356461542820.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></li>
<li>stripes:这里列出了所有stripes的元数据信息，包括index data, row data和stripe footer。<br>  <img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16356462325881.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></li>
</ul>
</li>
</ul>
</li>
</ol>
<h3 id="9-4-4-Parquet格式"><a href="#9-4-4-Parquet格式" class="headerlink" title="9.4.4 Parquet格式"></a>9.4.4 Parquet格式</h3><p>Parquet文件是以二进制方式存储的，所以是不可以直接读取的，文件中包括该文件的数据和元数据，因此Parquet格式文件是自解析的。</p>
<ol>
<li>行组(Row Group)：每一个行组包含一定的行数，在一个HDFS文件中至少存储一个行组，类似于orc的stripe的概念。</li>
<li>列块(Column Chunk)：在一个行组中每一列保存在一个列块中，行组中的所有列连续的存储在这个行组文件中。一个列块中的值都是相同类型的，不同的列块可能使用不同的算法进行压缩。</li>
<li>页(Page)：每一个列块划分为多个页，一个页是最小的编码的单位，在同一个列块的不同页可能使用不同的编码方式。</li>
<li>通常情况下，在存储Parquet数据的时候会按照Block大小设置行组的大小，由于一般情况下每一个Mapper任务处理数据的最小单位是一个Block，这样可以把每一个行组由一个Mapper任务处理，增大任务执行并行度。Parquet文件的格式。</li>
</ol>
<h3 id="9-4-5-主流文件存储格式对比实验"><a href="#9-4-5-主流文件存储格式对比实验" class="headerlink" title="9.4.5 主流文件存储格式对比实验"></a>9.4.5 主流文件存储格式对比实验</h3><p>数据文件 <a href="media/16346959668071/log.data">log.data</a></p>
<ol>
<li>textfile<ol>
<li>建表导数据 存储数据格式为TEXTFILE <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> log_text</span><br><span class="line">(</span><br><span class="line">    track_time  string,</span><br><span class="line">    url         string,</span><br><span class="line">    session_id  string,</span><br><span class="line">    referer     string,</span><br><span class="line">    ip          string,</span><br><span class="line">    end_user_id string,</span><br><span class="line">    city_id     string</span><br><span class="line">)</span><br><span class="line"><span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span></span><br><span class="line">stored <span class="keyword">as</span> textfile;</span><br><span class="line"></span><br><span class="line">load data <span class="keyword">local</span> inpath <span class="string">&#x27;/opt/module/hive-3.1.2/datas/log.data&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span> log_text ;</span><br></pre></td></tr></table></figure></li>
<li>查看占用空间占用 18.1 M <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> dfs <span class="operator">-</span>ls <span class="operator">-</span>h <span class="operator">/</span><span class="keyword">user</span><span class="operator">/</span>hive<span class="operator">/</span>warehouse<span class="operator">/</span>mock_data.db<span class="operator">/</span>log_text;</span><br><span class="line"><span class="operator">+</span><span class="comment">----------------------------------------------------+</span></span><br><span class="line"><span class="operator">|</span>                     DFS Output                     <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------------------------------------------------+</span></span><br><span class="line"><span class="operator">|</span> Found <span class="number">1</span> items                                      <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="operator">-</span>rw<span class="operator">-</span>r<span class="comment">--r--   3 atguigu supergroup     18.1 M 2021-10-30 10:34 /user/hive/warehouse/mock_data.db/log_text/log.data |</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------------------------------------------------+</span></span><br><span class="line"><span class="number">2</span> <span class="keyword">rows</span> selected (<span class="number">0.008</span> seconds)</span><br></pre></td></tr></table></figure></li>
</ol>
</li>
<li>orc<ol>
<li>建表导数据，指定orc格式 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> log_orc (</span><br><span class="line">    track_time  string,</span><br><span class="line">    url         string,</span><br><span class="line">    session_id  string,</span><br><span class="line">    referer     string,</span><br><span class="line">    ip          string,</span><br><span class="line">    end_user_id string,</span><br><span class="line">    city_id     string</span><br><span class="line">)</span><br><span class="line"><span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span></span><br><span class="line">stored <span class="keyword">as</span> orc</span><br><span class="line">tblproperties (&quot;orc.compress&quot; <span class="operator">=</span> &quot;NONE&quot;); <span class="comment">-- 设置orc存储不使用压缩</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">table</span> log_orc <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> log_text ;</span><br></pre></td></tr></table></figure></li>
<li>查看文件大小 7.7 M <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> dfs <span class="operator">-</span>ls <span class="operator">-</span>h <span class="operator">/</span><span class="keyword">user</span><span class="operator">/</span>hive<span class="operator">/</span>warehouse<span class="operator">/</span>mock_data.db<span class="operator">/</span>log_orc;</span><br><span class="line"><span class="operator">+</span><span class="comment">----------------------------------------------------+</span></span><br><span class="line"><span class="operator">|</span>                     DFS Output                     <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------------------------------------------------+</span></span><br><span class="line"><span class="operator">|</span> Found <span class="number">1</span> items                                      <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="operator">-</span>rw<span class="operator">-</span>r<span class="comment">--r--   3 atguigu supergroup      7.7 M 2021-10-30 10:37 /user/hive/warehouse/mock_data.db/log_orc/000000_0 |</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------------------------------------------------+</span></span><br><span class="line"><span class="number">2</span> <span class="keyword">rows</span> selected (<span class="number">0.005</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span></span><br></pre></td></tr></table></figure></li>
</ol>
</li>
<li>parquet<ol>
<li>建表导数据，指定parquet格式 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> log_parquet (</span><br><span class="line">    track_time  string,</span><br><span class="line">    url         string,</span><br><span class="line">    session_id  string,</span><br><span class="line">    referer     string,</span><br><span class="line">    ip          string,</span><br><span class="line">    end_user_id string,</span><br><span class="line">    city_id     string</span><br><span class="line">)</span><br><span class="line"><span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span></span><br><span class="line">stored <span class="keyword">as</span> parquet;</span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">table</span> log_parquet <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> log_text ;</span><br></pre></td></tr></table></figure></li>
<li>查看文件大小 13.1 M <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> dfs <span class="operator">-</span>ls <span class="operator">-</span>h <span class="operator">/</span><span class="keyword">user</span><span class="operator">/</span>hive<span class="operator">/</span>warehouse<span class="operator">/</span>mock_data.db<span class="operator">/</span>log_parquet;</span><br><span class="line"><span class="operator">+</span><span class="comment">----------------------------------------------------+</span></span><br><span class="line"><span class="operator">|</span>                     DFS Output                     <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------------------------------------------------+</span></span><br><span class="line"><span class="operator">|</span> Found <span class="number">1</span> items                                      <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="operator">-</span>rw<span class="operator">-</span>r<span class="comment">--r--   3 atguigu supergroup     13.1 M 2021-10-30 10:49 /user/hive/warehouse/mock_data.db/log_parquet/000000_0 |</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------------------------------------------------+</span></span><br><span class="line"><span class="number">2</span> <span class="keyword">rows</span> selected (<span class="number">0.009</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span></span><br></pre></td></tr></table></figure></li>
</ol>
</li>
<li>存储文件大小的对比总结：<br> ORC &gt;  Parquet &gt;  textFile</li>
<li>存储文件的查询速度测试<br> ORC &gt;  Parquet &gt;  textFile <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">select</span> <span class="built_in">count</span>(<span class="number">1</span>) <span class="keyword">from</span> person <span class="keyword">where</span> id<span class="operator">=</span><span class="number">4</span>;</span><br><span class="line"><span class="operator">+</span><span class="comment">------+</span></span><br><span class="line"><span class="operator">|</span> _c0  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">------+</span></span><br><span class="line"><span class="operator">|</span> <span class="number">1</span>    <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">------+</span></span><br><span class="line"><span class="number">1</span> <span class="type">row</span> selected (<span class="number">21.249</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">set</span> hive.optimize.index.filter<span class="operator">=</span><span class="literal">true</span>;</span><br><span class="line"><span class="keyword">No</span> <span class="keyword">rows</span> affected (<span class="number">0.005</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">select</span> <span class="built_in">count</span>(<span class="number">1</span>) <span class="keyword">from</span> person_orc <span class="keyword">where</span> id<span class="operator">=</span><span class="number">4</span>;</span><br><span class="line"><span class="operator">+</span><span class="comment">------+</span></span><br><span class="line"><span class="operator">|</span> _c0  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">------+</span></span><br><span class="line"><span class="operator">|</span> <span class="number">1</span>    <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">------+</span></span><br><span class="line"><span class="number">1</span> <span class="type">row</span> selected (<span class="number">17.151</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">select</span> <span class="built_in">count</span>(<span class="number">1</span>) <span class="keyword">from</span> person_parquet <span class="keyword">where</span> id<span class="operator">=</span><span class="number">4</span>;</span><br><span class="line"><span class="operator">+</span><span class="comment">------+</span></span><br><span class="line"><span class="operator">|</span> _c0  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">------+</span></span><br><span class="line"><span class="operator">|</span> <span class="number">1</span>    <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">------+</span></span><br><span class="line"><span class="number">1</span> <span class="type">row</span> selected (<span class="number">18.485</span> seconds)</span><br></pre></td></tr></table></figure>
<h2 id="9-5-存储和压缩结合"><a href="#9-5-存储和压缩结合" class="headerlink" title="9.5 存储和压缩结合"></a>9.5 存储和压缩结合</h2><h3 id="9-5-1-测试存储和压缩"><a href="#9-5-1-测试存储和压缩" class="headerlink" title="9.5.1 测试存储和压缩"></a>9.5.1 测试存储和压缩</h3></li>
</ol>
<ul>
<li><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+ORC">官方文档</a></li>
<li>ORC存储方式的压缩：<table>
<thead>
<tr>
<th>Key</th>
<th>Default</th>
<th>Notes</th>
</tr>
</thead>
<tbody><tr>
<td>orc.compress</td>
<td>ZLIB</td>
<td>high level compression (one of NONE, ZLIB, SNAPPY)</td>
</tr>
<tr>
<td>orc.compress.size</td>
<td>262,144</td>
<td>number of bytes in each compression chunk</td>
</tr>
<tr>
<td>orc.stripe.size</td>
<td>268,435,456</td>
<td>number of bytes in each stripe</td>
</tr>
<tr>
<td>orc.row.index.stride</td>
<td>10,000</td>
<td>number of rows between index entries (must be &gt;= 1000)</td>
</tr>
<tr>
<td>orc.create.index</td>
<td>true</td>
<td>whether to create row indexes</td>
</tr>
<tr>
<td>orc.bloom.filter.columns</td>
<td>“”</td>
<td>comma separated list of column names for which bloom filter should be created</td>
</tr>
<tr>
<td>orc.bloom.filter.fpp</td>
<td>0.05</td>
<td>false positive probability for bloom filter (must &gt;0.0 and &lt;1.0)</td>
</tr>
</tbody></table>
</li>
<li>注意：所有关于ORCFile的参数都是在HQL语句的TBLPROPERTIES字段里面出现</li>
<li>测试<ol>
<li>创建ZLIB压缩的ORC存储方式<ul>
<li>建表导数据  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> log_orc_zlib (</span><br><span class="line">    track_time  string,</span><br><span class="line">    url         string,</span><br><span class="line">    session_id  string,</span><br><span class="line">    referer     string,</span><br><span class="line">    ip          string,</span><br><span class="line">    end_user_id string,</span><br><span class="line">    city_id     string</span><br><span class="line">)</span><br><span class="line"><span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span></span><br><span class="line">stored <span class="keyword">as</span> orc</span><br><span class="line">tblproperties (&quot;orc.compress&quot; <span class="operator">=</span> &quot;ZLIB&quot;);</span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> log_orc_zlib <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> log_text;</span><br></pre></td></tr></table></figure></li>
<li>查看文件大小 2.8 M  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> dfs <span class="operator">-</span>ls <span class="operator">-</span>h <span class="operator">/</span><span class="keyword">user</span><span class="operator">/</span>hive<span class="operator">/</span>warehouse<span class="operator">/</span>mock_data.db<span class="operator">/</span>log_orc_zlib;</span><br><span class="line"><span class="operator">+</span><span class="comment">----------------------------------------------------+</span></span><br><span class="line"><span class="operator">|</span>                     DFS Output                     <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------------------------------------------------+</span></span><br><span class="line"><span class="operator">|</span> Found <span class="number">1</span> items                                      <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="operator">-</span>rw<span class="operator">-</span>r<span class="comment">--r--   3 atguigu supergroup      2.8 M 2021-10-30 10:50 /user/hive/warehouse/mock_data.db/log_orc_zlib/000000_0 |</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------------------------------------------------+</span></span><br><span class="line"><span class="number">2</span> <span class="keyword">rows</span> selected (<span class="number">0.011</span> seconds)</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>创建一个SNAPPY压缩的ORC存储方式<ul>
<li>建表导数据  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> log_orc_snappy (</span><br><span class="line">    track_time  string,</span><br><span class="line">    url         string,</span><br><span class="line">    session_id  string,</span><br><span class="line">    referer     string,</span><br><span class="line">    ip          string,</span><br><span class="line">    end_user_id string,</span><br><span class="line">    city_id     string</span><br><span class="line">)</span><br><span class="line"><span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span></span><br><span class="line">stored <span class="keyword">as</span> orc</span><br><span class="line">tblproperties (&quot;orc.compress&quot; <span class="operator">=</span> &quot;SNAPPY&quot;);</span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> log_orc_snappy <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> log_text;</span><br></pre></td></tr></table></figure></li>
<li>查看文件大小 3.7 M  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> dfs <span class="operator">-</span>ls <span class="operator">-</span>h <span class="operator">/</span><span class="keyword">user</span><span class="operator">/</span>hive<span class="operator">/</span>warehouse<span class="operator">/</span>mock_data.db<span class="operator">/</span>log_orc_snappy;</span><br><span class="line"><span class="operator">+</span><span class="comment">----------------------------------------------------+</span></span><br><span class="line"><span class="operator">|</span>                     DFS Output                     <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------------------------------------------------+</span></span><br><span class="line"><span class="operator">|</span> Found <span class="number">1</span> items                                      <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="operator">-</span>rw<span class="operator">-</span>r<span class="comment">--r--   3 atguigu supergroup      3.7 M 2021-10-30 10:51 /user/hive/warehouse/mock_data.db/log_orc_snappy/000000_0 |</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------------------------------------------------+</span></span><br><span class="line"><span class="number">2</span> <span class="keyword">rows</span> selected (<span class="number">0.009</span> seconds)</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>创建一个SNAPPY压缩的parquet存储方式<ul>
<li>建表导数据  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> log_parquet_snappy (</span><br><span class="line">    track_time  string,</span><br><span class="line">    url         string,</span><br><span class="line">    session_id  string,</span><br><span class="line">    referer     string,</span><br><span class="line">    ip          string,</span><br><span class="line">    end_user_id string,</span><br><span class="line">    city_id     string</span><br><span class="line">)</span><br><span class="line"><span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span></span><br><span class="line">stored <span class="keyword">as</span> parquet</span><br><span class="line">tblproperties (&quot;parquet.compression&quot; <span class="operator">=</span> &quot;SNAPPY&quot;);</span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> log_parquet_snappy <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> log_text;</span><br></pre></td></tr></table></figure></li>
<li>查看文件大小 6.4 M  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> dfs <span class="operator">-</span>ls <span class="operator">-</span>h <span class="operator">/</span><span class="keyword">user</span><span class="operator">/</span>hive<span class="operator">/</span>warehouse<span class="operator">/</span>mock_data.db<span class="operator">/</span>log_parquet_snappy;</span><br><span class="line"><span class="operator">+</span><span class="comment">----------------------------------------------------+</span></span><br><span class="line"><span class="operator">|</span>                     DFS Output                     <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------------------------------------------------+</span></span><br><span class="line"><span class="operator">|</span> Found <span class="number">1</span> items                                      <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="operator">-</span>rw<span class="operator">-</span>r<span class="comment">--r--   3 atguigu supergroup      6.4 M 2021-10-31 12:29 /user/hive/warehouse/mock_data.db/log_parquet_snappy/000000_0 |</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------------------------------------------------+</span></span><br><span class="line"><span class="number">2</span> <span class="keyword">rows</span> selected (<span class="number">0.078</span> seconds)</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>存储方式和压缩总结<ul>
<li>在实际的项目开发当中，hive表的数据存储格式一般选择：orc或parquet。压缩方式一般选择snappy，lzo。<h1 id="十、性能优化"><a href="#十、性能优化" class="headerlink" title="十、性能优化"></a>十、性能优化</h1><h2 id="10-1-执行计划（Explain）"><a href="#10-1-执行计划（Explain）" class="headerlink" title="10.1 执行计划（Explain）"></a>10.1 执行计划（Explain）</h2></li>
</ul>
</li>
</ol>
</li>
</ul>
<ol>
<li>概念：获取SQL执行的规划，主要用于分析SQL需要的优化依据信息</li>
<li>基本语法 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">EXPLAIN [EXTENDED <span class="operator">|</span> DEPENDENCY <span class="operator">|</span> <span class="keyword">AUTHORIZATION</span>] query</span><br></pre></td></tr></table></figure></li>
<li>案例实操<ul>
<li>不生成MR任务(关注Fetch Operator)  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> explain <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> emp;</span><br><span class="line"><span class="operator">+</span><span class="comment">----------------------------------------------------+</span></span><br><span class="line"><span class="operator">|</span>                      Explain                       <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------------------------------------------------+</span></span><br><span class="line"><span class="operator">|</span> STAGE DEPENDENCIES:                                <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>   Stage<span class="number">-0</span> <span class="keyword">is</span> a root stage                          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>                                                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> STAGE PLANS:                                       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>   Stage: Stage<span class="number">-0</span>                                   <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>     <span class="keyword">Fetch</span> Operator                                 <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>       limit: <span class="number">-1</span>                                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>       Processor Tree:                              <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>         TableScan                                  <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>           alias: emp                               <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>           Statistics: Num <span class="keyword">rows</span>: <span class="number">1</span> Data size: <span class="number">6570</span> Basic stats: COMPLETE <span class="keyword">Column</span> stats: <span class="keyword">NONE</span> <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>           <span class="keyword">Select</span> Operator                          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>             expressions: emp_no (type: <span class="type">int</span>), emp_name (type: string), job (type: string), mgr (type: <span class="type">int</span>), hire_date (type: string), sal (type: <span class="keyword">double</span>), comm (type: <span class="keyword">double</span>), dept_no (type: <span class="type">int</span>) <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>             outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7 <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>             Statistics: Num <span class="keyword">rows</span>: <span class="number">1</span> Data size: <span class="number">6570</span> Basic stats: COMPLETE <span class="keyword">Column</span> stats: <span class="keyword">NONE</span> <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>             ListSink                               <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>                                                    <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------------------------------------------------+</span></span><br><span class="line"><span class="number">17</span> <span class="keyword">rows</span> selected (<span class="number">0.076</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span></span><br></pre></td></tr></table></figure></li>
<li>生成MR任务的（关注Map Operator Tree）  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> explain <span class="keyword">select</span> dept_no, <span class="built_in">avg</span>(sal) avg_sal <span class="keyword">from</span> emp <span class="keyword">group</span> <span class="keyword">by</span> dept_no;</span><br><span class="line"><span class="operator">+</span><span class="comment">----------------------------------------------------+</span></span><br><span class="line"><span class="operator">|</span>                      Explain                       <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------------------------------------------------+</span></span><br><span class="line"><span class="operator">|</span> STAGE DEPENDENCIES:                                <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>   Stage<span class="number">-1</span> <span class="keyword">is</span> a root stage                          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>   Stage<span class="number">-0</span> depends <span class="keyword">on</span> stages: Stage<span class="number">-1</span>               <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>                                                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> STAGE PLANS:                                       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>   Stage: Stage<span class="number">-1</span>                                   <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>     Map Reduce                                     <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>       Map Operator Tree:                           <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>           TableScan                                <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>             alias: emp                             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>             Statistics: Num <span class="keyword">rows</span>: <span class="number">1</span> Data size: <span class="number">6570</span> Basic stats: COMPLETE <span class="keyword">Column</span> stats: <span class="keyword">NONE</span> <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>             <span class="keyword">Select</span> Operator                        <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>               expressions: sal (type: <span class="keyword">double</span>), dept_no (type: <span class="type">int</span>) <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>               outputColumnNames: sal, dept_no      <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>               Statistics: Num <span class="keyword">rows</span>: <span class="number">1</span> Data size: <span class="number">6570</span> Basic stats: COMPLETE <span class="keyword">Column</span> stats: <span class="keyword">NONE</span> <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>               <span class="keyword">Group</span> <span class="keyword">By</span> Operator                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>                 aggregations: <span class="built_in">sum</span>(sal), <span class="built_in">count</span>(sal) <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>                 keys: dept_no (type: <span class="type">int</span>)          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>                 mode: hash                         <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>                 outputColumnNames: _col0, _col1, _col2 <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>                 Statistics: Num <span class="keyword">rows</span>: <span class="number">1</span> Data size: <span class="number">6570</span> Basic stats: COMPLETE <span class="keyword">Column</span> stats: <span class="keyword">NONE</span> <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>                 Reduce Output Operator             <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>                   key expressions: _col0 (type: <span class="type">int</span>) <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>                   sort <span class="keyword">order</span>: <span class="operator">+</span>                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>                   Map<span class="operator">-</span>reduce <span class="keyword">partition</span> columns: _col0 (type: <span class="type">int</span>) <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>                   Statistics: Num <span class="keyword">rows</span>: <span class="number">1</span> Data size: <span class="number">6570</span> Basic stats: COMPLETE <span class="keyword">Column</span> stats: <span class="keyword">NONE</span> <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>                   <span class="keyword">value</span> expressions: _col1 (type: <span class="keyword">double</span>), _col2 (type: <span class="type">bigint</span>) <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>       Execution mode: vectorized                   <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>       Reduce Operator Tree:                        <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>         <span class="keyword">Group</span> <span class="keyword">By</span> Operator                          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>           aggregations: <span class="built_in">sum</span>(VALUE._col0), <span class="built_in">count</span>(VALUE._col1) <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>           keys: KEY._col0 (type: <span class="type">int</span>)              <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>           mode: mergepartial                       <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>           outputColumnNames: _col0, _col1, _col2   <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>           Statistics: Num <span class="keyword">rows</span>: <span class="number">1</span> Data size: <span class="number">6570</span> Basic stats: COMPLETE <span class="keyword">Column</span> stats: <span class="keyword">NONE</span> <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>           <span class="keyword">Select</span> Operator                          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>             expressions: _col0 (type: <span class="type">int</span>), (_col1 <span class="operator">/</span> _col2) (type: <span class="keyword">double</span>) <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>             outputColumnNames: _col0, _col1        <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>             Statistics: Num <span class="keyword">rows</span>: <span class="number">1</span> Data size: <span class="number">6570</span> Basic stats: COMPLETE <span class="keyword">Column</span> stats: <span class="keyword">NONE</span> <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>             File Output Operator                   <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>               compressed: <span class="literal">false</span>                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>               Statistics: Num <span class="keyword">rows</span>: <span class="number">1</span> Data size: <span class="number">6570</span> Basic stats: COMPLETE <span class="keyword">Column</span> stats: <span class="keyword">NONE</span> <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>               <span class="keyword">table</span>:                               <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>                   input format: org.apache.hadoop.mapred.SequenceFileInputFormat <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>                   output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>                   serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>                                                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>   Stage: Stage<span class="number">-0</span>                                   <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>     <span class="keyword">Fetch</span> Operator                                 <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>       limit: <span class="number">-1</span>                                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>       Processor Tree:                              <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>         ListSink                                   <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>                                                    <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------------------------------------------------+</span></span><br><span class="line"><span class="number">53</span> <span class="keyword">rows</span> selected (<span class="number">0.063</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span></span><br></pre></td></tr></table></figure></li>
<li>查看详细执行计划  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> explain extended <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> emp;</span><br><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> explain extended <span class="keyword">select</span> deptno, <span class="built_in">avg</span>(sal) avg_sal <span class="keyword">from</span> emp <span class="keyword">group</span> <span class="keyword">by</span> deptno;</span><br></pre></td></tr></table></figure>
<h2 id="10-2-Fetch抓取"><a href="#10-2-Fetch抓取" class="headerlink" title="10.2 Fetch抓取"></a>10.2 Fetch抓取</h2></li>
</ul>
</li>
</ol>
<ul>
<li>Fetch抓取是指，Hive中对某些情况的查询可以不必使用MapReduce计算。</li>
<li>例如：SELECT * FROM employees;在这种情况下，Hive可以简单地读取employee对应的存储目录下的文件，然后输出查询结果到控制台。</li>
<li>在hive-default.xml.template文件中hive.fetch.task.conversion默认是more，老版本hive默认是minimal，该属性修改为more以后，在全局查找、字段查找、limit查找等都不走mapreduce。  <figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.fetch.task.conversion<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>more<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span></span><br><span class="line">      Expects one of [none, minimal, more].</span><br><span class="line">      Some select queries can be converted to single FETCH task minimizing latency.</span><br><span class="line">      Currently the query should be single sourced not having any subquery and should not have any aggregations or distincts (which incurs RS), lateral views and joins.</span><br><span class="line">      0. none : disable hive.fetch.task.conversion</span><br><span class="line">      1. minimal : SELECT STAR, FILTER on partition columns, LIMIT only</span><br><span class="line">      2. more  : SELECT, FILTER, LIMIT only (support TABLESAMPLE and virtual columns)</span><br><span class="line">    <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li>
</ul>
<ol>
<li>案例实操：<ol>
<li>把hive.fetch.task.conversion设置成none，然后执行查询语句，都会执行mapreduce程序。 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; <span class="built_in">set</span> hive.fetch.task.conversion=none;</span><br><span class="line">hive (default)&gt; select * from emp;</span><br><span class="line">hive (default)&gt; select ename from emp;</span><br><span class="line">hive (default)&gt; select ename from emp <span class="built_in">limit</span> 3;</span><br></pre></td></tr></table></figure></li>
<li>把hive.fetch.task.conversion设置成more，然后执行查询语句，如下查询方式都不会执行mapreduce程序。 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hive (default)&gt; <span class="built_in">set</span> hive.fetch.task.conversion=more;</span><br><span class="line">hive (default)&gt; select * from emp;</span><br><span class="line">hive (default)&gt; select ename from emp;</span><br><span class="line">hive (default)&gt; select ename from emp <span class="built_in">limit</span> 3;</span><br></pre></td></tr></table></figure>
<h2 id="10-3-本地模式"><a href="#10-3-本地模式" class="headerlink" title="10.3 本地模式"></a>10.3 本地模式</h2></li>
</ol>
</li>
</ol>
<ul>
<li>大多数的Hadoop Job是需要Hadoop提供的完整的可扩展性来处理大数据集的。不过，有时Hive的输入数据量是非常小的。在这种情况下，为查询触发执行任务消耗的时间可能会比实际job的执行时间要多的多。对于大多数这种情况，Hive可以通过本地模式在单台机器上处理所有的任务。对于小数据集，执行时间可以明显被缩短。</li>
<li>用户可以通过设置hive.exec.mode.local.auto的值为true，来让Hive在适当的时候自动启动这个优化。  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- //开启本地mr</span></span><br><span class="line"><span class="keyword">set</span> hive.exec.mode.local.auto<span class="operator">=</span><span class="literal">true</span>;</span><br><span class="line"><span class="comment">-- 设置local mr的最大输入数据量，当输入数据量小于这个值时采用local  mr的方式，默认为134217728，即128M</span></span><br><span class="line"><span class="keyword">set</span> hive.exec.mode.local.auto.inputbytes.max<span class="operator">=</span><span class="number">134217728</span>;</span><br><span class="line"><span class="comment">-- 设置local mr的最大输入文件个数，当输入文件个数小于这个值时采用local mr的方式，默认为4</span></span><br><span class="line"><span class="keyword">set</span> hive.exec.mode.local.auto.input.files.max<span class="operator">=</span><span class="number">4</span>;</span><br></pre></td></tr></table></figure></li>
</ul>
<ol>
<li>开启本地模式，并执行查询语句 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">set</span> hive.exec.mode.local.auto<span class="operator">=</span><span class="literal">true</span>; </span><br><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> emp cluster <span class="keyword">by</span> deptno;</span><br><span class="line"><span class="type">Time</span> taken: <span class="number">1.328</span> seconds, Fetched: <span class="number">14</span> <span class="type">row</span>(s)</span><br></pre></td></tr></table></figure></li>
<li>关闭本地模式，并执行查询语句 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">set</span> hive.exec.mode.local.auto<span class="operator">=</span><span class="literal">false</span>; </span><br><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> emp cluster <span class="keyword">by</span> deptno;</span><br><span class="line"><span class="type">Time</span> taken: <span class="number">20.09</span> seconds, Fetched: <span class="number">14</span> <span class="type">row</span>(s)</span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="10-4-表的优化"><a href="#10-4-表的优化" class="headerlink" title="10.4 表的优化"></a>10.4 表的优化</h2><h3 id="10-4-1-小表大表Join-MapJoin-内连接场景"><a href="#10-4-1-小表大表Join-MapJoin-内连接场景" class="headerlink" title="10.4.1 小表大表Join(MapJoin)内连接场景"></a>10.4.1 小表大表Join(MapJoin)内连接场景</h3><ul>
<li>将key相对分散，并且数据量小的表放在join的左边，这样可以有效减少内存溢出错误发生的几率；再进一步，可以使用map join让小的维度表（1000条以下的记录条数）先进内存。在map端完成join。<blockquote>
<p>实际测试发现：新版的hive已经对小表JOIN大表和大表JOIN小表进行了优化。小表放在左边和右边已经没有明显区别。</p>
</blockquote>
</li>
<li>案例实操<ol>
<li>需求测试大表JOIN小表和小表JOIN大表的效率</li>
<li>开启MapJoin参数设置<ol>
<li>设置自动选择Mapjoin <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.auto.convert.join <span class="operator">=</span> <span class="literal">true</span>; 默认为<span class="literal">true</span></span><br></pre></td></tr></table></figure></li>
<li>大表小表的阈值设置（默认25M以下认为是小表）： <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.mapjoin.smalltable.filesize <span class="operator">=</span> <span class="number">25000000</span>;</span><br></pre></td></tr></table></figure></li>
<li>MapJoin工作机制<br> <img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16355531696295.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></li>
<li>建大表、小表和JOIN后表的语句 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"> <span class="comment">-- 创建大表</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> bigtable (</span><br><span class="line">     id        <span class="type">bigint</span>,</span><br><span class="line">     t         <span class="type">bigint</span>,</span><br><span class="line">     uid       string,</span><br><span class="line">     keyword   string,</span><br><span class="line">     url_rank  <span class="type">int</span>,</span><br><span class="line">     click_num <span class="type">int</span>,</span><br><span class="line">     click_url string</span><br><span class="line"> ) <span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span>;</span><br><span class="line"> <span class="comment">-- 创建小表</span></span><br><span class="line"> <span class="keyword">create</span> <span class="keyword">table</span> smalltable (</span><br><span class="line">     id        <span class="type">bigint</span>,</span><br><span class="line">     t         <span class="type">bigint</span>,</span><br><span class="line">     uid       string,</span><br><span class="line">     keyword   string,</span><br><span class="line">     url_rank  <span class="type">int</span>,</span><br><span class="line">     click_num <span class="type">int</span>,</span><br><span class="line">     click_url string</span><br><span class="line"> ) <span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span>;</span><br><span class="line"> </span><br><span class="line"> <span class="comment">-- 创建join后表的语句</span></span><br><span class="line"> <span class="keyword">create</span> <span class="keyword">table</span> jointable (</span><br><span class="line">     id        <span class="type">bigint</span>,</span><br><span class="line">     t         <span class="type">bigint</span>,</span><br><span class="line">     uid       string,</span><br><span class="line">     keyword   string,</span><br><span class="line">     url_rank  <span class="type">int</span>,</span><br><span class="line">     click_num <span class="type">int</span>,</span><br><span class="line">     click_url string</span><br><span class="line"> ) <span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span>;</span><br></pre></td></tr></table></figure></li>
<li>分别向大表和小表中导入数据 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> load data <span class="keyword">local</span> inpath <span class="string">&#x27;/opt/module/hive-3.1.2/datas/bigtable&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span> bigtable;</span><br><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span>load data <span class="keyword">local</span> inpath <span class="string">&#x27;/opt/module/hive-3.1.2/datas/smalltable&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span> smalltable;</span><br></pre></td></tr></table></figure></li>
<li>小表JOIN大表语句 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> jointable</span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">       b.id, </span><br><span class="line">       b.t, </span><br><span class="line">       b.uid, </span><br><span class="line">       b.keyword, </span><br><span class="line">       b.url_rank, </span><br><span class="line">       b.click_num, </span><br><span class="line">       b.click_url</span><br><span class="line"><span class="keyword">from</span> smalltable s</span><br><span class="line">         <span class="keyword">join</span> bigtable b <span class="keyword">on</span> b.id <span class="operator">=</span> s.id;</span><br></pre></td></tr></table></figure></li>
<li>执行大表JOIN小表语句 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> jointable</span><br><span class="line"><span class="keyword">select</span> </span><br><span class="line">       b.id, </span><br><span class="line">       b.t, </span><br><span class="line">       b.uid, </span><br><span class="line">       b.keyword, </span><br><span class="line">       b.url_rank, </span><br><span class="line">       b.click_num, </span><br><span class="line">       b.click_url</span><br><span class="line"><span class="keyword">from</span> bigtable  b</span><br><span class="line">    <span class="keyword">join</span> smalltable  s <span class="keyword">on</span> s.id <span class="operator">=</span> b.id;</span><br></pre></td></tr></table></figure>
<h3 id="10-4-2-大表Join大表"><a href="#10-4-2-大表Join大表" class="headerlink" title="10.4.2 大表Join大表"></a>10.4.2 大表Join大表</h3></li>
</ol>
</li>
</ol>
</li>
</ul>
<ol>
<li>大表和大表join时，MR一定执行ReduceJoin操作，需要注意优化的点就是ReduceJoin带来的问题！！！<h4 id="10-4-2-1-空Key过滤（针对数据倾斜的一种解决方案）"><a href="#10-4-2-1-空Key过滤（针对数据倾斜的一种解决方案）" class="headerlink" title="10.4.2.1 空Key过滤（针对数据倾斜的一种解决方案）"></a>10.4.2.1 空Key过滤（针对数据倾斜的一种解决方案）</h4></li>
<li>当大表和大表Join走MR的时候，相同Key对应的values会进入一个Reduce，如果出现大量相同搞得key，会导致数据倾斜</li>
<li>案例实操<ol>
<li>创建原始数据表、空id表、合并后数据表 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 创建空id表</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> nullidtable (</span><br><span class="line">    id        <span class="type">bigint</span>,</span><br><span class="line">    t         <span class="type">bigint</span>,</span><br><span class="line">    uid       string,</span><br><span class="line">    keyword   string,</span><br><span class="line">    url_rank  <span class="type">int</span>,</span><br><span class="line">    click_num <span class="type">int</span>,</span><br><span class="line">    click_url string</span><br><span class="line">) <span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span>;</span><br></pre></td></tr></table></figure></li>
<li>分别加载原始数据和空id数据到对应表中 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> load data <span class="keyword">local</span> inpath <span class="string">&#x27;/opt/module/hive-3.1.2/datas/nullid&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span> nullidtable;</span><br></pre></td></tr></table></figure></li>
<li>测试不过滤空id <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">insert</span> overwrite <span class="keyword">table</span> jointable <span class="keyword">select</span> n.<span class="operator">*</span> <span class="keyword">from</span> nullidtable n <span class="keyword">left</span> <span class="keyword">join</span> bigtable o <span class="keyword">on</span> n.id <span class="operator">=</span> o.id;</span><br></pre></td></tr></table></figure></li>
<li>测试过滤空id <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">insert</span> overwrite <span class="keyword">table</span> jointable <span class="keyword">select</span> n.<span class="operator">*</span> <span class="keyword">from</span> (<span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> nullidtable <span class="keyword">where</span> id <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">null</span> ) n  <span class="keyword">left</span> <span class="keyword">join</span> bigtable o <span class="keyword">on</span> n.id <span class="operator">=</span> o.id;</span><br></pre></td></tr></table></figure>
<h4 id="10-4-2-1-SMB-Sort-Merge-Bucket-join"><a href="#10-4-2-1-SMB-Sort-Merge-Bucket-join" class="headerlink" title="10.4.2.1 SMB(Sort Merge Bucket join)"></a>10.4.2.1 SMB(Sort Merge Bucket join)</h4></li>
</ol>
</li>
<li>数据量超级大的时候可能导致Join时间过长，或者直接导致Job失败</li>
<li>原理：从建表的时候就将其创建为分桶表，两表的关联字段作为分桶字段，后续做Join的时候就对桶进行Join，避免了每一条数据进行Join</li>
<li>案例实操<ul>
<li>创建第二张大表  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> bigtable2 (</span><br><span class="line">    id        <span class="type">bigint</span>,</span><br><span class="line">    t         <span class="type">bigint</span>,</span><br><span class="line">    uid       string,</span><br><span class="line">    keyword   string,</span><br><span class="line">    url_rank  <span class="type">int</span>,</span><br><span class="line">    click_num <span class="type">int</span>,</span><br><span class="line">    click_url string</span><br><span class="line">)</span><br><span class="line"><span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span>;</span><br><span class="line">    </span><br><span class="line">load data <span class="keyword">local</span> inpath <span class="string">&#x27;/home/atguigu/hive/table_optimize/bigtable&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span> bigtable2;</span><br></pre></td></tr></table></figure></li>
<li>测试大表直接JOIN 耗时51.176  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> jointable</span><br><span class="line"><span class="keyword">select</span> b.id,</span><br><span class="line">       b.t,</span><br><span class="line">       b.uid,</span><br><span class="line">       b.keyword,</span><br><span class="line">       b.url_rank,</span><br><span class="line">       b.click_num,</span><br><span class="line">       b.click_url</span><br><span class="line"><span class="keyword">from</span> bigtable s</span><br><span class="line"><span class="keyword">join</span> bigtable2 b <span class="keyword">on</span> b.id <span class="operator">=</span> s.id;</span><br><span class="line"></span><br><span class="line"><span class="keyword">No</span> <span class="keyword">rows</span> affected (<span class="number">51.176</span> seconds)</span><br></pre></td></tr></table></figure></li>
<li>创建分桶表1, 创建分桶表2, 桶的个数不要超过可用CPU的核数  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> bigtable_buck1 (</span><br><span class="line">    id        <span class="type">bigint</span>,</span><br><span class="line">    t         <span class="type">bigint</span>,</span><br><span class="line">    uid       string,</span><br><span class="line">    keyword   string,</span><br><span class="line">    url_rank  <span class="type">int</span>,</span><br><span class="line">    click_num <span class="type">int</span>,</span><br><span class="line">    click_url string</span><br><span class="line">)</span><br><span class="line">clustered <span class="keyword">by</span> (id)</span><br><span class="line">sorted <span class="keyword">by</span> (id)</span><br><span class="line"><span class="keyword">into</span> <span class="number">8</span> buckets</span><br><span class="line"><span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> bigtable_buck2 (</span><br><span class="line">    id        <span class="type">bigint</span>,</span><br><span class="line">    t         <span class="type">bigint</span>,</span><br><span class="line">    uid       string,</span><br><span class="line">    keyword   string,</span><br><span class="line">    url_rank  <span class="type">int</span>,</span><br><span class="line">    click_num <span class="type">int</span>,</span><br><span class="line">    click_url string</span><br><span class="line">)</span><br><span class="line">clustered <span class="keyword">by</span> (id)</span><br><span class="line">sorted <span class="keyword">by</span> (id)</span><br><span class="line"><span class="keyword">into</span> <span class="number">8</span> buckets</span><br><span class="line"><span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span>;</span><br><span class="line"></span><br><span class="line">load data inpath <span class="string">&#x27;/bigtable&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span> bigtable_buck1;</span><br><span class="line">load data inpath <span class="string">&#x27;/bigtable&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span> bigtable_buck2;</span><br></pre></td></tr></table></figure></li>
<li>设置参数  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.optimize.bucketmapjoin <span class="operator">=</span> <span class="literal">true</span>;</span><br><span class="line"><span class="keyword">set</span> hive.optimize.bucketmapjoin.sortedmerge <span class="operator">=</span> <span class="literal">true</span>;</span><br><span class="line"><span class="keyword">set</span> hive.input.format<span class="operator">=</span>org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;</span><br></pre></td></tr></table></figure></li>
<li>测试 耗时23.272 seconds  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> jointable</span><br><span class="line"><span class="keyword">select</span> b.id,</span><br><span class="line">       b.t,</span><br><span class="line">       b.uid,</span><br><span class="line">       b.keyword,</span><br><span class="line">       b.url_rank,</span><br><span class="line">       b.click_num,</span><br><span class="line">       b.click_url</span><br><span class="line"><span class="keyword">from</span> bigtable_buck1 s</span><br><span class="line"><span class="keyword">join</span> bigtable_buck2 b <span class="keyword">on</span> b.id <span class="operator">=</span> s.id;</span><br><span class="line"></span><br><span class="line"><span class="keyword">No</span> <span class="keyword">rows</span> affected (<span class="number">23.272</span> seconds)</span><br></pre></td></tr></table></figure>
<h3 id="10-4-3-Group-By"><a href="#10-4-3-Group-By" class="headerlink" title="10.4.3 Group By"></a>10.4.3 Group By</h3></li>
</ul>
</li>
</ol>
<ul>
<li>分组聚合操作时，如果一个组内出现大量的数据，都会交给一个reduce处理，很可能会造成数据倾斜<br>  <img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16355799864007.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></li>
<li>解决思路<ol>
<li>是否在Map端进行聚合，默认为True<br> <code>set hive.map.aggr = true</code></li>
<li>在Map端进行聚合操作的条目数目<br><code>set   = 100000</code> </li>
<li>有数据倾斜的时候进行负载均衡（默认是false）<br> <code>set hive.groupby.skewindata = true</code><br> <font color ='red' >当选项设定为 true，生成的查询计划会有两个MR Job</font>。第一个MR Job中，Map的输出结果会随机分布到Reduce中，每个Reduce做部分聚合操作，并输出结果，这样处理的结果是<font color ='red' >相同的Group By Key有可能被分发到不同的Reduce中</font>，从而达到负载均衡的目的；第二个MR Job再根据预处理的数据结果按照Group By Key分布到Reduce中（这个过程可以保证相同的Group By Key被分布到同一个Reduce中），最后完成最终的聚合操作。</li>
</ol>
<ul>
<li>思路：会多执行一个Job做数据的负载均衡 解决数据倾斜！</li>
</ul>
</li>
<li>测试<ul>
<li>直接执行  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> </span><br><span class="line">       substr(birth, <span class="number">0</span>, <span class="number">7</span>), </span><br><span class="line">       <span class="built_in">count</span>(<span class="number">1</span>)</span><br><span class="line"><span class="keyword">from</span> person</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> substr(birth, <span class="number">0</span>, <span class="number">7</span>)</span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> <span class="number">2</span> <span class="keyword">desc</span>;</span><br><span class="line"></span><br><span class="line">INFO  : MapReduce Jobs Launched:</span><br><span class="line">INFO  : Stage<span class="operator">-</span>Stage<span class="number">-1</span>: Map: <span class="number">1</span>  Reduce: <span class="number">1</span>   Cumulative CPU: <span class="number">2.12</span> sec   HDFS Read: <span class="number">14687</span> HDFS Write: <span class="number">138</span> SUCCESS</span><br><span class="line">INFO  : Total MapReduce CPU <span class="type">Time</span> Spent: <span class="number">2</span> seconds <span class="number">120</span> msec</span><br></pre></td></tr></table></figure></li>
<li>修改参数执行   <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.groupby.skewindata <span class="operator">=</span> <span class="literal">true</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">select</span> </span><br><span class="line">       substr(birth, <span class="number">0</span>, <span class="number">7</span>), </span><br><span class="line">       <span class="built_in">count</span>(<span class="number">1</span>)</span><br><span class="line"><span class="keyword">from</span> person</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> substr(birth, <span class="number">0</span>, <span class="number">7</span>)</span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> <span class="number">2</span> <span class="keyword">desc</span>;</span><br><span class="line"></span><br><span class="line">INFO  : MapReduce Jobs Launched:</span><br><span class="line">INFO  : Stage<span class="operator">-</span>Stage<span class="number">-1</span>: Map: <span class="number">1</span>  Reduce: <span class="number">1</span>   Cumulative CPU: <span class="number">2.47</span> sec   HDFS Read: <span class="number">14399</span> HDFS Write: <span class="number">153</span> SUCCESS</span><br><span class="line">INFO  : Stage<span class="operator">-</span>Stage<span class="number">-2</span>: Map: <span class="number">1</span>  Reduce: <span class="number">1</span>   Cumulative CPU: <span class="number">2.08</span> sec   HDFS Read: <span class="number">7574</span> HDFS Write: <span class="number">138</span> SUCCESS</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>
<h3 id="10-4-4-Count-Distinct-去重统计"><a href="#10-4-4-Count-Distinct-去重统计" class="headerlink" title="10.4.4 Count(Distinct) 去重统计"></a>10.4.4 Count(Distinct) 去重统计</h3><p>数据量小的时候无所谓，数据量大的情况下，由于COUNT DISTINCT操作需要用一个Reduce Task来完成，这一个Reduce需要处理的数据量太大，就会导致整个Job很难完成，一般COUNT DISTINCT使用先GROUP BY再COUNT的方式替换,但是需要注意group by造成的数据倾斜问题.</p>
<ol>
<li>设置5个reduce个数 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> mapreduce.job.reduces <span class="operator">=</span> <span class="number">5</span>;</span><br></pre></td></tr></table></figure></li>
<li>执行去重id查询 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="built_in">count</span>(<span class="keyword">distinct</span> id) <span class="keyword">from</span> bigtable;</span><br><span class="line">Stage<span class="operator">-</span>Stage<span class="number">-1</span>: Map: <span class="number">1</span>  Reduce: <span class="number">1</span>   Cumulative CPU: <span class="number">7.12</span> sec   HDFS Read: <span class="number">120741990</span> HDFS Write: <span class="number">7</span> SUCCESS</span><br><span class="line">Total MapReduce CPU <span class="type">Time</span> Spent: <span class="number">7</span> seconds <span class="number">120</span> msec</span><br><span class="line">OK</span><br><span class="line">c0</span><br><span class="line"><span class="number">100001</span></span><br><span class="line"><span class="type">Time</span> taken: <span class="number">23.607</span> seconds, Fetched: <span class="number">1</span> <span class="type">row</span>(s)</span><br></pre></td></tr></table></figure></li>
<li>采用GROUP by去重id <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="built_in">count</span>(id) <span class="keyword">from</span> (<span class="keyword">select</span> id <span class="keyword">from</span> bigtable <span class="keyword">group</span> <span class="keyword">by</span> id) a;</span><br><span class="line">Stage<span class="operator">-</span>Stage<span class="number">-1</span>: Map: <span class="number">1</span>  Reduce: <span class="number">5</span>   Cumulative CPU: <span class="number">17.53</span> sec   HDFS Read: <span class="number">120752703</span> HDFS Write: <span class="number">580</span> SUCCESS</span><br><span class="line">Stage<span class="operator">-</span>Stage<span class="number">-2</span>: Map: <span class="number">1</span>  Reduce: <span class="number">1</span>   Cumulative CPU: <span class="number">4.29</span> sec2   HDFS Read: <span class="number">9409</span> HDFS Write: <span class="number">7</span> SUCCESS</span><br><span class="line">Total MapReduce CPU <span class="type">Time</span> Spent: <span class="number">21</span> seconds <span class="number">820</span> msec</span><br><span class="line">OK</span><br><span class="line">_c0</span><br><span class="line"><span class="number">100001</span></span><br><span class="line"><span class="type">Time</span> taken: <span class="number">50.795</span> seconds, Fetched: <span class="number">1</span> <span class="type">row</span>(s)</span><br></pre></td></tr></table></figure></li>
<li>虽然会多用一个Job来完成，但在数据量大的情况下，可以避免Reduce因为数据量大导致失败，所以是值得的。</li>
</ol>
<h3 id="10-4-5-笛卡尔积"><a href="#10-4-5-笛卡尔积" class="headerlink" title="10.4.5 笛卡尔积"></a>10.4.5 笛卡尔积</h3><p>尽量避免笛卡尔积，join的时候不加on条件，或者无效的on条件，Hive只能使用1个reducer来完成笛卡尔积。</p>
<h3 id="10-4-6-行列过滤"><a href="#10-4-6-行列过滤" class="headerlink" title="10.4.6 行列过滤"></a>10.4.6 行列过滤</h3><ul>
<li>列处理：在SELECT中，只拿需要的列，如果有分区，尽量使用分区过滤，少用SELECT *。</li>
<li>行处理：在分区剪裁中，当使用外关联时，如果将副表的过滤条件写在Where后面，那么就会先全表关联，之后再过滤，比如：</li>
<li>案例实操：<ol>
<li>测试先关联两张表，再用where条件过滤 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> </span><br><span class="line">       o.id </span><br><span class="line"><span class="keyword">from</span> bigtable b</span><br><span class="line"><span class="keyword">join</span> bigtable o <span class="keyword">on</span>  o.id <span class="operator">=</span> b.id</span><br><span class="line"><span class="keyword">where</span> o.id <span class="operator">&lt;=</span> <span class="number">10</span>;</span><br><span class="line"></span><br><span class="line"><span class="number">1</span>,<span class="number">081</span> <span class="keyword">rows</span> selected (<span class="number">18.979</span> seconds)</span><br></pre></td></tr></table></figure></li>
<li>通过子查询后，再关联表 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> </span><br><span class="line">       b.id</span><br><span class="line"><span class="keyword">from</span> bigtable b</span><br><span class="line"><span class="keyword">join</span> (<span class="keyword">select</span> </span><br><span class="line">             id</span><br><span class="line">    <span class="keyword">from</span> bigtable</span><br><span class="line">    <span class="keyword">where</span> id <span class="operator">&lt;=</span> <span class="number">10</span>) o </span><br><span class="line"><span class="keyword">on</span> b.id <span class="operator">=</span> o.id;</span><br><span class="line"></span><br><span class="line"><span class="number">1</span>,<span class="number">081</span> <span class="keyword">rows</span> selected (<span class="number">17.753</span> seconds)</span><br></pre></td></tr></table></figure>
<h3 id="10-4-7-分区"><a href="#10-4-7-分区" class="headerlink" title="10.4.7 分区"></a>10.4.7 分区</h3>详见7.1章。</li>
</ol>
</li>
</ul>
<h3 id="10-4-8-分桶"><a href="#10-4-8-分桶" class="headerlink" title="10.4.8 分桶"></a>10.4.8 分桶</h3><pre><code>详见7.2章。
</code></pre>
<h2 id="10-5-合理设置Map及Reduce数"><a href="#10-5-合理设置Map及Reduce数" class="headerlink" title="10.5 合理设置Map及Reduce数"></a>10.5 合理设置Map及Reduce数</h2><ol>
<li>通常情况下，作业会产生一个或者多个map任务。<ul>
<li>主要的决定因素有：input的文件总个数，input的文件大小，集群设置的文件块大小。</li>
</ul>
</li>
<li>是不是map数越多越好？<ul>
<li>答案是否定的。如果一个任务有很多小文件（远远小于块大小128m），则每个小文件也会被当做一个片，用一个map任务来完成，而一个map任务启动和初始化的时间远远大于逻辑处理的时间，就会造成很大的资源浪费。而且，同时可执行的map数是受限的。</li>
</ul>
</li>
<li>是不是保证每个map处理接近128m的文件块，就高枕无忧了？<ul>
<li>答案也是不一定。比如有一个127m的文件，正常会用一个map去完成，但这个文件只有一个或者两个小字段，却有几千万的记录，如果map处理的逻辑比较复杂，用一个map任务去做，肯定也比较耗时。</li>
</ul>
</li>
</ol>
<ul>
<li>针对上面的问题2和3，我们需要采取两种方式来解决：即减少map数和增加map数；</li>
</ul>
<h3 id="10-5-1-复杂文件增加Map数"><a href="#10-5-1-复杂文件增加Map数" class="headerlink" title="10.5.1 复杂文件增加Map数"></a>10.5.1 复杂文件增加Map数</h3><ul>
<li>当input的文件都很大，任务逻辑复杂，map执行非常慢的时候，可以考虑增加Map数，来使得每个map处理的数据量减少，从而提高任务的执行效率。</li>
<li>增加map的方法为：根据<br>computeSliteSize(Math.max(minSize,Math.min(maxSize,blocksize)))=blocksize=128M公式，调整maxSize最大值。让maxSize最大值低于blocksize就可以增加map的个数。</li>
<li>案例实操：<ol>
<li>执行查询 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="built_in">count</span>(<span class="operator">*</span>) <span class="keyword">from</span> bigtable;</span><br><span class="line"></span><br><span class="line">Hadoop job information <span class="keyword">for</span> Stage<span class="number">-1</span>: number <span class="keyword">of</span> mappers: <span class="number">1</span>; number <span class="keyword">of</span> reducers: <span class="number">1</span></span><br></pre></td></tr></table></figure></li>
<li>设置最大切片值为100个字节 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> mapreduce.input.fileinputformat.split.maxsize<span class="operator">=</span><span class="number">10240</span>;</span><br><span class="line"></span><br><span class="line">Hadoop job information <span class="keyword">for</span> Stage<span class="number">-1</span>: number <span class="keyword">of</span> mappers: <span class="number">12613</span>; number <span class="keyword">of</span> reducers: <span class="number">1</span></span><br></pre></td></tr></table></figure>
<h3 id="10-5-2-小文件进行合并"><a href="#10-5-2-小文件进行合并" class="headerlink" title="10.5.2 小文件进行合并"></a>10.5.2 小文件进行合并</h3></li>
</ol>
</li>
</ul>
<ol>
<li>在map执行前合并小文件，减少map数：CombineHiveInputFormat具有对小文件进行合并的功能（系统默认的格式）。HiveInputFormat没有对小文件合并功能。 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.input.format<span class="operator">=</span> org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;</span><br></pre></td></tr></table></figure></li>
<li>在Map-Reduce的任务结束时合并小文件的设置：<ul>
<li>在map-only任务结束时合并小文件，默认true  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SET</span> hive.merge.mapfiles <span class="operator">=</span> <span class="literal">true</span>;</span><br></pre></td></tr></table></figure></li>
<li>在map-reduce任务结束时合并小文件，默认false  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SET</span> hive.merge.mapredfiles <span class="operator">=</span> <span class="literal">true</span>;</span><br></pre></td></tr></table></figure></li>
<li>合并文件的大小，默认256M  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SET</span> hive.merge.size.per.task <span class="operator">=</span> <span class="number">268435456</span>;</span><br></pre></td></tr></table></figure></li>
<li>当输出文件的平均大小小于该值时，启动一个独立的map-reduce任务进行文件merge  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SET</span> hive.merge.smallfiles.avgsize <span class="operator">=</span> <span class="number">16777216</span>;</span><br></pre></td></tr></table></figure>
<h3 id="10-5-3-合理设置Reduce数"><a href="#10-5-3-合理设置Reduce数" class="headerlink" title="10.5.3 合理设置Reduce数"></a>10.5.3 合理设置Reduce数</h3></li>
</ul>
</li>
<li>调整reduce个数方法一<ol>
<li>每个Reduce处理的数据量默认是256MB <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive.exec.reducers.bytes.per.reducer<span class="operator">=</span><span class="number">256000000</span></span><br></pre></td></tr></table></figure></li>
<li>每个任务最大的reduce数，默认为1009 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hive.exec.reducers.max<span class="operator">=</span><span class="number">1009</span></span><br></pre></td></tr></table></figure></li>
<li>计算reducer数的公式 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">N<span class="operator">=</span><span class="built_in">min</span>(参数<span class="number">2</span>，总输入数据量<span class="operator">/</span>参数<span class="number">1</span>)</span><br></pre></td></tr></table></figure></li>
</ol>
</li>
<li>调整reduce个数方法二<ol>
<li>在hadoop的mapred-default.xml文件中修改，设置每个job的Reduce个数 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> mapreduce.job.reduces <span class="operator">=</span> <span class="number">15</span>;</span><br></pre></td></tr></table></figure></li>
</ol>
</li>
<li>reduce个数并不是越多越好<ol>
<li>过多的启动和初始化reduce也会消耗时间和资源；</li>
<li>另外，有多少个reduce，就会有多少个输出文件，如果生成了很多个小文件，那么如果这些小文件作为下一个任务的输入，则也会出现小文件过多的问题；</li>
<li>在设置reduce个数的时候也需要考虑这两个原则：处理大数据量利用合适的reduce数；使单个reduce任务处理数据量大小要合适；</li>
</ol>
</li>
</ol>
<h3 id="10-6-并行执行"><a href="#10-6-并行执行" class="headerlink" title="10.6 并行执行"></a>10.6 并行执行</h3><ul>
<li>Hive会将一个查询转化成一个或者多个阶段。这样的阶段可以是MapReduce阶段、抽样阶段、合并阶段、limit阶段。或者Hive执行过程中可能需要的其他阶段。默认情况下，Hive一次只会执行一个阶段。不过，某个特定的job可能包含众多的阶段，而这些阶段可能并非完全互相依赖的，也就是说有些阶段是可以并行执行的，这样可能使得整个job的执行时间缩短。不过，如果有更多的阶段可以并行执行，那么job可能就越快完成。</li>
<li>通过设置参数hive.exec.parallel值为true，就可以开启并发执行。不过，在共享集群中，需要注意下，如果job中并行阶段增多，那么集群利用率就会增加。当然，得是在系统资源比较空闲的时候才有优势，否则，没资源，并行也起不来。  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 打开任务并行执行</span></span><br><span class="line"><span class="keyword">set</span> hive.exec.parallel<span class="operator">=</span><span class="literal">true</span>;</span><br><span class="line"><span class="comment">-- 同一个sql允许最大并行度，默认为8。</span></span><br><span class="line"><span class="keyword">set</span> hive.exec.parallel.thread.number<span class="operator">=</span><span class="number">8</span>; </span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="10-7-严格模式"><a href="#10-7-严格模式" class="headerlink" title="10.7 严格模式"></a>10.7 严格模式</h3><p>Hive可以通过设置防止一些危险操作：</p>
<ol>
<li>分区表不使用分区过滤<ul>
<li>将hive.strict.checks.no.partition.filter设置为true时，对于分区表，除非where语句中含有分区字段过滤条件来限制范围，否则不允许执行。换句话说，就是用户不允许扫描所有分区。进行这个限制的原因是，通常分区表都拥有非常大的数据集，而且数据增加迅速。没有进行分区限制的查询可能会消耗令人不可接受的巨大资源来处理这个表。</li>
</ul>
</li>
<li>使用order by没有limit过滤<ul>
<li>将hive.strict.checks.orderby.no.limit设置为true时，对于使用了order by语句的查询，要求必须使用limit语句。因为order by为了执行排序过程会将所有的结果数据分发到同一个Reducer中进行处理，强制要求用户增加这个LIMIT语句可以防止Reducer额外执行很长一段时间。</li>
</ul>
</li>
<li>笛卡尔积<ul>
<li>将hive.strict.checks.cartesian.product设置为true时，会限制笛卡尔积的查询。对关系型数据库非常了解的用户可能期望在 执行JOIN查询的时候不使用ON语句而是使用where语句，这样关系数据库的执行优化器就可以高效地将WHERE语句转化成那个ON语句。不幸的是，Hive并不会执行这种优化，因此，如果表足够大，那么这个查询就会出现不可控的情况。</li>
</ul>
</li>
</ol>
<h2 id="10-8-JVM重用"><a href="#10-8-JVM重用" class="headerlink" title="10.8 JVM重用"></a>10.8 JVM重用</h2><pre><code>详见hadoop优化文档中jvm重用
</code></pre>
<h2 id="10-9-压缩"><a href="#10-9-压缩" class="headerlink" title="10.9 压缩"></a>10.9 压缩</h2><pre><code>详见第9章。
</code></pre>
<h1 id="十一、Hive实战"><a href="#十一、Hive实战" class="headerlink" title="十一、Hive实战"></a>十一、Hive实战</h1><h2 id="11-1-需求描述"><a href="#11-1-需求描述" class="headerlink" title="11.1 需求描述"></a>11.1 需求描述</h2><p>统计硅谷影音视频网站的常规指标，各种TopN指标：</p>
<ul>
<li>统计视频观看数Top10</li>
<li>统计视频类别热度Top10</li>
<li>统计出视频观看数最高的20个视频的所属类别以及类别包含Top20视频的个数</li>
<li>统计视频观看数Top50所关联视频的所属类别Rank</li>
<li>统计每个类别中的视频热度Top10,以Music为例</li>
<li>统计每个类别视频观看数Top10</li>
<li>统计上传视频最多的用户Top10以及他们上传的视频观看次数在前20的视频 </li>
</ul>
<h2 id="11-2-数据结构"><a href="#11-2-数据结构" class="headerlink" title="11.2 数据结构"></a>11.2 数据结构</h2><ol>
<li>视频表<table>
<thead>
<tr>
<th>字段</th>
<th>备注</th>
<th>详细描述</th>
</tr>
</thead>
<tbody><tr>
<td>videoId</td>
<td>视频唯一id（String）</td>
<td>11位字符串</td>
</tr>
<tr>
<td>uploader</td>
<td>视频上传者（String）</td>
<td>上传视频的用户名String</td>
</tr>
<tr>
<td>age</td>
<td>视频年龄（int）</td>
<td>视频在平台上的整数天</td>
</tr>
<tr>
<td>category</td>
<td>视频类别（Array<String>）</td>
<td>上传视频指定的视频分类</td>
</tr>
<tr>
<td>length</td>
<td>视频长度（Int）</td>
<td>整形数字标识的视频长度</td>
</tr>
<tr>
<td>views</td>
<td>观看次数（Int）</td>
<td>视频被浏览的次数</td>
</tr>
<tr>
<td>rate</td>
<td>视频评分（Double）</td>
<td>满分5分</td>
</tr>
<tr>
<td>Ratings</td>
<td>流量（Int）</td>
<td>视频的流量，整型数字</td>
</tr>
<tr>
<td>conments</td>
<td>评论数（Int）</td>
<td>一个视频的整数评论数</td>
</tr>
<tr>
<td>relatedId</td>
<td>相关视频id（Array<String>）</td>
<td>相关视频的id，最多20个</td>
</tr>
</tbody></table>
</li>
<li>用户表<table>
<thead>
<tr>
<th>字段</th>
<th>备注</th>
<th>字段类型</th>
</tr>
</thead>
<tbody><tr>
<td>uploader</td>
<td>上传者用户名</td>
<td>string</td>
</tr>
<tr>
<td>videos</td>
<td>上传视频数</td>
<td>int</td>
</tr>
<tr>
<td>friends</td>
<td>朋友数量</td>
<td>int</td>
</tr>
</tbody></table>
</li>
</ol>
<h2 id="11-3-准备工作"><a href="#11-3-准备工作" class="headerlink" title="11.3 准备工作"></a>11.3 准备工作</h2><h3 id="11-3-1-ETL"><a href="#11-3-1-ETL" class="headerlink" title="11.3.1 ETL"></a>11.3.1 ETL</h3><h3 id="11-3-2-准备表"><a href="#11-3-2-准备表" class="headerlink" title="11.3.2 准备表"></a>11.3.2 准备表</h3><ol>
<li>需要准备的表<ul>
<li>创建原始数据表：gulivideo_ori，gulivideo_user_ori，</li>
<li>创建最终表：gulivideo_orc，gulivideo_user_orc</li>
</ul>
</li>
<li>创建原始数据表：<ul>
<li>创建视频原始数据表：gulivideo_ori  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> gulivideo_ori (</span><br><span class="line">    videoId   string,</span><br><span class="line">    uploader  string,</span><br><span class="line">    age       <span class="type">int</span>,</span><br><span class="line">    category  <span class="keyword">array</span><span class="operator">&lt;</span>string<span class="operator">&gt;</span>,</span><br><span class="line">    length    <span class="type">int</span>,</span><br><span class="line">    `views`     <span class="type">int</span>,</span><br><span class="line">    rate      <span class="type">float</span>,</span><br><span class="line">    ratings   <span class="type">int</span>,</span><br><span class="line">    comments  <span class="type">int</span>,</span><br><span class="line">    relatedId <span class="keyword">array</span><span class="operator">&lt;</span>string<span class="operator">&gt;</span></span><br><span class="line">)</span><br><span class="line"><span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> &quot;\t&quot;</span><br><span class="line">collection items terminated <span class="keyword">by</span> &quot;&amp;&quot;</span><br><span class="line">stored <span class="keyword">as</span> textfile;</span><br></pre></td></tr></table></figure></li>
<li>创建用户原始数据表: gulivideo_user_ori  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> gulivideo_user_ori(</span><br><span class="line">    uploader string,</span><br><span class="line">    videos <span class="type">int</span>,</span><br><span class="line">    friends <span class="type">int</span>)</span><br><span class="line"><span class="type">row</span> format delimited </span><br><span class="line">fields terminated <span class="keyword">by</span> &quot;\t&quot; </span><br><span class="line">stored <span class="keyword">as</span> textfile;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>创建orc存储格式带snappy压缩的表：<ul>
<li>gulivideo_orc  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> gulivideo_orc(</span><br><span class="line">    videoId string, </span><br><span class="line">    uploader string, </span><br><span class="line">    age <span class="type">int</span>, </span><br><span class="line">    category <span class="keyword">array</span><span class="operator">&lt;</span>string<span class="operator">&gt;</span>, </span><br><span class="line">    length <span class="type">int</span>, </span><br><span class="line">    views <span class="type">int</span>, </span><br><span class="line">    rate <span class="type">float</span>, </span><br><span class="line">    ratings <span class="type">int</span>, </span><br><span class="line">    comments <span class="type">int</span>,</span><br><span class="line">    relatedId <span class="keyword">array</span><span class="operator">&lt;</span>string<span class="operator">&gt;</span>)</span><br><span class="line">stored <span class="keyword">as</span> orc</span><br><span class="line">tblproperties(&quot;orc.compress&quot;<span class="operator">=</span>&quot;SNAPPY&quot;);</span><br></pre></td></tr></table></figure></li>
<li>gulivideo_user_orc  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> gulivideo_user_orc(</span><br><span class="line">    uploader string,</span><br><span class="line">    videos <span class="type">int</span>,</span><br><span class="line">    friends <span class="type">int</span>)</span><br><span class="line"><span class="type">row</span> format delimited </span><br><span class="line">fields terminated <span class="keyword">by</span> &quot;\t&quot; </span><br><span class="line">stored <span class="keyword">as</span> orc</span><br><span class="line">tblproperties(&quot;orc.compress&quot;<span class="operator">=</span>&quot;SNAPPY&quot;);</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>向ori表插入数据 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">load data inpath &quot;/gulivideo/video/output&quot; <span class="keyword">into</span> <span class="keyword">table</span> gulivideo_ori;</span><br><span class="line">load data inpath &quot;/gulivideo/user&quot; <span class="keyword">into</span> <span class="keyword">table</span> gulivideo_user_ori;</span><br></pre></td></tr></table></figure></li>
<li>向orc表插入数据 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">table</span> gulivideo_orc <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> gulivideo_ori;</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">table</span> gulivideo_user_orc <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> gulivideo_user_ori;</span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="11-3-3-安装Tez引擎"><a href="#11-3-3-安装Tez引擎" class="headerlink" title="11.3.3 安装Tez引擎"></a>11.3.3 安装Tez引擎</h3><p>Tez是一个Hive的运行引擎，性能优于MR。为什么优于MR呢？看下<br><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16356724726440.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
<ul>
<li>用Hive直接编写MR程序，假设有四个有依赖关系的MR作业，上图中，绿色是Reduce Task，云状表示写屏蔽，需要将中间结果持久化写到HDFS。</li>
<li>Tez可以将多个有依赖的作业转换为一个作业，这样只需写一次HDFS，且中间节点较少，从而大大提升作业的计算性能。</li>
<li>安装步骤<ol>
<li>将tez安装包拷贝到集群，并解压tar包 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 software]$ mkdir /opt/module/tez</span><br><span class="line">[atguigu@hadoop102 software]$ tar -zxvf /opt/software/tez-0.10.1-SNAPSHOT-minimal.tar.gz -C /opt/module/tez</span><br></pre></td></tr></table></figure></li>
<li>上传tez依赖到HDFS <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 software]$ hadoop fs -mkdir /tez</span><br><span class="line">[atguigu@hadoop102 software]$ hadoop fs -put /opt/software/tez-0.10.1-SNAPSHOT.tar.gz /tez</span><br></pre></td></tr></table></figure></li>
<li>新建$HADOOP_HOME/etc/hadoop/tez-site.xml添加如下内容： <figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;</span></span><br><span class="line"><span class="meta">&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>tez.lib.uris<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>$&#123;fs.defaultFS&#125;/tez/tez-0.10.1-SNAPSHOT.tar.gz<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>tez.use.cluster.hadoop-libs<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>tez.am.resource.memory.mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>1024<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>tez.am.resource.cpu.vcores<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>tez.container.max.java.heap.fraction<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>0.4<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>tez.task.resource.memory.mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>1024<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>tez.task.resource.cpu.vcores<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure></li>
</ol>
</li>
</ul>
<ol start="4">
<li>修改Hadoop环境变量$HADOOP_HOME/etc/hadoop/shellprofile.d/tez.sh <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 添加Tez的Jar包相关信息</span></span><br><span class="line">hadoop_add_profile tez</span><br><span class="line"><span class="keyword">function</span> _tez_hadoop_classpath</span><br><span class="line">&#123;</span><br><span class="line">    hadoop_add_classpath <span class="string">&quot;<span class="variable">$HADOOP_HOME</span>/etc/hadoop&quot;</span> after</span><br><span class="line">    hadoop_add_classpath <span class="string">&quot;/opt/module/tez/*&quot;</span> after</span><br><span class="line">    hadoop_add_classpath <span class="string">&quot;/opt/module/tez/lib/*&quot;</span> after</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>修改Hive的计算引擎$HIVE_HOME/conf/hive-site.xml <figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.execution.engine<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>tez<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.tez.container.size<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>1024<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li>
<li>解决日志Jar包冲突 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 software]$ rm /opt/module/tez/lib/slf4j-log4j12-1.7.10.jar</span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="11-4-业务分析"><a href="#11-4-业务分析" class="headerlink" title="11.4 业务分析"></a>11.4 业务分析</h2><ul>
<li>统计视频观看数Top10  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> videoId,</span><br><span class="line">       uploader,</span><br><span class="line">       age,</span><br><span class="line">       category,</span><br><span class="line">       length,</span><br><span class="line">       `views`,</span><br><span class="line">       rate,</span><br><span class="line">       ratings,</span><br><span class="line">       comments,</span><br><span class="line">       relatedId</span><br><span class="line"><span class="keyword">from</span> gulivideo_orc</span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> `views` <span class="keyword">desc</span></span><br><span class="line">limit <span class="number">10</span>;</span><br></pre></td></tr></table></figure></li>
<li>统计视频类别热度Top10  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line">       category_name,</span><br><span class="line">       <span class="built_in">count</span>(videoId) video_count</span><br><span class="line"><span class="keyword">from</span> (</span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">    videoId,category_name</span><br><span class="line"><span class="keyword">from</span> gulivideo_orc</span><br><span class="line"> <span class="keyword">lateral</span> <span class="keyword">view</span> explode(category) explode_category <span class="keyword">as</span> category_name</span><br><span class="line">) t1</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> category_name</span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> video_count <span class="keyword">desc</span> limit <span class="number">10</span>;</span><br></pre></td></tr></table></figure></li>
<li>统计出视频观看数最高的20个视频的所属类别以及类别包含Top20视频的个数  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line">       t2.category_name,</span><br><span class="line">       <span class="built_in">count</span>(videoId) video_count</span><br><span class="line"><span class="keyword">from</span> (</span><br><span class="line">         <span class="keyword">select</span></span><br><span class="line">                t1.videoId,</span><br><span class="line">                category_name</span><br><span class="line">         <span class="keyword">from</span> (</span><br><span class="line">                  <span class="keyword">select</span></span><br><span class="line">                         videoId,</span><br><span class="line">                         category,</span><br><span class="line">                         `views`</span><br><span class="line">                  <span class="keyword">from</span> gulivideo_orc</span><br><span class="line">                  <span class="keyword">order</span> <span class="keyword">by</span> `views` <span class="keyword">desc</span></span><br><span class="line">                  limit <span class="number">20</span></span><br><span class="line">              ) t1</span><br><span class="line">                  <span class="keyword">lateral</span> <span class="keyword">view</span> explode(t1.category) explode_category <span class="keyword">as</span> category_name</span><br><span class="line">     ) t2</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> t2.category_name</span><br><span class="line">;</span><br></pre></td></tr></table></figure>
</li>
<li>统计视频观看数Top50所关联视频的所属类别排名  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> category_name,</span><br><span class="line">       video_count,</span><br><span class="line">       <span class="built_in">rank</span>() <span class="keyword">over</span> (<span class="keyword">order</span> <span class="keyword">by</span> video_count) video_count_rank,</span><br><span class="line">       video_views,</span><br><span class="line">       <span class="built_in">rank</span>() <span class="keyword">over</span> (<span class="keyword">order</span> <span class="keyword">by</span> video_views) video_views_rank</span><br><span class="line"><span class="keyword">from</span> (</span><br><span class="line">         <span class="keyword">select</span> category_name,</span><br><span class="line">                <span class="built_in">count</span>(t5.single_relatedId)  video_count,</span><br><span class="line">                <span class="built_in">sum</span>(t5.related_video_views) video_views</span><br><span class="line">         <span class="keyword">from</span> (</span><br><span class="line">                  <span class="keyword">select</span> t4.orign_video_id,</span><br><span class="line">                         t4.orign_video_views,</span><br><span class="line">                         t4.single_relatedId,</span><br><span class="line">                         t4.related_video_views,</span><br><span class="line">                         category_name</span><br><span class="line">                  <span class="keyword">from</span> (</span><br><span class="line">                           <span class="keyword">select</span> t2.videoId orign_video_id,</span><br><span class="line">                                  t2.views   orign_video_views,</span><br><span class="line">                                  t2.single_relatedId,</span><br><span class="line">                                  t3.views   related_video_views,</span><br><span class="line">                                  t3.category</span><br><span class="line">                           <span class="keyword">from</span> (<span class="keyword">select</span> t1.videoId,</span><br><span class="line">                                        t1.views,</span><br><span class="line">                                        single_relatedId</span><br><span class="line">                                 <span class="keyword">from</span> (</span><br><span class="line">                                          <span class="keyword">select</span> videoId,</span><br><span class="line">                                                 `views`,</span><br><span class="line">                                                 relatedId</span><br><span class="line">                                          <span class="keyword">from</span> gulivideo_orc</span><br><span class="line">                                          <span class="keyword">order</span> <span class="keyword">by</span> `views` <span class="keyword">desc</span></span><br><span class="line">                                          limit <span class="number">50</span></span><br><span class="line">                                      ) t1</span><br><span class="line">                                          <span class="keyword">lateral</span> <span class="keyword">view</span> explode(t1.relatedId) explode_relatedId <span class="keyword">as</span> single_relatedId) t2</span><br><span class="line">                                    <span class="keyword">left</span> <span class="keyword">join</span> gulivideo_orc t3 <span class="keyword">on</span> t2.single_relatedId <span class="operator">=</span> t3.videoId</span><br><span class="line">                           <span class="keyword">where</span> t3.videoId <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">null</span></span><br><span class="line">                       ) t4</span><br><span class="line">                           <span class="keyword">lateral</span> <span class="keyword">view</span> explode(t4.category) explode_category <span class="keyword">as</span> category_name</span><br><span class="line">              ) t5</span><br><span class="line">         <span class="keyword">group</span> <span class="keyword">by</span> t5.category_name</span><br><span class="line">     ) t6</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
<li>统计每个类别中的视频热度Top10,以Music为例  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 统计每个类别中的视频热度Top10,以Music为例</span></span><br><span class="line"><span class="keyword">select</span> videoId,</span><br><span class="line">       `views`,</span><br><span class="line">       category_name</span><br><span class="line"><span class="keyword">from</span> gulivideo_orc</span><br><span class="line"><span class="keyword">lateral</span> <span class="keyword">VIEW</span> explode(category) gulivideo_orc_tmp <span class="keyword">AS</span> category_name</span><br><span class="line"><span class="keyword">where</span> array_contains(category, &quot;Music&quot;)</span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> `views` <span class="keyword">desc</span></span><br><span class="line">limit <span class="number">10</span>;</span><br><span class="line"><span class="comment">-- 统计每个类别中的视频热度Top10,以Music为例2</span></span><br><span class="line"><span class="keyword">SELECT</span> t1.videoId,</span><br><span class="line">       t1.views,</span><br><span class="line">       t1.category_name</span><br><span class="line"><span class="keyword">FROM</span> (</span><br><span class="line">         <span class="keyword">SELECT</span> videoId,</span><br><span class="line">                `views`,</span><br><span class="line">                category_name</span><br><span class="line">         <span class="keyword">FROM</span> gulivideo_orc</span><br><span class="line">                  <span class="keyword">lateral</span> <span class="keyword">VIEW</span> explode(category) gulivideo_orc_tmp <span class="keyword">AS</span> category_name</span><br><span class="line">     ) t1</span><br><span class="line"><span class="keyword">WHERE</span> t1.category_name <span class="operator">=</span> &quot;Music&quot;</span><br><span class="line"><span class="keyword">ORDER</span> <span class="keyword">BY</span> t1.views <span class="keyword">DESC</span></span><br><span class="line">LIMIT <span class="number">10</span>;</span><br></pre></td></tr></table></figure></li>
<li>统计每个类别视频观看数Top10  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="operator">*</span></span><br><span class="line"><span class="keyword">from</span> (</span><br><span class="line">         <span class="keyword">SELECT</span> t1.videoId,</span><br><span class="line">                t1.views,</span><br><span class="line">                t1.category_name,</span><br><span class="line">                <span class="built_in">rank</span>() <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> t1.category_name <span class="keyword">order</span> <span class="keyword">by</span> t1.views <span class="keyword">desc</span>) video_rank</span><br><span class="line">         <span class="keyword">FROM</span> (</span><br><span class="line">                  <span class="keyword">SELECT</span> videoId,</span><br><span class="line">                         `views`,</span><br><span class="line">                         category_name</span><br><span class="line">                  <span class="keyword">FROM</span> gulivideo_orc</span><br><span class="line">                           <span class="keyword">lateral</span> <span class="keyword">VIEW</span> explode(category) gulivideo_orc_tmp <span class="keyword">AS</span> category_name</span><br><span class="line">              ) t1</span><br><span class="line">     ) t2</span><br><span class="line"><span class="keyword">where</span> t2.video_rank <span class="operator">&lt;=</span> <span class="number">10</span></span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> t2.category_name, t2.video_rank</span><br></pre></td></tr></table></figure></li>
<li>统计上传视频最多的用户Top10以及他们上传的视频观看次数在前20的视频  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="operator">*</span></span><br><span class="line"><span class="keyword">from</span> (</span><br><span class="line">         <span class="keyword">select</span> uploader,</span><br><span class="line">                videoId,</span><br><span class="line">                `views`,</span><br><span class="line">                <span class="built_in">percent_rank</span>() <span class="keyword">over</span> (<span class="keyword">order</span> <span class="keyword">by</span> `views` )                                       views_percent_rank,</span><br><span class="line">                <span class="built_in">count</span>(videoId) <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> uploader) video_count,</span><br><span class="line">                <span class="built_in">dense_rank</span>() <span class="keyword">over</span> (<span class="keyword">order</span> <span class="keyword">by</span> <span class="built_in">count</span>(videoId) <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> uploader) <span class="keyword">desc</span>) video_count_rank</span><br><span class="line">         <span class="keyword">from</span> gulivideo_orc</span><br><span class="line">     ) t1</span><br><span class="line"><span class="keyword">where</span> t1.views_percent_rank <span class="operator">&gt;=</span> <span class="number">0.8</span></span><br><span class="line">  <span class="keyword">and</span> t1.video_count_rank <span class="operator">&lt;</span> <span class="number">10</span>;</span><br><span class="line">    </span><br><span class="line"><span class="keyword">SELECT</span> t2.videoId,</span><br><span class="line">       t2.views,</span><br><span class="line">       t2.uploader</span><br><span class="line"><span class="keyword">FROM</span> (</span><br><span class="line">         <span class="keyword">SELECT</span> uploader,</span><br><span class="line">                videos</span><br><span class="line">         <span class="keyword">FROM</span> gulivideo_user_orc</span><br><span class="line">         <span class="keyword">ORDER</span> <span class="keyword">BY</span> videos <span class="keyword">DESC</span></span><br><span class="line">         LIMIT <span class="number">10</span></span><br><span class="line">     ) t1</span><br><span class="line">         <span class="keyword">JOIN</span> gulivideo_orc t2</span><br><span class="line">              <span class="keyword">ON</span> t1.uploader <span class="operator">=</span> t2.uploader</span><br><span class="line"><span class="keyword">ORDER</span> <span class="keyword">BY</span> t2.views</span><br><span class="line">        <span class="keyword">DESC</span></span><br><span class="line">LIMIT <span class="number">20</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span></span><br><span class="line"><span class="keyword">from</span> (</span><br><span class="line">         <span class="keyword">SELECT</span> t2.videoId,</span><br><span class="line">                t2.views,</span><br><span class="line">                t2.uploader,</span><br><span class="line">                <span class="built_in">rank</span>() <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> t2.uploader <span class="keyword">order</span> <span class="keyword">by</span> t2.views <span class="keyword">desc</span> ) views_rank</span><br><span class="line">         <span class="keyword">FROM</span> (</span><br><span class="line">                  <span class="keyword">SELECT</span> uploader,</span><br><span class="line">                         videos</span><br><span class="line">                  <span class="keyword">FROM</span> gulivideo_user_orc</span><br><span class="line">                  <span class="keyword">ORDER</span> <span class="keyword">BY</span> videos <span class="keyword">DESC</span></span><br><span class="line">                  LIMIT <span class="number">10</span></span><br><span class="line">              ) t1</span><br><span class="line">                  <span class="keyword">JOIN</span> gulivideo_orc t2</span><br><span class="line">                       <span class="keyword">ON</span> t1.uploader <span class="operator">=</span> t2.uploader</span><br><span class="line">     ) t3</span><br><span class="line"><span class="keyword">where</span> t3.views_rank <span class="operator">&lt;=</span> <span class="number">20</span></span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> uploader, views_rank</span><br></pre></td></tr></table></figure></li>
</ul>
]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>Hive</tag>
        <tag>数仓</tag>
      </tags>
  </entry>
  <entry>
    <title>Hadoop</title>
    <url>/2021/11/07/Hadoop/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="Hadoop"><a href="#Hadoop" class="headerlink" title="Hadoop"></a>Hadoop</h1><h1 id="一、大数据概论"><a href="#一、大数据概论" class="headerlink" title="一、大数据概论"></a>一、大数据概论</h1><h2 id="1-1-大数据概念"><a href="#1-1-大数据概念" class="headerlink" title="1.1 大数据概念"></a>1.1 大数据概念</h2><ul>
<li>大数据: 指无法在一定时间范围内用常规软件工具进行捕捉、管理和处理的数据集合，是需要新处理模式才能具有更强的决策力、洞察发现力和流程优化能力的海量、高增长率和多样化的信息资产</li>
<li>解决海量数据的存储和分析计算</li>
</ul>
<h2 id="1-2-大数据特点"><a href="#1-2-大数据特点" class="headerlink" title="1.2.大数据特点"></a>1.2.大数据特点</h2><ol>
<li>Volume: 大量</li>
<li>Velocity: 高速</li>
<li>Variety: 多样</li>
<li>Value: 低价值密度</li>
</ol>
<h2 id="1-3-大数据应用场景"><a href="#1-3-大数据应用场景" class="headerlink" title="1.3.大数据应用场景"></a>1.3.大数据应用场景</h2><ol>
<li>物流仓储、零售、旅游···</li>
</ol>
<h2 id="1-4-大数据部门组织结构"><a href="#1-4-大数据部门组织结构" class="headerlink" title="1.4.大数据部门组织结构"></a>1.4.大数据部门组织结构</h2><p><img src="https://i.loli.net/2021/11/11/S94W8YQMsxfFEv6.jpg"></p>
<h1 id="二、从Hadoop框架了解大数据生态"><a href="#二、从Hadoop框架了解大数据生态" class="headerlink" title="二、从Hadoop框架了解大数据生态"></a>二、从Hadoop框架了解大数据生态</h1><h2 id="2-1-Hadoop是什么"><a href="#2-1-Hadoop是什么" class="headerlink" title="2.1 Hadoop是什么"></a>2.1 Hadoop是什么</h2><ul>
<li>Hadoop是Apache基金会开发的<font color ='red' >分布式系统基础架构</font></li>
<li>解决海量数据的<font color ='red' ><font color ='red' ></font></font></li>
<li>广义上指更广泛的Hadoop生态圈</li>
</ul>
<h2 id="2-2-Hadoop发展历史"><a href="#2-2-Hadoop发展历史" class="headerlink" title="2.2 Hadoop发展历史"></a>2.2 Hadoop发展历史</h2><ol>
<li>Lucene开发者Doug Cutting实现全文搜索</li>
<li>2001年Lucene成为Apache基金会子项目</li>
<li>Lucene处理海量数据暴露数据存储困难，检索速度慢的问题</li>
<li>微型版Nutch解决问题</li>
<li>Google <ul>
<li>GFS–&gt;HDFS </li>
<li>Map-Reduce–&gt;MR</li>
<li>BigTable–&gt;Hbase</li>
</ul>
</li>
<li>2003-2004年，Google公开了部分GFS和MapReduce思想的细节，以此为基础Doug Cutting等人用了2年业余时间实现了DFS和MapReduce机制，使Nutch性能飙升。</li>
<li>2005 年Hadoop 作为 Lucene的子项目 Nutch的一部分正式引入Apache基金会。</li>
<li>2006 年 3 月份，Map-Reduce和Nutch Distributed File System （NDFS）分别被纳入到 Hadoop 项目中，Hadoop就此正式诞生，标志着大数据时代来临。</li>
<li>名字来源于Doug Cutting儿子的玩具大象</li>
</ol>
<h2 id="2-3-Hadoop发行版本"><a href="#2-3-Hadoop发行版本" class="headerlink" title="2.3 Hadoop发行版本"></a>2.3 Hadoop发行版本</h2><p>Hadoop 三大发行版本：Apache、Cloudera、Hortonworks。</p>
<ul>
<li>Apache 版本最原始（最基础）的版本，对于入门学习最好。</li>
<li>Cloudera 内部集成了很多大数据框架，对应产品 CDH。</li>
<li>Hortonworks 文档较好，对应产品 HDP。</li>
<li>Hortonworks 现在已经被 Cloudera 公司收购，推出新的品牌 CDP。</li>
</ul>
<h2 id="2-4-Hadoop优势"><a href="#2-4-Hadoop优势" class="headerlink" title="2.4 Hadoop优势"></a>2.4 Hadoop优势</h2><ol>
<li>高可靠性：Hadoop维护多个数据副本，单节点出现故障不会导致数据丢失</li>
<li>高拓展性：在集群间分配任务数据，可以方便的拓展数以千计的节点</li>
<li>高效性：在MapReduce的思想下，Hadoop是并行工作，加快任务处理速度</li>
<li>高容错性：能将失败的任务重新分配</li>
</ol>
<h2 id="2-5-Hadoop组成"><a href="#2-5-Hadoop组成" class="headerlink" title="2.5 Hadoop组成"></a>2.5 Hadoop组成</h2><p>1.x 2.x组成<br><img src="https://i.loli.net/2021/11/11/KSWUMNeDrAEtXCQ.jpg"></p>
<h3 id="2-5-1-HDFS架构概述"><a href="#2-5-1-HDFS架构概述" class="headerlink" title="2.5.1 HDFS架构概述"></a>2.5.1 HDFS架构概述</h3><p>Hadoop Distributed File System，简称 HDFS，是一个分布式文件系统。负责海量数据的存储。</p>
<ol>
<li>NameNode（nn）：存储文件的元数据，如文件名，文件目录结构，文件属性（生成时间、副本数、文件权限），以及每个文件的块列表和块所在的DataNode等。</li>
<li>DataNode(dn)：在本地文件系统存储文件块数据，以及块数据的校验和。</li>
<li>Secondary NameNode(2nn)：每隔一段时间对NameNode元数据备份。<h3 id="2-5-2-YARN"><a href="#2-5-2-YARN" class="headerlink" title="2.5.2 YARN"></a>2.5.2 YARN</h3>Yet Another Resource Negotiator 简称 YARN ，另一种资源协调者，是 Hadoop 的资源管理器。<br><img src="https://i.loli.net/2021/11/11/gARKvL9V1uQfGCH.jpg"></li>
<li>ResourceManager（RM）：整个集群资源（内存、CPU等）的老大<ul>
<li>处理客户端请求</li>
<li>监控NodeManager</li>
<li>启动、监控ApplicationMaster</li>
<li>资源的分配与调度</li>
</ul>
</li>
<li>ApplicationMaster（AM）：单个任务运行的老大<ul>
<li>负责数据的切分</li>
<li>为用用程序申请资源并分配</li>
</ul>
</li>
<li>NodeManager（NM）：单个节点服务器资源老大</li>
<li>Container：容器，相当一台独立的服务器，里面封装了任务运行所需要的资源，如内存、CPU、磁盘、网络等。</li>
</ol>
<h3 id="2-5-3-MapReduce"><a href="#2-5-3-MapReduce" class="headerlink" title="2.5.3 MapReduce"></a>2.5.3 MapReduce</h3><p>MapReduce将计算过程分为两个阶段：Map和Reduce<br><img src="https://i.loli.net/2021/11/11/TprRumH1ih3sDk4.jpg"></p>
<ol>
<li>Map阶段并行处理输入数据</li>
<li>Reduce阶段对Map结果进行汇总</li>
</ol>
<h2 id="2-6-大数据技术生态体系"><a href="#2-6-大数据技术生态体系" class="headerlink" title="2.6 大数据技术生态体系"></a>2.6 大数据技术生态体系</h2><p><img src="https://i.loli.net/2021/11/11/73FdceRLbJAKfOp.jpg"></p>
<ul>
<li>Sqoop：Sqoop是一款开源的工具，主要用于在Hadoop、Hive与传统的数据库（MySql）间进行数据的传递，可以将一个关系型数据库（例如 ：MySQL，Oracle 等）中的数据导进到Hadoop的HDFS中，也可以将HDFS的数据导进到关系型数据库中。</li>
<li>Flume：Flume是一个高可用的，高可靠的，分布式的海量日志采集、聚合和传输的系统，Flume支持在日志系统中定制各类数据发送方，用于收集数据； </li>
<li>Kafka：Kafka是一种高吞吐量的分布式发布订阅消息系统； </li>
<li>Spark：Spark是当前最流行的开源大数据内存计算框架。可以基于Hadoop上存储的大数据进行计算。</li>
<li>Flink：Flink是当前最流行的开源大数据内存计算框架。用于实时计算的场景较多。</li>
<li>Oozie：Oozie是一个管理Hdoop作业（job）的工作流程调度管理系统。</li>
<li>Hbase：HBase是一个分布式的、面向列的开源数据库。HBase不同于一般的关系数据库，它是一个适合于非结构化数据存储的数据库。</li>
<li>Hive：Hive是基于Hadoop的一个数据仓库工具，可以将结构化的数据文件映射为一张数据库表，并提供简单的SQL查询功能，可以将SQL语句转换为MapReduce任务进行运行。 其优点是学习成本低，可以通过类SQL语句快速实现简单的MapReduce统计，不必开发专门的MapReduce应用，十分适合数据仓库的统计分析。</li>
<li>ZooKeeper：它是一个针对大型分布式系统的可靠协调系统，提供的功能包括：配置维护、名字服务、分布式同步、组服务等。</li>
</ul>
<h2 id="2-7-推荐系统框架图"><a href="#2-7-推荐系统框架图" class="headerlink" title="2.7 推荐系统框架图"></a>2.7 推荐系统框架图</h2><p><img src="https://i.loli.net/2021/11/11/M7ezhvJfyHpTPWb.jpg"></p>
<h1 id="三、-Hadoop运行环境搭建"><a href="#三、-Hadoop运行环境搭建" class="headerlink" title="三、 Hadoop运行环境搭建"></a>三、 Hadoop运行环境搭建</h1><h2 id="3-1-最小化安装Centos"><a href="#3-1-最小化安装Centos" class="headerlink" title="3.1 最小化安装Centos"></a>3.1 最小化安装Centos</h2><p>·······</p>
<h2 id="3-2-配置虚拟机模板"><a href="#3-2-配置虚拟机模板" class="headerlink" title="3.2 配置虚拟机模板"></a>3.2 配置虚拟机模板</h2><ul>
<li>固定IP,修改主机名</li>
<li>Yum安装工具  <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">yum install -y epel-release psmisc nc net-tools rsync vim lrzsz ntp libzstd openssl-static tree iotop git</span><br></pre></td></tr></table></figure></li>
<li>关闭防火墙  <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">systemctl stop firewalld</span><br><span class="line">systemctl disable firewalld</span><br></pre></td></tr></table></figure></li>
<li>创建用户  <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">adduser atguigu</span><br><span class="line">passwd atguigu</span><br></pre></td></tr></table></figure></li>
<li>添加sudo权限/etc/sudoers添加内容  <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">atguigu  ALL=(ALL)   ALL</span><br></pre></td></tr></table></figure></li>
</ul>
<h2 id="3-3-安装JDK"><a href="#3-3-安装JDK" class="headerlink" title="3.3 安装JDK"></a>3.3 安装JDK</h2><ol>
<li>上传jdk-8u212-linux-x64.tar.gz到/opt/software下</li>
<li>解压JDK到/opt/module目录下<code>tar -zxvf jdk-8u212-linux-x64.tar.gz -C /opt/module/</code></li>
<li>配置JDK环境变量新建/etc/profile.d/set_env.sh文件 添加如下内容 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 ~]$ cat /etc/profile.d/set_env.sh</span><br><span class="line">#JAVA_HOME</span><br><span class="line">export JAVA_HOME=/opt/module/jdk1.8.0_212</span><br><span class="line">export PATH=$PATH:$JAVA_HOME/bin</span><br></pre></td></tr></table></figure></li>
<li>测试JDK是否安装成功 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@dw-server-001 <span class="built_in">log</span>]$ java -version</span><br><span class="line">java version <span class="string">&quot;1.8.0_212&quot;</span></span><br><span class="line">Java(TM) SE Runtime Environment (build 1.8.0_212-b10)</span><br><span class="line">Java HotSpot(TM) 64-Bit Server VM (build 25.212-b10, mixed mode)</span><br><span class="line">[atguigu@dw-server-001 <span class="built_in">log</span>]$</span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="3-4-安装Hadoop"><a href="#3-4-安装Hadoop" class="headerlink" title="3.4 安装Hadoop"></a>3.4 安装Hadoop</h2><ol>
<li>上传 hadoop-3.1.3.tar.gz到/opt/software下</li>
<li>解压到/opt/module下面<code>tar -zxvf hadoop-3.1.3.tar.gz -C /opt/module/</code></li>
<li>配置环境变量/etc/profile.d/set_env.sh文件 添加如下内容 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#HADOOP_HOME</span><br><span class="line">export HADOOP_HOME=/opt/module/hadoop-3.1.3</span><br><span class="line">export PATH=$PATH:$HADOOP_HOME/bin</span><br><span class="line">export PATH=$PATH:$HADOOP_HOME/sbin</span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="3-5-Hadoop目录结构"><a href="#3-5-Hadoop目录结构" class="headerlink" title="3.5 Hadoop目录结构"></a>3.5 Hadoop目录结构</h2><ol>
<li>查看Hadoop目录结构 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 hadoop-3.1.3]$ ll</span><br><span class="line">总用量 52</span><br><span class="line">drwxr-xr-x. 2 atguigu atguigu  4096 5月  22 2017 bin</span><br><span class="line">drwxr-xr-x. 3 atguigu atguigu  4096 5月  22 2017 etc</span><br><span class="line">drwxr-xr-x. 2 atguigu atguigu  4096 5月  22 2017 include</span><br><span class="line">drwxr-xr-x. 3 atguigu atguigu  4096 5月  22 2017 lib</span><br><span class="line">drwxr-xr-x. 2 atguigu atguigu  4096 5月  22 2017 libexec</span><br><span class="line">-rw-r--r--. 1 atguigu atguigu 15429 5月  22 2017 LICENSE.txt</span><br><span class="line">-rw-r--r--. 1 atguigu atguigu   101 5月  22 2017 NOTICE.txt</span><br><span class="line">-rw-r--r--. 1 atguigu atguigu  1366 5月  22 2017 README.txt</span><br><span class="line">drwxr-xr-x. 2 atguigu atguigu  4096 5月  22 2017 sbin</span><br><span class="line">drwxr-xr-x. 4 atguigu atguigu  4096 5月  22 2017 share</span><br></pre></td></tr></table></figure></li>
<li>重要目录<ul>
<li>bin目录：存放对Hadoop相关服务（HDFS,YARN）进行操作的脚本</li>
<li>etc目录：Hadoop的配置文件目录，存放Hadoop的配置文件</li>
<li>lib目录：存放Hadoop的本地库（对数据进行压缩解压缩功能）</li>
<li>sbin目录：存放启动或停止Hadoop相关服务的脚本</li>
<li>share目录：存放Hadoop的依赖jar包、文档、和官方案例</li>
</ul>
</li>
</ol>
<h1 id="四、Hadoop分布式部署"><a href="#四、Hadoop分布式部署" class="headerlink" title="四、Hadoop分布式部署"></a>四、Hadoop分布式部署</h1><ol>
<li>Hadoop的运行模式介绍<ul>
<li>本地模式: hadoop默认安装后启动就是本地模式，就是将来的数据存在Linux本地，并且运行MR程序的时候也是在本地机器上运行</li>
<li>伪分布式模式: 伪分布式其实就只在一台机器上启动HDFS集群，启动YARN集群，并且数据存在HDFS集群上，以及运行MR程序也是在YARN上运行，计算后的结果也是输出到HDFS上。本质上就是利用一台服务器中多个java进程去模拟多个服务</li>
<li>完全分布式: 完全分布式其实就是多台机器上分别启动HDFS集群，启动YARN集群，并且数据存在HDFS集群上的以及运行MR程序也是在YARN上运行，计算后的结果也是输出到HDFS上。</li>
</ul>
</li>
<li>本地运行模式入门（官方wordcount）,Hadoop的默认的运行模式<ul>
<li>需求：统计以文件中单词出现的次数</li>
<li>实现步骤：<ul>
<li>在当前hadoop的安装目录创建一个文件</li>
<li>运行hadoop官方提供的wordcount案例   <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount wcinput wcoutput </span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>
</li>
</ol>
<h2 id="4-1-准备虚拟机"><a href="#4-1-准备虚拟机" class="headerlink" title="4.1 准备虚拟机"></a>4.1 准备虚拟机</h2><ol>
<li>静态IP <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">BOOTPROTO=static</span><br><span class="line">IPADDR=192.168.1.102</span><br><span class="line">GATEWAY=192.168.1.2</span><br><span class="line">DNS1=192.168.1.2</span><br></pre></td></tr></table></figure></li>
<li>关闭防火墙 <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">systemctl stop firewalld</span><br><span class="line">systemctl disable firewalld</span><br></pre></td></tr></table></figure></li>
<li>设置hostname <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hostnamectl set-hostname hadoop001</span><br></pre></td></tr></table></figure></li>
<li>修改/etc/hosts 添加解析内容 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">192.168.2.6 hadoop001</span><br><span class="line">192.168.2.7 hadoop002</span><br><span class="line">192.168.2.8 hadoop003</span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="4-2-准备分发脚本xsync"><a href="#4-2-准备分发脚本xsync" class="headerlink" title="4.2 准备分发脚本xsync"></a>4.2 准备分发脚本xsync</h2><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta">#</span><span class="bash">1. 判断参数个数</span></span><br><span class="line">if [ $# -lt 1 ]</span><br><span class="line">then</span><br><span class="line">  echo Not Enough Arguement!</span><br><span class="line">  exit;</span><br><span class="line">fi</span><br><span class="line"><span class="meta">#</span><span class="bash">2. 遍历集群所有机器</span></span><br><span class="line">for host in hadoop001 hadoop103 hadoop104</span><br><span class="line">do</span><br><span class="line">  echo ====================  $host  ====================</span><br><span class="line"><span class="meta">  #</span><span class="bash">3. 遍历所有目录，挨个发送</span></span><br><span class="line">  for file in $@</span><br><span class="line">  do</span><br><span class="line">    #4. 判断文件是否存在</span><br><span class="line">    if [ -e $file ]</span><br><span class="line">    then</span><br><span class="line">      #5. 获取父目录</span><br><span class="line">      pdir=$(cd -P $(dirname $file); pwd)</span><br><span class="line">      #6. 获取当前文件的名称</span><br><span class="line">      fname=$(basename $file)</span><br><span class="line">      ssh $host &quot;mkdir -p $pdir&quot;</span><br><span class="line">      rsync -av $pdir/$fname $host:$pdir</span><br><span class="line">    else</span><br><span class="line">      echo $file does not exists!</span><br><span class="line">    fi</span><br><span class="line">  done</span><br><span class="line">done</span><br></pre></td></tr></table></figure>
<h2 id="4-3-免密登录"><a href="#4-3-免密登录" class="headerlink" title="4.3 免密登录"></a>4.3 免密登录</h2><ol>
<li><p>生成公私钥<code>ssh-keygen</code></p>
</li>
<li><p>拷贝公钥到目标服务器</p>
 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ssh-copy-id hadoop001</span><br><span class="line">ssh-copy-id hadoop103</span><br><span class="line">ssh-copy-id hadoop104</span><br></pre></td></tr></table></figure></li>
<li><p>.ssh文件夹下（~/.ssh）的文件功能解释</p>
<table>
<thead>
<tr>
<th>文件</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>known_hosts</td>
<td>记录ssh访问过计算机的公钥(public key)</td>
</tr>
<tr>
<td>id_rsa</td>
<td>生成的私钥</td>
</tr>
<tr>
<td>id_rsa.pub</td>
<td>生成的公钥</td>
</tr>
<tr>
<td>authorized_keys</td>
<td>存放授权过的无密登录服务器公钥</td>
</tr>
</tbody></table>
</li>
</ol>
<h2 id="4-4-集群配置"><a href="#4-4-集群配置" class="headerlink" title="4.4 集群配置"></a>4.4 集群配置</h2><ol>
<li><p>集群部署规划</p>
<ul>
<li>注意：NameNode和SecondaryNameNode不要安装在同一台服务器</li>
<li>注意：ResourceManager也很消耗内存，不要和NameNode、SecondaryNameNode配置在同一台机器上。<table>
<thead>
<tr>
<th align="left">服务器</th>
<th align="left">HDFS</th>
<th align="left">Yarn</th>
</tr>
</thead>
<tbody><tr>
<td align="left">hadoop001</td>
<td align="left">NameNode、DataNode</td>
<td align="left">NodeManager</td>
</tr>
<tr>
<td align="left">hadoop103</td>
<td align="left">DataNode</td>
<td align="left">ResourceManager、NodeManager</td>
</tr>
<tr>
<td align="left">hadoop104</td>
<td align="left">SecondaryNameNode、DataNode</td>
<td align="left">NodeManager</td>
</tr>
</tbody></table>
</li>
</ul>
</li>
<li><p>配置文件说明<br> Hadoop配置文件分两类：默认配置文件和自定义配置文件，只有用户想修改某一默认配置值时，才需要修改自定义配置文件，更改相应属性值。</p>
<ul>
<li>默认配置文件：<table>
<thead>
<tr>
<th align="left">配置文件</th>
<th align="left">默认文件存放在Hadoop的jar包中的位置</th>
</tr>
</thead>
<tbody><tr>
<td align="left">core-default.xml</td>
<td align="left">hadoop-common-3.1.3.jar/ core-default.xml</td>
</tr>
<tr>
<td align="left">hdfs-default.xml</td>
<td align="left">hadoop-hdfs-3.1.3.jar/ hdfs-default.xml</td>
</tr>
<tr>
<td align="left">yarn-default.xml</td>
<td align="left">hadoop-yarn-common-3.1.3.jar/yarn-default.xml</td>
</tr>
<tr>
<td align="left">mapred-default.xml</td>
<td align="left">hadoop-mapreduce-client-core-3.1.3.jar/ mapred-default.xml</td>
</tr>
</tbody></table>
</li>
<li>自定义配置文件：<br>  core-site.xml、hdfs-site.xml、yarn-site.xml、mapred-site.xml四个配置文件存放在$HADOOP_HOME/etc/hadoop这个路径下，可以根据项目需求重新进行修改配置。</li>
<li>常用端口号说明<table>
<thead>
<tr>
<th align="left">Daemon</th>
<th align="left">App</th>
<th align="left">Hadoop2</th>
<th align="left">Hadoop3</th>
</tr>
</thead>
<tbody><tr>
<td align="left">NameNode Port</td>
<td align="left">Hadoop HDFS NameNode</td>
<td align="left">8020/9000</td>
<td align="left">9820</td>
</tr>
<tr>
<td align="left"></td>
<td align="left">Hadoop HDFS NameNode HTTP UI</td>
<td align="left">50070</td>
<td align="left">9870</td>
</tr>
<tr>
<td align="left">SecondaryNameNode Port</td>
<td align="left">Secondary NameNode</td>
<td align="left">50091</td>
<td align="left">9869</td>
</tr>
<tr>
<td align="left"></td>
<td align="left">Secondary NameNode HTTP UI</td>
<td align="left">50090</td>
<td align="left">9868</td>
</tr>
<tr>
<td align="left">DataNode Port</td>
<td align="left">Hadoop HDFS DataNode IPC</td>
<td align="left">50020</td>
<td align="left">9867</td>
</tr>
<tr>
<td align="left"></td>
<td align="left">Hadoop HDFS DataNod</td>
<td align="left">50010</td>
<td align="left">9866</td>
</tr>
<tr>
<td align="left"></td>
<td align="left">Hadoop HDFS DataNode HTTP UI</td>
<td align="left">50075</td>
<td align="left">9864</td>
</tr>
</tbody></table>
</li>
</ul>
</li>
<li><p><strong>配置集群<code>$HADOOP_HOME/etc/hadoop</code></strong></p>
<ul>
<li>核心配置文件core-site.xml  <figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;</span></span><br><span class="line"><span class="meta">&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 指定NameNode的地址 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://hadoop001:9820<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 指定hadoop数据的存储目录 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/module/hadoop-3.1.3/data<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- 配置HDFS网页登录使用的静态用户为atguigu --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.http.staticuser.user<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>atguigu<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- 配置该atguigu(superUser)允许通过代理访问的主机节点 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.atguigu.hosts<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 配置该atguigu(superUser)允许通过代理用户所属组 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.atguigu.groups<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 配置该atguigu(superUser)允许通过代理的用户--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.atguigu.users<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure></li>
<li>HDFS配置文件hdfs-site.xml  <figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;</span></span><br><span class="line"><span class="meta">&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- nn web端访问地址--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.http-address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop001:9870<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 2nn web端访问地址--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.secondary.http-address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop104:9868<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure></li>
<li>YARN配置文件yarn-site.xml  <figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;</span></span><br><span class="line"><span class="meta">&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 指定MR走shuffle --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 指定ResourceManager的地址--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop103<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 环境变量的继承 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.env-whitelist<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- yarn容器允许分配的最大最小内存 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.minimum-allocation-mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>512<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.maximum-allocation-mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>4096<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- yarn容器允许管理的物理内存大小 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.resource.memory-mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>4096<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 关闭yarn对物理内存和虚拟内存的限制检查 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.pmem-check-enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.vmem-check-enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure></li>
<li>MapReduce配置文件mapred-site.xml  <figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;</span></span><br><span class="line"><span class="meta">&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 指定MapReduce程序运行在Yarn上 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure></li>
<li>集群配置workers  <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hadoop001</span><br><span class="line">hadoop002</span><br><span class="line">hadoop003</span><br></pre></td></tr></table></figure></li>
<li>同步配置  <figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">xsync /opt/module/hadoop-3.1.3/etc</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ol>
<h2 id="4-5-启动集群"><a href="#4-5-启动集群" class="headerlink" title="4.5 启动集群"></a>4.5 启动集群</h2><ol>
<li>如果集群是第一次启动，需要在hadoop001节点格式化NameNode（注意格式化NameNode，会产生新的集群id，导致NameNode和DataNode的集群id不一致，集群找不到已往数据。如果集群在运行过程中报错，需要重新格式化NameNode的话，一定要先停止namenode和datanode进程，并且要删除所有机器的data和logs目录，然后再进行格式化。） <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 ~]$ hdfs namenode -format</span><br></pre></td></tr></table></figure></li>
<li>启动Hadoop集群<ol>
<li>单点 启动/停止集群<ul>
<li>启动HDFS集群<ul>
<li>hadoop001启动namenode <pre><code><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hdfs --daemon start namenode</span><br></pre></td></tr></table></figure>
</code></pre>
<ul>
<li>hadoop001 hadoop002 hadoop003 分别启动 datanode  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">    hdfs --daemon start datanode</span><br><span class="line">    ```		  </span><br><span class="line">- hadoop003 启动secondarynamenode</span><br><span class="line">    ```bash</span><br><span class="line">    hdfs --daemon start secondarynamenode</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>
</li>
<li>启动YARN集群<ul>
<li>hadoop002启动resourcemanager <pre><code><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ssh dw-server-002 yarn --daemon start resourcemanager</span><br></pre></td></tr></table></figure>
</code></pre>
<ul>
<li>hadoop001 hadoop002 hadoop003 分别启动 nodemanager  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">yarn --daemon start nodemanager</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>
</li>
<li>停止HDFS集群<ul>
<li>hadoop001停止namenode <pre><code><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hdfs --daemon stop namenode</span><br></pre></td></tr></table></figure>
</code></pre>
</li>
<li>hadoop001 hadoop002 hadoop003 分别停止 datanode<pre><code><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hdfs --daemon stop datanode</span><br></pre></td></tr></table></figure>
</code></pre>
</li>
<li>hadoop003 停止secondarynamenode<pre><code><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hdfs --daemon stop secondarynamenode</span><br></pre></td></tr></table></figure>
</code></pre>
</li>
</ul>
</li>
<li>停止YARN集群<ul>
<li>hadoop001停止resourcemanager   <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">yarn --daemon stop resourcemanager</span><br></pre></td></tr></table></figure></li>
<li>hadoop001 hadoop002 hadoop003 分别停止 nodemanager  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">yarn --daemon stop nodemanager</span><br></pre></td></tr></table></figure>
<blockquote>
<p>格式化HDFS集群的 NameNode的注意事项</p>
<ol>
<li>集群只有首次搭建后需要对NameNode进行格式化操作</li>
<li>如果集群在后期使用过程需要重新格式化，一定切记删除所有机器hadoop安装目录下的 data logs 目录。</li>
</ol>
</blockquote>
</li>
</ul>
</li>
</ul>
</li>
<li>集群启动/停止的操作<ol>
<li>检查/opt/module/hadoop-3.1.3/etc/hadoop/workers文件</li>
<li>同步配置xsync /opt/module/hadoop-3.1.3/etc/hadoop/workers</li>
<li>免密登陆</li>
<li>利用hadoop提供的 群启/群停 脚本完成集群操作<ul>
<li>群启： start-dfs.sh   start-yarn.sh</li>
<li>群停： stop-dfs.sh    stop-yarn.sh<blockquote>
<p>注意事项：</p>
<ul>
<li>启动hdfs的时候要在NameNode所在的机器执行脚本</li>
<li>启动yarn的时候要在resourcemanager所在的机器执行脚本</li>
</ul>
</blockquote>
</li>
</ul>
</li>
</ol>
</li>
<li>检查集群启动情况<ol>
<li>Web端查看HDFS的NameNode<ul>
<li>浏览器中输入：<a href="http://hadoop001:9870/">http://hadoop001:9870</a></li>
<li>查看HDFS上存储的数据信息</li>
</ul>
</li>
<li>Web端查看YARN的ResourceManager<ul>
<li>浏览器中输入：<a href="http://hadoop002:8088/">http://hadoop002:8088</a></li>
<li>查看YARN上运行的Job信息</li>
</ul>
</li>
</ol>
</li>
<li>集群基本测试<ol>
<li>上传文件到集群<ul>
<li>上传小文件  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 ~]$ hadoop fs -mkdir /input</span><br><span class="line">[atguigu@hadoop001 ~]$ hadoop fs -put <span class="variable">$HADOOP_HOME</span>/wcinput/word.txt /input</span><br></pre></td></tr></table></figure></li>
<li>上传大文件  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 ~]$ hadoop fs -put  /opt/software/jdk-8u212-linux-x64.tar.gz  /</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>上传文件后查看文件存放位置<ul>
<li>查看HDFS文件在磁盘上的存储路径  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 subdir0]$ <span class="built_in">pwd</span></span><br><span class="line">/opt/module/hadoop-3.1.3/data/dfs/data/current/BP-938951106-192.168.10.107-1495462844069/current/finalized/subdir0/subdir0</span><br></pre></td></tr></table></figure></li>
<li>查看HDFS在磁盘存储文件内容  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 subdir0]$ cat blk_1073741825</span><br><span class="line">hadoop yarn</span><br><span class="line">hadoop mapreduce </span><br><span class="line">atguigu</span><br><span class="line">atguigu     </span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>下载 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop104 software]$ hadoop fs -get /jdk-8u212-linux-x64.tar.gz ./</span><br></pre></td></tr></table></figure></li>
<li>执行wordcount程序 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 hadoop-3.1.3]$ hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount /input /output</span><br></pre></td></tr></table></figure></li>
</ol>
</li>
</ol>
</li>
</ol>
<h2 id="4-6-配置历史服务器"><a href="#4-6-配置历史服务器" class="headerlink" title="4.6 配置历史服务器"></a>4.6 配置历史服务器</h2><ul>
<li>概念：历史服务器是针对MR程序执行的历史记录</li>
<li>配置步骤：<ul>
<li>修改 mapred-site.xml 文件  <figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 历史服务器端地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop001:10020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 历史服务器web端地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.webapp.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop001:19888<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li>
<li>分发 mapred-site.xml</li>
<li>把历史服务器的 启停 操作配置到 自定义的群启群停脚本hadoop_cluster.sh  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"><span class="keyword">if</span> [ <span class="variable">$#</span> -lt 1 ]</span><br><span class="line"><span class="keyword">then</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="string">&quot;No Args Input...&quot;</span></span><br><span class="line">    <span class="built_in">exit</span> ;</span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"><span class="keyword">case</span> <span class="variable">$1</span> <span class="keyword">in</span></span><br><span class="line"><span class="string">&quot;start&quot;</span>)</span><br><span class="line">        <span class="built_in">echo</span> <span class="string">&quot; =================== 启动 hadoop集群 ===================&quot;</span></span><br><span class="line"></span><br><span class="line">        <span class="built_in">echo</span> <span class="string">&quot; --------------- 启动 hdfs ---------------&quot;</span></span><br><span class="line">        ssh hadoop001 <span class="string">&quot;/opt/module/hadoop-3.1.3/sbin/start-dfs.sh&quot;</span></span><br><span class="line">        <span class="built_in">echo</span> <span class="string">&quot; --------------- 启动 yarn ---------------&quot;</span></span><br><span class="line">        ssh hadoop002 <span class="string">&quot;/opt/module/hadoop-3.1.3/sbin/start-yarn.sh&quot;</span></span><br><span class="line">        <span class="built_in">echo</span> <span class="string">&quot; --------------- 启动 historyserver ---------------&quot;</span></span><br><span class="line">        ssh hadoop001 <span class="string">&quot;/opt/module/hadoop-3.1.3/bin/mapred --daemon start historyserver&quot;</span></span><br><span class="line">;;</span><br><span class="line"><span class="string">&quot;stop&quot;</span>)</span><br><span class="line">        <span class="built_in">echo</span> <span class="string">&quot; =================== 关闭 hadoop集群 ===================&quot;</span></span><br><span class="line"></span><br><span class="line">        <span class="built_in">echo</span> <span class="string">&quot; --------------- 关闭 historyserver ---------------&quot;</span></span><br><span class="line">        ssh hadoop001 <span class="string">&quot;/opt/module/hadoop-3.1.3/bin/mapred --daemon stop historyserver&quot;</span></span><br><span class="line">        <span class="built_in">echo</span> <span class="string">&quot; --------------- 关闭 yarn ---------------&quot;</span></span><br><span class="line">        ssh hadoop002 <span class="string">&quot;/opt/module/hadoop-3.1.3/sbin/stop-yarn.sh&quot;</span></span><br><span class="line">        <span class="built_in">echo</span> <span class="string">&quot; --------------- 关闭 hdfs ---------------&quot;</span></span><br><span class="line">        ssh hadoop001 <span class="string">&quot;/opt/module/hadoop-3.1.3/sbin/stop-dfs.sh&quot;</span></span><br><span class="line">;;</span><br><span class="line">*)</span><br><span class="line">    <span class="built_in">echo</span> <span class="string">&quot;Input Args Error...&quot;</span></span><br><span class="line">;;</span><br><span class="line"><span class="keyword">esac</span></span><br></pre></td></tr></table></figure></li>
<li>启动历史服务器  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 hadoop]$ mapred --daemon start historyserver</span><br></pre></td></tr></table></figure></li>
<li>查看历史服务器JobHistory是否启动<ul>
<li><a href="http://hadoop001:19888/jobhistory">http://hadoop001:19888/jobhistory</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="4-7-配置日志的聚集"><a href="#4-7-配置日志的聚集" class="headerlink" title="4.7 配置日志的聚集"></a>4.7 配置日志的聚集</h2><ul>
<li>概念：日志是针对 MR 程序运行是所产生的的日志</li>
<li>目的：方便后期分析问题 有更好的 执行过过程的依据</li>
<li>开启日志聚集的功能<ul>
<li>修改配置文件  yarn-site.xml  <figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 开启日志聚集功能 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log-aggregation-enable<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 设置日志聚集服务器地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log.server.url<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>http://hadoop001:19888/jobhistory/logs<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 设置日志保留时间为7天 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log-aggregation.retain-seconds<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>604800<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
<ul>
<li>分发 yarn-site.xml</li>
<li>重启集群（historyserver  、 yarn集群）</li>
<li>日志聚集后，会在HDFS的 /tmp 目录下存储</li>
<li>查看日志<ul>
<li><a href="http://hadoop001:19888/jobhistory">http://hadoop001:19888/jobhistory</a>         </li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title>HDFS</title>
    <url>/2021/11/07/HDFS/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="HDFS"><a href="#HDFS" class="headerlink" title="HDFS"></a>HDFS</h1><h1 id="一、HDFS概述"><a href="#一、HDFS概述" class="headerlink" title="一、HDFS概述"></a>一、HDFS概述</h1><h2 id="1-1-背景及定义"><a href="#1-1-背景及定义" class="headerlink" title="1.1 背景及定义"></a>1.1 背景及定义</h2><h3 id="1-1-1-背景"><a href="#1-1-1-背景" class="headerlink" title="1.1.1 背景"></a>1.1.1 背景</h3><ol>
<li>海量数据无法单台服务器存储</li>
<li>需要管理多台机器上的文件</li>
</ol>
<h3 id="1-1-2-定义"><a href="#1-1-2-定义" class="headerlink" title="1.1.2 定义"></a>1.1.2 定义</h3><ol>
<li>HDFS: Hadoop Distributed File System 分布式文件系统，多台服务器联合实现功能，集群中各自承担不同角色</li>
<li>适合一次写入，多次读出的场景，<font color ='red' >支持文件追加，不支持文件修改</font>。适合做数据分析，不适合做网盘应用</li>
</ol>
<hr>
<h2 id="1-2-优缺点"><a href="#1-2-优缺点" class="headerlink" title="1.2 优缺点"></a>1.2 优缺点</h2><ul>
<li><p>优点</p>
<ol>
<li><p>高容错性</p>
<ul>
<li><p>数据自动保存多个副本，通过增加副本提高容错性<br>  <img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16338313699180.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
</li>
<li><p>某个副本丢失以后，可以自动回复<br>  <img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16338314377557.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
</li>
</ul>
</li>
<li><p>适合处理大数据</p>
<ul>
<li>数据规模：能处理GB,TB,PB级别的数据</li>
<li>文件规模：能处理百万千万规模以上的文件数量</li>
</ul>
</li>
<li><p>可构建在廉价机器上，通过多副本机制提高可靠性</p>
</li>
</ol>
</li>
<li><p>缺点</p>
<ol>
<li>不适合低延迟数据访问，比如毫秒级的数据存储 </li>
<li>无法高效的对大量小文件进行存储<ul>
<li>大量小文件会占用NameNode大量内存保存元数据信息（目录和块信息）</li>
<li>小文件存储的寻址时间超过读取时间，不符合HDFS设计目标</li>
</ul>
</li>
<li>不支持并发写入，文件随机修改<ul>
<li>同一个文件只能单线程写，不支持多线程并发写</li>
<li>仅支持数据append，不支持文件随机修改</li>
</ul>
</li>
</ol>
</li>
</ul>
<hr>
<h2 id="1-3-组成架构"><a href="#1-3-组成架构" class="headerlink" title="1.3 组成架构"></a>1.3 组成架构</h2><p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16338314790454.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
<h3 id="1-3-1-NameNode（NN）"><a href="#1-3-1-NameNode（NN）" class="headerlink" title="1.3.1 NameNode（NN）"></a>1.3.1 NameNode（NN）</h3><ol>
<li>Master，它是一个主管、管理者。负责：</li>
<li>管理HDFS的名称空间</li>
<li>配置副本策略</li>
<li>管理数据块（Block）映射信息</li>
<li>处理客户端读写请求</li>
</ol>
<h3 id="1-3-2-DataNode（DN）"><a href="#1-3-2-DataNode（DN）" class="headerlink" title="1.3.2 DataNode（DN）"></a>1.3.2 DataNode（DN）</h3><ol>
<li>Slave。NameNode下达命令，DataNode执行实际的操作</li>
<li>存储实际的数据块</li>
<li>执行数据块的读/写操作</li>
</ol>
<h3 id="1-3-3-Client客户端"><a href="#1-3-3-Client客户端" class="headerlink" title="1.3.3 Client客户端"></a>1.3.3 Client客户端</h3><ol>
<li>文件切分。文件上传HDFS的时候，Client将文件切分成一个一个的Block，然后进行上传</li>
<li>与NameNode交互，获取文件的位置信息</li>
<li>与DataNode交互，读取或者写入数据；</li>
<li>Client提供一些命令来管理HDFS，比如NameNode格式化</li>
<li>Client可以通过一些命令来访问HDFS，比如对HDFS增删查改操作</li>
</ol>
<h3 id="1-3-4-Secondary-NameNode-2NN"><a href="#1-3-4-Secondary-NameNode-2NN" class="headerlink" title="1.3.4 Secondary NameNode(2NN)"></a>1.3.4 Secondary NameNode(2NN)</h3><ol>
<li>并非NameNode的热备。当NameNode挂掉的时候，它并不能马上替换NameNode并提供服务。</li>
<li>辅助NameNode，分担其工作量，比如定期合并Fsimage和Edits，并推送给NameNode</li>
<li>在紧急情况下，可辅助恢复NameNode。</li>
</ol>
<hr>
<h2 id="1-4-HDFS文件块大小"><a href="#1-4-HDFS文件块大小" class="headerlink" title="1.4 HDFS文件块大小"></a>1.4 HDFS文件块大小</h2><p>HDFS中的文件在物理上是分块存储（Block），块的大小可以通过配置参数(dfs.blocksize）来规定，默认大小在Hadoop2.x/3.x版本中是128M，1.x版本中是64M。<br><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16338322175049.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"><br><em>思考：为什么块的大小不能设置太小，也不能设置太大？</em></p>
<blockquote>
<ol>
<li>HDFS的块设置太小，会增加寻址时间，程序一直在找块的开始位置；</li>
<li>如果块设置的太大，从磁盘传输数据的时间会明显大于定位这个块开始位置所需的时间。导致程序在处理这块数据时，会非常慢。</li>
</ol>
<p>总结：HDFS块的大小设置主要取决于磁盘传输速率。</p>
</blockquote>
<hr>
<h1 id="二、Shell操作HDFS"><a href="#二、Shell操作HDFS" class="headerlink" title="二、Shell操作HDFS"></a>二、Shell操作HDFS</h1><h2 id="2-1-基本语法"><a href="#2-1-基本语法" class="headerlink" title="2.1 基本语法"></a>2.1 基本语法</h2><p><code>hadoop fs [genericOptions] [commandOptions]</code></p>
<hr>
<h2 id="2-2-命令参考"><a href="#2-2-命令参考" class="headerlink" title="2.2 命令参考"></a>2.2 命令参考</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 ~]$ hadoop fs</span><br><span class="line">Usage: hadoop fs [generic options]</span><br><span class="line">	[-appendToFile &lt;localsrc&gt; ... &lt;dst&gt;]</span><br><span class="line">	[-cat [-ignoreCrc] &lt;src&gt; ...]</span><br><span class="line">	[-checksum &lt;src&gt; ...]</span><br><span class="line">	[-chgrp [-R] GROUP PATH...]</span><br><span class="line">	[-chmod [-R] &lt;MODE[,MODE]... | OCTALMODE&gt; PATH...]</span><br><span class="line">	[-chown [-R] [OWNER][:[GROUP]] PATH...]</span><br><span class="line">	[-copyFromLocal [-f] [-p] [-l] [-d] [-t &lt;thread count&gt;] &lt;localsrc&gt; ... &lt;dst&gt;]</span><br><span class="line">	[-copyToLocal [-f] [-p] [-ignoreCrc] [-crc] &lt;src&gt; ... &lt;localdst&gt;]</span><br><span class="line">	[-count [-q] [-h] [-v] [-t [&lt;storage <span class="built_in">type</span>&gt;]] [-u] [-x] [-e] &lt;path&gt; ...]</span><br><span class="line">	[-cp [-f] [-p | -p[topax]] [-d] &lt;src&gt; ... &lt;dst&gt;]</span><br><span class="line">	[-createSnapshot &lt;snapshotDir&gt; [&lt;snapshotName&gt;]]</span><br><span class="line">	[-deleteSnapshot &lt;snapshotDir&gt; &lt;snapshotName&gt;]</span><br><span class="line">	[-df [-h] [&lt;path&gt; ...]]</span><br><span class="line">	[-du [-s] [-h] [-v] [-x] &lt;path&gt; ...]</span><br><span class="line">	[-expunge]</span><br><span class="line">	[-find &lt;path&gt; ... &lt;expression&gt; ...]</span><br><span class="line">	[-get [-f] [-p] [-ignoreCrc] [-crc] &lt;src&gt; ... &lt;localdst&gt;]</span><br><span class="line">	[-getfacl [-R] &lt;path&gt;]</span><br><span class="line">	[-getfattr [-R] &#123;-n name | -d&#125; [-e en] &lt;path&gt;]</span><br><span class="line">	[-getmerge [-nl] [-skip-empty-file] &lt;src&gt; &lt;localdst&gt;]</span><br><span class="line">	[-head &lt;file&gt;]</span><br><span class="line">	[-<span class="built_in">help</span> [cmd ...]]</span><br><span class="line">	[-ls [-C] [-d] [-h] [-q] [-R] [-t] [-S] [-r] [-u] [-e] [&lt;path&gt; ...]]</span><br><span class="line">	[-mkdir [-p] &lt;path&gt; ...]</span><br><span class="line">	[-moveFromLocal &lt;localsrc&gt; ... &lt;dst&gt;]</span><br><span class="line">	[-moveToLocal &lt;src&gt; &lt;localdst&gt;]</span><br><span class="line">	[-mv &lt;src&gt; ... &lt;dst&gt;]</span><br><span class="line">	[-put [-f] [-p] [-l] [-d] &lt;localsrc&gt; ... &lt;dst&gt;]</span><br><span class="line">	[-renameSnapshot &lt;snapshotDir&gt; &lt;oldName&gt; &lt;newName&gt;]</span><br><span class="line">	[-rm [-f] [-r|-R] [-skipTrash] [-safely] &lt;src&gt; ...]</span><br><span class="line">	[-rmdir [--ignore-fail-on-non-empty] &lt;dir&gt; ...]</span><br><span class="line">	[-setfacl [-R] [&#123;-b|-k&#125; &#123;-m|-x &lt;acl_spec&gt;&#125; &lt;path&gt;]|[--<span class="built_in">set</span> &lt;acl_spec&gt; &lt;path&gt;]]</span><br><span class="line">	[-setfattr &#123;-n name [-v value] | -x name&#125; &lt;path&gt;]</span><br><span class="line">	[-setrep [-R] [-w] &lt;rep&gt; &lt;path&gt; ...]</span><br><span class="line">	[-<span class="built_in">stat</span> [format] &lt;path&gt; ...]</span><br><span class="line">	[-tail [-f] [-s &lt;sleep interval&gt;] &lt;file&gt;]</span><br><span class="line">	[-<span class="built_in">test</span> -[defsz] &lt;path&gt;]</span><br><span class="line">	[-text [-ignoreCrc] &lt;src&gt; ...]</span><br><span class="line">	[-touch [-a] [-m] [-t TIMESTAMP ] [-c] &lt;path&gt; ...]</span><br><span class="line">	[-touchz &lt;path&gt; ...]</span><br><span class="line">	[-truncate [-w] &lt;length&gt; &lt;path&gt; ...]</span><br><span class="line">	[-usage [cmd ...]]</span><br><span class="line">    </span><br><span class="line">Generic options supported are:</span><br><span class="line">-conf &lt;configuration file&gt;        specify an application configuration file</span><br><span class="line">-D &lt;property=value&gt;               define a value <span class="keyword">for</span> a given property</span><br><span class="line">-fs &lt;file:///|hdfs://namenode:port&gt; specify default filesystem URL to use, overrides <span class="string">&#x27;fs.defaultFS&#x27;</span> property from configurations.</span><br><span class="line">-jt &lt;<span class="built_in">local</span>|resourcemanager:port&gt;  specify a ResourceManager</span><br><span class="line">-files &lt;file1,...&gt;                specify a comma-separated list of files to be copied to the map reduce cluster</span><br><span class="line">-libjars &lt;jar1,...&gt;               specify a comma-separated list of jar files to be included <span class="keyword">in</span> the classpath</span><br><span class="line">-archives &lt;archive1,...&gt;          specify a comma-separated list of archives to be unarchived on the compute machines</span><br><span class="line">    </span><br><span class="line">The general <span class="built_in">command</span> line syntax is:</span><br><span class="line"><span class="built_in">command</span> [genericOptions] [commandOptions]</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="2-3-常用命令"><a href="#2-3-常用命令" class="headerlink" title="2.3 常用命令"></a>2.3 常用命令</h2><h3 id="2-3-1-上传"><a href="#2-3-1-上传" class="headerlink" title="2.3.1 上传"></a>2.3.1 上传</h3><ul>
<li>-moveFromLocal：从本地剪切粘贴到 HDFS  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 sanguo]$ ls</span><br><span class="line">guanyu.txt  liubei.txt  sanguo.txt  zhangfei.txt  zhaoyun.txt</span><br><span class="line">[atguigu@hadoop001 sanguo]$ hadoop fs -moveFromLocal zhangfei.txt /sanguo</span><br><span class="line">2021-10-10 11:01:49,884 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = <span class="literal">false</span>, remoteHostTrusted = <span class="literal">false</span></span><br><span class="line">[atguigu@hadoop001 sanguo]$ ls</span><br><span class="line">guanyu.txt  liubei.txt  sanguo.txt  zhaoyun.txt</span><br><span class="line">[atguigu@hadoop001 sanguo]$</span><br></pre></td></tr></table></figure></li>
<li>-copyFromLocal：从本地文件系统中拷贝文件到 HDFS 路径去  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 ~]$ hadoop fs -copyFromLocal zhaoyun.txt /sanguo</span><br><span class="line">2021-10-10 10:21:55,357 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = <span class="literal">false</span>, remoteHostTrusted = <span class="literal">false</span></span><br><span class="line">[atguigu@hadoop001 ~]$</span><br></pre></td></tr></table></figure></li>
<li>-put：等同于 copyFromLocal，生产环境更习惯用 put  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 sanguo]$ hadoop fs -put caocao.txt /sanguo</span><br><span class="line">2021-10-10 12:05:07,051 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = <span class="literal">false</span>, remoteHostTrusted = <span class="literal">false</span></span><br><span class="line">[atguigu@hadoop001 sanguo]$ hadoop fs -ls /sanguo</span><br><span class="line">Found 8 items</span><br><span class="line">-rw-r--r--   3 atguigu supergroup         34 2021-10-10 12:05 /sanguo/caocao.txt</span><br><span class="line">-rw-r--r--   3 atguigu supergroup         48 2021-10-10 10:26 /sanguo/guanyu.txt</span><br><span class="line">-rw-r--r--   3 atguigu supergroup  195013152 2021-10-10 10:21 /sanguo/jdk-8u212-linux-x64.tar.gz</span><br><span class="line">-rw-r--r--   3 atguigu supergroup         22 2021-10-10 10:25 /sanguo/liubei.txt</span><br><span class="line">drwxr-xr-x   - atguigu supergroup          0 2021-10-10 10:33 /sanguo/shu</span><br><span class="line">drwxr-xr-x   - atguigu supergroup          0 2021-10-10 10:33 /sanguo/wu</span><br><span class="line">-rw-r--r--   3 atguigu supergroup         22 2021-10-10 11:01 /sanguo/zhangfei.txt</span><br><span class="line">-rw-r--r--   6 atguigu supergroup         28 2021-10-10 10:21 /sanguo/zhaoyun.txt</span><br><span class="line">[atguigu@hadoop001 sanguo]$</span><br></pre></td></tr></table></figure></li>
<li>-appendToFile：追加一个文件到已经存在的文件末尾</li>
</ul>
<h3 id="2-3-2-查看"><a href="#2-3-2-查看" class="headerlink" title="2.3.2 查看"></a>2.3.2 查看</h3><h3 id="2-3-3-下载"><a href="#2-3-3-下载" class="headerlink" title="2.3.3 下载"></a>2.3.3 下载</h3><h3 id="2-3-4-删除"><a href="#2-3-4-删除" class="headerlink" title="2.3.4 删除"></a>2.3.4 删除</h3><h3 id="2-3-5-HDFS-直接操作"><a href="#2-3-5-HDFS-直接操作" class="headerlink" title="2.3.5 HDFS 直接操作"></a>2.3.5 HDFS 直接操作</h3><hr>
<h1 id="四、HDFS的数据流"><a href="#四、HDFS的数据流" class="headerlink" title="四、HDFS的数据流"></a>四、HDFS的数据流</h1><h2 id="4-1-HDFS写数据流程"><a href="#4-1-HDFS写数据流程" class="headerlink" title="4.1 HDFS写数据流程"></a>4.1 HDFS写数据流程</h2><h3 id="4-1-1-剖析文件写入"><a href="#4-1-1-剖析文件写入" class="headerlink" title="4.1.1 剖析文件写入"></a>4.1.1 剖析文件写入</h3><p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16338477292310.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
<ol>
<li>请求上传文件<ol>
<li>创建Hadoop客户端DistributionFileSystem</li>
</ol>
</li>
<li>请求NN上传文件<ol>
<li>NN校验：权限，路径，文件名</li>
<li>NN响应允许上传</li>
</ol>
</li>
<li>请求NN上传Block</li>
<li>NN根据副本数量按照<font color ='red' >机架感知</font>规则返回副本存储节点DN</li>
<li>客户端创建输出流FSDataOutputStream，根据<font color ='red' >网络拓扑计算节点距离</font>，请求<font color ='red' ><font color ='red' ></font>最近节点</font>建立Block传输通道<ol>
<li>客户端请求DN1建立通道</li>
<li>DN1请求DN2建立通道</li>
<li>DN2请求DN3建立通道</li>
</ol>
</li>
<li>通道建立应答成功<ol>
<li>DN3应答DN2成功</li>
<li>DN2应答DN1成功</li>
<li>DN1应答客户端成功</li>
</ol>
</li>
<li>传输数据Packet<ol>
<li>block 分解成多个 packet （每个64K）,packet分解成多个chunk(每个512B)，每个chunk都有个校验和chacksum</li>
<li>DFSOutputStream负责把数据写入dataQueue，写入单位为packet</li>
<li>DataStreamer从dataQueue中提取packet,发送到管道中的第一个datanode,同时将该packet写入 ackQueue中（block是有副本的，在写block之前就已经确定了，这些副本要写到哪些datanode上，这些datanode形成一个数据管道，DataStreamer只会把数据写入管道的第一个datanode,然后第一个dataNode向第二个datanode写数据，第二个再向第三个写，它们之间使用socket传输数据）</li>
<li>ResponseProcessor会从datanodes接收ack（此ack是Datanode接收packet成功后的确定），ResponseProcessor接收到所有Datanote的ack后，就从ackQueue中移除相应的packet。</li>
<li>如果遇到异常，所有的packets都从ackQueue移除，并排除异常的Datanode，再重新申请一个管道 。</li>
<li>然后 DataStreamer又重新从dataQueue中，获得packets并发送。</li>
<li><a href="https://www.jianshu.com/p/419a2068987c">参考</a></li>
</ol>
</li>
<li>当一个Block传输完成之后，客户端再次请求NameNode上传第二个Block的服务器。（重复执行3-7步）。</li>
</ol>
<h3 id="4-1-2-网络拓扑-节点距离计算"><a href="#4-1-2-网络拓扑-节点距离计算" class="headerlink" title="4.1.2 网络拓扑-节点距离计算"></a>4.1.2 网络拓扑-节点距离计算</h3><ul>
<li>在HDFS写数据的过程中，NameNode会选择距离待上传数据最近距离的DataNode接收数据。</li>
<li>节点距离：两个节点到达最近的共同祖先的距离总和。<br><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16339649640120.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></li>
</ul>
<h3 id="4-1-3-机架感知（副本存储节点选择）"><a href="#4-1-3-机架感知（副本存储节点选择）" class="headerlink" title="4.1.3 机架感知（副本存储节点选择）"></a>4.1.3 机架感知（副本存储节点选择）</h3><p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16339657833860.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
<ul>
<li>第一个副本在Client所处的节点上。如果客户端在集群外，随机选一个。</li>
<li>第二个副本在另一个机架的随机一个节点</li>
<li>第三个副本在第二个副本所在机架的随机节点</li>
</ul>
<blockquote>
<p>思考</p>
<ol>
<li>HDFS根据请求返回DataNode的节点的策略？– 机架感知</li>
</ol>
<ul>
<li>如果当前Client所在机器有DataNode节点，那就返回当前机器DN1,否则从集群中随机一台。</li>
<li>根据第一台机器的位置，然后再其他机架上随机一台，在第二台机器所在机架上再随机一台。</li>
<li>以上策略的缘由：为了提高数据的可靠性，同时一定程度也保证数据传输的效率！</li>
</ul>
<ol start="2">
<li>客户端建立传输通道的时候如何确定和哪一台DataNode先建立连接？– 网络拓扑</li>
</ol>
<ul>
<li>找离client最近的一台机器先建立通道。</li>
</ul>
<ol start="3">
<li>Client为什么是以串行的方式建立通道？</li>
</ol>
<ul>
<li>本质上就是为了降低client的IO开销</li>
</ul>
<ol start="4">
<li>数据传输的时候如何保证数据成功？（了解）</li>
</ol>
<ul>
<li>采用了ack回执的策略保证了数据完整成功上传。</li>
</ul>
</blockquote>
<hr>
<h2 id="4-2-HDFS读数据流程"><a href="#4-2-HDFS读数据流程" class="headerlink" title="4.2 HDFS读数据流程"></a>4.2 HDFS读数据流程</h2><p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16339658069723.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
<ol>
<li>客户端通过DistributedFileSystem向NameNode请求下载文件，NameNode通过查询元数据，找到文件块所在的DataNode地址。</li>
<li>挑选一台DataNode（就近原则，然后随机）服务器，请求读取数据。</li>
<li>DataNode开始传输数据给客户端（从磁盘里面读取数据输入流，以Packet为单位来做校验）。</li>
<li>客户端以Packet为单位接收，先在本地缓存，然后写入目标文件。</li>
</ol>
<hr>
<h1 id="五、NameNode和SecondaryNameNode"><a href="#五、NameNode和SecondaryNameNode" class="headerlink" title="五、NameNode和SecondaryNameNode"></a>五、NameNode和SecondaryNameNode</h1><h2 id="5-1-NN和2NN工作机制"><a href="#5-1-NN和2NN工作机制" class="headerlink" title="5.1 NN和2NN工作机制"></a>5.1 NN和2NN工作机制</h2><h3 id="5-1-1-元数据信息要保存在哪？"><a href="#5-1-1-元数据信息要保存在哪？" class="headerlink" title="5.1.1 元数据信息要保存在哪？"></a>5.1.1 元数据信息要保存在哪？</h3><ol>
<li><p>保存到磁盘</p>
<ul>
<li>优点：数据安全</li>
<li>不足：读写速度慢 效率低！</li>
</ul>
</li>
<li><p>保存内存</p>
<ul>
<li>优点： 读写效率高！</li>
<li>不足：数据不安全</li>
</ul>
</li>
<li><p>最终的解决方案： 磁盘 + 内存<br><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16339951016390.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
<ul>
<li>第一阶段：NameNode启动<ol>
<li>第一次启动NameNode格式化后，创建Fsimage和Edits文件。如果不是第一次启动，直接加载编辑日志和镜像文件到内存。</li>
<li>客户端对元数据进行增删改的请求。</li>
<li>NameNode记录操作日志，更新滚动日志。</li>
<li>NameNode在内存中对元数据进行增删改。 </li>
</ol>
</li>
<li>第二阶段：Secondary NameNode工作<ol>
<li>Secondary NameNode询问NameNode是否需要CheckPoint。直接带回NameNode是否检查结果。</li>
<li>Secondary NameNode请求执行CheckPoint。</li>
<li>NameNode滚动正在写的Edits日志。</li>
<li>将滚动前的编辑日志和镜像文件拷贝到Secondary NameNode。</li>
<li>Secondary NameNode加载编辑日志和镜像文件到内存，并合并。</li>
<li>生成新的镜像文件fsimage.chkpoint。</li>
<li>拷贝fsimage.chkpoint到NameNode。</li>
<li>NameNode将fsimage.chkpoint重新命名成fsimage。</li>
</ol>
</li>
</ul>
</li>
</ol>
<hr>
<h2 id="5-2-Fsimage和Edits解析"><a href="#5-2-Fsimage和Edits解析" class="headerlink" title="5.2 Fsimage和Edits解析"></a>5.2 Fsimage和Edits解析</h2><p>NameNode被格式化之后，将在/opt/module/hadoop-2.7.2/data/tmp/dfs/name/current目录中产生如下文件</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 subdir0]$ <span class="built_in">cd</span> /opt/module/hadoop-3.1.3/data/dfs/name/current/</span><br><span class="line">[atguigu@hadoop001 current]$ ll</span><br><span class="line">总用量 6320</span><br><span class="line">-rw-rw-r--. 1 atguigu atguigu 1048576 10月 12 07:26 edits_0000000000000000666-0000000000000000666</span><br><span class="line">-rw-rw-r--. 1 atguigu atguigu      42 10月 12 09:45 edits_0000000000000000667-0000000000000000668</span><br><span class="line">-rw-rw-r--. 1 atguigu atguigu 1048576 10月 12 09:45 edits_inprogress_0000000000000000669</span><br><span class="line">-rw-rw-r--. 1 atguigu atguigu    4452 10月 12 09:05 fsimage_0000000000000000666</span><br><span class="line">-rw-rw-r--. 1 atguigu atguigu      62 10月 12 09:05 fsimage_0000000000000000666.md5</span><br><span class="line">-rw-rw-r--. 1 atguigu atguigu    4452 10月 12 09:45 fsimage_0000000000000000668</span><br><span class="line">-rw-rw-r--. 1 atguigu atguigu      62 10月 12 09:45 fsimage_0000000000000000668.md5</span><br><span class="line">-rw-rw-r--. 1 atguigu atguigu       4 10月 12 09:45 seen_txid</span><br><span class="line">-rw-rw-r--. 1 atguigu atguigu     215 10月 12 09:05 VERSION</span><br><span class="line">[atguigu@hadoop001 current]$</span><br></pre></td></tr></table></figure>
<ol>
<li>Fsimage文件：HDFS文件系统元数据的一个永久性的检查点，其中包含HDFS文件系统的所有目录和文件inode的序列化信息。 </li>
<li>Edits文件：存放HDFS文件系统的所有更新操作的路径，文件系统客户端执行的所有写操作首先会被记录到Edits文件中。 </li>
<li>seen_txid文件保存的是一个数字，就是最后一个edits_的数字 </li>
<li>每次NameNode启动的时候都会将Fsimage文件读入内存，加载Edits里面的更新操作，保证内存中的元数据信息是最新的、同步的，可以看成NameNode启动的时候就将Fsimage和Edits文件进行了合并</li>
<li>解析Fsimage文件<ol>
<li>oiv查看Fsimage文件<ol>
<li>查看oiv和oev命令 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 current]$ hdfs --<span class="built_in">help</span> | grep -E <span class="string">&#x27;oev|oiv&#x27;</span></span><br><span class="line">oev                  apply the offline edits viewer to an edits file</span><br><span class="line">oiv                  apply the offline fsimage viewer to an fsimage</span><br><span class="line">[atguigu@hadoop001 current]$</span><br></pre></td></tr></table></figure></li>
<li>基本语法<br> <code>hdfs oiv -p 文件类型 -i镜像文件 -o 转换后文件输出路径</code></li>
<li>操作 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 current]$ hdfs oiv -p xml -i fsimage_0000000000000000666 -o ~/fsimage/fsimage.xml</span><br><span class="line">2021-10-12 10:02:40,652 INFO offlineImageViewer.FSImageHandler: Loading 3 strings</span><br><span class="line">[atguigu@hadoop001 current]$</span><br></pre></td></tr></table></figure></li>
<li>部分内容 <figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">INodeSection</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">lastInodeId</span>&gt;</span>16507<span class="tag">&lt;/<span class="name">lastInodeId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">numInodes</span>&gt;</span>54<span class="tag">&lt;/<span class="name">numInodes</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">inode</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">id</span>&gt;</span>16385<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">type</span>&gt;</span>DIRECTORY<span class="tag">&lt;/<span class="name">type</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span><span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">mtime</span>&gt;</span>1633959143943<span class="tag">&lt;/<span class="name">mtime</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">permission</span>&gt;</span>atguigu:supergroup:0755<span class="tag">&lt;/<span class="name">permission</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">nsquota</span>&gt;</span>9223372036854775807<span class="tag">&lt;/<span class="name">nsquota</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dsquota</span>&gt;</span>-1<span class="tag">&lt;/<span class="name">dsquota</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">inode</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">inode</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">id</span>&gt;</span>16470<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">type</span>&gt;</span>DIRECTORY<span class="tag">&lt;/<span class="name">type</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>sanguo<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">mtime</span>&gt;</span>1633838707139<span class="tag">&lt;/<span class="name">mtime</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">permission</span>&gt;</span>atguigu:supergroup:0755<span class="tag">&lt;/<span class="name">permission</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">nsquota</span>&gt;</span>-1<span class="tag">&lt;/<span class="name">nsquota</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dsquota</span>&gt;</span>-1<span class="tag">&lt;/<span class="name">dsquota</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">inode</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">inode</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">id</span>&gt;</span>16471<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">type</span>&gt;</span>FILE<span class="tag">&lt;/<span class="name">type</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>jdk-8u212-linux-x64.tar.gz<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">replication</span>&gt;</span>3<span class="tag">&lt;/<span class="name">replication</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">mtime</span>&gt;</span>1633832463721<span class="tag">&lt;/<span class="name">mtime</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">atime</span>&gt;</span>1633832462757<span class="tag">&lt;/<span class="name">atime</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">preferredBlockSize</span>&gt;</span>134217728<span class="tag">&lt;/<span class="name">preferredBlockSize</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">permission</span>&gt;</span>atguigu:supergroup:0644<span class="tag">&lt;/<span class="name">permission</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">blocks</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">block</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">id</span>&gt;</span>1073741861<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">genstamp</span>&gt;</span>1037<span class="tag">&lt;/<span class="name">genstamp</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">numBytes</span>&gt;</span>134217728<span class="tag">&lt;/<span class="name">numBytes</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">block</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">block</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">id</span>&gt;</span>1073741862<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">genstamp</span>&gt;</span>1038<span class="tag">&lt;/<span class="name">genstamp</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">numBytes</span>&gt;</span>60795424<span class="tag">&lt;/<span class="name">numBytes</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">block</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">blocks</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">storagePolicyId</span>&gt;</span>0<span class="tag">&lt;/<span class="name">storagePolicyId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">inode</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">INodeSection</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">INodeDirectorySection</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">directory</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">parent</span>&gt;</span>16385<span class="tag">&lt;/<span class="name">parent</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">child</span>&gt;</span>16470<span class="tag">&lt;/<span class="name">child</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">child</span>&gt;</span>16475<span class="tag">&lt;/<span class="name">child</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">child</span>&gt;</span>16386<span class="tag">&lt;/<span class="name">child</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">child</span>&gt;</span>16507<span class="tag">&lt;/<span class="name">child</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">child</span>&gt;</span>16392<span class="tag">&lt;/<span class="name">child</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">child</span>&gt;</span>16403<span class="tag">&lt;/<span class="name">child</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">child</span>&gt;</span>16430<span class="tag">&lt;/<span class="name">child</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">child</span>&gt;</span>16453<span class="tag">&lt;/<span class="name">child</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">directory</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">directory</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">parent</span>&gt;</span>16470<span class="tag">&lt;/<span class="name">parent</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">child</span>&gt;</span>16484<span class="tag">&lt;/<span class="name">child</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">child</span>&gt;</span>16473<span class="tag">&lt;/<span class="name">child</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">child</span>&gt;</span>16471<span class="tag">&lt;/<span class="name">child</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">child</span>&gt;</span>16474<span class="tag">&lt;/<span class="name">child</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">child</span>&gt;</span>16477<span class="tag">&lt;/<span class="name">child</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">child</span>&gt;</span>16478<span class="tag">&lt;/<span class="name">child</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">child</span>&gt;</span>16480<span class="tag">&lt;/<span class="name">child</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">child</span>&gt;</span>16472<span class="tag">&lt;/<span class="name">child</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">directory</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">INodeDirectorySection</span>&gt;</span></span><br></pre></td></tr></table></figure></li>
</ol>
<blockquote>
<p>思考：可以看出，Fsimage中没有记录块所对应DataNode，为什么？<br>  在集群启动后，要求DataNode上报数据块信息，并间隔一段时间后再次上报。</p>
</blockquote>
</li>
<li>oev查看Edits文件<ol>
<li>基本语法<br> <code>hdfs oev -p 文件类型 -i编辑日志 -o 转换后文件输出路径</code></li>
<li>操作 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 current]$ hdfs oev -p xml -i edits_0000000000000000666-0000000000000000666 -o ~/fsimage/edits.xml</span><br><span class="line">[atguigu@hadoop001 current]$ ll ~/fsimage/edits.xml</span><br><span class="line">-rw-rw-r--. 1 atguigu atguigu 221 10月 12 12:05 /home/atguigu/fsimage/edits.xml</span><br><span class="line">[atguigu@hadoop001 current]$</span><br></pre></td></tr></table></figure></li>
<li>内容 <figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; standalone=&quot;yes&quot;?&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">EDITS</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">EDITS_VERSION</span>&gt;</span>-64<span class="tag">&lt;/<span class="name">EDITS_VERSION</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">RECORD</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">OPCODE</span>&gt;</span>OP_START_LOG_SEGMENT<span class="tag">&lt;/<span class="name">OPCODE</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">DATA</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">TXID</span>&gt;</span>328<span class="tag">&lt;/<span class="name">TXID</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">DATA</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;/<span class="name">RECORD</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">RECORD</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">OPCODE</span>&gt;</span>OP_MKDIR<span class="tag">&lt;/<span class="name">OPCODE</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">DATA</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">TXID</span>&gt;</span>329<span class="tag">&lt;/<span class="name">TXID</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">LENGTH</span>&gt;</span>0<span class="tag">&lt;/<span class="name">LENGTH</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">INODEID</span>&gt;</span>16470<span class="tag">&lt;/<span class="name">INODEID</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">PATH</span>&gt;</span>/sanguo<span class="tag">&lt;/<span class="name">PATH</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">TIMESTAMP</span>&gt;</span>1633832427513<span class="tag">&lt;/<span class="name">TIMESTAMP</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">PERMISSION_STATUS</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">USERNAME</span>&gt;</span>atguigu<span class="tag">&lt;/<span class="name">USERNAME</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">GROUPNAME</span>&gt;</span>supergroup<span class="tag">&lt;/<span class="name">GROUPNAME</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">MODE</span>&gt;</span>493<span class="tag">&lt;/<span class="name">MODE</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;/<span class="name">PERMISSION_STATUS</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">DATA</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;/<span class="name">RECORD</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">RECORD</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">OPCODE</span>&gt;</span>OP_ADD<span class="tag">&lt;/<span class="name">OPCODE</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">DATA</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">TXID</span>&gt;</span>330<span class="tag">&lt;/<span class="name">TXID</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">LENGTH</span>&gt;</span>0<span class="tag">&lt;/<span class="name">LENGTH</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">INODEID</span>&gt;</span>16471<span class="tag">&lt;/<span class="name">INODEID</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">PATH</span>&gt;</span>/sanguo/jdk-8u212-linux-x64.tar.gz._COPYING_<span class="tag">&lt;/<span class="name">PATH</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">REPLICATION</span>&gt;</span>3<span class="tag">&lt;/<span class="name">REPLICATION</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">MTIME</span>&gt;</span>1633832462757<span class="tag">&lt;/<span class="name">MTIME</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">ATIME</span>&gt;</span>1633832462757<span class="tag">&lt;/<span class="name">ATIME</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">BLOCKSIZE</span>&gt;</span>134217728<span class="tag">&lt;/<span class="name">BLOCKSIZE</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">CLIENT_NAME</span>&gt;</span>DFSClient_NONMAPREDUCE_-1241738550_1<span class="tag">&lt;/<span class="name">CLIENT_NAME</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">CLIENT_MACHINE</span>&gt;</span>192.168.2.6<span class="tag">&lt;/<span class="name">CLIENT_MACHINE</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">OVERWRITE</span>&gt;</span>true<span class="tag">&lt;/<span class="name">OVERWRITE</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">PERMISSION_STATUS</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">USERNAME</span>&gt;</span>atguigu<span class="tag">&lt;/<span class="name">USERNAME</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">GROUPNAME</span>&gt;</span>supergroup<span class="tag">&lt;/<span class="name">GROUPNAME</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">MODE</span>&gt;</span>420<span class="tag">&lt;/<span class="name">MODE</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;/<span class="name">PERMISSION_STATUS</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">ERASURE_CODING_POLICY_ID</span>&gt;</span>0<span class="tag">&lt;/<span class="name">ERASURE_CODING_POLICY_ID</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">RPC_CLIENTID</span>&gt;</span>a0a774c6-277a-43ec-9f31-a8c2cbcaa322<span class="tag">&lt;/<span class="name">RPC_CLIENTID</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">RPC_CALLID</span>&gt;</span>3<span class="tag">&lt;/<span class="name">RPC_CALLID</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">DATA</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;/<span class="name">RECORD</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">RECORD</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">OPCODE</span>&gt;</span>OP_ALLOCATE_BLOCK_ID<span class="tag">&lt;/<span class="name">OPCODE</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">DATA</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">TXID</span>&gt;</span>331<span class="tag">&lt;/<span class="name">TXID</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">BLOCK_ID</span>&gt;</span>1073741861<span class="tag">&lt;/<span class="name">BLOCK_ID</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">DATA</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;/<span class="name">RECORD</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">RECORD</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">OPCODE</span>&gt;</span>OP_SET_GENSTAMP_V2<span class="tag">&lt;/<span class="name">OPCODE</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">DATA</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">TXID</span>&gt;</span>332<span class="tag">&lt;/<span class="name">TXID</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">GENSTAMPV2</span>&gt;</span>1037<span class="tag">&lt;/<span class="name">GENSTAMPV2</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">DATA</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;/<span class="name">RECORD</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">RECORD</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">OPCODE</span>&gt;</span>OP_ADD_BLOCK<span class="tag">&lt;/<span class="name">OPCODE</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">DATA</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">TXID</span>&gt;</span>333<span class="tag">&lt;/<span class="name">TXID</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">PATH</span>&gt;</span>/sanguo/jdk-8u212-linux-x64.tar.gz._COPYING_<span class="tag">&lt;/<span class="name">PATH</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">BLOCK</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">BLOCK_ID</span>&gt;</span>1073741861<span class="tag">&lt;/<span class="name">BLOCK_ID</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">NUM_BYTES</span>&gt;</span>0<span class="tag">&lt;/<span class="name">NUM_BYTES</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">GENSTAMP</span>&gt;</span>1037<span class="tag">&lt;/<span class="name">GENSTAMP</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;/<span class="name">BLOCK</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">RPC_CLIENTID</span>/&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">RPC_CALLID</span>&gt;</span>-2<span class="tag">&lt;/<span class="name">RPC_CALLID</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">DATA</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;/<span class="name">RECORD</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">RECORD</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">OPCODE</span>&gt;</span>OP_ALLOCATE_BLOCK_ID<span class="tag">&lt;/<span class="name">OPCODE</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">DATA</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">TXID</span>&gt;</span>334<span class="tag">&lt;/<span class="name">TXID</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">BLOCK_ID</span>&gt;</span>1073741862<span class="tag">&lt;/<span class="name">BLOCK_ID</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">DATA</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;/<span class="name">RECORD</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">RECORD</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">OPCODE</span>&gt;</span>OP_SET_GENSTAMP_V2<span class="tag">&lt;/<span class="name">OPCODE</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">DATA</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">TXID</span>&gt;</span>335<span class="tag">&lt;/<span class="name">TXID</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">GENSTAMPV2</span>&gt;</span>1038<span class="tag">&lt;/<span class="name">GENSTAMPV2</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">DATA</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;/<span class="name">RECORD</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">RECORD</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">OPCODE</span>&gt;</span>OP_ADD_BLOCK<span class="tag">&lt;/<span class="name">OPCODE</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">DATA</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">TXID</span>&gt;</span>336<span class="tag">&lt;/<span class="name">TXID</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">PATH</span>&gt;</span>/sanguo/jdk-8u212-linux-x64.tar.gz._COPYING_<span class="tag">&lt;/<span class="name">PATH</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">BLOCK</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">BLOCK_ID</span>&gt;</span>1073741861<span class="tag">&lt;/<span class="name">BLOCK_ID</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">NUM_BYTES</span>&gt;</span>134217728<span class="tag">&lt;/<span class="name">NUM_BYTES</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">GENSTAMP</span>&gt;</span>1037<span class="tag">&lt;/<span class="name">GENSTAMP</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;/<span class="name">BLOCK</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">BLOCK</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">BLOCK_ID</span>&gt;</span>1073741862<span class="tag">&lt;/<span class="name">BLOCK_ID</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">NUM_BYTES</span>&gt;</span>0<span class="tag">&lt;/<span class="name">NUM_BYTES</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">GENSTAMP</span>&gt;</span>1038<span class="tag">&lt;/<span class="name">GENSTAMP</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;/<span class="name">BLOCK</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">RPC_CLIENTID</span>/&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">RPC_CALLID</span>&gt;</span>-2<span class="tag">&lt;/<span class="name">RPC_CALLID</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">DATA</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;/<span class="name">RECORD</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">RECORD</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">OPCODE</span>&gt;</span>OP_CLOSE<span class="tag">&lt;/<span class="name">OPCODE</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">DATA</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">TXID</span>&gt;</span>337<span class="tag">&lt;/<span class="name">TXID</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">LENGTH</span>&gt;</span>0<span class="tag">&lt;/<span class="name">LENGTH</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">INODEID</span>&gt;</span>0<span class="tag">&lt;/<span class="name">INODEID</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">PATH</span>&gt;</span>/sanguo/jdk-8u212-linux-x64.tar.gz._COPYING_<span class="tag">&lt;/<span class="name">PATH</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">REPLICATION</span>&gt;</span>3<span class="tag">&lt;/<span class="name">REPLICATION</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">MTIME</span>&gt;</span>1633832463721<span class="tag">&lt;/<span class="name">MTIME</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">ATIME</span>&gt;</span>1633832462757<span class="tag">&lt;/<span class="name">ATIME</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">BLOCKSIZE</span>&gt;</span>134217728<span class="tag">&lt;/<span class="name">BLOCKSIZE</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">CLIENT_NAME</span>/&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">CLIENT_MACHINE</span>/&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">OVERWRITE</span>&gt;</span>false<span class="tag">&lt;/<span class="name">OVERWRITE</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">BLOCK</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">BLOCK_ID</span>&gt;</span>1073741861<span class="tag">&lt;/<span class="name">BLOCK_ID</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">NUM_BYTES</span>&gt;</span>134217728<span class="tag">&lt;/<span class="name">NUM_BYTES</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">GENSTAMP</span>&gt;</span>1037<span class="tag">&lt;/<span class="name">GENSTAMP</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;/<span class="name">BLOCK</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">BLOCK</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">BLOCK_ID</span>&gt;</span>1073741862<span class="tag">&lt;/<span class="name">BLOCK_ID</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">NUM_BYTES</span>&gt;</span>60795424<span class="tag">&lt;/<span class="name">NUM_BYTES</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">GENSTAMP</span>&gt;</span>1038<span class="tag">&lt;/<span class="name">GENSTAMP</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;/<span class="name">BLOCK</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">PERMISSION_STATUS</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">USERNAME</span>&gt;</span>atguigu<span class="tag">&lt;/<span class="name">USERNAME</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">GROUPNAME</span>&gt;</span>supergroup<span class="tag">&lt;/<span class="name">GROUPNAME</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">MODE</span>&gt;</span>420<span class="tag">&lt;/<span class="name">MODE</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;/<span class="name">PERMISSION_STATUS</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">DATA</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;/<span class="name">RECORD</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">RECORD</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">OPCODE</span>&gt;</span>OP_RENAME_OLD<span class="tag">&lt;/<span class="name">OPCODE</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">DATA</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">TXID</span>&gt;</span>338<span class="tag">&lt;/<span class="name">TXID</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">LENGTH</span>&gt;</span>0<span class="tag">&lt;/<span class="name">LENGTH</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">SRC</span>&gt;</span>/sanguo/jdk-8u212-linux-x64.tar.gz._COPYING_<span class="tag">&lt;/<span class="name">SRC</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">DST</span>&gt;</span>/sanguo/jdk-8u212-linux-x64.tar.gz<span class="tag">&lt;/<span class="name">DST</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">TIMESTAMP</span>&gt;</span>1633832463726<span class="tag">&lt;/<span class="name">TIMESTAMP</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">RPC_CLIENTID</span>&gt;</span>a0a774c6-277a-43ec-9f31-a8c2cbcaa322<span class="tag">&lt;/<span class="name">RPC_CLIENTID</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">RPC_CALLID</span>&gt;</span>9<span class="tag">&lt;/<span class="name">RPC_CALLID</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">DATA</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;/<span class="name">RECORD</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">RECORD</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">OPCODE</span>&gt;</span>OP_END_LOG_SEGMENT<span class="tag">&lt;/<span class="name">OPCODE</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">DATA</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">TXID</span>&gt;</span>339<span class="tag">&lt;/<span class="name">TXID</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">DATA</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;/<span class="name">RECORD</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">EDITS</span>&gt;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<blockquote>
<p>思考：NameNode如何确定下次开机启动的时候合并哪些Edits？<br>通过最新合并的fsimage_的序号（例如fsimage_0000000000000000584）和seen_txid存放的序号（如585）取它们中间的edits序号合并即可<br><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16340122811449.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
</blockquote>
</li>
</ol>
</li>
</ol>
</li>
</ol>
<hr>
<h2 id="5-3-CheckPoint时间设置"><a href="#5-3-CheckPoint时间设置" class="headerlink" title="5.3 CheckPoint时间设置"></a>5.3 CheckPoint时间设置</h2><p>配置文件：hdfs-default.xml</p>
<ol>
<li>默认配置SecondaryNameNode一小时同步一次 <figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.checkpoint.period<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>3600s<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li>
<li>一分钟检查一次操作次数，达到100W次触发一次同步 <figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.checkpoint.txns<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>1000000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>操作动作次数<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.checkpoint.check.period<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>60s<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span> 1分钟检查一次操作次数<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">&lt;/property &gt;</span><br></pre></td></tr></table></figure></li>
</ol>
<hr>
<h2 id="5-4-NameNode故障处理"><a href="#5-4-NameNode故障处理" class="headerlink" title="5.4 NameNode故障处理"></a>5.4 NameNode故障处理</h2><h3 id="5-4-1-使用SecondaryNameNode中的数据恢复"><a href="#5-4-1-使用SecondaryNameNode中的数据恢复" class="headerlink" title="5.4.1 使用SecondaryNameNode中的数据恢复"></a>5.4.1 使用SecondaryNameNode中的数据恢复</h3><p>将SecondaryNameNode中的数据拷贝到NameNode存储元数据的位置</p>
<ol>
<li>杀掉NameNode进程 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 name]$ jps</span><br><span class="line">1921 NodeManager</span><br><span class="line">4834 NameNode</span><br><span class="line">1638 DataNode</span><br><span class="line">4909 Jps</span><br><span class="line">2095 JobHistoryServer</span><br><span class="line">[atguigu@hadoop001 name]$ <span class="built_in">kill</span> -9 1921</span><br><span class="line">[atguigu@hadoop001 name]$ jps</span><br><span class="line">4834 NameNode</span><br><span class="line">1638 DataNode</span><br><span class="line">4922 Jps</span><br><span class="line">2095 JobHistoryServer</span><br><span class="line">[atguigu@hadoop001 name]$</span><br></pre></td></tr></table></figure></li>
<li>删除NameNode数据 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 name]$ <span class="built_in">pwd</span></span><br><span class="line">/opt/module/hadoop-3.1.3/data/dfs/name</span><br><span class="line">[atguigu@hadoop001 name]$ ls</span><br><span class="line">current  in_use.lock</span><br><span class="line">[atguigu@hadoop001 name]$ rm -rf *</span><br><span class="line">[atguigu@hadoop001 name]$ ll</span><br><span class="line">总用量 0</span><br><span class="line">[atguigu@hadoop001 name]$</span><br></pre></td></tr></table></figure></li>
<li>拷贝SecondaryNameNode数据到NameNode<br> SecondaryNameNode: <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop003 namesecondary]$ <span class="built_in">pwd</span></span><br><span class="line">/opt/module/hadoop-3.1.3/data/dfs/namesecondary</span><br><span class="line">[atguigu@hadoop003 namesecondary]$ ls</span><br><span class="line">current  in_use.lock</span><br><span class="line">[atguigu@hadoop003 namesecondary]$</span><br></pre></td></tr></table></figure>
 NameNode: <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 name]$ <span class="built_in">pwd</span></span><br><span class="line">/opt/module/hadoop-3.1.3/data/dfs/name</span><br><span class="line">[atguigu@hadoop001 name]$ scp -r hadoop003:/opt/module/hadoop-3.1.3/data/dfs/namesecondary/* ./</span><br><span class="line">edits_0000000000000000415-0000000000000000416          100%   42    71.3KB/s   00:00</span><br><span class="line">edits_0000000000000000618-0000000000000000623          100%  311   726.8KB/s   00:00</span><br><span class="line">edits_0000000000000000671-0000000000000000672          100%   42    96.1KB/s   00:00</span><br><span class="line">edits_0000000000000000002-0000000000000000217          100%   27KB  28.4MB/s   00:00</span><br><span class="line">edits_0000000000000000664-0000000000000000665          100%   42    96.8KB/s   00:00</span><br><span class="line">edits_0000000000000000667-0000000000000000668          100%   42   101.8KB/s   00:00</span><br><span class="line">VERSION                                                100%  215   519.5KB/s   00:00</span><br><span class="line">edits_0000000000000000218-0000000000000000219          100%   42   105.8KB/s   00:00</span><br><span class="line">edits_0000000000000000410-0000000000000000411          100%   42   102.3KB/s   00:00</span><br><span class="line">edits_0000000000000000645-0000000000000000649          100%  291   761.2KB/s   00:00</span><br><span class="line">edits_0000000000000000662-0000000000000000663          100%   42   113.7KB/s   00:00</span><br><span class="line">edits_0000000000000000220-0000000000000000221          100%   42   111.9KB/s   00:00</span><br><span class="line">edits_0000000000000000412-0000000000000000413          100%   42   102.2KB/s   00:00</span><br><span class="line">edits_0000000000000000654-0000000000000000655          100%   42   105.8KB/s   00:00</span><br><span class="line">edits_0000000000000000222-0000000000000000223          100%   42   102.6KB/s   00:00</span><br><span class="line">edits_0000000000000000408-0000000000000000409          100%   42   106.7KB/s   00:00</span><br><span class="line">edits_0000000000000000656-0000000000000000657          100%   42   109.0KB/s   00:00</span><br><span class="line">edits_0000000000000000225-0000000000000000325          100% 1024KB  88.8MB/s   00:00</span><br><span class="line">edits_0000000000000000326-0000000000000000327          100%   42   130.9KB/s   00:00</span><br><span class="line">edits_0000000000000000639-0000000000000000644          100%  310     1.0MB/s   00:00</span><br><span class="line">edits_0000000000000000650-0000000000000000651          100%   42   164.8KB/s   00:00</span><br><span class="line">edits_0000000000000000669-0000000000000000670          100%   42   170.4KB/s   00:00</span><br><span class="line">edits_0000000000000000328-0000000000000000339          100%  902     3.5MB/s   00:00</span><br><span class="line">fsimage_0000000000000000670                            100% 4452    13.4MB/s   00:00</span><br><span class="line">fsimage_0000000000000000672.md5                        100%   62   255.0KB/s   00:00</span><br><span class="line">edits_0000000000000000340-0000000000000000382          100% 3512    11.5MB/s   00:00</span><br><span class="line">edits_0000000000000000417-0000000000000000525          100% 6663    19.3MB/s   00:00</span><br><span class="line">edits_0000000000000000658-0000000000000000659          100%   42   166.4KB/s   00:00</span><br><span class="line">fsimage_0000000000000000670.md5                        100%   62   251.4KB/s   00:00</span><br><span class="line">edits_0000000000000000383-0000000000000000405          100% 1708     5.9MB/s   00:00</span><br><span class="line">edits_0000000000000000526-0000000000000000617          100% 5206    14.9MB/s   00:00</span><br><span class="line">edits_0000000000000000660-0000000000000000661          100%   42   168.7KB/s   00:00</span><br><span class="line">fsimage_0000000000000000672                            100% 4452    13.6MB/s   00:00</span><br><span class="line">edits_0000000000000000406-0000000000000000407          100%   42   167.7KB/s   00:00</span><br><span class="line">edits_0000000000000000624-0000000000000000638          100%  934     3.6MB/s   00:00</span><br><span class="line">edits_0000000000000000652-0000000000000000653          100%   42   162.7KB/s   00:00</span><br><span class="line">in_use.lock                                            100%   14    53.2KB/s   00:00</span><br><span class="line">[atguigu@hadoop001 name]$ ls</span><br><span class="line">current  in_use.lock</span><br><span class="line">[atguigu@hadoop001 name]$</span><br></pre></td></tr></table></figure></li>
<li>重新启动NameNode <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 name]$ jps</span><br><span class="line">4977 Jps</span><br><span class="line">1638 DataNode</span><br><span class="line">2095 JobHistoryServer</span><br><span class="line">[atguigu@hadoop001 name]$ hdfs --daemon start namenode</span><br><span class="line">[atguigu@hadoop001 name]$ jps</span><br><span class="line">1638 DataNode</span><br><span class="line">5030 NameNode</span><br><span class="line">5101 Jps</span><br><span class="line">2095 JobHistoryServer</span><br><span class="line">[atguigu@hadoop001 name]$</span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="5-4-2-使用importCheckpoint恢复"><a href="#5-4-2-使用importCheckpoint恢复" class="headerlink" title="5.4.2 使用importCheckpoint恢复"></a>5.4.2 使用importCheckpoint恢复</h3><p>使用-importCheckpoint选项启动NameNode守护进程，从而将SecondaryNameNode中数据拷贝到NameNode目录中。</p>
<ol>
<li>修改hdfs-site.xml <figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.checkpoint.period<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>120<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/module/hadoop-3.1.3/data/dfs/name<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li>
<li>杀掉NameNode进程</li>
<li>删除NameNode存储的数据（/opt/module/hadoop-3.1.3/data/dfs/name） <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-3.1.3]$ rm -rf /opt/module/hadoop-3.1.3/data/dfs/name/*</span><br></pre></td></tr></table></figure></li>
<li>如果SecondaryNameNode不和NameNode在一个主机节点上，需要将SecondaryNameNode存储数据的目录拷贝到NameNode存储数据的平级目录，并删除in_use.lock文件 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 dfs]$ scp -r atguigu@hadoop104:/opt/module/hadoop-3.1.3/data/dfs/namesecondary ./</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop102 namesecondary]$ rm -rf in_use.lock</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop102 dfs]$ <span class="built_in">pwd</span></span><br><span class="line">/opt/module/hadoop-3.1.3/data/dfs</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop102 dfs]$ ls</span><br><span class="line">data  name  namesecondary</span><br></pre></td></tr></table></figure></li>
<li>导入检查点数据（等待一会ctrl+c结束掉） <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-3.1.3]$ bin/hdfs namenode -importCheckpoint</span><br></pre></td></tr></table></figure></li>
<li>启动NameNode <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-3.1.3]$ hdfs --daemon start namenode</span><br></pre></td></tr></table></figure></li>
</ol>
<hr>
<h2 id="5-5-集群安全模式"><a href="#5-5-集群安全模式" class="headerlink" title="5.5 集群安全模式"></a>5.5 集群安全模式</h2><ol>
<li>NameNode启动：NameNode启动时，首先降镜像文件FsImage载入内存，并执行编辑日志Edits中的各项操作。在内存中成功建立完整的文件系统元数据之后，创建一个空的编辑日志，生成新的Fsimage文件。此时，NameNode开始监听DateNode请求，这个过程期间，NameNode一直运行安全模式，即对于客户端来说是只读的</li>
<li>DateNode启动：系统中的数据块位置不由NameNode维护，而是以块列表的形式存储在DateNode中。在系统的正常操作期间，NameNode会在内存中保留所有块位置的信息。在安全模式下，各个DateNode会向NameNode发送最新的块列表信息，NomeNode获取到足够多的块信息之后，即可高效运行文件系统</li>
<li>安全模式退出判断：如果满足<font color ='red' >最小副本条件</font>（在整个文件系统中99.9%的块满足最小副本级别（默认值：dfs.replication.min=1））。在启动一个刚刚格式化的HDFS集群时，因为系统中还没有任何块，所以NameNode不会进入安全模式。</li>
<li>手动进入安全模式 <ol>
<li>基本语法<br> 集群处于安全模式，不能执行重要操作（写操作）。集群启动完成后，自动退出安全模式。 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 查看安全模式状态</span></span><br><span class="line">[atguigu@hadoop001 name]$ hdfs dfsadmin -safemode get</span><br><span class="line">Safe mode is OFF</span><br><span class="line"><span class="comment"># 进入安全模式</span></span><br><span class="line">[atguigu@hadoop001 name]$ hdfs dfsadmin -safemode enter</span><br><span class="line">Safe mode is ON</span><br><span class="line"><span class="comment"># 查看安全模式状态</span></span><br><span class="line">[atguigu@hadoop001 name]$ hdfs dfsadmin -safemode get</span><br><span class="line">Safe mode is ON</span><br><span class="line"><span class="comment"># 离开安全模式</span></span><br><span class="line">[atguigu@hadoop001 name]$ hdfs dfsadmin -safemode leave</span><br><span class="line">Safe mode is OFF</span><br><span class="line"><span class="comment"># 查看安全模式状态</span></span><br><span class="line">[atguigu@hadoop001 name]$ hdfs dfsadmin -safemode get</span><br><span class="line">Safe mode is OFF</span><br><span class="line"><span class="comment"># 进入安全模式</span></span><br><span class="line">[atguigu@hadoop001 name]$ hdfs dfsadmin -safemode enter</span><br><span class="line">Safe mode is ON</span><br><span class="line"><span class="comment"># 等待安全模式退出，阻塞当前进程</span></span><br><span class="line">[atguigu@hadoop001 name]$ hdfs dfsadmin -safemode <span class="built_in">wait</span></span><br><span class="line"></span><br><span class="line">^C</span><br><span class="line">[atguigu@hadoop001 name]$ hdfs dfsadmin -safemode leave</span><br><span class="line">Safe mode is OFF</span><br><span class="line">[atguigu@hadoop001 name]$ hdfs dfsadmin -safemode get</span><br><span class="line">Safe mode is OFF</span><br><span class="line"><span class="comment"># 安全模式退出，wait进程继续</span></span><br><span class="line">[atguigu@hadoop001 name]$ hdfs dfsadmin -safemode <span class="built_in">wait</span></span><br><span class="line">Safe mode is OFF</span><br><span class="line">[atguigu@hadoop001 name]$</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
<li>示例<br> <img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16340146742555.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></li>
</ol>
</li>
</ol>
<hr>
<h2 id="5-6-NameNode多目录配置"><a href="#5-6-NameNode多目录配置" class="headerlink" title="5.6 NameNode多目录配置"></a>5.6 NameNode多目录配置</h2><ol>
<li>NameNode的本地目录可以配置成多个，且每个目录存放内容相同，增加了可靠性（不同目录最好分布在不同的磁盘）</li>
<li>具体配置如下<ol>
<li>在hdfs-site.xml文件中添加如下内容 <figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>file://$&#123;hadoop.tmp.dir&#125;/dfs/name1,file://$&#123;hadoop.tmp.dir&#125;/dfs/name2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li>
<li>停止集群，删除三台节点的data和logs中所有数据。 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-3.1.3]$ rm -rf data/ logs/</span><br><span class="line">[atguigu@hadoop103 hadoop-3.1.3]$ rm -rf data/ logs/</span><br><span class="line">[atguigu@hadoop104 hadoop-3.1.3]$ rm -rf data/ logs/</span><br></pre></td></tr></table></figure></li>
<li>格式化集群并启动。 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-3.1.3]$ bin/hdfs namenode -format</span><br><span class="line">[atguigu@hadoop102 hadoop-3.1.3]$ sbin/start-dfs.sh</span><br></pre></td></tr></table></figure></li>
<li>查看结果 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 dfs]$ ll</span><br><span class="line">总用量 12</span><br><span class="line">drwx------. 3 atguigu atguigu 4096 12月 11 08:03 data</span><br><span class="line">drwxrwxr-x. 3 atguigu atguigu 4096 12月 11 08:03 name1</span><br><span class="line">drwxrwxr-x. 3 atguigu atguigu 4096 12月 11 08:03 name2</span><br></pre></td></tr></table></figure></li>
</ol>
</li>
</ol>
<hr>
<h1 id="六、DataNode"><a href="#六、DataNode" class="headerlink" title="六、DataNode"></a>六、DataNode</h1><h2 id="6-1-DateNode工作机制"><a href="#6-1-DateNode工作机制" class="headerlink" title="6.1 DateNode工作机制"></a>6.1 DateNode工作机制</h2><p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16340068224418.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
<ol>
<li>一个数据块在DataNode上以文件形式存储在磁盘上，包括两个文件，一个是数据本身，一个是元数据包括数据块的长度，块数据的校验和，以及时间戳。</li>
<li>DataNode启动后向NameNode注册，通过后，周期性（6小时）的向NameNode上报所有的块信息。 <figure class="highlight xml"><table><tr><td class="code"><pre><span class="line">hdfs-default.xml</span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.blockreport.intervalMsec<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>21600000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>Determines block reporting interval in milliseconds.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li>
<li>心跳是每3秒一次，心跳返回结果带有NameNode给该DataNode的命令如复制块数据到另一台机器，或删除某个数据块。 <figure class="highlight xml"><table><tr><td class="code"><pre><span class="line">hdfs-default.xml</span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.heartbeat.interval<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>3s<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span></span><br><span class="line">    Determines datanode heartbeat interval in seconds.</span><br><span class="line">    Can use the following suffix (case insensitive):</span><br><span class="line">    ms(millis), s(sec), m(min), h(hour), d(day)</span><br><span class="line">    to specify the time (such as 2s, 2m, 1h, etc.).</span><br><span class="line">    Or provide complete number in seconds (such as 30 for 30 seconds).</span><br><span class="line">    If no time unit is specified then seconds is assumed.</span><br><span class="line">  <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li>
<li>如果超过10分钟30秒没有收到某个DataNode的心跳，则认为该节点不可用。</li>
<li>集群运行中可以安全加入和退出一些机器。</li>
</ol>
<hr>
<h2 id="6-2-数据完整性"><a href="#6-2-数据完整性" class="headerlink" title="6.2 数据完整性"></a>6.2 数据完整性</h2><p>思考：如果电脑磁盘里面存储的数据是控制高铁信号灯的红灯信号（1）和绿灯信号（0），但是存储该数据的磁盘坏了，一直显示是绿灯，是否很危险？同理DataNode节点上的数据损坏了，却没有发现，是否也很危险，那么如何解决呢？如下是DataNode节点保证数据完整性的方法。</p>
<ol>
<li>当DataNode读取Block的时候，它会计算CheckSum。</li>
<li>如果计算后的CheckSum，与Block创建时值不一样，说明Block已经损坏。</li>
<li>Client读取其他DataNode上的Block。</li>
<li>常见的校验算法 crc（32），md5（128），sha1（160）</li>
<li>DataNode在其文件创建后周期验证CheckSum。</li>
</ol>
<hr>
<h2 id="6-3-掉线时限参数设置"><a href="#6-3-掉线时限参数设置" class="headerlink" title="6.3 掉线时限参数设置"></a>6.3 掉线时限参数设置</h2><p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16340157434025.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
<ol>
<li>需要注意的是hdfs-site.xml 配置文件中的heartbeat.recheck.interval的单位为毫秒，dfs.heartbeat.interval的单位为秒。 <figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.heartbeat.recheck-interval<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>300000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.heartbeat.interval<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>3<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li>
<li>效果如图<br> <img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16340162105064.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></li>
</ol>
<hr>
<h2 id="6-4-服役新节点"><a href="#6-4-服役新节点" class="headerlink" title="6.4 服役新节点"></a>6.4 服役新节点</h2><ol>
<li>需求：<br> 随着公司业务的增长，数据量越来越大，原有的数据节点的容量已经不能满足存储数据的需求，需要在原有集群基础上动态添加新的数据节点</li>
<li>环境准备<ol>
<li>在hadoop104主机上再克隆一台hadoop105主机</li>
<li>修改IP地址和主机名称</li>
<li>删除原来HDFS文件系统留存的文件（/opt/module/hadoop-3.1.3/data和logs）</li>
<li>source一下配置文件 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop105 hadoop-3.1.3]$ <span class="built_in">source</span> /etc/profile</span><br></pre></td></tr></table></figure></li>
</ol>
</li>
<li>具体步骤<ol>
<li>直接启动DataNode，即可关联到集群 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop105 hadoop-3.1.3]$ hdfs --daemon start datanode</span><br><span class="line">[atguigu@hadoop105 hadoop-3.1.3]$ yarn --daemon start nodemanager</span><br></pre></td></tr></table></figure></li>
<li>在hadoop105上上传文件 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop105 hadoop-3.1.3]$ hadoop fs -put /opt/module/hadoop-3.1.3/LICENSE.txt /</span><br></pre></td></tr></table></figure></li>
<li>如果数据不均衡，可以用命令实现集群的再平衡 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 sbin]$ ./start-balancer.sh</span><br><span class="line">starting balancer, logging to /opt/module/hadoop-3.1.3/logs/hadoop-atguigu-balancer-hadoop102.out</span><br><span class="line">Time Stamp               Iteration<span class="comment">#  Bytes Already Moved  Bytes Left To Move  Bytes Being Moved</span></span><br></pre></td></tr></table></figure></li>
</ol>
</li>
</ol>
<hr>
<h2 id="6-5-退役旧数据节点"><a href="#6-5-退役旧数据节点" class="headerlink" title="6.5 退役旧数据节点"></a>6.5 退役旧数据节点</h2><h3 id="6-5-1-添加白名单和黑名单"><a href="#6-5-1-添加白名单和黑名单" class="headerlink" title="6.5.1 添加白名单和黑名单"></a>6.5.1 添加白名单和黑名单</h3><ul>
<li>白名单和黑名单是hadoop管理集群主机的一种机制。</li>
<li>添加到白名单的主机节点，都允许访问NameNode，不在白名单的主机节点，都会被退出。</li>
<li>添加到黑名单的主机节点，不允许访问NameNode，会在数据迁移后退出。</li>
<li>实际情况下，白名单用于确定允许访问NameNode的DataNode节点，内容配置一般与workers文件内容一致。 黑名单用于在集群运行过程中退役DataNode节点。<br>配置白名单和黑名单的具体步骤如下：<ol>
<li>在NameNode节点的/opt/module/hadoop-3.1.3/etc/hadoop目录下分别创建whitelist 和blacklist文件，白名单whitelist添加节点内容，blacklist黑名单暂时为空。 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 hadoop]$ <span class="built_in">pwd</span></span><br><span class="line">/opt/module/hadoop-3.1.3/etc/hadoop</span><br><span class="line">[atguigu@hadoop001 hadoop]$ touch blacklist</span><br><span class="line">[atguigu@hadoop001 hadoop]$ touch whitelist</span><br><span class="line">[atguigu@hadoop001 hadoop]$ ll *list</span><br><span class="line">-rw-rw-r--. 1 atguigu atguigu 0 10月 12 18:50 blacklist</span><br><span class="line">-rw-rw-r--. 1 atguigu atguigu 0 10月 12 18:50 whitelist</span><br><span class="line">[atguigu@hadoop001 hadoop]$ vim whitelist</span><br><span class="line">[atguigu@hadoop001 hadoop]$ cat whitelist</span><br><span class="line">hadoop001</span><br><span class="line">hadoop002</span><br><span class="line">hadoop003</span><br><span class="line">hadoop004</span><br><span class="line">[atguigu@hadoop001 hadoop]$</span><br></pre></td></tr></table></figure></li>
<li>在hdfs-site.xml配置文件中增加dfs.hosts和 dfs.hosts.exclude配置参数 <figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 白名单 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.hosts<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/module/hadoop-3.1.3/etc/hadoop/whitelist<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 黑名单 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.hosts.exclude<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/module/hadoop-3.1.3/etc/hadoop/blacklist<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li>
<li>分发配置文件whitelist，blacklist，hdfs-site.xml (注意：004节点也要发一份) <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 hadoop]$ xsync /opt/module/hadoop-3.1.3/etc/hadoop/</span><br><span class="line">[atguigu@hadoop001 hadoop]$ scp -r * hadoop004:/opt/module/hadoop-3.1.3/etc/hadoop/</span><br></pre></td></tr></table></figure></li>
<li>重新启动集群(注意：105节点没有添加到workers，因此要单独起停) <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-3.1.3]$ stop-dfs.sh</span><br><span class="line">[atguigu@hadoop102 hadoop-3.1.3]$ start-dfs.sh</span><br><span class="line">[atguigu@hadoop105 hadoop-3.1.3]$ hdfs –daemon start datanode</span><br></pre></td></tr></table></figure></li>
<li>在web浏览器上查看目前正常工作的DN节点<br> <img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16340373303979.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></li>
</ol>
</li>
</ul>
<h3 id="6-5-2-黑名单退役"><a href="#6-5-2-黑名单退役" class="headerlink" title="6.5.2 黑名单退役"></a>6.5.2 黑名单退役</h3><ol>
<li>编辑/opt/module/hadoop-3.1.3/etc/hadoop目录下的blacklist文件 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 hadoop]$ vim blacklist</span><br><span class="line">[atguigu@hadoop001 hadoop]$ cat blacklist</span><br><span class="line">hadoop004</span><br><span class="line">[atguigu@hadoop001 hadoop]$</span><br></pre></td></tr></table></figure></li>
<li>分发blacklist到所有节点 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 hadoop]$ xsync /opt/module/hadoop-3.1.3/etc/hadoop/</span><br><span class="line">[atguigu@hadoop001 hadoop]$ scp -r * hadoop004:/opt/module/hadoop-3.1.3/etc/hadoop/</span><br></pre></td></tr></table></figure></li>
<li>刷新NameNode、刷新ResourceManager <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 hadoop]$ hdfs dfsadmin -refreshNodes</span><br><span class="line">Refresh nodes successful</span><br><span class="line">[atguigu@hadoop001 hadoop]$ yarn rmadmin -refreshNodes</span><br><span class="line">2021-10-12 19:20:20,896 INFO client.RMProxy: Connecting to ResourceManager at hadoop002/192.168.2.7:8033</span><br><span class="line">[atguigu@hadoop001 hadoop]$</span><br></pre></td></tr></table></figure></li>
<li>检查Web浏览器，退役节点的状态为decommission in progress（退役中），说明数据节点正在复制块到其他节点<img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16340378419483.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></li>
<li>等待退役节点状态为decommissioned（所有块已经复制完成），停止该节点及节点资源管理器。注意：如果副本数是3，服役的节点小于等于3，是不能退役成功的，需要修改副本数后才能退役<img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16340378878057.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></li>
<li>如果数据不均衡，可以用命令实现集群的再平衡 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 hadoop-3.1.3]$ sbin/start-balancer.sh</span><br></pre></td></tr></table></figure>
<blockquote>
<p>注意：不允许白名单和黑名单中同时出现同一个主机名称，既然使用了黑名单blacklist成功退役了hadoop105节点，因此要将白名单whitelist里面的hadoop105去掉。</p>
</blockquote>
</li>
</ol>
<hr>
<h2 id="6-6-DataNode多目录配置"><a href="#6-6-DataNode多目录配置" class="headerlink" title="6.6 DataNode多目录配置"></a>6.6 DataNode多目录配置</h2><ol>
<li>DataNode可以配置成多个目录，每个目录存储的数据不一样。即：数据不是副本</li>
<li>具体配置如下<ol>
<li>在hdfs-site.xml文件中添加如下内容 <figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.datanode.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>file://$&#123;hadoop.tmp.dir&#125;/dfs/data1,file://$&#123;hadoop.tmp.dir&#125;/dfs/data2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li>
<li>停止集群，删除三台节点的data和logs中所有数据。 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-3.1.3]$ rm -rf data/ logs/</span><br><span class="line">[atguigu@hadoop103 hadoop-3.1.3]$ rm -rf data/ logs/</span><br><span class="line">[atguigu@hadoop104 hadoop-3.1.3]$ rm -rf data/ logs/</span><br></pre></td></tr></table></figure></li>
<li>格式化集群并启动。 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-3.1.3]$ bin/hdfs namenode –format</span><br><span class="line">[atguigu@hadoop102 hadoop-3.1.3]$ sbin/start-dfs.sh</span><br></pre></td></tr></table></figure></li>
<li>查看结果 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 dfs]$ ll</span><br><span class="line">总用量 12</span><br><span class="line">drwx------. 3 atguigu atguigu 4096 4月   4 14:22 data1</span><br><span class="line">drwx------. 3 atguigu atguigu 4096 4月   4 14:22 data2</span><br><span class="line">drwxrwxr-x. 3 atguigu atguigu 4096 12月 11 08:03 name1</span><br><span class="line">drwxrwxr-x. 3 atguigu atguigu 4096 12月 11 08:03 name2</span><br></pre></td></tr></table></figure></li>
</ol>
</li>
</ol>
<hr>
<h2 id="问答"><a href="#问答" class="headerlink" title="问答"></a>问答</h2><h3 id="一、描述一下HDFS的数据写入流程"><a href="#一、描述一下HDFS的数据写入流程" class="headerlink" title="一、描述一下HDFS的数据写入流程"></a>一、描述一下HDFS的数据写入流程</h3><p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16340443194587.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
<ol>
<li>客户端调用DistributedFileSystem#create方法通过DfsClient请求NameNode上传文件</li>
<li>NameNode校验：权限，路径，文件名通过后在对应路径创建空文件，写入Edits操作记录，返回文件信息</li>
<li>DistributedFileSystem#create 返回值为FSDataOutputStream输出流，FSDataOutputStream#create方法会初始化DFSOutputStream和DataStreamer；</li>
<li>调用DFSOutputStream#addBlock请求NN申请新的Block</li>
<li>NN根据机架感知-网络拓扑计算节点距离返回合适的DN列表</li>
<li>客户端调用FSDataOutputStream.PositionCache#write -&gt; DFSOutputStream#writeChunk写入数据</li>
<li>读取满一个packet(128个chunk)放入DFSOutputStream#enqueueCurrentPacketFull</li>
<li>DataStreamer#setPipeline初始化pipeline 首先和最近的DN1建立连接，然后DN1传输给DN2,DN2传输给DN3</li>
<li>DataStreamer把packet从dataqueue移动到ackqueue</li>
<li>DN3校验后返回ack给DN2,DN2校验后返回ack给DN1,DN1校验后返回ack给客户端</li>
<li>ResponseProcessor.ResponseProcessor接收响应并把packet从ackqueue移除</li>
<li>出现异常后ackqueue会重新移动到dataqueue再次创建连接重新发送</li>
<li>全部packet发送完成后当前block上传结束</li>
<li>读取下一个block重复4-13步骤</li>
<li>全部读取完成通知NN上传结束</li>
</ol>
<ul>
<li>block(文件块): 类似Linux中常见文件系统的最小操作单位, HDFS也是如此, block就是HDFS操作文件的最小单元, 默认128MB/个 (逻辑上设置的大小)</li>
<li>chunk(校验块): 文件块(block)实际存储/校验的最小单位, 严谨说chunk分为数据域和校验域两个部分组成, 但一般情况大家指的是数据域<ul>
<li>数据域: 默认512字节, 比如你写513个字节数据, 它占据一个block (而此block内部是两个chunk组成) , 又称为chunk data</li>
<li>校验域: 固定4字节, 存放校验码, 它与数据域一一对应, 又称chunk checksum</li>
</ul>
</li>
<li>packet(数据包): HDFS各组件间数据传输的基本单位, 默认64KB, 类似网络传输中数据包的概念, 主要是 header + body/data 两个部分组成<ul>
<li>packet header : 存储传输数据过程中的一些基本信息(数据包长度/版本/标志位等), 变长</li>
<li>packet body: 存储实际传输的数据(chunk), 同样packet里实际分布的最小单位也是chunk</li>
</ul>
</li>
</ul>
<h3 id="二、描述一下HDFS的数据读取流程"><a href="#二、描述一下HDFS的数据读取流程" class="headerlink" title="二、描述一下HDFS的数据读取流程"></a>二、描述一下HDFS的数据读取流程</h3><p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16340854804776.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
<ol>
<li>客户端向namenode发RPC请求, 读取文件对应blocks的DN信息</li>
<li>namenode检查文件是否存在，如果存在则获取文件的元信息（按顺序返回BlockId以及对应所在的datanode列表）</li>
<li>客户端收到block对应的DN信息后选取一个网络访问最近的DN, 依次读取每个数据块(客户端这里有校验逻辑, 如果发现异常会汇报坏块)</li>
<li>客户端与DN建立socket连接，通过流以packet为单位传输对应的数据块, 客户端收到数据写入本地磁盘</li>
<li>依次传输剩下的数据块，直到整个文件读取完成, 最后关闭输入流</li>
</ol>
<h3 id="三、简述HDFS的架构，其中每个服务的作用"><a href="#三、简述HDFS的架构，其中每个服务的作用" class="headerlink" title="三、简述HDFS的架构，其中每个服务的作用"></a>三、简述HDFS的架构，其中每个服务的作用</h3><ol>
<li>NameNode: HDFS集群的管理者<ol>
<li>管理HDFS的命名空间</li>
<li>配置副本策略</li>
<li>维护元数据信息：文件基本属性，目录层级，文件-块的映射</li>
<li>处理客户端的读写请求</li>
</ol>
</li>
<li>DataNode：HDFS集群的数据存储节点<ol>
<li>存储Block数据</li>
<li>保证数据完整性</li>
<li>接受NameNode指令，执行实际操作</li>
<li>接受客户端请求读写Block</li>
</ol>
</li>
<li>SecondaryNameNode: NameNode的备份<ol>
<li>并非NameNode的热备。当NameNode挂掉的时候，它并不能马上替换NameNode并提供服务。</li>
<li>辅助NameNode，分担其工作量，比如定期合并Fsimage和Edits，并推送给NameNode</li>
<li>在紧急情况下，可辅助恢复NameNode。</li>
</ol>
</li>
<li>Client<ol>
<li>文件切分。文件上传HDFS的时候，Client将文件切分成一个一个的Block，然后进行上传</li>
<li>与NameNode交互，获取文件的位置信息</li>
<li>与DataNode交互，读取或者写入数据；</li>
<li>Client提供一些命令来管理HDFS，比如NameNode格式化</li>
<li>Client可以通过一些命令来访问HDFS，比如对HDFS增删查改操作</li>
</ol>
</li>
</ol>
<h3 id="四、HDFS中如何实现元数据的维护？"><a href="#四、HDFS中如何实现元数据的维护？" class="headerlink" title="四、HDFS中如何实现元数据的维护？"></a>四、HDFS中如何实现元数据的维护？</h3><ol>
<li>存储<ul>
<li>内存：保存全量元数据信息</li>
<li>磁盘：FsImage + Edits = 完整的元数据信息<ol>
<li>FsImage：NameNode内存中的元数据镜像</li>
<li>Edits：NameNode已经执行的的操作记录</li>
</ol>
</li>
</ul>
</li>
<li>NameNode启动操作<ol>
<li>加载FsImage到内存</li>
<li>在内存中根据seen_txid执行Edits记录生成完整元数据</li>
<li>内存中完整的元数据信息写入新的FsImage</li>
<li>生成新的Edits</li>
</ol>
</li>
<li>SecondaryNameNode定期合并<ol>
<li>触发合并时机<ol>
<li>默认配置一小时合并一次</li>
<li>每分钟检查操作次数，到达100W条，立即触发一次</li>
</ol>
</li>
<li>合并流程 <ol>
<li>SecondaryNameNode请求NameNode进行合并</li>
<li>NameNode停止使用当前的Edits文件，生成新的Edits文件继续执行操作</li>
<li>SecondaryNameNode拉取FsImage和Edits，加载到内存总合并，生成新的FsImage.checkpoint</li>
<li>SecondaryNameNode推送FsImage.checkpoint到NameNode</li>
<li>NameNode重命名FsImage.checkpoint为FsImage，合并流程结束</li>
</ol>
</li>
</ol>
</li>
<li>正常运行<ol>
<li>client对数据进行操作时，将这些操作记录到Edits</li>
</ol>
</li>
</ol>
<h3 id="五、2NN如何对NN的元数据进行合并？"><a href="#五、2NN如何对NN的元数据进行合并？" class="headerlink" title="五、2NN如何对NN的元数据进行合并？"></a>五、2NN如何对NN的元数据进行合并？</h3><ol>
<li>触发合并时机<ol>
<li>默认配置一小时合并一次</li>
<li>每分钟检查操作次数，到达100W条，立即触发一次</li>
</ol>
</li>
<li>合并流程 <ol>
<li>SecondaryNameNode请求NameNode进行合并</li>
<li>NameNode停止使用当前的Edits文件，生成新的Edits文件继续执行操作</li>
<li>SecondaryNameNode拉取FsImage和Edits，加载到内存总合并，生成新的FsImage.checkpoint</li>
<li>SecondaryNameNode推送FsImage.checkpoint到NameNode</li>
<li>NameNode重命名FsImage.checkpoint为FsImage，合并流程结束</li>
</ol>
</li>
</ol>
<h3 id="六、假如NN的数据发生的丢失，依靠2NN恢复完全靠谱？"><a href="#六、假如NN的数据发生的丢失，依靠2NN恢复完全靠谱？" class="headerlink" title="六、假如NN的数据发生的丢失，依靠2NN恢复完全靠谱？"></a>六、假如NN的数据发生的丢失，依靠2NN恢复完全靠谱？</h3><ol>
<li>可能会造成部分数据丢失原因如下：<ol>
<li>NN数据发生丢之前，完整元数据保存在内存中，在磁盘中Fsimage+edits=完整元数据</li>
<li>如果edits包含操作记录，且未合并到Fsimage，2NN中的Fsimage和NN的Fsimage相同</li>
<li>如果丢失edits，使用2NN的Fsimage恢复NN的Fsimage，会丢失NN的Edits部分的数据</li>
</ol>
</li>
</ol>
<h3 id="七、HDFS集群的安全模式有了解吗？"><a href="#七、HDFS集群的安全模式有了解吗？" class="headerlink" title="七、HDFS集群的安全模式有了解吗？"></a>七、HDFS集群的安全模式有了解吗？</h3><ol>
<li>HDFS集群的安全模式是对数据完整性的一种保护措施</li>
<li>NameNode启动进入安全模式，对客户端保持只读<ol>
<li>加载磁盘Fsimage到内存</li>
<li>执行edits操作记录，生成完整的元数据信息</li>
<li>生成新的edits</li>
<li>内存中的元数据信息，生成新的Fsimage</li>
<li>开始监听DateNode请求</li>
</ol>
</li>
<li>DataNode启动<ol>
<li>向NameNode注册，加入集群</li>
<li>注册成功向NameNode发送Block数据块信息</li>
</ol>
</li>
<li>NameNode接收到足够多的Block数据块位置信息退出安全模式<ol>
<li>满足“最小副本条件”， NameNode会在30秒钟之后就退出安全模式<ol>
<li>最小副本条件指的是在整个文件系统中99.9%的块满足最小副本级别（默认值：dfs.replication.min=1）</li>
</ol>
</li>
</ol>
</li>
<li>clien正常读写</li>
</ol>
<h3 id="八、如何在合理利用HDFS安全模式的操作命令完成一些工作？"><a href="#八、如何在合理利用HDFS安全模式的操作命令完成一些工作？" class="headerlink" title="八、如何在合理利用HDFS安全模式的操作命令完成一些工作？"></a>八、如何在合理利用HDFS安全模式的操作命令完成一些工作？</h3><ol>
<li>利用安全模式的wait命令实现等待集群恢复后自动执行某些操作</li>
<li><code>hdfs dfsadmin -safemode wait</code></li>
</ol>
<h3 id="九、描述一下NN和DN的关系，以及DN的工作流程"><a href="#九、描述一下NN和DN的关系，以及DN的工作流程" class="headerlink" title="九、描述一下NN和DN的关系，以及DN的工作流程"></a>九、描述一下NN和DN的关系，以及DN的工作流程</h3><ol>
<li>NN负责管理集群，维护元数据信息，接收客户端请求</li>
<li>DN负责保存Block数据块信息，保证数据完整性，接收客户端读写block请求</li>
<li>DataNode启动后会向NameNode注册，注册成功后，发送Block数据块的信息</li>
<li>DataNode运行过程中会周期性（6小时，hdfs-default.xml&gt;dfs.blockreport.intervalMsecp配置）的向NameNode上报Block数据块的信息</li>
<li>DataNode运行过程中每3秒（dfs.heartbeat.interval）会向NameNode发送心跳，并带回NameNode的命令</li>
<li>NameNode超过配置时间（2<em>心跳重新检测间隔默认五分钟+10</em>心跳间隔默认三秒=10分钟30秒）未收到心跳会下线DataNode</li>
<li>服役新节点通过配置白名单和works文件</li>
<li>退役节点通过配置黑名单，NameNode执行刷新集群，退役节点变成退役中（移动数据块），移动完成，变成退役状态</li>
</ol>
]]></content>
      <categories>
        <category>HDFS</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>HDFS</tag>
      </tags>
  </entry>
  <entry>
    <title>HBase</title>
    <url>/2021/11/07/HBase/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="HBase"><a href="#HBase" class="headerlink" title="HBase"></a>HBase</h1><h1 id="第1章-HBase简介"><a href="#第1章-HBase简介" class="headerlink" title="第1章 HBase简介"></a>第1章 HBase简介</h1><h2 id="1-1-HBase定义"><a href="#1-1-HBase定义" class="headerlink" title="1.1 HBase定义"></a>1.1 HBase定义</h2><p>HBase是一种分布式、可扩展、支持海量数据存储的NoSQL数据库。</p>
<ul>
<li>HBase是一种面向列簇存储的非关系型数据库。</li>
<li>用于存储结构化和非结构化的数据，适用于单表非关系型数据的存储，不适合做关联查询，类似JOIN等操作。</li>
<li>基于HDFS，数据持久化存储的体现形式是HFile，存放于DataNode中，被ResionServer以region的形式进行管理。</li>
<li>延迟较低，接入在线业务使用，面对大量的企业数据，HBase可以实现单表大量数据的存储，同时提供了高效的数据访问速度。</li>
</ul>
<h2 id="1-2-HBase数据模型"><a href="#1-2-HBase数据模型" class="headerlink" title="1.2 HBase数据模型"></a>1.2 HBase数据模型</h2><p>逻辑上，HBase的数据模型同关系型数据库很类似，数据存储在一张表中，有行有列。但从HBase的底层物理存储结构（K-V）来看，HBase更像是一个multi-dimensional map。</p>
<h3 id="1-2-1-HBase逻辑结构"><a href="#1-2-1-HBase逻辑结构" class="headerlink" title="1.2.1 HBase逻辑结构"></a>1.2.1 HBase逻辑结构</h3><p><img src="https://i.loli.net/2021/11/07/MIeQBJnp3RSNElH.jpg"></p>
<h3 id="1-2-2HBase物理存储结构"><a href="#1-2-2HBase物理存储结构" class="headerlink" title="1.2.2HBase物理存储结构"></a>1.2.2HBase物理存储结构</h3><p><img src="https://i.loli.net/2021/11/07/BbljIXvo31RaptL.jpg"></p>
<h3 id="1-2-3-数据模型"><a href="#1-2-3-数据模型" class="headerlink" title="1.2.3 数据模型"></a>1.2.3 数据模型</h3><ol>
<li>Name Space<ul>
<li>命名空间，类似于关系型数据库的DatabBase概念，每个命名空间下有多个表。HBase有两个自带的命名空间，分别是hbase和default，hbase中存放的是HBase内置的表，default表是用户默认使用的命名空间。</li>
</ul>
</li>
<li>Region<ul>
<li>类似于关系型数据库的表概念。不同的是，HBase定义表时只需要声明列族即可，不需要声明具体的列。这意味着，往HBase写入数据时，字段可以动态、按需指定。因此，和关系型数据库相比，HBase能够轻松应对字段变更的场景。</li>
</ul>
</li>
<li>Row<ul>
<li>HBase表中的每行数据都由一个RowKey和多个Column（列）组成，数据是按照RowKey的字典顺序存储的，并且查询数据时只能根据RowKey进行检索，所以RowKey的设计十分重要。</li>
</ul>
</li>
<li>Row Key<ul>
<li>Rowkey 的概念和 mysql 中的主键类似，Hbase 使用 Rowkey 来唯一的区分某一行的数据。Hbase只支持3种查询方式： 1、基于Rowkey的单行查询，2、基于Rowkey的范围扫描 ，3、全表扫描</li>
<li>因此，Rowkey对Hbase的性能影响非常大。设计的时候要兼顾基于Rowkey的单行查询也要键入Rowkey的范围扫描。</li>
<li>Rowkey 行键可以是任意字符串(最大长度是64KB，实际应用中长度一般为 10-100bytes)，最好是16。在HBase 内部，Rowkey 保存为字节数组。HBase会对表中的数据按照 Rowkey 字典序排序</li>
</ul>
</li>
<li>Column Family（列簇）<ul>
<li>Hbase 通过列簇划分数据的存储，列簇下面可以包含任意多的列，实现灵活的数据存取。列簇是由一个一个的列组成（任意多），在列数据为空的情况下，不会占用存储空间。</li>
<li>Hbase 创建表的时候必须指定列簇。就像关系型数据库创建的时候必须指定具体的列是一样的。</li>
<li>Hbase的列簇不是越多越好，官方推荐的是列簇最好小于或者等于3。一般是1个列簇。</li>
<li>新的列簇成员（列）可以随后动态加入，Family下面可以有多个Qualifier，所以可以简单的理解为，HBase中的列是二级列，也就是说Family是第一级列，Qualifier是第二级列。</li>
<li>权限控制、存储以及调优都是在列簇层面进行的；</li>
<li>HBase把同一列簇里面的数据存储在同一目录下，由几个文件保存。</li>
</ul>
</li>
<li>Column<ul>
<li>HBase中的每个列都由Column Family(列族)和Column Qualifier（列限定符）进行限定，例如info：name，info：age。建表时，只需指明列族，而列限定符无需预先定义。</li>
</ul>
</li>
<li>Time Stamp<ul>
<li>用于标识数据的不同版本（version），每条数据写入时，如果不指定时间戳，系统会自动为其加上该字段，其值为写入HBase的时间。</li>
</ul>
</li>
<li>Cell<ul>
<li>由{rowkey, column Family：column Qualifier, time Stamp} 唯一确定的单元。cell中的数据是没有类型的，全部是字节数组形式存贮。</li>
</ul>
</li>
</ol>
<h2 id="1-3-HBase基本架构"><a href="#1-3-HBase基本架构" class="headerlink" title="1.3 HBase基本架构"></a>1.3 HBase基本架构</h2><p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16359030360669.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
<ol>
<li>Region Server<ul>
<li>Region Server为 Region的管理者，其实现类为HRegionServer，主要作用如下:<ul>
<li>对于数据的操作：get, put, delete；</li>
<li>对于Region的操作：splitRegion、compactRegion。</li>
</ul>
</li>
</ul>
</li>
<li>Master<ul>
<li>Master是所有Region Server的管理者，其实现类为HMaster，主要作用如下：<ul>
<li>对于表的操作：create, delete, alter</li>
<li>对于RegionServer的操作：分配regions到每个RegionServer，监控每个RegionServer的状态，负载均衡和故障转移。</li>
</ul>
</li>
</ul>
</li>
<li>Zookeeper<ul>
<li>HBase通过Zookeeper来做Master的高可用、RegionServer的监控、元数据的入口以及集群配置的维护等工作。</li>
</ul>
</li>
<li>HDFS<ul>
<li>HDFS为HBase提供最终的底层数据存储服务，同时为HBase提供高可用的支持。</li>
</ul>
</li>
</ol>
<h1 id="第2章-HBase快速入门"><a href="#第2章-HBase快速入门" class="headerlink" title="第2章 HBase快速入门"></a>第2章 HBase快速入门</h1><h2 id="2-1-HBase安装部署"><a href="#2-1-HBase安装部署" class="headerlink" title="2.1 HBase安装部署"></a>2.1 HBase安装部署</h2><h3 id="2-1-1-Zookeeper正常部署"><a href="#2-1-1-Zookeeper正常部署" class="headerlink" title="2.1.1 Zookeeper正常部署"></a>2.1.1 Zookeeper正常部署</h3><ul>
<li>首先保证Zookeeper集群的正常部署，并启动之：  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 zookeeper-3.5.7]$ bin/zkServer.sh start</span><br><span class="line">[atguigu@hadoop103 zookeeper-3.5.7]$ bin/zkServer.sh start</span><br><span class="line">[atguigu@hadoop104 zookeeper-3.5.7]$ bin/zkServer.sh start</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="2-1-2-Hadoop正常部署"><a href="#2-1-2-Hadoop正常部署" class="headerlink" title="2.1.2 Hadoop正常部署"></a>2.1.2 Hadoop正常部署</h3><ul>
<li>Hadoop集群的正常部署并启动：  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 hadoop-3.1.3]$ sbin/start-dfs.sh</span><br><span class="line">[atguigu@hadoop103 hadoop-3.1.3]$ sbin/start-yarn.sh</span><br></pre></td></tr></table></figure></li>
<li>集群启动异常<ol>
<li>时间同步 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ntpdate ntp.aliyun.com</span><br></pre></td></tr></table></figure></li>
<li>hdfs坏块 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">        </span><br></pre></td></tr></table></figure></li>
</ol>
</li>
</ul>
<h3 id="2-1-3-HBase的解压"><a href="#2-1-3-HBase的解压" class="headerlink" title="2.1.3 HBase的解压"></a>2.1.3 HBase的解压</h3><ol>
<li>解压Hbase到指定目录： <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 software]$ tar -zxvf hbase-2.2.4-bin.tar.gz -C /opt/module</span><br><span class="line">[atguigu@hadoop102 software]$ mv /opt/module/hbase-2.2.4 /opt/module/hbase</span><br></pre></td></tr></table></figure>
</li>
<li>配置环境变量 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 ~]$ sudo vim /etc/profile.d/my_env.sh</span><br><span class="line"><span class="comment"># 添加</span></span><br><span class="line"><span class="comment">#HBASE_HOME</span></span><br><span class="line"><span class="built_in">export</span> HBASE_HOME=/opt/module/hbase</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$HBASE_HOME</span>/bin</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="2-1-4-HBase的配置文件"><a href="#2-1-4-HBase的配置文件" class="headerlink" title="2.1.4 HBase的配置文件"></a>2.1.4 HBase的配置文件</h3><p>修改HBase对应的配置文件。</p>
<ol>
<li>hbase-env.sh修改内容： <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">export</span> HBASE_MANAGES_ZK=<span class="literal">false</span></span><br></pre></td></tr></table></figure></li>
<li>hbase-site.xml修改内容： <figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.rootdir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://hadoop102:8020/hbase<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.cluster.distributed<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.zookeeper.quorum<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop102,hadoop103,hadoop104<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.unsafe.stream.capability.enforce<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    </span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.wal.provider<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>filesystem<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure></li>
<li>regionservers： <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hadoop102</span><br><span class="line">hadoop103</span><br><span class="line">hadoop104</span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="2-1-5-HBase远程发送到其他集群"><a href="#2-1-5-HBase远程发送到其他集群" class="headerlink" title="2.1.5 HBase远程发送到其他集群"></a>2.1.5 HBase远程发送到其他集群</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 module]$ xsync hbase/</span><br></pre></td></tr></table></figure>

<h3 id="2-1-6-HBase服务的启动"><a href="#2-1-6-HBase服务的启动" class="headerlink" title="2.1.6 HBase服务的启动"></a>2.1.6 HBase服务的启动</h3><ol>
<li>启动方式1 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 hbase]$ bin/hbase-daemon.sh start master</span><br><span class="line">[atguigu@hadoop102 hbase]$ bin/hbase-daemon.sh start regionserver</span><br></pre></td></tr></table></figure>
<blockquote>
<p>提示：如果集群之间的节点时间不同步，会导致regionserver无法启动，抛出ClockOutOfSyncException异常。<br> 修复提示：</p>
</blockquote>
<ul>
<li>a、同步时间服务<ul>
<li>请参看帮助文档：《尚硅谷大数据技术之Hadoop入门》</li>
</ul>
</li>
<li>b、属性：hbase.master.maxclockskew设置更大的值  <figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.master.maxclockskew<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>180000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>Time difference of regionserver from master<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>启动方式2 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">启动服务：</span><br><span class="line">[atguigu@hadoop102 hbase]$ bin/start-hbase.sh</span><br><span class="line">停止服务：</span><br><span class="line">[atguigu@hadoop102 hbase]$ bin/stop-hbase.sh</span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="2-1-7-查看HBase页面"><a href="#2-1-7-查看HBase页面" class="headerlink" title="2.1.7 查看HBase页面"></a>2.1.7 查看HBase页面</h3><p>启动成功后，可以通过“host:port”的方式来访问HBase管理页面，例如：<br><a href="http://hadoop001:16010/">http://hadoop001:16010</a> </p>
<h3 id="2-1-8高可用-可选"><a href="#2-1-8高可用-可选" class="headerlink" title="2.1.8高可用(可选)"></a>2.1.8高可用(可选)</h3><p>在HBase中HMaster负责监控HRegionServer的生命周期，均衡RegionServer的负载，如果HMaster挂掉了，那么整个HBase集群将陷入不健康的状态，并且此时的工作状态并不会维持太久。所以HBase支持对HMaster的高可用配置。</p>
<ol>
<li>关闭HBase集群（如果没有开启则跳过此步） <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 hbase]$ bin/stop-hbase.sh</span><br></pre></td></tr></table></figure></li>
<li>在conf目录下创建backup-masters文件 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 hbase]$ touch conf/backup-masters</span><br></pre></td></tr></table></figure></li>
<li>在backup-masters文件中配置高可用HMaster节点 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 hbase]$ <span class="built_in">echo</span> hadoop103 &gt; conf/backup-masters</span><br></pre></td></tr></table></figure></li>
<li>将整个conf目录scp到其他节点 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 hbase]$ scp -r conf/ hadoop103:/opt/module/hbase/</span><br><span class="line">[atguigu@hadoop102 hbase]$ scp -r conf/ hadoop104:/opt/module/hbase/</span><br></pre></td></tr></table></figure></li>
<li>打开页面测试查看<br> <a href="http://hadooo102:16010/">http://hadooo102:16010</a> </li>
</ol>
<h2 id="2-2-HBase-Shell操作"><a href="#2-2-HBase-Shell操作" class="headerlink" title="2.2 HBase Shell操作"></a>2.2 HBase Shell操作</h2><h3 id="2-2-1-基本操作"><a href="#2-2-1-基本操作" class="headerlink" title="2.2.1 基本操作"></a>2.2.1 基本操作</h3><ol>
<li>进入HBase客户端命令行 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 hbase]$ bin/hbase shell</span><br></pre></td></tr></table></figure></li>
<li>查看帮助命令 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hbase(main):001:0&gt; <span class="built_in">help</span></span><br></pre></td></tr></table></figure></li>
<li>查看当前数据库中有哪些表 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hbase(main):002:0&gt; list</span><br></pre></td></tr></table></figure></li>
<li>查看命名空间 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hbase(main):004:0&gt; list_namespace</span><br><span class="line">NAMESPACE</span><br><span class="line">api_test</span><br><span class="line">default</span><br><span class="line">hbase</span><br><span class="line">3 row(s)</span><br><span class="line">Took 0.0494 seconds</span><br><span class="line">hbase(main):005:0&gt;</span><br></pre></td></tr></table></figure></li>
<li>创建命名空间 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hbase(main):005:0&gt; create_namespace <span class="string">&#x27;test&#x27;</span></span><br><span class="line">Took 0.2427 seconds</span><br><span class="line">hbase(main):006:0&gt; list_namespace</span><br><span class="line">NAMESPACE</span><br><span class="line">api_test</span><br><span class="line">default</span><br><span class="line">hbase</span><br><span class="line"><span class="built_in">test</span></span><br><span class="line">4 row(s)</span><br><span class="line">Took 0.0081 seconds</span><br><span class="line">hbase(main):007:0&gt;</span><br></pre></td></tr></table></figure></li>
<li>删除命名空间: 必须是空的 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hbase(main):007:0&gt; drop_namespace <span class="string">&#x27;test&#x27;</span></span><br><span class="line">Took 0.2408 seconds</span><br><span class="line">hbase(main):008:0&gt; list_namespace</span><br><span class="line">NAMESPACE</span><br><span class="line">api_test</span><br><span class="line">default</span><br><span class="line">hbase</span><br><span class="line">3 row(s)</span><br><span class="line">Took 0.0102 seconds</span><br><span class="line">hbase(main):009:0&gt;</span><br></pre></td></tr></table></figure></li>
<li>查看命名空间中的表格 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hbase(main):011:0&gt; list_namespace_tables <span class="string">&#x27;hbase&#x27;</span></span><br><span class="line">TABLE</span><br><span class="line">meta</span><br><span class="line">namespace</span><br><span class="line">2 row(s)</span><br><span class="line">Took 0.0051 seconds</span><br><span class="line">=&gt; [<span class="string">&quot;meta&quot;</span>, <span class="string">&quot;namespace&quot;</span>]</span><br><span class="line">hbase(main):012:0&gt;</span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="2-2-2-表的操作"><a href="#2-2-2-表的操作" class="headerlink" title="2.2.2 表的操作"></a>2.2.2 表的操作</h3><ol>
<li>创建表 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hbase(main):002:0&gt; create <span class="string">&#x27;student&#x27;</span>,<span class="string">&#x27;info&#x27;</span></span><br></pre></td></tr></table></figure></li>
<li>插入数据到表 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hbase(main):003:0&gt; put <span class="string">&#x27;student&#x27;</span>,<span class="string">&#x27;1001&#x27;</span>,<span class="string">&#x27;info:sex&#x27;</span>,<span class="string">&#x27;male&#x27;</span></span><br><span class="line">hbase(main):004:0&gt; put <span class="string">&#x27;student&#x27;</span>,<span class="string">&#x27;1001&#x27;</span>,<span class="string">&#x27;info:age&#x27;</span>,<span class="string">&#x27;18&#x27;</span></span><br><span class="line">hbase(main):005:0&gt; put <span class="string">&#x27;student&#x27;</span>,<span class="string">&#x27;1002&#x27;</span>,<span class="string">&#x27;info:name&#x27;</span>,<span class="string">&#x27;Janna&#x27;</span></span><br><span class="line">hbase(main):006:0&gt; put <span class="string">&#x27;student&#x27;</span>,<span class="string">&#x27;1002&#x27;</span>,<span class="string">&#x27;info:sex&#x27;</span>,<span class="string">&#x27;female&#x27;</span></span><br><span class="line">hbase(main):007:0&gt; put <span class="string">&#x27;student&#x27;</span>,<span class="string">&#x27;1002&#x27;</span>,<span class="string">&#x27;info:age&#x27;</span>,<span class="string">&#x27;20&#x27;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
<li>扫描查看表数据 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hbase(main):008:0&gt; scan <span class="string">&#x27;student&#x27;</span></span><br><span class="line">hbase(main):009:0&gt; scan <span class="string">&#x27;student&#x27;</span>,&#123;STARTROW =&gt; <span class="string">&#x27;1001&#x27;</span>, STOPROW  =&gt; <span class="string">&#x27;1001&#x27;</span>&#125;</span><br><span class="line">hbase(main):010:0&gt; scan <span class="string">&#x27;student&#x27;</span>,&#123;STARTROW =&gt; <span class="string">&#x27;1001&#x27;</span>&#125;</span><br></pre></td></tr></table></figure></li>
<li>查看表结构 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hbase(main):011:0&gt; describe <span class="string">&#x27;student&#x27;</span></span><br></pre></td></tr></table></figure></li>
<li>更新指定字段的数据 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hbase(main):012:0&gt; put <span class="string">&#x27;student&#x27;</span>,<span class="string">&#x27;1001&#x27;</span>,<span class="string">&#x27;info:name&#x27;</span>,<span class="string">&#x27;Nick&#x27;</span></span><br><span class="line">hbase(main):013:0&gt; put <span class="string">&#x27;student&#x27;</span>,<span class="string">&#x27;1001&#x27;</span>,<span class="string">&#x27;info:age&#x27;</span>,<span class="string">&#x27;100&#x27;</span></span><br></pre></td></tr></table></figure></li>
<li>查看“指定行”或“指定列族:列”的数据 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hbase(main):014:0&gt; get <span class="string">&#x27;student&#x27;</span>,<span class="string">&#x27;1001&#x27;</span></span><br><span class="line">hbase(main):015:0&gt; get <span class="string">&#x27;student&#x27;</span>,<span class="string">&#x27;1001&#x27;</span>,<span class="string">&#x27;info:name&#x27;</span></span><br></pre></td></tr></table></figure></li>
<li>统计表数据行数 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hbase(main):021:0&gt; count <span class="string">&#x27;student&#x27;</span></span><br></pre></td></tr></table></figure></li>
<li>删除数据<ul>
<li>删除某rowkey的全部数据：  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hbase(main):016:0&gt; deleteall <span class="string">&#x27;student&#x27;</span>,<span class="string">&#x27;1001&#x27;</span></span><br></pre></td></tr></table></figure></li>
<li>删除某rowkey的某一列数据：  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hbase(main):017:0&gt; delete <span class="string">&#x27;student&#x27;</span>,<span class="string">&#x27;1002&#x27;</span>,<span class="string">&#x27;info:sex&#x27;</span></span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>清空表数据 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hbase(main):018:0&gt; truncate <span class="string">&#x27;student&#x27;</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>提示：清空表的操作顺序为先disable，然后再truncate。</p>
</blockquote>
</li>
<li>删除表<ul>
<li>首先需要先让该表为disable状态：  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hbase(main):019:0&gt; <span class="built_in">disable</span> <span class="string">&#x27;student&#x27;</span></span><br></pre></td></tr></table></figure></li>
<li>然后才能drop这个表：  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hbase(main):020:0&gt; drop <span class="string">&#x27;student&#x27;</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>提示：如果直接drop表，会报错：ERROR: Table student is enabled. Disable it first.</p>
</blockquote>
</li>
</ul>
</li>
<li>变更表信息<ul>
<li>将info列族中的数据存放3个版本：  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hbase(main):022:0&gt; alter <span class="string">&#x27;student&#x27;</span>,&#123;NAME=&gt;<span class="string">&#x27;info&#x27;</span>,VERSIONS=&gt;3&#125;</span><br><span class="line">hbase(main):022:0&gt; get <span class="string">&#x27;student&#x27;</span>,<span class="string">&#x27;1001&#x27;</span>,&#123;COLUMN=&gt;<span class="string">&#x27;info:name&#x27;</span>,VERSIONS=&gt;3&#125;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ol>
<h1 id="第3章-HBase-API"><a href="#第3章-HBase-API" class="headerlink" title="第3章 HBase API"></a>第3章 HBase API</h1><h2 id="3-1-DDL"><a href="#3-1-DDL" class="headerlink" title="3.1 DDL"></a>3.1 DDL</h2><h3 id="3-1-1-创建命名空间"><a href="#3-1-1-创建命名空间" class="headerlink" title="3.1.1 创建命名空间"></a>3.1.1 创建命名空间</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">create_namespace</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    Configuration configuration = HBaseConfiguration.create();</span><br><span class="line">    configuration.set(<span class="string">&quot;hbase.zookeeper.quorum&quot;</span>, <span class="string">&quot;hadoop001,hadoop002,hadoop003,hadoop004,hadoop005&quot;</span>);</span><br><span class="line">    <span class="keyword">try</span> (</span><br><span class="line">            Connection connection = ConnectionFactory.createConnection(configuration);</span><br><span class="line">            Admin admin = connection.getAdmin();</span><br><span class="line"></span><br><span class="line">    ) &#123;</span><br><span class="line">        <span class="keyword">final</span> String namespace = <span class="string">&quot;api_test&quot;</span>;</span><br><span class="line">        NamespaceDescriptor.Builder builder = NamespaceDescriptor.create(namespace);</span><br><span class="line">        admin.createNamespace(builder.build());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="3-1-2-查看命名空间"><a href="#3-1-2-查看命名空间" class="headerlink" title="3.1.2 查看命名空间"></a>3.1.2 查看命名空间</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">list_namespace</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    Configuration configuration = HBaseConfiguration.create();</span><br><span class="line">    configuration.set(<span class="string">&quot;hbase.zookeeper.quorum&quot;</span>, <span class="string">&quot;hadoop001,hadoop002,hadoop003,hadoop004,hadoop005&quot;</span>);</span><br><span class="line">    <span class="keyword">try</span> (</span><br><span class="line">            Connection connection = ConnectionFactory.createConnection(configuration);</span><br><span class="line">            Admin admin = connection.getAdmin();</span><br><span class="line"></span><br><span class="line">    ) &#123;</span><br><span class="line">        NamespaceDescriptor[] namespaceDescriptors = admin.listNamespaceDescriptors();</span><br><span class="line">        <span class="keyword">for</span> (NamespaceDescriptor namespaceDescriptor : namespaceDescriptors) &#123;</span><br><span class="line">            System.out.println(namespaceDescriptor.getName());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="3-1-3-删除命名空间"><a href="#3-1-3-删除命名空间" class="headerlink" title="3.1.3 删除命名空间"></a>3.1.3 删除命名空间</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">drop_namespace</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    Configuration configuration = HBaseConfiguration.create();</span><br><span class="line">    configuration.set(<span class="string">&quot;hbase.zookeeper.quorum&quot;</span>, <span class="string">&quot;hadoop001,hadoop002,hadoop003,hadoop004,hadoop005&quot;</span>);</span><br><span class="line">    <span class="keyword">try</span> (</span><br><span class="line">            Connection connection = ConnectionFactory.createConnection(configuration);</span><br><span class="line">            Admin admin = connection.getAdmin();</span><br><span class="line"></span><br><span class="line">    ) &#123;</span><br><span class="line">        admin.deleteNamespace(<span class="string">&quot;api_test&quot;</span>);</span><br><span class="line">        NamespaceDescriptor[] namespaceDescriptors = admin.listNamespaceDescriptors();</span><br><span class="line">        <span class="keyword">for</span> (NamespaceDescriptor namespaceDescriptor : namespaceDescriptors) &#123;</span><br><span class="line">            System.out.println(namespaceDescriptor);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


<h3 id="3-1-4-判断表是否存在"><a href="#3-1-4-判断表是否存在" class="headerlink" title="3.1.4 判断表是否存在"></a>3.1.4 判断表是否存在</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">tableExists</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    Configuration configuration = HBaseConfiguration.create();</span><br><span class="line">    configuration.set(<span class="string">&quot;hbase.zookeeper.quorum&quot;</span>, <span class="string">&quot;hadoop001,hadoop002,hadoop003,hadoop004,hadoop005&quot;</span>);</span><br><span class="line">    <span class="keyword">try</span> (</span><br><span class="line">            Connection connection = ConnectionFactory.createConnection(configuration);</span><br><span class="line">            Admin admin = connection.getAdmin();</span><br><span class="line"></span><br><span class="line">    ) &#123;</span><br><span class="line">        TableName tableName = TableName.valueOf(<span class="string">&quot;api_test_table&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">final</span> <span class="keyword">boolean</span> b = admin.tableExists(tableName);</span><br><span class="line">        System.out.println(b);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="3-1-5-创建表"><a href="#3-1-5-创建表" class="headerlink" title="3.1.5 创建表"></a>3.1.5 创建表</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">create_table</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    Configuration configuration = HBaseConfiguration.create();</span><br><span class="line">    configuration.set(<span class="string">&quot;hbase.zookeeper.quorum&quot;</span>, <span class="string">&quot;hadoop001,hadoop002,hadoop003,hadoop004,hadoop005&quot;</span>);</span><br><span class="line">    <span class="keyword">try</span> (</span><br><span class="line">            Connection connection = ConnectionFactory.createConnection(configuration);</span><br><span class="line">            Admin admin = connection.getAdmin();</span><br><span class="line"></span><br><span class="line">    ) &#123;</span><br><span class="line">        TableName tableName = TableName.valueOf(<span class="string">&quot;api_test_table&quot;</span>);</span><br><span class="line">        TableDescriptorBuilder tableDescriptorBuilder = TableDescriptorBuilder.newBuilder(tableName);</span><br><span class="line">        ColumnFamilyDescriptorBuilder columnFamilyDescriptorBuilder = ColumnFamilyDescriptorBuilder.newBuilder(<span class="string">&quot;info&quot;</span>.getBytes(StandardCharsets.UTF_8));</span><br><span class="line">        columnFamilyDescriptorBuilder.setMaxVersions(<span class="number">3</span>);</span><br><span class="line">        tableDescriptorBuilder.setColumnFamily(columnFamilyDescriptorBuilder.build());</span><br><span class="line"></span><br><span class="line">        admin.createTable(tableDescriptorBuilder.build());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="3-1-6-删除表"><a href="#3-1-6-删除表" class="headerlink" title="3.1.6 删除表"></a>3.1.6 删除表</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">drop_table</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    Configuration configuration = HBaseConfiguration.create();</span><br><span class="line">    configuration.set(<span class="string">&quot;hbase.zookeeper.quorum&quot;</span>, <span class="string">&quot;hadoop001,hadoop002,hadoop003,hadoop004,hadoop005&quot;</span>);</span><br><span class="line">    <span class="keyword">try</span> (</span><br><span class="line">            Connection connection = ConnectionFactory.createConnection(configuration);</span><br><span class="line">            Admin admin = connection.getAdmin();</span><br><span class="line"></span><br><span class="line">    ) &#123;</span><br><span class="line">        TableName tableName = TableName.valueOf(<span class="string">&quot;api_test_table&quot;</span>);</span><br><span class="line">        admin.disableTable(tableName);</span><br><span class="line">        admin.deleteTable(tableName);</span><br><span class="line">        TableName[] tableNames = admin.listTableNames();</span><br><span class="line">        <span class="keyword">for</span> (TableName tableName1 : tableNames) &#123;</span><br><span class="line">            System.out.println(tableName1);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="3-1-7-修改表"><a href="#3-1-7-修改表" class="headerlink" title="3.1.7 修改表"></a>3.1.7 修改表</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">alter_table</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    Configuration configuration = HBaseConfiguration.create();</span><br><span class="line">    configuration.set(<span class="string">&quot;hbase.zookeeper.quorum&quot;</span>, <span class="string">&quot;hadoop001,hadoop002,hadoop003,hadoop004,hadoop005&quot;</span>);</span><br><span class="line">    <span class="keyword">try</span> (</span><br><span class="line">            Connection connection = ConnectionFactory.createConnection(configuration);</span><br><span class="line">            Admin admin = connection.getAdmin();</span><br><span class="line"></span><br><span class="line">    ) &#123;</span><br><span class="line">        TableName tableName = TableName.valueOf(<span class="string">&quot;api_test_table&quot;</span>);</span><br><span class="line"></span><br><span class="line">        ColumnFamilyDescriptorBuilder columnFamilyDescriptorBuilder</span><br><span class="line">                = ColumnFamilyDescriptorBuilder.newBuilder(<span class="string">&quot;job&quot;</span>.getBytes(StandardCharsets.UTF_8));</span><br><span class="line">        columnFamilyDescriptorBuilder.setMaxVersions(<span class="number">2</span>);</span><br><span class="line"></span><br><span class="line">        admin.addColumnFamily(tableName, columnFamilyDescriptorBuilder.build());</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        ColumnFamilyDescriptorBuilder infoFamilyDescriptorBuilder</span><br><span class="line">                = ColumnFamilyDescriptorBuilder.newBuilder(<span class="string">&quot;info&quot;</span>.getBytes(StandardCharsets.UTF_8));</span><br><span class="line">        infoFamilyDescriptorBuilder.setMaxVersions(<span class="number">2</span>);</span><br><span class="line"></span><br><span class="line">        admin.modifyColumnFamily(tableName, infoFamilyDescriptorBuilder.build());</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="3-1-8-查看表"><a href="#3-1-8-查看表" class="headerlink" title="3.1.8 查看表"></a>3.1.8 查看表</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">list_table</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    Configuration configuration = HBaseConfiguration.create();</span><br><span class="line">    configuration.set(<span class="string">&quot;hbase.zookeeper.quorum&quot;</span>, <span class="string">&quot;hadoop001,hadoop002,hadoop003,hadoop004,hadoop005&quot;</span>);</span><br><span class="line">    <span class="keyword">try</span> (</span><br><span class="line">            Connection connection = ConnectionFactory.createConnection(configuration);</span><br><span class="line">            Admin admin = connection.getAdmin();</span><br><span class="line"></span><br><span class="line">    ) &#123;</span><br><span class="line">        TableName[] tableNames = admin.listTableNames();</span><br><span class="line">        <span class="keyword">for</span> (TableName tableName : tableNames) &#123;</span><br><span class="line">            System.out.println(tableName);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="3-2-DML"><a href="#3-2-DML" class="headerlink" title="3.2 DML"></a>3.2 DML</h2><h3 id="3-2-1-插入数据"><a href="#3-2-1-插入数据" class="headerlink" title="3.2.1 插入数据"></a>3.2.1 插入数据</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">put</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    Configuration configuration = HBaseConfiguration.create();</span><br><span class="line">    configuration.set(<span class="string">&quot;hbase.zookeeper.quorum&quot;</span>, <span class="string">&quot;hadoop001,hadoop002,hadoop003,hadoop004,hadoop005&quot;</span>);</span><br><span class="line">    TableName tableName = TableName.valueOf(<span class="string">&quot;api_test_table&quot;</span>);</span><br><span class="line">    <span class="keyword">try</span> (</span><br><span class="line">            Connection connection = ConnectionFactory.createConnection(configuration);</span><br><span class="line">            Table table = connection.getTable(tableName);</span><br><span class="line">    ) &#123;</span><br><span class="line">        <span class="keyword">final</span> Put put_1001 = <span class="keyword">new</span> Put(<span class="string">&quot;1001&quot;</span>.getBytes(StandardCharsets.UTF_8));</span><br><span class="line">        put_1001.addColumn(<span class="string">&quot;info&quot;</span>.getBytes(StandardCharsets.UTF_8),</span><br><span class="line">                <span class="string">&quot;name&quot;</span>.getBytes(StandardCharsets.UTF_8),</span><br><span class="line">                <span class="string">&quot;张三&quot;</span>.getBytes(StandardCharsets.UTF_8));</span><br><span class="line">        table.put(put_1001);</span><br><span class="line"></span><br><span class="line">        put_1001.addColumn(<span class="string">&quot;info&quot;</span>.getBytes(StandardCharsets.UTF_8),</span><br><span class="line">                <span class="string">&quot;age&quot;</span>.getBytes(StandardCharsets.UTF_8),</span><br><span class="line">                <span class="string">&quot;18&quot;</span>.getBytes(StandardCharsets.UTF_8));</span><br><span class="line">        table.put(put_1001);</span><br><span class="line"></span><br><span class="line">        put_1001.addColumn(<span class="string">&quot;info&quot;</span>.getBytes(StandardCharsets.UTF_8),</span><br><span class="line">                <span class="string">&quot;gender&quot;</span>.getBytes(StandardCharsets.UTF_8),</span><br><span class="line">                <span class="string">&quot;male&quot;</span>.getBytes(StandardCharsets.UTF_8));</span><br><span class="line">        table.put(put_1001);</span><br><span class="line">        </span><br><span class="line">        put_1001.addColumn(<span class="string">&quot;info&quot;</span>.getBytes(StandardCharsets.UTF_8),</span><br><span class="line">                <span class="string">&quot;telephone&quot;</span>.getBytes(StandardCharsets.UTF_8),</span><br><span class="line">                <span class="string">&quot;18889898899&quot;</span>.getBytes(StandardCharsets.UTF_8));</span><br><span class="line"></span><br><span class="line">        table.put(put_1001);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="3-2-2-查询单条数据"><a href="#3-2-2-查询单条数据" class="headerlink" title="3.2.2 查询单条数据"></a>3.2.2 查询单条数据</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">get</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    Configuration configuration = HBaseConfiguration.create();</span><br><span class="line">    configuration.set(<span class="string">&quot;hbase.zookeeper.quorum&quot;</span>, <span class="string">&quot;hadoop001,hadoop002,hadoop003,hadoop004,hadoop005&quot;</span>);</span><br><span class="line">    TableName tableName = TableName.valueOf(<span class="string">&quot;api_test_table&quot;</span>);</span><br><span class="line">    <span class="keyword">try</span> (</span><br><span class="line">            Connection connection = ConnectionFactory.createConnection(configuration);</span><br><span class="line">            Table table = connection.getTable(tableName);</span><br><span class="line">    ) &#123;</span><br><span class="line">        <span class="keyword">final</span> Get get = <span class="keyword">new</span> Get(<span class="string">&quot;1001&quot;</span>.getBytes(StandardCharsets.UTF_8));</span><br><span class="line"></span><br><span class="line">        <span class="keyword">final</span> Result result = table.get(get);</span><br><span class="line">        <span class="keyword">final</span> Cell[] cells = result.rawCells();</span><br><span class="line">        <span class="keyword">for</span> (Cell cell : cells) &#123;</span><br><span class="line">            printCell(cell);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">printCell</span><span class="params">(Cell cell)</span> </span>&#123;</span><br><span class="line">    String row = <span class="keyword">new</span> String(CellUtil.cloneRow(cell), StandardCharsets.UTF_8);</span><br><span class="line">    String family = <span class="keyword">new</span> String(CellUtil.cloneFamily(cell), StandardCharsets.UTF_8);</span><br><span class="line">    String qualifier = <span class="keyword">new</span> String(CellUtil.cloneQualifier(cell), StandardCharsets.UTF_8);</span><br><span class="line">    String value = <span class="keyword">new</span> String(CellUtil.cloneValue(cell), StandardCharsets.UTF_8);</span><br><span class="line"></span><br><span class="line">    System.out.println(</span><br><span class="line">            <span class="string">&quot;row:&quot;</span> + row + <span class="string">&quot; &quot;</span> +</span><br><span class="line">                    <span class="string">&quot;family:&quot;</span> + family + <span class="string">&quot; &quot;</span> +</span><br><span class="line">                    <span class="string">&quot;qualifier:&quot;</span> + qualifier + <span class="string">&quot; &quot;</span> +</span><br><span class="line">                    <span class="string">&quot;value:&quot;</span> + value + <span class="string">&quot; &quot;</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="3-2-3-范围查询"><a href="#3-2-3-范围查询" class="headerlink" title="3.2.3 范围查询"></a>3.2.3 范围查询</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">scan</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    Configuration configuration = HBaseConfiguration.create();</span><br><span class="line">    configuration.set(<span class="string">&quot;hbase.zookeeper.quorum&quot;</span>, <span class="string">&quot;hadoop001,hadoop002,hadoop003,hadoop004,hadoop005&quot;</span>);</span><br><span class="line">    TableName tableName = TableName.valueOf(<span class="string">&quot;api_test_table&quot;</span>);</span><br><span class="line">    <span class="keyword">try</span> (</span><br><span class="line">            Connection connection = ConnectionFactory.createConnection(configuration);</span><br><span class="line">            Table table = connection.getTable(tableName);</span><br><span class="line">    ) &#123;</span><br><span class="line">        <span class="keyword">final</span> Scan scan = <span class="keyword">new</span> Scan();</span><br><span class="line">        scan.withStartRow(<span class="string">&quot;1001&quot;</span>.getBytes(StandardCharsets.UTF_8));</span><br><span class="line">        scan.withStopRow(<span class="string">&quot;1005&quot;</span>.getBytes(StandardCharsets.UTF_8));</span><br><span class="line"></span><br><span class="line">        <span class="keyword">final</span> ResultScanner scanner = table.getScanner(scan);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (Result result : scanner) &#123;</span><br><span class="line">            <span class="keyword">final</span> Cell[] cells = result.rawCells();</span><br><span class="line">            <span class="keyword">for</span> (Cell cell : cells) &#123;</span><br><span class="line">                printCell(cell);</span><br><span class="line">            &#125;</span><br><span class="line">            System.out.println();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="3-2-4-追加数据"><a href="#3-2-4-追加数据" class="headerlink" title="3.2.4 追加数据"></a>3.2.4 追加数据</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">append</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    Configuration configuration = HBaseConfiguration.create();</span><br><span class="line">    configuration.set(<span class="string">&quot;hbase.zookeeper.quorum&quot;</span>, <span class="string">&quot;hadoop001,hadoop002,hadoop003,hadoop004,hadoop005&quot;</span>);</span><br><span class="line">    TableName tableName = TableName.valueOf(<span class="string">&quot;api_test_table&quot;</span>);</span><br><span class="line">    <span class="keyword">try</span> (</span><br><span class="line">            Connection connection = ConnectionFactory.createConnection(configuration);</span><br><span class="line">            Table table = connection.getTable(tableName);</span><br><span class="line">    ) &#123;</span><br><span class="line">        <span class="keyword">final</span> Append append = <span class="keyword">new</span> Append(<span class="string">&quot;1001&quot;</span>.getBytes(StandardCharsets.UTF_8));</span><br><span class="line">        append.addColumn(<span class="string">&quot;info&quot;</span>.getBytes(StandardCharsets.UTF_8),</span><br><span class="line">                <span class="string">&quot;salary&quot;</span>.getBytes(StandardCharsets.UTF_8),</span><br><span class="line">                <span class="string">&quot;400000&quot;</span>.getBytes(StandardCharsets.UTF_8));</span><br><span class="line">        table.append(append);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="3-2-5-删除数据"><a href="#3-2-5-删除数据" class="headerlink" title="3.2.5 删除数据"></a>3.2.5 删除数据</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">delete</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    Configuration configuration = HBaseConfiguration.create();</span><br><span class="line">    configuration.set(<span class="string">&quot;hbase.zookeeper.quorum&quot;</span>, <span class="string">&quot;hadoop001,hadoop002,hadoop003,hadoop004,hadoop005&quot;</span>);</span><br><span class="line">    TableName tableName = TableName.valueOf(<span class="string">&quot;api_test_table&quot;</span>);</span><br><span class="line">    <span class="keyword">try</span> (</span><br><span class="line">            Connection connection = ConnectionFactory.createConnection(configuration);</span><br><span class="line">            Table table = connection.getTable(tableName);</span><br><span class="line">    ) &#123;</span><br><span class="line">        <span class="keyword">final</span> Delete delete = <span class="keyword">new</span> Delete(<span class="string">&quot;1001&quot;</span>.getBytes(StandardCharsets.UTF_8));</span><br><span class="line">        delete.addColumn(<span class="string">&quot;info&quot;</span>.getBytes(StandardCharsets.UTF_8),</span><br><span class="line">                <span class="string">&quot;name&quot;</span>.getBytes(StandardCharsets.UTF_8)</span><br><span class="line">        );</span><br><span class="line">        table.delete(delete);</span><br><span class="line">        get();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="3-2-6-清空数据"><a href="#3-2-6-清空数据" class="headerlink" title="3.2.6 清空数据"></a>3.2.6 清空数据</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">truncate</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    Configuration configuration = HBaseConfiguration.create();</span><br><span class="line">    configuration.set(<span class="string">&quot;hbase.zookeeper.quorum&quot;</span>, <span class="string">&quot;hadoop001,hadoop002,hadoop003,hadoop004,hadoop005&quot;</span>);</span><br><span class="line">    TableName tableName = TableName.valueOf(<span class="string">&quot;api_test_table&quot;</span>);</span><br><span class="line">    <span class="keyword">try</span> (</span><br><span class="line">            Connection connection = ConnectionFactory.createConnection(configuration);</span><br><span class="line">            <span class="keyword">final</span> Admin admin = connection.getAdmin()</span><br><span class="line">    ) &#123;</span><br><span class="line">        admin.disableTable(tableName);</span><br><span class="line">        admin.truncateTable(tableName, <span class="keyword">false</span>);</span><br><span class="line">        scan();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h1 id="第4章-HBase进阶"><a href="#第4章-HBase进阶" class="headerlink" title="第4章 HBase进阶"></a>第4章 HBase进阶</h1><h2 id="4-1-架构原理"><a href="#4-1-架构原理" class="headerlink" title="4.1 架构原理"></a>4.1 架构原理</h2><p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16360144290588.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
<h3 id="4-1-1-Client"><a href="#4-1-1-Client" class="headerlink" title="4.1.1 Client"></a>4.1.1 Client</h3><ol>
<li>HBase META表：记录了用户所有表拆分出来的的 Region 映射信息，META可以有多个 Regoin</li>
<li>Client 访问用户数据前需要首先访问 ZooKeeper，找到META表所在RegionServer，最后才能找到用户数据的位置去访问，中间需要多次网络操作，不过 client 端会做 cache 缓存。</li>
</ol>
<h3 id="4-1-2-ZooKeeper"><a href="#4-1-2-ZooKeeper" class="headerlink" title="4.1.2 ZooKeeper"></a>4.1.2 ZooKeeper</h3><ol>
<li>ZooKeeper 为 HBase 提供 Failover 机制，选举 Master，避免单点 Master 单点故障问题</li>
<li>存储所有 Region 的寻址入口：<del>-ROOT-表在哪台服务器上</del> META表的位置信息(zookeeper路径：<code>/hbase/meta-region-server</code>)</li>
<li>实时监控 RegionServer 的状态，将 RegionServer 的上线和下线信息实时通知给 Master</li>
</ol>
<h3 id="4-1-3-Meta表结构"><a href="#4-1-3-Meta表结构" class="headerlink" title="4.1.3 Meta表结构"></a>4.1.3 Meta表结构</h3><ol>
<li>hbase:metab: 表存放着整个集群的所有Region信息，客户端数据的读写需要定位到具体需要操作的Region，说白了就是一张字典表。meta表只会有一个Region，这是为了确保meta表多次操作的原子性。</li>
<li>Meta表结构与内容 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hbase(main):002:0&gt; scan <span class="string">&#x27;hbase:meta&#x27;</span></span><br><span class="line">ROW                                                                                         COLUMN+CELL</span><br><span class="line"> api_test_table                                                                             column=table:state, timestamp=1635927178467, value=\x08\x00</span><br><span class="line"> api_test_table,,1635927178047.d0d5f57b5659e0911c7aabbaa3a1bc04.                            column=info:regioninfo, timestamp=1635927185877, value=&#123;ENCODED =&gt; d0d5f57b5659e0911c7aabbaa3a1bc04, NAME =&gt; <span class="string">&#x27;api_test_table,,1635927178047.d0d5f57b5659e0911c7aabbaa3a1bc04.&#x27;</span>, STARTKEY =&gt; <span class="string">&#x27;&#x27;</span>, ENDKEY =&gt; <span class="string">&#x27;&#x27;</span>&#125;</span><br><span class="line"> api_test_table,,1635927178047.d0d5f57b5659e0911c7aabbaa3a1bc04.                            column=info:seqnumDuringOpen, timestamp=1635927185877, value=\x00\x00\x00\x00\x00\x00\x00\x05</span><br><span class="line"> api_test_table,,1635927178047.d0d5f57b5659e0911c7aabbaa3a1bc04.                            column=info:server, timestamp=1635927185877, value=hadoop001:16020</span><br><span class="line"> api_test_table,,1635927178047.d0d5f57b5659e0911c7aabbaa3a1bc04.                            column=info:serverstartcode, timestamp=1635927185877, value=1635912473426</span><br><span class="line"> api_test_table,,1635927178047.d0d5f57b5659e0911c7aabbaa3a1bc04.                            column=info:sn, timestamp=1635927185668, value=hadoop001,16020,1635912473426</span><br><span class="line"> api_test_table,,1635927178047.d0d5f57b5659e0911c7aabbaa3a1bc04.                            column=info:state, timestamp=1635927185877, value=OPEN</span><br><span class="line"> hbase:namespace                                                                            column=table:state, timestamp=1635908733224, value=\x08\x00</span><br><span class="line"> hbase:namespace,,1635908732447.5584783366fc148c901aaffe044a32ec.                           column=info:regioninfo, timestamp=1635922081791, value=&#123;ENCODED =&gt; 5584783366fc148c901aaffe044a32ec, NAME =&gt; <span class="string">&#x27;hbase:namespace,,1635908732447.5584783366fc148c901aaffe044a32ec.&#x27;</span>, STARTKEY =&gt; <span class="string">&#x27;&#x27;</span>, ENDKEY =&gt; <span class="string">&#x27;&#x27;</span>&#125;</span><br><span class="line"> hbase:namespace,,1635908732447.5584783366fc148c901aaffe044a32ec.                           column=info:seqnumDuringOpen, timestamp=1635922081791, value=\x00\x00\x00\x00\x00\x00\x00\x15</span><br><span class="line"> hbase:namespace,,1635908732447.5584783366fc148c901aaffe044a32ec.                           column=info:server, timestamp=1635922081791, value=hadoop004:16020</span><br><span class="line"> hbase:namespace,,1635908732447.5584783366fc148c901aaffe044a32ec.                           column=info:serverstartcode, timestamp=1635922081791, value=1635912472425</span><br><span class="line"> hbase:namespace,,1635908732447.5584783366fc148c901aaffe044a32ec.                           column=info:sn, timestamp=1635922081349, value=hadoop004,16020,1635912472425</span><br><span class="line"> hbase:namespace,,1635908732447.5584783366fc148c901aaffe044a32ec.                           column=info:state, timestamp=1635922081791, value=OPEN</span><br><span class="line"> student                                                                                    column=table:state, timestamp=1635910393993, value=\x08\x00</span><br><span class="line"> student,,1635910393240.55ab1df9560f226f1e7b4bc063e429ac.                                   column=info:regioninfo, timestamp=1635912481491, value=&#123;ENCODED =&gt; 55ab1df9560f226f1e7b4bc063e429ac, NAME =&gt; <span class="string">&#x27;student,,1635910393240.55ab1df9560f226f1e7b4bc063e429ac.&#x27;</span>, STARTKEY =&gt; <span class="string">&#x27;&#x27;</span>, ENDKEY =&gt; <span class="string">&#x27;&#x27;</span>&#125;</span><br><span class="line"> student,,1635910393240.55ab1df9560f226f1e7b4bc063e429ac.                                   column=info:seqnumDuringOpen, timestamp=1635912481491, value=\x00\x00\x00\x00\x00\x00\x00\x0F</span><br><span class="line"> student,,1635910393240.55ab1df9560f226f1e7b4bc063e429ac.                                   column=info:server, timestamp=1635912481491, value=hadoop003:16020</span><br><span class="line"> student,,1635910393240.55ab1df9560f226f1e7b4bc063e429ac.                                   column=info:serverstartcode, timestamp=1635912481491, value=1635912471638</span><br><span class="line"> student,,1635910393240.55ab1df9560f226f1e7b4bc063e429ac.                                   column=info:sn, timestamp=1635912480984, value=hadoop003,16020,1635912471638</span><br><span class="line"> student,,1635910393240.55ab1df9560f226f1e7b4bc063e429ac.                                   column=info:state, timestamp=1635912481491, value=OPEN</span><br><span class="line">6 row(s)</span><br><span class="line">Took 0.0450 seconds</span><br></pre></td></tr></table></figure></li>
<li>Meta表组成说明<ol>
<li>Rowkey组成: meta表中的一个Rowkey就代表了一个region。<ul>
<li>格式： <code>[table],[region start key],[region id]</code><ul>
<li>region id 由该 region 生成的时间戳（精确到毫秒）与 region encoded 组成</li>
<li>region encoded 由 region 所在的 表名, StartKey, 时间戳这三者的MD5值产生， HBase 在 HDFS 上存储 region 的路径就是 region encoded。</li>
</ul>
</li>
<li>[region start key]为空的，说明这是该table的第一个region。若对应region中startkey和endkey都为空的话，表明这个table只有一个region</li>
<li>示例<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">Rowkey: student,,1635910393240.55ab1df9560f226f1e7b4bc063e429ac.</span><br><span class="line">table: student</span><br><span class="line">region start key: 空</span><br><span class="line">region id timestamp: 1635910393240</span><br><span class="line">region id encoded: 55ab1df9560f226f1e7b4bc063e429ac</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>Column组成<table>
<thead>
<tr>
<th>列名</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>info:state</td>
<td>Region状态</td>
</tr>
<tr>
<td>info:sn</td>
<td>Region Server Node，由server和serverstartcode 组成</td>
</tr>
<tr>
<td>info:serverstartcode</td>
<td>Region Server启动Code，实质上就是Region Server启动的时间戳</td>
</tr>
<tr>
<td>info:server</td>
<td>Region Server 地址和端口</td>
</tr>
<tr>
<td>info:seqnumDuringOpen</td>
<td>表示Region在线时长的一个二进制串</td>
</tr>
<tr>
<td>info:regioninfo</td>
<td>Region 的详细信息，和 .regioninfo 内容相同</td>
</tr>
</tbody></table>
</li>
<li>info:regioninfo 是重要信息<ul>
<li>ENCODED：基于${表名},${起始键},${region时间戳}生成的32位md5字符串，region数据存储在hdfs上时使用的唯一编号，可以从meta表中根据该值定位到hdfs中的具体路径。 rowkey中最后的${encode编码}就是 ENCODED 的值，其是rowkey组成的一部分。</li>
<li>NAME：与 ROWKEY 值相同</li>
<li>STARTKEY：该 region 的起始键</li>
<li>ENDKEY：该 region 的结束键</li>
</ul>
</li>
</ol>
</li>
</ol>
<h3 id="4-1-4-Master"><a href="#4-1-4-Master" class="headerlink" title="4.1.4 Master"></a>4.1.4 Master</h3><ol>
<li>为 RegionServer 分配 Region</li>
<li>负责 RegionServer 的负载均衡</li>
<li>发现失效的 RegionServer 并重新分配其上的 Region</li>
<li>HDFS 上的垃圾文件（HBase）回收</li>
<li>处理 Schema 更新请求（表的创建，删除，修改，列簇的增加等等）</li>
</ol>
<h3 id="4-1-5-RegionServer"><a href="#4-1-5-RegionServer" class="headerlink" title="4.1.5 RegionServer"></a>4.1.5 RegionServer</h3><ol>
<li>RegionServer 维护 Master 分配给它的 Region，处理对这些 Region 的 IO 请求</li>
<li>RegionServer 负责 Split 在运行过程中变得过大的 Region，负责 Compact 操作</li>
</ol>
<ul>
<li>可以看到，client 访问 HBase 上数据的过程并不需要 master 参与（寻址访问 zookeeper 和 RegioneServer，数据读写访问 RegioneServer），Master 仅仅维护者 Table 和 Region 的元数据信息，负载很低。</li>
<li>META 存的是所有的 Region 的位置信息，那么 RegioneServer 当中 Region 在进行分裂之后 的新产生的 Region，是由 Master 来决定发到哪个 RegioneServer，这就意味着，只有 Master 知道 new Region 的位置信息，所以，由 Master 来管理META表当中的数据的 CRUD</li>
</ul>
<ol start="5">
<li>所以结合以上两点表明，在没有 Region 分裂的情况，Master 宕机一段时间是可以忍受的。</li>
</ol>
<h3 id="4-1-6-HRegion"><a href="#4-1-6-HRegion" class="headerlink" title="4.1.6 HRegion"></a>4.1.6 HRegion</h3><ol>
<li>table在行的方向上分隔为多个Region。Region是HBase中分布式存储和负载均衡的最小单元，即不同的region可以分别在不同的Region Server上，但同一个Region是不会拆分到多个server上。</li>
<li>Region按大小分裂（split），每个表一般是只有一个region。随着数据不断插入表，region不断增大，当region的某个列族达到一个阈值时就会分裂两个新的region。</li>
<li>每个region由以下信息标识：&lt; 表名,startRowkey,创建时间&gt;</li>
<li>由元数据表META记录该region的endRowkey</li>
</ol>
<h3 id="4-1-7-Store"><a href="#4-1-7-Store" class="headerlink" title="4.1.7 Store"></a>4.1.7 Store</h3><ol>
<li>每一个region由一个或多个store组成，至少是一个store，hbase会把一起访问的数据放在一个store里面，即为每个 ColumnFamily建一个store，如果有几个ColumnFamily，也就有几个Store。</li>
<li>一个Store由一个memStore和0或者 多个StoreFile组成。</li>
<li>HBase以store的大小来判断是否需要切分region</li>
</ol>
<h3 id="4-1-8-MemStore"><a href="#4-1-8-MemStore" class="headerlink" title="4.1.8 MemStore"></a>4.1.8 MemStore</h3><ol>
<li>写缓存，由于HFile中的数据要求是有序的，所以数据是先存储在MemStore中，排好序后，等到达刷写时机才会刷写到HFile，每次刷写都会形成一个新的HFile。</li>
<li>memStore 是放在内存里的。保存修改的数据即keyValues。当memStore的大小达到一个阀值（默认128MB）时，memStore会被flush到文 件，即生成一个快照。目前hbase 会有一个线程来负责memStore的flush操作。</li>
<li>进入MemStore的数据对rowkey进行字典序排序</li>
</ol>
<h3 id="4-1-9-StoreFile"><a href="#4-1-9-StoreFile" class="headerlink" title="4.1.9 StoreFile"></a>4.1.9 StoreFile</h3><ol>
<li>保存实际数据的物理文件，StoreFile以HFile的形式存储在HDFS上。每个Store会有一个或多个StoreFile（HFile），数据在每个StoreFile中都是有序的。</li>
<li>memStore内存中的数据写到文件后就是StoreFile，StoreFile底层是以HFile的格式保存。当storefile文件的数量增长到一定阈值后，系统会进行合并（minor、major compaction），在合并过程中会进行版本合并和删除工作（majar），形成更大的storefile。</li>
</ol>
<h3 id="4-1-10-HFile"><a href="#4-1-10-HFile" class="headerlink" title="4.1.10 HFile"></a>4.1.10 HFile</h3><ol>
<li>HBase中KeyValue数据的存储格式，HFile是Hadoop的 二进制格式文件，实际上StoreFile就是对Hfile做了轻量级包装，即StoreFile底层就是HFile。</li>
</ol>
<h3 id="4-1-11-HLog"><a href="#4-1-11-HLog" class="headerlink" title="4.1.11 HLog"></a>4.1.11 HLog</h3><ol>
<li>HLog(WAL log)：WAL意为write ahead log，用来做灾难恢复使用，HLog记录数据的所有变更，一旦region server 宕机，就可以从log中进行恢复。</li>
<li>由于数据要经MemStore排序后才能刷写到HFile，但把数据保存在内存中会有很高的概率导致数据丢失，为了解决这个问题，数据会先写在一个叫做Write-Ahead logfile的文件中，然后再写入MemStore中。所以在系统出现故障的时候，数据可以通过这个日志文件重建。</li>
<li>HLog文件就是一个普通的Hadoop Sequence File， Sequence File的value是key时HLogKey对象，其中记录了写入数据的归属信息，除了table和region名字外，还同时包括sequence number和timestamp，timestamp是写入时间，sequence number的起始值为0，或者是最近一次存入文件系统中的sequence number。 Sequence File的value是HBase的KeyValue对象，即对应HFile中的KeyValue。</li>
</ol>
<h2 id="4-2-写流程"><a href="#4-2-写流程" class="headerlink" title="4.2 写流程"></a>4.2 写流程</h2><p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16360175069448.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
<ol>
<li>Client先访问zookeeper，获取hbase:meta表位于哪个Region Server。</li>
<li>访问对应的Region Server，获取hbase:meta表，根据读请求的namespace:table/rowkey，查询出目标数据位于哪个Region Server中的哪个Region中。并将该table的region信息以及meta表的位置信息缓存在客户端的meta cache，方便下次访问。</li>
<li>与目标Region Server进行通讯；</li>
<li>将数据顺序写入（追加）到WAL；</li>
<li>将数据写入对应的MemStore，数据会在MemStore进行排序（rowKey 字典序）；</li>
<li>向客户端发送ack；</li>
<li>等达到MemStore的刷写时机后，将数据刷写到HFile。</li>
</ol>
<h2 id="4-3-MemStore-Flush"><a href="#4-3-MemStore-Flush" class="headerlink" title="4.3 MemStore Flush"></a>4.3 MemStore Flush</h2><p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16360739438242.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"><br>MemStore刷写时机：</p>
<ul>
<li><p>MemStore 级别限制：</p>
<ul>
<li><code>hbase.hregion.memstore.flush.size</code></li>
<li>默认值：128M</li>
<li>当某个memstore的大小达到128M(默认情况下)时，<font color ='red' >其所在region的所有memstore都会刷写</font>，这个时候是不阻塞写操作的</li>
</ul>
</li>
<li><p>Region 级别限制：</p>
<ul>
<li><code>hbase.hregion.memstore.flush.size</code> * <code>hbase.hregion.memstore.block.multiplier</code> <ul>
<li>默认值: 128M * 4 = 512M</li>
<li>当一个Region的MemStore总量达到512M（默认情况下）时，会阻塞这个region的写操作，并强制刷写到HFile。触发这个刷新只会发生在MemStore即将写满128M时put了一个巨大的记录的情况，这时会阻塞写操作，强制刷写成功才能继续写入</li>
</ul>
</li>
</ul>
</li>
<li><p>RegionServer 级别限制：</p>
<ul>
<li><p><code>java_heapsize</code> * <code>hbase.regionserver.global.memstore.size</code> </p>
<ul>
<li>默认值：java_heapsize * 0.4</li>
<li>当RegionServer上所有的MemStore占用到达heap的40%时，强制阻塞所有的写操作，将所有的MemStore刷写到HFile</li>
</ul>
</li>
<li><p><code>java_heapsize</code> * <code>hbase.regionserver.global.memstore.size</code> * <code>hbase.regionserver.global.memstore.size.upper.limit</code></p>
<ul>
<li>默认值：java_heapsize * 0.4 * 0.95</li>
<li>当region server中memstore的总大小达到上述内存限制时，region server会把它的所有region按照其所有memstore的大小顺序（由大到小）依次进行刷写。直到region server中所有memstore的总大小减小到<code>hbase.regionserver.global.memstore.size.lower.limit</code>以下。</li>
</ul>
</li>
<li><p><code>hbase.regionserver.optionalcacheflushinterval</code></p>
<ul>
<li>默认值：3600000，1小时</li>
<li>到达自动刷写的时间，也会触发memstore flush。</li>
</ul>
</li>
<li><p><code>hbase.regionserver.max.logs</code></p>
<ul>
<li>默认值：32</li>
<li>当WAL文件的数量超过32，region会按照时间顺序依次进行刷写，直到WAL文件数量减小到hbase.regionserver.max.log以下（该属性名已经废弃，现无需手动设置，最大值为32）。</li>
</ul>
</li>
</ul>
</li>
<li><p>手动执行:</p>
<ul>
<li>可以通过 shell 命令 flush ‘tableName’ 或者 flush ‘regionName’ 分别对一个表或者一个 Region 进行 flush。 </li>
</ul>
</li>
</ul>
<h2 id="4-4-读流程"><a href="#4-4-读流程" class="headerlink" title="4.4 读流程"></a>4.4 读流程</h2><p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16360830898136.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
<h3 id="4-4-1-读流程"><a href="#4-4-1-读流程" class="headerlink" title="4.4.1 读流程"></a>4.4.1 读流程</h3><ol>
<li>Client先访问zookeeper，获取hbase:meta表位于哪个Region Server。</li>
<li>访问对应的Region Server，获取hbase:meta表，根据读请求的namespace:table/rowkey，查询出目标数据位于哪个Region Server中的哪个Region中。并将该table的region信息以及meta表的位置信息缓存在客户端的meta cache，方便下次访问。</li>
<li>与目标Region Server进行通讯；</li>
<li>分别在Block Cache（读缓存），MemStore和Store File（HFile）中查询目标数据，并将查到的所有数据进行合并。此处所有数据是指同一条数据的不同版本（time stamp）或者不同的类型（Put/Delete）。<ul>
<li>如果在 Memstore 中找不到，则在 Storefile 中扫描 为了能快速的判断要查询的数据在不在这个 StoreFile 中，应用了 BloomFilter</li>
<li>BloomFilter，布隆过滤器：迅速判断一个元素是不是在一个庞大的集合内，但是他有一个 弱点：它有一定的误判率</li>
<li>误判率：原本不存在与该集合的元素，布隆过滤器有可能会判断说它存在，但是，如果布隆过滤器，判断说某一个元素不存在该集合，那么该元素就一定不在该集合内）</li>
</ul>
</li>
<li>将从文件中查询到的数据块（Block，HFile数据存储单元，默认大小为64KB）缓存到Block Cache。</li>
<li>将合并后的最终结果返回给客户端。</li>
</ol>
<h3 id="4-4-1-Block-Cache读缓存"><a href="#4-4-1-Block-Cache读缓存" class="headerlink" title="4.4.1 Block Cache读缓存"></a>4.4.1 Block Cache读缓存</h3><ol>
<li>读缓存：RegionServer级别，占堆内存的40%(默认)</li>
<li>当写操作导致缓存数据过期时，RegionServer会将过期的块释放  </li>
<li>如果没有命中BlockCache，会去memstore和所有的storefile里面查找数据</li>
</ol>
<h3 id="4-4-1-1-LruBlockCache"><a href="#4-4-1-1-LruBlockCache" class="headerlink" title="4.4.1.1 LruBlockCache"></a>4.4.1.1 LruBlockCache</h3><p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16360911357506.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
<ul>
<li><p>LruBlockCache内部较为简单，主要就是一个map，如上图所示，由hfilename+offset来唯一标识一个block；</p>
</li>
<li><p>LruBlockCache所能够使用的内存为堆的一定比例，通过hfile.block.cache.size设置，默认是0.4；</p>
</li>
<li><p>maxSize = heapSize * hfile.block.cache.size，以下参数都根据maxSize计算；<br><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16360926195851.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
<ul>
<li>acceptSize<ul>
<li>使用量达到一定比例时会触发驱逐，该阈值通过hbase.lru.blockcache.acceptable.factor设置，默认是0.99；</li>
</ul>
</li>
<li>minSize<ul>
<li>驱逐后最少剩余比例，该阈值通过hbase.lru.blockcache.min.factor设置，默认是0.95；</li>
</ul>
</li>
<li>hardLimit<ul>
<li>使用量达到一定比例时则拒绝写入，该阈值通过hbase.lru.blockcache.hard.capacity.limit.factor设置，默认是1.2，这意味允许一定的超出；</li>
</ul>
</li>
</ul>
</li>
<li><p>关于驱逐：</p>
<ol>
<li>block分为3种类型，由BlockPriority字段区分，取值为single、mutli、inMem，空间分配默认为0.25：0.5：0.25；</li>
<li>系统表以及其它指定了InMem的表所含block会标记为inMem，其它block初次存入时标记为single，再次访问时会修改为multi；</li>
<li>存放时只要还有空间即可放入，空间分配比例只是在驱逐发生时进行计算使用；</li>
<li>驱逐时，会用minSize乘以各类型的比例，得到各类型最少要保留的minSize；</li>
<li>根据目前的算法，驱逐后的size，应该是略大于minSize的一个值，伪代码如下； <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">expectFreeSize = usedSize - minSize;//预期释放总大小</span><br><span class="line">freedSize = 0;//当前已释放总大小</span><br><span class="line">n=3;//类型数量</span><br><span class="line"><span class="keyword">for</span> <span class="built_in">type</span> <span class="keyword">in</span> (<span class="string">&#x27;single&#x27;</span>,<span class="string">&#x27;multi&#x27;</span>,<span class="string">&#x27;inMem&#x27;</span>):</span><br><span class="line">    overFlow = type.usedSize - type.minSize</span><br><span class="line">    toBeFree = min(overFlow,(expectFreeSize - freedSize)/n)</span><br><span class="line">    free(toBeFree)</span><br><span class="line">    freedSize += toBeFree</span><br><span class="line">    n--;</span><br></pre></td></tr></table></figure></li>
</ol>
</li>
</ul>
<h3 id="4-4-1-2-CombinedBlockCache-LruBlockCache-BucketCache"><a href="#4-4-1-2-CombinedBlockCache-LruBlockCache-BucketCache" class="headerlink" title="4.4.1.2 CombinedBlockCache = LruBlockCache + BucketCache"></a>4.4.1.2 CombinedBlockCache = LruBlockCache + BucketCache</h3><ol>
<li>BucketCache<ol>
<li>LruBlockCache的优点是实现简单，缺点是block的存入和释放伴随着内存的申请和释放，会带来内存碎片和gc过多的问题；</li>
<li>BucketCache采用了类似池的思路，预先申请内存并划分为一个个的bucket，这些bucket会一直存在并重复使用；</li>
</ol>
</li>
</ol>
<p>总体的读写流程如下图所示：<br><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16360926651279.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"><br>Block缓存写入流程：</p>
<ol>
<li>将block写入RAMCache，然后系统会根据blockkey进行hash，根据hash结果将block分配到一组blockingQueue中；</li>
<li>HBase会同时启动多个WriteThead，分别关联一个blockingQueue，并发的执行异步写入；</li>
<li>每个WriteThead读取到block数据后，调用bucketAllocator为这些block分配内存空间；</li>
<li>BucketAllocator会选择与block大小对应的bucket进行存放，并且返回对应的物理地址偏移量offset；</li>
<li>WriteThead将block以及分配好的物理地址偏移量传给IOEngine模块，执行具体的内存写入操作；</li>
<li>写入成功后，将类似这样的映射关系写入BackingMap中，方便后续查找时根据blockkey可以直接定位；</li>
</ol>
<p>Block缓存读取流程：</p>
<ol>
<li>首先从RAMCache中查找，对于还没有来得及写入到bucket的缓存block，一定存储在RAMCache中；</li>
<li>如果在RAMCache中没有找到，再在BackingMap中根据blockKey找到对应entry；</li>
<li>根据entry中的offset可以直接从内存中查找对应的block数据；</li>
</ol>
<p>其中最核心的组件是BucketAllocator和IoEngine，前者负责block的逻辑地址分配，后者负责block的实际物理存放，内部结构如下：<br><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16361213582414.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
<ol>
<li>hbase中blocksize是可以灵活设置的，bucketCache预设了一组支持的大小，从4K~512k不等；</li>
<li>一个Bucket只能存放一种size的block，一种size对应一个BucketSizeInfo进行管理；</li>
<li>初始化时，每种size先分配1个bucket，剩余的都分配给最大的那个size，如黑色箭头所示；</li>
<li>分配过程中当前size如果空间不够，会挪用其它size的空闲bucket，如棕色箭头所示，这意味着有可能某个Bucket一开始存放了32k的block</li>
<li>释放后空闲，被挪用后变成存放64k的block；</li>
<li>ioEngine有多种实现，可支持onheap、offheap、disk等；</li>
</ol>
<p>关于驱逐：</p>
<ol>
<li>2种情况下会触发<ol>
<li>1是已使用超过95%（acceptableFactor），</li>
<li>2是某个size的block分配不了(总量虽然没达到阈值，但不存在完全空闲的bucket供挪用)；</li>
<li>驱逐后的最少剩余比例为85%（minFactor），遍历各个bucketSizeInfo，把超过85%的部分加起来，再乘以一个系数0.1（extraFreeFactor），就是要释放的大小；</li>
<li>具体计算方法复用了LruBlockCache的代码，也是按照single、multi、inMem及其比例进行计算和释放；</li>
<li>实际清理动作是修改一些状态数据，比如Bucket对象的freeList、freeCount，以及backMapping的键值对等，并不需要对底层的byteBuffer做什么操作；</li>
<li>对于refCount大于0的block，会先将其markedForEvict置为true，待各个使用方读取完成后调用returnBlock进行释放；</li>
</ol>
</li>
</ol>
<h2 id="4-5-StoreFile-Compaction"><a href="#4-5-StoreFile-Compaction" class="headerlink" title="4.5 StoreFile Compaction"></a>4.5 StoreFile Compaction</h2><p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16360927692130.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
<ul>
<li>由于memstore每次刷写都会生成一个新的HFile，且同一个字段的不同版本（timestamp）和不同类型（Put/Delete）有可能会分布在不同的HFile中，因此查询时需要遍历所有的HFile。为了减少HFile的个数，以及清理掉过期和删除的数据，会进行StoreFile Compaction。</li>
<li>Compaction分为两种，分别是Minor Compaction和Major Compaction。<ul>
<li>Minor Compaction会将临近的若干个较小的HFile合并成一个较大的HFile，但不会清理过期和删除的数据；不会对cell合并；性能要求低</li>
<li>Major Compaction会将一个Store下的所有的HFile合并成一个大HFile，并且会清理掉过期和删除的数据，对所有cell进行排序，性能要求高</li>
</ul>
</li>
<li>触发时机：<ul>
<li>Minor Compaction自动执行，超过3个以上的storeFile，根据ExploringCompactionPolicy算法判断触发</li>
<li>Major Compaction：默认七天一次，推荐手动执行</li>
</ul>
</li>
</ul>
<h2 id="4-6-Region-Split"><a href="#4-6-Region-Split" class="headerlink" title="4.6 Region Split"></a>4.6 Region Split</h2><ul>
<li>默认情况下，每个Table起初只有一个Region，随着数据的不断写入，Region会自动进行拆分。刚拆分时，两个子Region都位于当前的Region Server，但处于负载均衡的考虑，HMaster有可能会将某个Region转移给其他的Region Server。</li>
<li>Region Split时机：<ol>
<li>当1个region中的某个Store下所有StoreFile的总大小超过<code>hbase.hregion.max.filesize（默认值：10G）</code>，该Region就会进行拆分（0.94版本之前）。</li>
<li>当1个region中的某个Store下所有StoreFile的总大小超过<code>Min(R^3 * 2 * &quot;hbase.hregion.memstore.flush.size&quot;,hbase.hregion.max.filesize&quot;)</code>，该Region就会进行拆分，其中R为当前Region Server中属于该Table的Region个数（0.94版本之后）。快速分裂：使表能尽快分布到所有Region</li>
<li>Hbase 2.0引入了新的split策略：如果当前RegionServer上改表只有一个Region，按照<code>2 * hbase.hregion.memstore.flush.size 默认128M</code>分裂，否则按照<code>hbase.hregion.max.filesize 默认值10G</code>分裂。防止Region过多，导致小文件过多</li>
</ol>
</li>
<li>Region Split过程<br>  在子region文件夹下生成两个子文件夹daughterA、daughterB，并在两个文件夹内生成reference文件，分别指向父region中对应的文件随着Compaction的进行，RefenceFile会逐渐被删除，此时父Region数据没用了， 会被删除，Split结束</li>
</ul>
<h2 id="4-7-Region-Merge"><a href="#4-7-Region-Merge" class="headerlink" title="4.7 Region Merge"></a>4.7 Region Merge</h2><p>如果Region数量过多，可以手动合并Region</p>
<h1 id="第5章-HBase优化"><a href="#第5章-HBase优化" class="headerlink" title="第5章 HBase优化"></a>第5章 HBase优化</h1><h2 id="5-1-预分区"><a href="#5-1-预分区" class="headerlink" title="5.1 预分区"></a>5.1 预分区</h2><p>每一个region维护着StartRow与EndRow，如果加入的数据符合某个Region维护的RowKey范围，则该数据交给这个Region维护。那么依照这个原则，我们可以将数据所要投放的分区提前大致的规划好，以提高HBase性能。</p>
<ol>
<li>手动设定预分区 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hbase&gt; create <span class="string">&#x27;staff1&#x27;</span>,<span class="string">&#x27;info&#x27;</span>,<span class="string">&#x27;partition1&#x27;</span>,SPLITS =&gt; [<span class="string">&#x27;1000&#x27;</span>,<span class="string">&#x27;2000&#x27;</span>,<span class="string">&#x27;3000&#x27;</span>,<span class="string">&#x27;4000&#x27;</span>]</span><br></pre></td></tr></table></figure></li>
<li>生成16进制序列预分区 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hbase&gt; create <span class="string">&#x27;staff2&#x27;</span>,<span class="string">&#x27;info&#x27;</span>,<span class="string">&#x27;partition2&#x27;</span>,&#123;NUMREGIONS =&gt; 15, SPLITALGO =&gt; <span class="string">&#x27;HexStringSplit&#x27;</span>&#125;</span><br></pre></td></tr></table></figure></li>
<li>按照文件中设置的规则预分区<ul>
<li>创建splits.txt文件内容如下：  <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">aaaa</span><br><span class="line">bbbb</span><br><span class="line">cccc</span><br><span class="line">dddd</span><br></pre></td></tr></table></figure></li>
<li>然后执行：  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="string">&#x27;staff3&#x27;</span>,<span class="string">&#x27;partition3&#x27;</span>,SPLITS_FILE <span class="operator">=</span><span class="operator">&gt;</span> <span class="string">&#x27;/home/atguigu/hbase/splits.txt&#x27;</span></span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>使用JavaAPI创建预分区 <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//自定义算法，产生一系列hash散列值存储在二维数组中</span></span><br><span class="line"><span class="keyword">byte</span>[][] splitKeys = 某个散列值函数</span><br><span class="line"><span class="comment">//创建HbaseAdmin实例</span></span><br><span class="line">HBaseAdmin hAdmin = <span class="keyword">new</span> HBaseAdmin(HbaseConfiguration.create());</span><br><span class="line"><span class="comment">//创建HTableDescriptor实例</span></span><br><span class="line">HTableDescriptor tableDesc = <span class="keyword">new</span> HTableDescriptor(tableName);</span><br><span class="line"><span class="comment">//通过HTableDescriptor实例和散列值二维数组创建带有预分区的Hbase表</span></span><br><span class="line">hAdmin.createTable(tableDesc, splitKeys);</span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="5-2-RowKey设计"><a href="#5-2-RowKey设计" class="headerlink" title="5.2 RowKey设计"></a>5.2 RowKey设计</h2><p>一条数据的唯一标识就是RowKey，那么这条数据存储于哪个分区，取决于RowKey处于哪个一个预分区的区间内，设计RowKey的主要目的 ，就是让数据均匀的分布于所有的region中，在一定程度上防止数据倾斜。接下来我们就谈一谈RowKey常用的设计方案。</p>
<ol>
<li>生成随机数、hash、散列值, 比如： <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">原本rowKey为1001的，SHA1后变成：dd01903921ea24941c26a48f2cec24e0bb0e8cc7</span><br><span class="line">原本rowKey为3001的，SHA1后变成：49042c54de64a1e9bf0b33e00245660ef92dc7bd</span><br><span class="line">原本rowKey为5001的，SHA1后变成：7b61dec07e02c188790670af43e717f0f46e8913</span><br></pre></td></tr></table></figure>
<ul>
<li>在做此操作之前，一般我们会选择从数据集中抽取样本，来决定什么样的rowKey来Hash后作为每个分区的临界值。</li>
</ul>
</li>
<li>字符串反转 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">20170524000001转成10000042507102</span><br><span class="line">20170524000002转成20000042507102</span><br></pre></td></tr></table></figure>
 这样也可以在一定程度上散列逐步put进来的数据。</li>
<li>字符串拼接 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">20170524000001_a12e</span><br><span class="line">20170524000001_93i7</span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="5-3-内存优化"><a href="#5-3-内存优化" class="headerlink" title="5.3 内存优化"></a>5.3 内存优化</h2><p>HBase操作过程中需要大量的内存开销，毕竟Table是可以缓存在内存中的，一般会分配整个可用内存的70%给HBase的Java堆。但是不建议分配非常大的堆内存，因为GC过程持续太久会导致RegionServer处于长期不可用状态，一般16~48G内存就可以了，如果因为框架占用内存过高导致系统内存不足，框架一样会被系统服务拖死。</p>
<h2 id="5-4-基础优化"><a href="#5-4-基础优化" class="headerlink" title="5.4 基础优化"></a>5.4 基础优化</h2><ol>
<li>允许在HDFS的文件中追加内容<ul>
<li>hdfs-site.xml、hbase-site.xml</li>
<li>属性：dfs.support.append</li>
<li>解释：开启HDFS追加同步，可以优秀的配合HBase的数据同步和持久化。默认值为true。</li>
</ul>
</li>
<li>优化DataNode允许的最大文件打开数<ul>
<li>hdfs-site.xml</li>
<li>属性：dfs.datanode.max.transfer.threads</li>
<li>解释：HBase一般都会同一时间操作大量的文件，根据集群的数量和规模以及数据动作，设置为4096或者更高。默认值：4096</li>
</ul>
</li>
<li>优化延迟高的数据操作的等待时间<ul>
<li>hdfs-site.xml</li>
<li>属性：dfs.image.transfer.timeout</li>
<li>解释：如果对于某一次数据操作来讲，延迟非常高，socket需要等待更长的时间，建议把该值设置为更大的值（默认60000毫秒），以确保socket不会被timeout掉。</li>
</ul>
</li>
<li>优化数据的写入效率<ul>
<li>mapred-site.xml</li>
<li>属性：<ul>
<li>mapreduce.map.output.compress</li>
<li>mapreduce.map.output.compress.codec</li>
</ul>
</li>
<li>解释：开启这两个数据可以大大提高文件的写入效率，减少写入时间。第一个属性值修改为true，第二个属性值修改为：org.apache.hadoop.io.compress.GzipCodec或者其他压缩方式。</li>
</ul>
</li>
<li>设置RPC监听数量<ul>
<li>hbase-site.xml</li>
<li>属性：Hbase.regionserver.handler.count</li>
<li>解释：默认值为30，用于指定RPC监听的数量，可以根据客户端的请求数进行调整，读写请求较多时，增加此值。</li>
</ul>
</li>
<li>优化HStore文件大小<ul>
<li>hbase-site.xml</li>
<li>属性：hbase.hregion.max.filesize</li>
<li>解释：默认值10737418240（10GB），如果需要运行HBase的MR任务，可以减小此值，因为一个region对应一个map任务，如果单个region过大，会导致map任务执行时间过长。该值的意思就是，如果HFile的大小达到这个数值，则这个region会被切分为两个Hfile。</li>
</ul>
</li>
<li>优化HBase客户端缓存<ul>
<li>hbase-site.xml</li>
<li>属性：hbase.client.write.buffer</li>
<li>解释：用于指定Hbase客户端缓存，增大该值可以减少RPC调用次数，但是会消耗更多内存，反之则反之。一般我们需要设定一定的缓存大小，以达到减少RPC次数的目的。</li>
</ul>
</li>
<li>指定scan.next扫描HBase所获取的行数<ul>
<li>hbase-site.xml</li>
<li>属性：hbase.client.scanner.caching</li>
<li>解释：用于指定scan.next方法获取的默认行数，值越大，消耗内存越大。</li>
</ul>
</li>
<li>flush、compact、split机制<br> 当MemStore达到阈值，将Memstore中的数据Flush进Storefile；compact机制则是把flush出来的小文件合并成大的Storefile文件。split则是当Region达到阈值，会把过大的Region一分为二。<ul>
<li>属性：<ul>
<li>hbase.hregion.memstore.flush.size：134217728<ul>
<li>128M就是Memstore的默认阈值</li>
<li>这个参数的作用是当单个HRegion内所有的Memstore大小总和超过指定值时，flush该HRegion的所有memstore。RegionServer的flush是通过将请求添加一个队列，模拟生产消费模型来异步处理的。那这里就有一个问题，当队列来不及消费，产生大量积压请求时，可能会导致内存陡增，最坏的情况是触发OOM。</li>
</ul>
</li>
<li>hbase.regionserver.global.memstore.upperLimit：0.8</li>
<li>hbase.regionserver.global.memstore.lowerLimit：0.6<ul>
<li>当MemStore使用内存总量达到hbase.regionserver.global.memstore.upperLimit指定值时，将会有多个MemStores flush到文件中，MemStore flush 顺序是按照大小降序执行的，直到刷新到MemStore使用内存略小于lowerLimit</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ol>
<h1 id="第6章-整合Phoenix"><a href="#第6章-整合Phoenix" class="headerlink" title="第6章 整合Phoenix"></a>第6章 整合Phoenix</h1><h2 id="6-1-Phoenix简介"><a href="#6-1-Phoenix简介" class="headerlink" title="6.1 Phoenix简介"></a>6.1 Phoenix简介</h2><h3 id="6-1-1-Phoenix定义"><a href="#6-1-1-Phoenix定义" class="headerlink" title="6.1.1 Phoenix定义"></a>6.1.1 Phoenix定义</h3><p>Phoenix是HBase的开源SQL皮肤。可以使用标准JDBC API代替HBase客户端API来创建表，插入数据和查询HBase数据。</p>
<h3 id="6-1-2-Phoenix特点"><a href="#6-1-2-Phoenix特点" class="headerlink" title="6.1.2 Phoenix特点"></a>6.1.2 Phoenix特点</h3><ol>
<li>容易集成：如Spark，Hive，Pig，Flume和Map Reduce；</li>
<li>操作简单：DML命令以及通过DDL命令创建和操作表和版本化增量更改；</li>
<li>支持HBase二级索引创建。</li>
</ol>
<h3 id="6-1-3-Phoenix架构"><a href="#6-1-3-Phoenix架构" class="headerlink" title="6.1.3 Phoenix架构"></a>6.1.3 Phoenix架构</h3><p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16361629213598.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
<h2 id="6-2-Phoenix快速入门"><a href="#6-2-Phoenix快速入门" class="headerlink" title="6.2 Phoenix快速入门"></a>6.2 Phoenix快速入门</h2><h3 id="6-2-1-安装"><a href="#6-2-1-安装" class="headerlink" title="6.2.1 安装"></a>6.2.1 安装</h3><ol>
<li>官网地址:<a href="http://phoenix.apache.org/">http://phoenix.apache.org/</a></li>
<li>Phoenix部署<ol>
<li>上传并解压tar包 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 module]$ tar -zxvf apache-phoenix-5.0.0-HBase-2.0-bin.tar.gz -C /opt/module/</span><br><span class="line">[atguigu@hadoop001 module]$ mv apache-phoenix-5.0.0-HBase-2.0-bin phoenix-5.0.0</span><br></pre></td></tr></table></figure></li>
<li>复制server包并拷贝到各个节点的hbase/lib <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 phoenix-5.0.0]$ cp /opt/module/phoenix-5.0.0/phoenix-5.0.0-HBase-2.0-server.jar /opt/module/hbase-2.0.5/lib/</span><br><span class="line">[atguigu@hadoop001 phoenix-5.0.0]$ xsync /opt/module/hbase-2.0.5/lib/phoenix-5.0.0-HBase-2.0-server.jar</span><br></pre></td></tr></table></figure></li>
<li>配置环境变量 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">#phoenix</span></span><br><span class="line"><span class="built_in">export</span> PHOENIX_HOME=/opt/module/phoenix-5.0.0</span><br><span class="line"><span class="built_in">export</span> PHOENIX_CLASSPATH=<span class="variable">$PHOENIX_HOME</span></span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$PHOENIX_HOME</span>/bin</span><br></pre></td></tr></table></figure></li>
<li>连接Phoenix <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 software]$ sqlline.py hadoop001,hadoop002,hadoop003,hadoop004,hadoop005:2181</span><br><span class="line">Setting property: [incremental, <span class="literal">false</span>]</span><br><span class="line">Setting property: [isolation, TRANSACTION_READ_COMMITTED]</span><br><span class="line">issuing: !connect jdbc:phoenix:hadoop001,hadoop002,hadoop003,hadoop004,hadoop005:2181 none none org.apache.phoenix.jdbc.PhoenixDriver</span><br><span class="line">Connecting to jdbc:phoenix:hadoop001,hadoop002,hadoop003,hadoop004,hadoop005:2181</span><br><span class="line">SLF4J: Class path contains multiple SLF4J bindings.</span><br><span class="line">SLF4J: Found binding <span class="keyword">in</span> [jar:file:/opt/module/phoenix-5.0.0/phoenix-5.0.0-HBase-2.0-client.jar!/org/slf4j/impl/StaticLoggerBinder.class]</span><br><span class="line">SLF4J: Found binding <span class="keyword">in</span> [jar:file:/opt/module/ha-hadoop-3.1.3/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]</span><br><span class="line">SLF4J: See http://www.slf4j.org/codes.html<span class="comment">#multiple_bindings for an explanation.</span></span><br><span class="line">21/11/06 10:06:53 WARN util.NativeCodeLoader: Unable to load native-hadoop library <span class="keyword">for</span> your platform... using builtin-java classes <span class="built_in">where</span> applicable</span><br><span class="line">Connected to: Phoenix (version 5.0)</span><br><span class="line">Driver: PhoenixEmbeddedDriver (version 5.0)</span><br><span class="line">Autocommit status: <span class="literal">true</span></span><br><span class="line">Transaction isolation: TRANSACTION_READ_COMMITTED</span><br><span class="line">Building list of tables and columns <span class="keyword">for</span> tab-completion (<span class="built_in">set</span> fastconnect to <span class="literal">true</span> to skip)...</span><br><span class="line">133/133 (100%) Done</span><br><span class="line">Done</span><br><span class="line">sqlline version 1.2.0</span><br><span class="line">0: jdbc:phoenix:hadoop001,hadoop002,hadoop003&gt;</span><br></pre></td></tr></table></figure></li>
</ol>
</li>
</ol>
<h3 id="6-2-2-Phoenix-Shell操作"><a href="#6-2-2-Phoenix-Shell操作" class="headerlink" title="6.2.2 Phoenix Shell操作"></a>6.2.2 Phoenix Shell操作</h3><ol>
<li>表的操作<ul>
<li>显示所有表  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:phoenix:hadoop001,hadoop002,hadoop003<span class="operator">&gt;</span> <span class="operator">!</span><span class="keyword">table</span></span><br><span class="line"><span class="operator">+</span><span class="comment">------------+--------------+-------------+---------------+----------+------------+----------------------------+-----------------+--------------+-----------------+---------------+</span></span><br><span class="line"><span class="operator">|</span> TABLE_CAT  <span class="operator">|</span> TABLE_SCHEM  <span class="operator">|</span> TABLE_NAME  <span class="operator">|</span>  TABLE_TYPE   <span class="operator">|</span> REMARKS  <span class="operator">|</span> TYPE_NAME  <span class="operator">|</span> SELF_REFERENCING_COL_NAME  <span class="operator">|</span> REF_GENERATION  <span class="operator">|</span> INDEX_STATE  <span class="operator">|</span> IMMUTABLE_ROWS  <span class="operator">|</span> SALT_BUCKETS  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">------------+--------------+-------------+---------------+----------+------------+----------------------------+-----------------+--------------+-----------------+---------------+</span></span><br><span class="line"><span class="operator">|</span>            <span class="operator">|</span> <span class="keyword">SYSTEM</span>       <span class="operator">|</span> CATALOG     <span class="operator">|</span> <span class="keyword">SYSTEM</span> <span class="keyword">TABLE</span>  <span class="operator">|</span>          <span class="operator">|</span>            <span class="operator">|</span>                            <span class="operator">|</span>                 <span class="operator">|</span>              <span class="operator">|</span> <span class="literal">false</span>           <span class="operator">|</span> <span class="keyword">null</span>          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>            <span class="operator">|</span> <span class="keyword">SYSTEM</span>       <span class="operator">|</span> <span class="keyword">FUNCTION</span>    <span class="operator">|</span> <span class="keyword">SYSTEM</span> <span class="keyword">TABLE</span>  <span class="operator">|</span>          <span class="operator">|</span>            <span class="operator">|</span>                            <span class="operator">|</span>                 <span class="operator">|</span>              <span class="operator">|</span> <span class="literal">false</span>           <span class="operator">|</span> <span class="keyword">null</span>          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>            <span class="operator">|</span> <span class="keyword">SYSTEM</span>       <span class="operator">|</span> LOG         <span class="operator">|</span> <span class="keyword">SYSTEM</span> <span class="keyword">TABLE</span>  <span class="operator">|</span>          <span class="operator">|</span>            <span class="operator">|</span>                            <span class="operator">|</span>                 <span class="operator">|</span>              <span class="operator">|</span> <span class="literal">true</span>            <span class="operator">|</span> <span class="number">32</span>            <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>            <span class="operator">|</span> <span class="keyword">SYSTEM</span>       <span class="operator">|</span> SEQUENCE    <span class="operator">|</span> <span class="keyword">SYSTEM</span> <span class="keyword">TABLE</span>  <span class="operator">|</span>          <span class="operator">|</span>            <span class="operator">|</span>                            <span class="operator">|</span>                 <span class="operator">|</span>              <span class="operator">|</span> <span class="literal">false</span>           <span class="operator">|</span> <span class="keyword">null</span>          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>            <span class="operator">|</span> <span class="keyword">SYSTEM</span>       <span class="operator">|</span> STATS       <span class="operator">|</span> <span class="keyword">SYSTEM</span> <span class="keyword">TABLE</span>  <span class="operator">|</span>          <span class="operator">|</span>            <span class="operator">|</span>                            <span class="operator">|</span>                 <span class="operator">|</span>              <span class="operator">|</span> <span class="literal">false</span>           <span class="operator">|</span> <span class="keyword">null</span>          <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">------------+--------------+-------------+---------------+----------+------------+----------------------------+-----------------+--------------+-----------------+---------------+</span></span><br><span class="line"><span class="number">0</span>: jdbc:phoenix:hadoop001,hadoop002,hadoop003<span class="operator">&gt;</span> <span class="operator">!</span>tables</span><br><span class="line"><span class="operator">+</span><span class="comment">------------+--------------+-------------+---------------+----------+------------+----------------------------+-----------------+--------------+-----------------+---------------+</span></span><br><span class="line"><span class="operator">|</span> TABLE_CAT  <span class="operator">|</span> TABLE_SCHEM  <span class="operator">|</span> TABLE_NAME  <span class="operator">|</span>  TABLE_TYPE   <span class="operator">|</span> REMARKS  <span class="operator">|</span> TYPE_NAME  <span class="operator">|</span> SELF_REFERENCING_COL_NAME  <span class="operator">|</span> REF_GENERATION  <span class="operator">|</span> INDEX_STATE  <span class="operator">|</span> IMMUTABLE_ROWS  <span class="operator">|</span> SALT_BUCKETS  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">------------+--------------+-------------+---------------+----------+------------+----------------------------+-----------------+--------------+-----------------+---------------+</span></span><br><span class="line"><span class="operator">|</span>            <span class="operator">|</span> <span class="keyword">SYSTEM</span>       <span class="operator">|</span> CATALOG     <span class="operator">|</span> <span class="keyword">SYSTEM</span> <span class="keyword">TABLE</span>  <span class="operator">|</span>          <span class="operator">|</span>            <span class="operator">|</span>                            <span class="operator">|</span>                 <span class="operator">|</span>              <span class="operator">|</span> <span class="literal">false</span>           <span class="operator">|</span> <span class="keyword">null</span>          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>            <span class="operator">|</span> <span class="keyword">SYSTEM</span>       <span class="operator">|</span> <span class="keyword">FUNCTION</span>    <span class="operator">|</span> <span class="keyword">SYSTEM</span> <span class="keyword">TABLE</span>  <span class="operator">|</span>          <span class="operator">|</span>            <span class="operator">|</span>                            <span class="operator">|</span>                 <span class="operator">|</span>              <span class="operator">|</span> <span class="literal">false</span>           <span class="operator">|</span> <span class="keyword">null</span>          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>            <span class="operator">|</span> <span class="keyword">SYSTEM</span>       <span class="operator">|</span> LOG         <span class="operator">|</span> <span class="keyword">SYSTEM</span> <span class="keyword">TABLE</span>  <span class="operator">|</span>          <span class="operator">|</span>            <span class="operator">|</span>                            <span class="operator">|</span>                 <span class="operator">|</span>              <span class="operator">|</span> <span class="literal">true</span>            <span class="operator">|</span> <span class="number">32</span>            <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>            <span class="operator">|</span> <span class="keyword">SYSTEM</span>       <span class="operator">|</span> SEQUENCE    <span class="operator">|</span> <span class="keyword">SYSTEM</span> <span class="keyword">TABLE</span>  <span class="operator">|</span>          <span class="operator">|</span>            <span class="operator">|</span>                            <span class="operator">|</span>                 <span class="operator">|</span>              <span class="operator">|</span> <span class="literal">false</span>           <span class="operator">|</span> <span class="keyword">null</span>          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span>            <span class="operator">|</span> <span class="keyword">SYSTEM</span>       <span class="operator">|</span> STATS       <span class="operator">|</span> <span class="keyword">SYSTEM</span> <span class="keyword">TABLE</span>  <span class="operator">|</span>          <span class="operator">|</span>            <span class="operator">|</span>                            <span class="operator">|</span>                 <span class="operator">|</span>              <span class="operator">|</span> <span class="literal">false</span>           <span class="operator">|</span> <span class="keyword">null</span>          <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">------------+--------------+-------------+---------------+----------+------------+----------------------------+-----------------+--------------+-----------------+---------------+</span></span><br><span class="line"><span class="number">0</span>: jdbc:phoenix:hadoop001,hadoop002,hadoop003<span class="operator">&gt;</span></span><br></pre></td></tr></table></figure></li>
<li>创建表  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 直接指定单个列作为RowKey</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> IF <span class="keyword">NOT</span> <span class="keyword">EXISTS</span> STUDENT (</span><br><span class="line">    id <span class="type">VARCHAR</span> <span class="keyword">primary</span> key,</span><br><span class="line">    name <span class="type">VARCHAR</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 指定多个列的联合作为RowKey</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> IF <span class="keyword">NOT</span> <span class="keyword">EXISTS</span> US_POPULATION (</span><br><span class="line">    State <span class="type">CHAR</span>(<span class="number">2</span>) <span class="keyword">NOT</span> <span class="keyword">NULL</span>,</span><br><span class="line">    City <span class="type">VARCHAR</span> <span class="keyword">NOT</span> <span class="keyword">NULL</span>,</span><br><span class="line">    Population <span class="type">BIGINT</span></span><br><span class="line"><span class="keyword">CONSTRAINT</span> my_pk <span class="keyword">PRIMARY</span> KEY (state, city));</span><br></pre></td></tr></table></figure>
<blockquote>
<p>在phoenix中，表名等会自动转换为大写，若要小写，使用双引号如”us_population”。</p>
</blockquote>
</li>
<li>插入数据</li>
<li>查询记录  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> student;</span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> student <span class="keyword">where</span> id<span class="operator">=</span><span class="string">&#x27;1001&#x27;</span>;</span><br></pre></td></tr></table></figure></li>
<li>删除记录  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">delete</span> <span class="keyword">from</span> student <span class="keyword">where</span> id<span class="operator">=</span><span class="string">&#x27;1001&#x27;</span>;</span><br></pre></td></tr></table></figure></li>
<li>删除表  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> student;</span><br></pre></td></tr></table></figure></li>
<li>退出命令行  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="operator">!</span>quit</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>表的映射<br> 默认情况下，直接在HBase中创建的表，通过Phoenix是查看不到的。如果要在Phoenix中操作直接在HBase中创建的表，则需要在Phoenix中进行表的映射。映射方式有两种：视图映射和表映射。<ul>
<li>视图映射<ol>
<li>Phoenix创建的视图是只读的，所以只能用来做查询，无法通过视图对源数据进行修改等操作</li>
<li>HBase创建表 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">hbase(main):<span class="number">031</span>:<span class="number">0</span><span class="operator">&gt;</span> <span class="keyword">create</span> <span class="string">&#x27;test&#x27;</span>,<span class="string">&#x27;info1&#x27;</span>,<span class="string">&#x27;info2&#x27;</span></span><br><span class="line">Created <span class="keyword">table</span> test</span><br><span class="line">Took <span class="number">0.7934</span> seconds</span><br><span class="line"><span class="operator">=</span><span class="operator">&gt;</span> Hbase::<span class="keyword">Table</span> <span class="operator">-</span> test</span><br><span class="line">hbase(main):<span class="number">032</span>:<span class="number">0</span><span class="operator">&gt;</span> put <span class="string">&#x27;test&#x27;</span>,<span class="string">&#x27;10001&#x27;</span>,<span class="string">&#x27;info1:name&#x27;</span>,<span class="string">&#x27;zhangsan&#x27;</span></span><br><span class="line">Took <span class="number">0.0326</span> seconds</span><br><span class="line">hbase(main):<span class="number">033</span>:<span class="number">0</span><span class="operator">&gt;</span> put <span class="string">&#x27;test&#x27;</span>,<span class="string">&#x27;10002&#x27;</span>,<span class="string">&#x27;info1:name&#x27;</span>,<span class="string">&#x27;zhangsan&#x27;</span></span><br><span class="line">Took <span class="number">0.0040</span> seconds</span><br><span class="line">hbase(main):<span class="number">034</span>:<span class="number">0</span><span class="operator">&gt;</span></span><br></pre></td></tr></table></figure></li>
<li>Phoenix创建视图，查询 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:phoenix:thin:url<span class="operator">=</span>http:<span class="operator">/</span><span class="operator">/</span>localhost:<span class="number">876</span><span class="operator">&gt;</span> <span class="keyword">create</span> <span class="keyword">view</span> &quot;test&quot;(id <span class="type">varchar</span> <span class="keyword">primary</span> key,&quot;info1&quot;.&quot;name&quot; <span class="type">varchar</span>, &quot;info2&quot;.&quot;address&quot; <span class="type">varchar</span>);</span><br><span class="line"><span class="keyword">No</span> <span class="keyword">rows</span> affected (<span class="number">5.836</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:phoenix:thin:url<span class="operator">=</span>http:<span class="operator">/</span><span class="operator">/</span>localhost:<span class="number">876</span><span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> &quot;test&quot;;</span><br><span class="line"><span class="operator">+</span><span class="comment">--------+-----------+----------+</span></span><br><span class="line"><span class="operator">|</span>   ID   <span class="operator">|</span>   name    <span class="operator">|</span> address  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">--------+-----------+----------+</span></span><br><span class="line"><span class="operator">|</span> <span class="number">10001</span>  <span class="operator">|</span> zhangsan  <span class="operator">|</span>          <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> <span class="number">10002</span>  <span class="operator">|</span> zhangsan  <span class="operator">|</span>          <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">--------+-----------+----------+</span></span><br><span class="line"><span class="number">2</span> <span class="keyword">rows</span> selected (<span class="number">0.044</span> seconds)</span><br></pre></td></tr></table></figure></li>
<li>删除视图<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:phoenix:thin:url<span class="operator">=</span>http:<span class="operator">/</span><span class="operator">/</span>localhost:<span class="number">876</span><span class="operator">&gt;</span> <span class="keyword">drop</span> <span class="keyword">view</span> &quot;test&quot;;</span><br><span class="line"> <span class="keyword">No</span> <span class="keyword">rows</span> affected (<span class="number">0.015</span> seconds)</span><br><span class="line"> <span class="number">0</span>: jdbc:phoenix:thin:url<span class="operator">=</span>http:<span class="operator">/</span><span class="operator">/</span>localhost:<span class="number">876</span><span class="operator">&gt;</span></span><br></pre></td></tr></table></figure></li>
</ol>
</li>
<li>表映射<ol>
<li>HBase中不存在表时，可以直接使用create table指令创建需要的表,系统将会自动在Phoenix和HBase中创建person_infomation的表，并会根据指令内的参数对表结构进行初始化。</li>
<li>当HBase中已经存在表时，可以以类似创建视图的方式创建关联表，只需要将create view改为create table即可。<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:phoenix:hadoop101,hadoop102,hadoop103<span class="operator">&gt;</span> <span class="keyword">create</span> <span class="keyword">table</span> &quot;test&quot;(id <span class="type">varchar</span> <span class="keyword">primary</span> key,&quot;info1&quot;.&quot;name&quot; <span class="type">varchar</span>, &quot;info2&quot;.&quot;address&quot; <span class="type">varchar</span>) column_encoded_bytes<span class="operator">=</span><span class="number">0</span>;</span><br></pre></td></tr></table></figure></li>
</ol>
</li>
</ul>
</li>
</ol>
<h3 id="6-2-3-Phoenix-JDBC操作"><a href="#6-2-3-Phoenix-JDBC操作" class="headerlink" title="6.2.3 Phoenix JDBC操作"></a>6.2.3 Phoenix JDBC操作</h3><ol>
<li>创建项目并导入依赖 <figure class="highlight xml"><table><tr><td class="code"><pre><span class="line">    <span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- https://mvnrepository.com/artifact/org.apache.phoenix/phoenix-queryserver-client --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.phoenix<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>phoenix-queryserver-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>5.0.0-HBase-2.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br></pre></td></tr></table></figure></li>
<li>编写代码 <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">PhoenixJDBC</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> SQLException </span>&#123;</span><br><span class="line">        <span class="keyword">final</span> String connectionUrl = ThinClientUtil.getConnectionUrl(<span class="string">&quot;hadoop001&quot;</span>, <span class="number">8765</span>);</span><br><span class="line">        System.out.println(connectionUrl);</span><br><span class="line">        <span class="keyword">try</span> (</span><br><span class="line"></span><br><span class="line">                <span class="keyword">final</span> Connection connection = DriverManager.getConnection(connectionUrl);</span><br><span class="line">                <span class="keyword">final</span> PreparedStatement preparedStatement = connection.prepareStatement(<span class="string">&quot;select * from student&quot;</span>);</span><br><span class="line"></span><br><span class="line">        )&#123;</span><br><span class="line">            <span class="keyword">final</span> ResultSet resultSet = preparedStatement.executeQuery();</span><br><span class="line">            <span class="keyword">while</span> (resultSet.next()) &#123;</span><br><span class="line">                <span class="keyword">final</span> String id = resultSet.getString(<span class="number">1</span>);</span><br><span class="line">                <span class="keyword">final</span> String name = resultSet.getString(<span class="number">2</span>);</span><br><span class="line">                System.out.println(<span class="string">&quot;id: &quot;</span> + id + <span class="string">&quot;,&quot;</span> + <span class="string">&quot;name: &quot;</span> + name);</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="6-3-Phoenix二级索引"><a href="#6-3-Phoenix二级索引" class="headerlink" title="6.3 Phoenix二级索引"></a>6.3 Phoenix二级索引</h2><p>已经有索引的情况下，</p>
<h3 id="6-3-1-HBase协处理器（扩展）"><a href="#6-3-1-HBase协处理器（扩展）" class="headerlink" title="6.3.1 HBase协处理器（扩展）"></a>6.3.1 HBase协处理器（扩展）</h3><ol>
<li>案例需求<br> 编写协处理器，实现在往A表插入数据的同时让HBase自身（协处理器）向B表中插入一条数据。</li>
<li>实现步骤<ol>
<li>创建一个maven项目，并引入以下依赖。 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="operator">&lt;</span>dependencies<span class="operator">&gt;</span></span><br><span class="line">    <span class="operator">&lt;</span>dependency<span class="operator">&gt;</span></span><br><span class="line">        <span class="operator">&lt;</span>groupId<span class="operator">&gt;</span>org.apache.hbase<span class="operator">&lt;</span><span class="operator">/</span>groupId<span class="operator">&gt;</span></span><br><span class="line">        <span class="operator">&lt;</span>artifactId<span class="operator">&gt;</span>hbase<span class="operator">-</span>client<span class="operator">&lt;</span><span class="operator">/</span>artifactId<span class="operator">&gt;</span></span><br><span class="line">        <span class="operator">&lt;</span>version<span class="operator">&gt;</span><span class="number">2.2</span><span class="number">.4</span><span class="operator">&lt;</span><span class="operator">/</span>version<span class="operator">&gt;</span></span><br><span class="line">    <span class="operator">&lt;</span><span class="operator">/</span>dependency<span class="operator">&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="operator">&lt;</span>dependency<span class="operator">&gt;</span></span><br><span class="line">        <span class="operator">&lt;</span>groupId<span class="operator">&gt;</span>org.apache.hbase<span class="operator">&lt;</span><span class="operator">/</span>groupId<span class="operator">&gt;</span></span><br><span class="line">        <span class="operator">&lt;</span>artifactId<span class="operator">&gt;</span>hbase<span class="operator">-</span>server<span class="operator">&lt;</span><span class="operator">/</span>artifactId<span class="operator">&gt;</span></span><br><span class="line">        <span class="operator">&lt;</span>version<span class="operator">&gt;</span><span class="number">2.2</span><span class="number">.4</span><span class="operator">&lt;</span><span class="operator">/</span>version<span class="operator">&gt;</span></span><br><span class="line">    <span class="operator">&lt;</span><span class="operator">/</span>dependency<span class="operator">&gt;</span></span><br><span class="line"><span class="operator">&lt;</span><span class="operator">/</span>dependencies<span class="operator">&gt;</span></span><br></pre></td></tr></table></figure></li>
<li>定义FruitTableCoprocessor类并继承BaseRegionObserver类 <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FruitTableCoprocessor</span> <span class="keyword">extends</span> <span class="title">BaseRegionObserver</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">postPut</span><span class="params">(ObserverContext&lt;RegionCoprocessorEnvironment&gt; e, Put put, WALEdit edit, Durability durability)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//获取连接</span></span><br><span class="line">        Connection connection = ConnectionFactory.createConnection(HBaseConfiguration.create());</span><br><span class="line"></span><br><span class="line">        <span class="comment">//获取表对象</span></span><br><span class="line">        Table table = connection.getTable(TableName.valueOf(<span class="string">&quot;fruit&quot;</span>));</span><br><span class="line"></span><br><span class="line">        <span class="comment">//插入数据</span></span><br><span class="line">        table.put(put);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//关闭资源</span></span><br><span class="line">        table.close();</span><br><span class="line">        connection.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ol>
</li>
</ol>
<h3 id="6-3-2-二级索引配置文件"><a href="#6-3-2-二级索引配置文件" class="headerlink" title="6.3.2 二级索引配置文件"></a>6.3.2 二级索引配置文件</h3><ul>
<li>添加如下配置到HBase的HRegionserver节点的hbase-site.xml  <figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!-- phoenix regionserver 配置参数--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.regionserver.wal.codec<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.hbase.regionserver.wal.IndexedWALEditCodec<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.region.server.rpc.scheduler.factory.class<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.hbase.ipc.PhoenixRpcSchedulerFactory<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>Factory to create the Phoenix RPC Scheduler that uses separate queues for index and metadata updates<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.rpc.controllerfactory.class<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.hbase.ipc.controller.ServerRpcControllerFactory<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>Factory to create the Phoenix RPC Scheduler that uses separate queues for index and metadata updates<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="6-3-3-全局二级索引"><a href="#6-3-3-全局二级索引" class="headerlink" title="6.3.3 全局二级索引"></a>6.3.3 全局二级索引</h3><ul>
<li><p>Global Index是默认的索引格式，创建全局索引时，会在HBase中建立一张新表。也就是说索引数据和数据表是存放在不同的表中的，因此<font color ='red' >全局索引适用于多读少写的业务场景</font>。</p>
</li>
<li><p>写数据的时候会消耗大量开销，因为索引表也要更新，而索引表是分布在不同的数据节点上的，跨节点的数据传输带来了较大的性能消耗。</p>
</li>
<li><p>在读数据的时候Phoenix会选择索引表来降低查询消耗的时间。</p>
</li>
<li><p>创建单个字段的全局索引</p>
  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> INDEX my_index <span class="keyword">ON</span> my_table (my_col);</span><br></pre></td></tr></table></figure>
<p>  <img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16361817666391.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"><br>  <img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16361818090972.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
<blockquote>
<p><font color ='red' >如果想查询的字段不是索引字段的话索引表不会被使用，也就是说不会带来查询速度的提升。</font></p>
</blockquote>
<ol>
<li>联合索引 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:phoenix:thin:url<span class="operator">=</span>http:<span class="operator">/</span><span class="operator">/</span>localhost:<span class="number">876</span><span class="operator">&gt;</span> <span class="keyword">create</span> index IDX_STU_NAME_AGE_IN <span class="keyword">on</span> STUDENT(name) INCLUDE(age);</span><br><span class="line"><span class="number">6</span> <span class="keyword">rows</span> affected (<span class="number">5.78</span> seconds)</span><br></pre></td></tr></table></figure></li>
<li>INCLOUD <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:phoenix:thin:url<span class="operator">=</span>http:<span class="operator">/</span><span class="operator">/</span>localhost:<span class="number">876</span><span class="operator">&gt;</span> <span class="keyword">create</span> index IDX_STU_NAME_AGE_IN <span class="keyword">on</span> STUDENT(name) INCLUDE(age);</span><br><span class="line"><span class="number">6</span> <span class="keyword">rows</span> affected (<span class="number">5.78</span> seconds)</span><br></pre></td></tr></table></figure></li>
</ol>
</li>
</ul>
<h3 id="6-3-4-本地二级索引"><a href="#6-3-4-本地二级索引" class="headerlink" title="6.3.4 本地二级索引"></a>6.3.4 本地二级索引</h3><p>Local Index适用于写操作频繁的场景。<br>索引数据和数据表的数据是存放在同一张表中（且是同一个Region），避免了在写操作的时候往不同服务器的索引表中写索引带来的额外开销。查询的字段不是索引字段索引表也会被使用，这会带来查询速度的提升。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">LOCAL</span> INDEX my_index <span class="keyword">ON</span> my_table (my_column);</span><br></pre></td></tr></table></figure>
<p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16361840708409.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
<h1 id="第7章-与Hive的集成"><a href="#第7章-与Hive的集成" class="headerlink" title="第7章 与Hive的集成"></a>第7章 与Hive的集成</h1><h2 id="7-1-HBase与Hive的对比"><a href="#7-1-HBase与Hive的对比" class="headerlink" title="7.1 HBase与Hive的对比"></a>7.1 HBase与Hive的对比</h2><ol>
<li>Hive<ul>
<li>数仓工具<br>  Hive的本质其实就相当于将HDFS中已经存储的文件在Mysql中做了一个双射关系，以方便使用HQL去管理查询。</li>
<li>用于数据分析、清洗<br>  Hive适用于离线的数据分析和清洗，延迟较高。</li>
<li>基于HDFS、MapReduce<br>  Hive存储的数据依旧在DataNode上，编写的HQL语句终将是转换为MapReduce代码执行。</li>
</ul>
</li>
<li>HBase<ol>
<li>数据库<br> 是一种面向列族存储的非关系型数据库。</li>
<li>用于存储结构化和非结构化的数据<br> 适用于单表非关系型数据的存储，不适合做关联查询，类似JOIN等操作。</li>
<li>基于HDFS<br> 数据持久化存储的体现形式是HFile，存放于DataNode中，被ResionServer以region的形式进行管理。</li>
<li>延迟较低，接入在线业务使用<br> 面对大量的企业数据，HBase可以直线单表大量数据的存储，同时提供了高效的数据访问速度。</li>
</ol>
</li>
</ol>
<h2 id="7-2-HBase与Hive集成使用"><a href="#7-2-HBase与Hive集成使用" class="headerlink" title="7.2 HBase与Hive集成使用"></a>7.2 HBase与Hive集成使用</h2><ol>
<li><p>HBase没有计算分析能力，用Hive辅助分析</p>
</li>
<li><p>HBase扮演HDFS的角色，提供数据存储<br>在hive-site.xml中修改zookeeper的属性，如下：</p>
 <figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.zookeeper.quorum<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop102,hadoop103,hadoop104<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.zookeeper.client.port<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>2181<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li>
<li><p>建立Hive表，关联HBase表，插入数据到Hive表的同时能够影响HBase表。</p>
<ol>
<li>在Hive中创建表同时关联HBase <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> hive_hbase_emp_table(</span><br><span class="line">    empno    <span class="type">int</span>,</span><br><span class="line">    ename    string,</span><br><span class="line">    job      string,</span><br><span class="line">    mgr      <span class="type">int</span>,</span><br><span class="line">    hiredate string,</span><br><span class="line">    sal      <span class="keyword">double</span>,</span><br><span class="line">    comm     <span class="keyword">double</span>,</span><br><span class="line">    deptno   <span class="type">int</span></span><br><span class="line">)</span><br><span class="line">STORED <span class="keyword">BY</span> <span class="string">&#x27;org.apache.hadoop.hive.hbase.HBaseStorageHandler&#x27;</span></span><br><span class="line"><span class="keyword">WITH</span> SERDEPROPERTIES (&quot;hbase.columns.mapping&quot; <span class="operator">=</span> &quot;:key,info:ename,info:job,info:mgr,info:hiredate,info:sal,info:comm,info:deptno&quot;)</span><br><span class="line">TBLPROPERTIES (&quot;hbase.table.name&quot; <span class="operator">=</span> &quot;hbase_emp_table&quot;);</span><br></pre></td></tr></table></figure>
<blockquote>
<p>提示：完成之后，可以分别进入Hive和HBase查看，都生成了对应的表</p>
</blockquote>
</li>
<li>在Hive中导入数据到hive_hbase_emp_table <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> hive_hbase_emp_table</span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> emp <span class="keyword">where</span> sal<span class="operator">&gt;</span><span class="number">3000</span>;</span><br></pre></td></tr></table></figure></li>
<li>查看Hive以及关联的HBase表中是否已经成功的同步插入了数据<ul>
<li>Hive：  <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> hive_hbase_emp_table;</span><br><span class="line"><span class="operator">+</span><span class="comment">-----------------------------+-----------------------------+---------------------------+---------------------------+--------------------------------+---------------------------+----------------------------+------------------------------+</span></span><br><span class="line"><span class="operator">|</span> hive_hbase_emp_table.empno  <span class="operator">|</span> hive_hbase_emp_table.ename  <span class="operator">|</span> hive_hbase_emp_table.job  <span class="operator">|</span> hive_hbase_emp_table.mgr  <span class="operator">|</span> hive_hbase_emp_table.hiredate  <span class="operator">|</span> hive_hbase_emp_table.sal  <span class="operator">|</span> hive_hbase_emp_table.comm  <span class="operator">|</span> hive_hbase_emp_table.deptno  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-----------------------------+-----------------------------+---------------------------+---------------------------+--------------------------------+---------------------------+----------------------------+------------------------------+</span></span><br><span class="line"><span class="operator">|</span> <span class="number">7839</span>                        <span class="operator">|</span> KING                        <span class="operator">|</span> PRESIDENT                 <span class="operator">|</span> <span class="keyword">NULL</span>                      <span class="operator">|</span> <span class="number">1981</span><span class="number">-11</span><span class="number">-17</span>                     <span class="operator">|</span> <span class="number">5000.0</span>                    <span class="operator">|</span> <span class="keyword">NULL</span>                       <span class="operator">|</span> <span class="number">10</span>                           <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">-----------------------------+-----------------------------+---------------------------+---------------------------+--------------------------------+---------------------------+----------------------------+------------------------------+</span></span><br><span class="line"><span class="number">1</span> <span class="type">row</span> selected (<span class="number">0.237</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span></span><br></pre></td></tr></table></figure></li>
<li>HBase：  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hbase(main):010:0&gt; scan <span class="string">&#x27;hbase_emp_table&#x27;</span></span><br><span class="line">ROW                                           COLUMN+CELL</span><br><span class="line"> 7839                                         column=info:deptno, timestamp=1636185055717, value=10</span><br><span class="line"> 7839                                         column=info:ename, timestamp=1636185055717, value=KING</span><br><span class="line"> 7839                                         column=info:hiredate, timestamp=1636185055717, value=1981-11-17</span><br><span class="line"> 7839                                         column=info:job, timestamp=1636185055717, value=PRESIDENT</span><br><span class="line"> 7839                                         column=info:sal, timestamp=1636185055717, value=5000.0</span><br><span class="line">1 row(s)</span><br><span class="line">Took 0.0418 seconds</span><br><span class="line">hbase(main):011:0&gt;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>在HBase中已经存储了某一张表hbase_emp_table，然后在Hive中创建一个外部表来关联HBase中的hbase_emp_table这张表，使之可以借助Hive来分析HBase这张表中的数据。<ol>
<li>在Hive中创建外部表 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">EXTERNAL</span> <span class="keyword">TABLE</span> relevance_hbase_emp (</span><br><span class="line">    empno    <span class="type">int</span>,</span><br><span class="line">    ename    string,</span><br><span class="line">    job      string,</span><br><span class="line">    mgr      <span class="type">int</span>,</span><br><span class="line">    hiredate string,</span><br><span class="line">    sal      <span class="keyword">double</span>,</span><br><span class="line">    comm     <span class="keyword">double</span>,</span><br><span class="line">    deptno   <span class="type">int</span></span><br><span class="line">)</span><br><span class="line">STORED <span class="keyword">BY</span> <span class="string">&#x27;org.apache.hadoop.hive.hbase.HBaseStorageHandler&#x27;</span></span><br><span class="line"><span class="keyword">WITH</span> SERDEPROPERTIES (&quot;hbase.columns.mapping&quot; <span class="operator">=</span> &quot;:key,info:ename,info:job,info:mgr,info:hiredate,info:sal,info:comm,info:deptno&quot;)</span><br><span class="line">TBLPROPERTIES (&quot;hbase.table.name&quot; <span class="operator">=</span> &quot;hbase_emp_table&quot;);</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
<li>关联后就可以使用Hive函数进行一些分析操作了 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> relevance_hbase_emp;</span><br><span class="line"><span class="operator">+</span><span class="comment">----------------------------+----------------------------+--------------------------+--------------------------+-------------------------------+--------------------------+---------------------------+-----------------------------+</span></span><br><span class="line"><span class="operator">|</span> relevance_hbase_emp.empno  <span class="operator">|</span> relevance_hbase_emp.ename  <span class="operator">|</span> relevance_hbase_emp.job  <span class="operator">|</span> relevance_hbase_emp.mgr  <span class="operator">|</span> relevance_hbase_emp.hiredate  <span class="operator">|</span> relevance_hbase_emp.sal  <span class="operator">|</span> relevance_hbase_emp.comm  <span class="operator">|</span> relevance_hbase_emp.deptno  <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------------------------+----------------------------+--------------------------+--------------------------+-------------------------------+--------------------------+---------------------------+-----------------------------+</span></span><br><span class="line"><span class="operator">|</span> <span class="number">7839</span>                       <span class="operator">|</span> KING                       <span class="operator">|</span> PRESIDENT                <span class="operator">|</span> <span class="keyword">NULL</span>                     <span class="operator">|</span> <span class="number">1981</span><span class="number">-11</span><span class="number">-17</span>                    <span class="operator">|</span> <span class="number">5000.0</span>                   <span class="operator">|</span> <span class="keyword">NULL</span>                      <span class="operator">|</span> <span class="number">10</span>                          <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">----------------------------+----------------------------+--------------------------+--------------------------+-------------------------------+--------------------------+---------------------------+-----------------------------+</span></span><br><span class="line"><span class="number">1</span> <span class="type">row</span> selected (<span class="number">0.248</span> seconds)</span><br><span class="line"><span class="number">0</span>: jdbc:hive2:<span class="operator">/</span><span class="operator">/</span>hadoop001:<span class="number">10000</span><span class="operator">&gt;</span></span><br></pre></td></tr></table></figure></li>
</ol>
</li>
</ol>
</li>
</ol>
]]></content>
      <categories>
        <category>HBase</category>
      </categories>
      <tags>
        <tag>数据库</tag>
        <tag>大数据</tag>
        <tag>HBase</tag>
        <tag>Phoenix</tag>
      </tags>
  </entry>
  <entry>
    <title>Flume</title>
    <url>/2021/11/07/Flume/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="Flume"><a href="#Flume" class="headerlink" title="Flume"></a>Flume</h1><h1 id="一、Flume-概述"><a href="#一、Flume-概述" class="headerlink" title="一、Flume 概述"></a>一、Flume 概述</h1><h2 id="1-1-Flume-定义"><a href="#1-1-Flume-定义" class="headerlink" title="1.1 Flume 定义"></a>1.1 Flume 定义</h2><ul>
<li>Flume 是 Cloudera 提供的一个高可用的，高可靠的，分布式的海量日志采集、聚合和传<br>输的系统。Flume 基于流式架构，灵活简单。<br>  <img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16357485584074.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></li>
</ul>
<h2 id="1-2-Flume-基础架构"><a href="#1-2-Flume-基础架构" class="headerlink" title="1.2 Flume 基础架构"></a>1.2 Flume 基础架构</h2><p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16357485788013.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
<h3 id="1-2-1-Agent"><a href="#1-2-1-Agent" class="headerlink" title="1.2.1 Agent"></a>1.2.1 Agent</h3><ul>
<li>Agent 是一个 JVM 进程，它以事件的形式将数据从源头送至目的。</li>
<li>Agent 主要有 3 个部分组成，Source、Channel、Sink。</li>
</ul>
<h3 id="1-2-2-Source"><a href="#1-2-2-Source" class="headerlink" title="1.2.2 Source"></a>1.2.2 Source</h3><ul>
<li>Source 是负责接收数据到 Flume Agent 的组件。Source 组件可以处理各种类型、各种格式的日志数据，包括 avro、thrift、exec、jms、spooling directory、netcat、taildir、sequence generator、syslog、http、legacy。</li>
</ul>
<h3 id="1-2-3-Sink"><a href="#1-2-3-Sink" class="headerlink" title="1.2.3 Sink"></a>1.2.3 Sink</h3><ul>
<li>Sink 不断地轮询 Channel 中的事件且批量地移除它们，并将这些事件批量写入到存储<br>或索引系统、或者被发送到另一个 Flume Agent。</li>
<li>Sink 组件目的地包括 hdfs、logger、avro、thrift、ipc、file、HBase、solr、自定义。</li>
</ul>
<h3 id="1-2-4-Channel"><a href="#1-2-4-Channel" class="headerlink" title="1.2.4 Channel"></a>1.2.4 Channel</h3><ul>
<li>Channel 是位于 Source 和 Sink 之间的缓冲区。因此，Channel 允许 Source 和 Sink 运作在不同的速率上。Channel 是线程安全的，可以同时处理几个 Source 的写入操作和几个</li>
<li>Sink 的读取操作。<ul>
<li>Flume 自带两种 Channel：Memory Channel 和 File Channel。<ul>
<li>Memory Channel 是内存中的队列。Memory Channel 在不需要关心数据丢失的情景下适用。如果需要关心数据丢失，那么 Memory Channel 就不应该使用，因为程序死亡、机器宕机或者重启都会导致数据丢失。</li>
<li>File Channel 将所有事件写到磁盘。因此在程序关闭或机器宕机的情况下不会丢失数据。</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="1-2-5-Event"><a href="#1-2-5-Event" class="headerlink" title="1.2.5 Event"></a>1.2.5 Event</h3><ul>
<li>传输单元，Flume 数据传输的基本单元，以 Event 的形式将数据从源头送至目的地。Event 由 Header 和 Body 两部分组成，Header 用来存放该 event 的一些属性，为 K-V 结构，</li>
<li>Body 用来存放该条数据，形式为字节数组。<br><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16357486063296.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></li>
</ul>
<h1 id="二、Flume入门"><a href="#二、Flume入门" class="headerlink" title="二、Flume入门"></a>二、Flume入门</h1><h2 id="2-1-Flume安装部署"><a href="#2-1-Flume安装部署" class="headerlink" title="2.1 Flume安装部署"></a>2.1 Flume安装部署</h2><h3 id="2-1-1-安装地址"><a href="#2-1-1-安装地址" class="headerlink" title="2.1.1 安装地址"></a>2.1.1 安装地址</h3><ol>
<li>Flume官网地址：<a href="http://flume.apache.org/">http://flume.apache.org/</a></li>
<li>文档查看地址：<a href="http://flume.apache.org/FlumeUserGuide.html">http://flume.apache.org/FlumeUserGuide.html</a></li>
<li>下载地址：<a href="http://archive.apache.org/dist/flume/">http://archive.apache.org/dist/flume/</a></li>
</ol>
<h3 id="2-1-2-安装部署"><a href="#2-1-2-安装部署" class="headerlink" title="2.1.2 安装部署"></a>2.1.2 安装部署</h3><ol>
<li>将apache-flume-1.9.0-bin.tar.gz上传到linux的/opt/software目录下</li>
<li>解压apache-flume-1.9.0-bin.tar.gz到/opt/module/目录下 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 software]$ tar -zxf /opt/software/apache-flume-1.9.0-bin.tar.gz -C /opt/module/</span><br></pre></td></tr></table></figure></li>
<li>修改apache-flume-1.9.0-bin的名称为flume <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 module]$ mv /opt/module/apache-flume-1.9.0-bin /opt/module/flume</span><br></pre></td></tr></table></figure></li>
<li>将lib文件夹下的guava-11.0.2.jar删除以兼容Hadoop 3.1.3 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 lib]$  rm /opt/module/flume/lib/guava-11.0.2.jar</span><br></pre></td></tr></table></figure></li>
<li>环境变量 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">#FLUME_HOME</span></span><br><span class="line"><span class="built_in">export</span> FLUME_HOME=/opt/module/flume-1.9.0</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$FLUME_HOME</span>/bin</span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="2-1-3-连接HDFS集群"><a href="#2-1-3-连接HDFS集群" class="headerlink" title="2.1.3 连接HDFS集群"></a>2.1.3 连接HDFS集群</h3><ol>
<li>使用场景：应用服务器部署FLume Agent采集应用运行日志，上送到Flume Server，由FlumeServer存储到HDFS集群<br> <img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16359837253952.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></li>
<li>配置步骤<ol>
<li>复制Hadoop集群的hdfs-site.xml、core-site.xml两个配置到$FLUME_HOME/conf目录 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">scp <span class="variable">$HADOOP_HOME</span>/etc/hadoop/core-site.xml atguigu@datax:/opt/module/flume-1.9.0/conf</span><br><span class="line"></span><br><span class="line">scp <span class="variable">$HADOOP_HOME</span>/etc/hadoop/hdfs-site.xml atguigu@datax:/opt/module/flume-1.9.0/conf</span><br></pre></td></tr></table></figure></li>
<li>复制hadoop-common-3.1.3.jar到$FLUME_HOME/lib目录 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">scp <span class="variable">$HADOOP_HOME</span>/share/hadoop/common/hadoop-common-3.1.3.jar atguigu@datax:/opt/module/flume-1.9.0/lib</span><br></pre></td></tr></table></figure></li>
<li>复制Hadoop common依赖包到$FLUME_HOME/lib目录 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">scp <span class="variable">$HADOOP_HOME</span>/share/hadoop/common/lib/*.jar atguigu@datax:/opt/module/flume-1.9.0/lib</span><br></pre></td></tr></table></figure></li>
<li>复制<code>$HADOOP_HOME/share/hadoop/hdfs/*.jar</code>到$FLUME_HOME/lib目录 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">scp <span class="variable">$HADOOP_HOME</span>/share/hadoop/hdfs/*.jar atguigu@datax:/opt/module/flume-1.9.0/lib</span><br></pre></td></tr></table></figure></li>
<li>准备配置 a1.conf <figure class="highlight ini"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Name the components on this agent</span></span><br><span class="line"><span class="comment"># a1:表示agent的名称</span></span><br><span class="line"><span class="comment"># r1:表示a1的Source的名称</span></span><br><span class="line"><span class="attr">a1.sources</span> = r1</span><br><span class="line"><span class="comment"># k1:表示a1的Sink的名称</span></span><br><span class="line"><span class="attr">a1.sinks</span> = k1</span><br><span class="line"><span class="comment"># c1:表示a1的Channel的名称</span></span><br><span class="line"><span class="attr">a1.channels</span> = c1</span><br><span class="line"></span><br><span class="line"><span class="comment"># Describe/configure the source</span></span><br><span class="line"><span class="comment"># 表示a1的输入源类型为netcat端口类型</span></span><br><span class="line"><span class="attr">a1.sources.r1.type</span> = netcat</span><br><span class="line"><span class="comment"># 表示a1的监听的主机</span></span><br><span class="line"><span class="attr">a1.sources.r1.bind</span> = <span class="number">0.0</span>.<span class="number">0.0</span></span><br><span class="line"><span class="comment"># 表示a1的监听的端口号</span></span><br><span class="line"><span class="attr">a1.sources.r1.port</span> = <span class="number">44444</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Describe the sink</span></span><br><span class="line"><span class="attr">a1.sinks.k1.type</span> = hdfs</span><br><span class="line"><span class="attr">a1.sinks.k1.hdfs.path</span> = hdfs://mycluster/flume/%Y%m%d/%H</span><br><span class="line"><span class="comment">#上传文件的前缀</span></span><br><span class="line"><span class="attr">a1.sinks.k1.hdfs.filePrefix</span> = logs-</span><br><span class="line"><span class="comment">#是否按照时间滚动文件夹</span></span><br><span class="line"><span class="attr">a1.sinks.k1.hdfs.round</span> = <span class="literal">true</span></span><br><span class="line"><span class="comment">#多少时间单位创建一个新的文件夹</span></span><br><span class="line"><span class="attr">a1.sinks.k1.hdfs.roundValue</span> = <span class="number">1</span></span><br><span class="line"><span class="comment">#重新定义时间单位</span></span><br><span class="line"><span class="attr">a1.sinks.k1.hdfs.roundUnit</span> = hour</span><br><span class="line"><span class="comment">#是否使用本地时间戳</span></span><br><span class="line"><span class="attr">a1.sinks.k1.hdfs.useLocalTimeStamp</span> = <span class="literal">true</span></span><br><span class="line"><span class="comment">#积攒多少个Event才flush到HDFS一次</span></span><br><span class="line"><span class="attr">a1.sinks.k1.hdfs.batchSize</span> = <span class="number">100</span></span><br><span class="line"><span class="comment">#设置文件类型，可支持压缩</span></span><br><span class="line"><span class="attr">a1.sinks.k1.hdfs.fileType</span> = DataStream</span><br><span class="line"><span class="comment">#多久生成一个新的文件</span></span><br><span class="line"><span class="attr">a1.sinks.k1.hdfs.rollInterval</span> = <span class="number">60</span></span><br><span class="line"><span class="comment">#设置每个文件的滚动大小</span></span><br><span class="line"><span class="attr">a1.sinks.k1.hdfs.rollSize</span> = <span class="number">134217700</span></span><br><span class="line"><span class="comment">#文件的滚动与Event数量无关</span></span><br><span class="line"><span class="attr">a1.sinks.k1.hdfs.rollCount</span> = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Use a channel which buffers events in memory</span></span><br><span class="line"><span class="attr">a1.channels.c1.type</span> = memory</span><br><span class="line"><span class="attr">a1.channels.c1.capacity</span> = <span class="number">1000</span></span><br><span class="line"><span class="attr">a1.channels.c1.transactionCapacity</span> = <span class="number">100</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Bind the source and sink to the channel</span></span><br><span class="line"><span class="attr">a1.sources.r1.channels</span> = c1</span><br><span class="line"><span class="attr">a1.sinks.k1.channel</span> = c1</span><br></pre></td></tr></table></figure></li>
<li>启动Flume <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">flume-ng agent -c <span class="variable">$FLUME_HOME</span>/conf -f <span class="variable">$FLUME_HOME</span>/conf/<span class="built_in">jobs</span>/a1.conf -n a1 -Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure></li>
<li>打开nc测试，检查HDFS文件</li>
</ol>
</li>
<li>异常记录<ol>
<li><code>java.lang.NoClassDefFoundError</code>类型的异常为缺少jar包导致<ul>
<li>解决方案：复制hadoop-common-3.1.3.jar到$FLUME_HOME/lib目录</li>
</ul>
</li>
<li><code>java.lang.NoSuchMethodError</code>类型异常为jar包冲突导致<ul>
<li>解决方案：相同jar包保留一个版本，具体保留那个需要测试是否存在兼容性问题，优先保留Flume原生依赖jar</li>
</ul>
</li>
<li><code>No FileSystem for scheme &quot;hdfs&quot;</code>异常 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">2021-11-04 10:32:45,923 (SinkRunner-PollingRunner-DefaultSinkProcessor) [WARN - org.apache.flume.sink.hdfs.HDFSEventSink.process(HDFSEventSink.java:454)] HDFS IO error</span><br><span class="line">org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme &quot;hdfs&quot;</span><br><span class="line">	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3281)</span><br><span class="line">	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3301)</span><br><span class="line">	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124)</span><br><span class="line">	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352)</span><br><span class="line">	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320)</span><br><span class="line">	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479)</span><br><span class="line">	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:361)</span><br><span class="line">	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:255)</span><br><span class="line">	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:247)</span><br><span class="line">	at org.apache.flume.sink.hdfs.BucketWriter$8$1.run(BucketWriter.java:727)</span><br><span class="line">	at org.apache.flume.auth.SimpleAuthenticator.execute(SimpleAuthenticator.java:50)</span><br><span class="line">	at org.apache.flume.sink.hdfs.BucketWriter$8.call(BucketWriter.java:724)</span><br><span class="line">	at java.util.concurrent.FutureTask.run(FutureTask.java:266)</span><br><span class="line">	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)</span><br><span class="line">	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)</span><br><span class="line">	at java.lang.Thread.run(Thread.java:748)</span><br></pre></td></tr></table></figure>
<ul>
<li>解决方案：复制<code>$HADOOP_HOME/share/hadoop/hdfs/*.jar</code>到$FLUME_HOME/lib目录  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">scp <span class="variable">$HADOOP_HOME</span>/share/hadoop/hdfs/*.jar atguigu@datax:/opt/module/flume-1.9.0/lib</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><code>java.net.UnknownHostException: mycluster</code> 找不到mycluster<ul>
<li>解决方案：复制Hadoop集群的hdfs-site.xml、core-site.xml两个配置到$FLUME_HOME/conf目录  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 ha-hadoop-3.1.3]$ scp <span class="variable">$HADOOP_HOME</span>/etc/hadoop/core-site.xml atguigu@datax:/opt/module/flume-1.9.0/conf</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop001 ha-hadoop-3.1.3]$ scp <span class="variable">$HADOOP_HOME</span>/etc/hadoop/hdfs-site.xml atguigu@datax:/opt/module/flume-1.9.0/conf</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><code>org.apache.hadoop.security.AccessControlException: Permission denied:</code> HDFS 权限不足，换用户 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">chown -R atguigu:atguigu flume-1.9.0/</span><br><span class="line">use atguigu</span><br></pre></td></tr></table></figure>
<h2 id="2-2-Flume入门案例"><a href="#2-2-Flume入门案例" class="headerlink" title="2.2 Flume入门案例"></a>2.2 Flume入门案例</h2><h3 id="2-2-1-监控端口数据官方案例"><a href="#2-2-1-监控端口数据官方案例" class="headerlink" title="2.2.1 监控端口数据官方案例"></a>2.2.1 监控端口数据官方案例</h3></li>
</ol>
</li>
<li>案例需求：<ul>
<li>使用Flume监听一个端口，收集该端口数据，并打印到控制台。 </li>
</ul>
</li>
<li>需求分析：<br> <img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16357500185897.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></li>
<li>实现步骤：<ol>
<li>安装netcat工具 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 module]$ sudo yum -y install nc</span><br></pre></td></tr></table></figure>
<ul>
<li>测试  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 开启端口</span></span><br><span class="line">[atguigu@hadoop001 ~]$ nc -l localhost 6666</span><br><span class="line">asdf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试连通</span></span><br><span class="line">[atguigu@hadoop001 ~]$ nc localhost 6666</span><br><span class="line">asdf</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>判断44444端口是否被占用 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 flume-telnet]$ sudo netstat -nlp | grep 44444</span><br></pre></td></tr></table></figure></li>
<li>在flume目录下创建job文件夹并进入job文件夹。 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 flume]$ mkdir job</span><br><span class="line">[atguigu@hadoop102 flume]$ <span class="built_in">cd</span> job/</span><br></pre></td></tr></table></figure></li>
<li>在job文件夹下创建Flume Agent配置文件flume-netcat-logger.conf。添加内容如下： <figure class="highlight ini"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Name the components on this agent</span></span><br><span class="line"><span class="comment"># a1:表示agent的名称</span></span><br><span class="line"><span class="comment"># r1:表示a1的Source的名称</span></span><br><span class="line"><span class="attr">a1.sources</span> = r1</span><br><span class="line"><span class="comment"># k1:表示a1的Sink的名称</span></span><br><span class="line"><span class="attr">a1.sinks</span> = k1</span><br><span class="line"><span class="comment"># c1:表示a1的Channel的名称</span></span><br><span class="line"><span class="attr">a1.channels</span> = c1</span><br><span class="line"></span><br><span class="line"><span class="comment"># Describe/configure the source</span></span><br><span class="line"><span class="comment"># 表示a1的输入源类型为netcat端口类型</span></span><br><span class="line"><span class="attr">a1.sources.r1.type</span> = netcat</span><br><span class="line"><span class="comment"># 表示a1的监听的主机</span></span><br><span class="line"><span class="attr">a1.sources.r1.bind</span> = localhost</span><br><span class="line"><span class="comment"># 表示a1的监听的端口号</span></span><br><span class="line"><span class="attr">a1.sources.r1.port</span> = <span class="number">44444</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Describe the sink</span></span><br><span class="line"><span class="comment"># 表示a1的输出目的地是控制台logger类型</span></span><br><span class="line"><span class="attr">a1.sinks.k1.type</span> = logger</span><br><span class="line"></span><br><span class="line"><span class="comment"># Use a channel which buffers events in memory</span></span><br><span class="line"><span class="comment"># 表示a1的channel类型是memory内存型</span></span><br><span class="line"><span class="attr">a1.channels.c1.type</span> = memory</span><br><span class="line"><span class="comment"># 表示a1的channel总容量1000个event</span></span><br><span class="line"><span class="attr">a1.channels.c1.capacity</span> = <span class="number">1000</span></span><br><span class="line"><span class="comment"># 表示a1的channel传输时收集到了100条event以后再去提交事务</span></span><br><span class="line"><span class="attr">a1.channels.c1.transactionCapacity</span> = <span class="number">100</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Bind the source and sink to the channel</span></span><br><span class="line"><span class="comment"># 表示将r1和c1连接起来</span></span><br><span class="line"><span class="attr">a1.sources.r1.channels</span> = c1</span><br><span class="line"><span class="comment"># 表示将k1和c1连接起来</span></span><br><span class="line"><span class="attr">a1.sinks.k1.channel</span> = c1</span><br></pre></td></tr></table></figure></li>
<li>先开启flume监听端口<ul>
<li>第一种写法：  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 flume]$ bin/flume-ng agent --conf conf/ --name a1 --conf-file job/flume-netcat-logger.conf -Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure></li>
<li>第二种写法：  <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 flume]$ bin/flume-ng agent -c conf/ -n a1 -f job/flume-netcat-logger.conf -Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure>
<ul>
<li>参数说明：<ul>
<li>–conf/-c：表示配置文件存储在conf/目录</li>
<li>–name/-n：表示给agent起名为a1</li>
<li>–conf-file/-f：flume本次启动读取的配置文件是在job文件夹下的flume-telnet.conf文件。</li>
<li>-Dflume.root.logger=INFO,console ：-D表示flume运行时动态修改flume.root.logger参数属性值，并将控制台日志打印级别设置为INFO级别。日志级别包括:log、info、warn、error。</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ol>
</li>
</ol>
<pre><code>7. 使用netcat工具向本机的44444端口发送内容
    <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 ~]$ nc localhost 44444</span><br><span class="line">hello</span><br><span class="line">OK</span><br><span class="line">atguigu</span><br><span class="line">OK</span><br><span class="line">123</span><br><span class="line">OK</span><br></pre></td></tr></table></figure>
8. 在Flume监听页面观察接收数据情况
    <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">2021-11-01 16:15:15,418 INFO  [lifecycleSupervisor-1-1] source.NetcatSource (NetcatSource.java:start(166)) - Created serverSocket:sun.nio.ch.ServerSocketChannelImpl[/127.0.0.1:44444]</span><br><span class="line">2021-11-01 16:15:47,687 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] sink.LoggerSink (LoggerSink.java:process(95)) - Event: &#123; headers:&#123;&#125; body: 68 65 6C 6C 6F                                  hello &#125;</span><br><span class="line">2021-11-01 16:15:51,920 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] sink.LoggerSink (LoggerSink.java:process(95)) - Event: &#123; headers:&#123;&#125; body: 61 74 67 75 69 67 75                            atguigu &#125;</span><br><span class="line">2021-11-01 16:15:53,944 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] sink.LoggerSink (LoggerSink.java:process(95)) - Event: &#123; headers:&#123;&#125; body: 31 32 33                                        123 &#125;</span><br></pre></td></tr></table></figure>
</code></pre>
<h3 id="2-2-2-实时监控单个追加文件"><a href="#2-2-2-实时监控单个追加文件" class="headerlink" title="2.2.2 实时监控单个追加文件"></a>2.2.2 实时监控单个追加文件</h3><ol>
<li>案例需求：实时监控Hive日志，并上传到HDFS中</li>
<li>需求分析：<br> <img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16357546764719.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></li>
<li>实现步骤：<ol>
<li>Flume要想将数据输出到HDFS，依赖Hadoop相关jar包。确认Hadoop和Java环境变量配置正确 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">#JAVA_HOME</span></span><br><span class="line"><span class="built_in">export</span> JAVA_HOME=/opt/module/jdk1.8.0_212</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$JAVA_HOME</span>/bin</span><br><span class="line"></span><br><span class="line"><span class="comment">#HADOOP_HOME</span></span><br><span class="line"><span class="built_in">export</span> HADOOP_HOME=/opt/module/ha-hadoop-3.1.3</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$HADOOP_HOME</span>/bin</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$HADOOP_HOME</span>/sbin</span><br></pre></td></tr></table></figure></li>
<li>创建flume-file-hdfs.conf文件<ul>
<li>注：要想读取Linux系统中的文件，就得按照Linux命令的规则执行命令。由于Hive日志在Linux系统中所以读取文件的类型选择：exec即execute执行的意思。表示执行Linux命令来读取文件。  <figure class="highlight ini"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Name the components on this agent</span></span><br><span class="line"><span class="attr">a2.sources</span> = r2</span><br><span class="line"><span class="attr">a2.sinks</span> = k2</span><br><span class="line"><span class="attr">a2.channels</span> = c2</span><br><span class="line"></span><br><span class="line"><span class="comment"># Describe/configure the source</span></span><br><span class="line"><span class="attr">a2.sources.r2.type</span> = exec</span><br><span class="line"><span class="attr">a2.sources.r2.command</span> = tail -f /opt/module/hive/logs/hive.log</span><br><span class="line"></span><br><span class="line"><span class="comment"># Describe the sink</span></span><br><span class="line"><span class="attr">a2.sinks.k2.type</span> = hdfs</span><br><span class="line"><span class="attr">a2.sinks.k2.hdfs.path</span> = hdfs://hadoop001:<span class="number">8020</span>/flume/%Y%m%d/%H</span><br><span class="line"><span class="comment">#上传文件的前缀</span></span><br><span class="line"><span class="attr">a2.sinks.k2.hdfs.filePrefix</span> = logs-</span><br><span class="line"><span class="comment">#是否按照时间滚动文件夹</span></span><br><span class="line"><span class="attr">a2.sinks.k2.hdfs.round</span> = <span class="literal">true</span></span><br><span class="line"><span class="comment">#多少时间单位创建一个新的文件夹</span></span><br><span class="line"><span class="attr">a2.sinks.k2.hdfs.roundValue</span> = <span class="number">1</span></span><br><span class="line"><span class="comment">#重新定义时间单位</span></span><br><span class="line"><span class="attr">a2.sinks.k2.hdfs.roundUnit</span> = hour</span><br><span class="line"><span class="comment">#是否使用本地时间戳</span></span><br><span class="line"><span class="attr">a2.sinks.k2.hdfs.useLocalTimeStamp</span> = <span class="literal">true</span></span><br><span class="line"><span class="comment">#积攒多少个Event才flush到HDFS一次</span></span><br><span class="line"><span class="attr">a2.sinks.k2.hdfs.batchSize</span> = <span class="number">100</span></span><br><span class="line"><span class="comment">#设置文件类型，可支持压缩</span></span><br><span class="line"><span class="attr">a2.sinks.k2.hdfs.fileType</span> = DataStream</span><br><span class="line"><span class="comment">#多久生成一个新的文件</span></span><br><span class="line"><span class="attr">a2.sinks.k2.hdfs.rollInterval</span> = <span class="number">60</span></span><br><span class="line"><span class="comment">#设置每个文件的滚动大小</span></span><br><span class="line"><span class="attr">a2.sinks.k2.hdfs.rollSize</span> = <span class="number">134217700</span></span><br><span class="line"><span class="comment">#文件的滚动与Event数量无关</span></span><br><span class="line"><span class="attr">a2.sinks.k2.hdfs.rollCount</span> = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Use a channel which buffers events in memory</span></span><br><span class="line"><span class="attr">a2.channels.c2.type</span> = memory</span><br><span class="line"><span class="attr">a2.channels.c2.capacity</span> = <span class="number">1000</span></span><br><span class="line"><span class="attr">a2.channels.c2.transactionCapacity</span> = <span class="number">100</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Bind the source and sink to the channel</span></span><br><span class="line"><span class="attr">a2.sources.r2.channels</span> = c2</span><br><span class="line"><span class="attr">a2.sinks.k2.channel</span> = c2</span><br></pre></td></tr></table></figure></li>
<li>注意: 对于所有与时间相关的转义序列，Event Header中必须存在以 “timestamp”的key（除非hdfs.useLocalTimeStamp设置为true，此方法会使用TimestampInterceptor自动添加timestamp）。a3.sinks.k3.hdfs.useLocalTimeStamp = true</li>
</ul>
</li>
<li>运行Flume <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">flume-ng agent --conf conf/ --name a2 --conf-file flume-file-hdfs.conf</span><br></pre></td></tr></table></figure></li>
<li>在HDFS上查看文件。 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 job]$ hadoop fs -ls /flume/20211101/16</span><br><span class="line">Found 2 items</span><br><span class="line">-rw-r--r--   3 atguigu supergroup      79991 2021-11-01 16:48 /flume/20211101/16/logs-.1635756437252</span><br><span class="line">-rw-r--r--   3 atguigu supergroup       5037 2021-11-01 16:49 /flume/20211101/16/logs-.1635756498528</span><br><span class="line">[atguigu@hadoop001 job]$</span><br></pre></td></tr></table></figure></li>
</ol>
</li>
</ol>
<h3 id="2-2-3-实时监控目录下多个新文件"><a href="#2-2-3-实时监控目录下多个新文件" class="headerlink" title="2.2.3 实时监控目录下多个新文件"></a>2.2.3 实时监控目录下多个新文件</h3><p>1）案例需求：使用Flume监听整个目录的文件，并上传至HDFS<br>2）需求分析：</p>
<h3 id="2-2-4-实时监控目录下的多个追加文件"><a href="#2-2-4-实时监控目录下的多个追加文件" class="headerlink" title="2.2.4 实时监控目录下的多个追加文件"></a>2.2.4 实时监控目录下的多个追加文件</h3><ul>
<li>Exec source适用于监控一个实时追加的文件，不能实现断点续传；</li>
<li>Spooldir Source适合用于同步新文件，但不适合对实时追加日志的文件进行监听并同步；</li>
<li>Taildir Source适合用于监听多个实时追加的文件，并且能够实现断点续传。</li>
</ul>
<ol>
<li>案例需求:使用Flume监听整个目录的实时追加文件，并上传至HDFS</li>
<li>需求分析:<br> <img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16357594199633.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></li>
<li>实现步骤：<ol>
<li>创建配置文件flume-taildir-hdfs.conf, 添加如下内容 <figure class="highlight ini"><table><tr><td class="code"><pre><span class="line"><span class="attr">a3.sources</span> = r3</span><br><span class="line"><span class="attr">a3.sinks</span> = k3</span><br><span class="line"><span class="attr">a3.channels</span> = c3</span><br><span class="line"></span><br><span class="line"><span class="comment"># Describe/configure the source</span></span><br><span class="line"><span class="attr">a3.sources.r3.type</span> = TAILDIR</span><br><span class="line"><span class="attr">a3.sources.r3.positionFile</span> = /opt/module/flume/tail_dir.json</span><br><span class="line"><span class="attr">a3.sources.r3.filegroups</span> = f1 f2</span><br><span class="line"><span class="attr">a3.sources.r3.filegroups.f1</span> = /opt/module/flume/files/.*file.*</span><br><span class="line"><span class="attr">a3.sources.r3.filegroups.f2</span> = /opt/module/flume/files2/.*log.*</span><br><span class="line"></span><br><span class="line"><span class="comment"># Describe the sink</span></span><br><span class="line"><span class="attr">a3.sinks.k3.type</span> = hdfs</span><br><span class="line"><span class="attr">a3.sinks.k3.hdfs.path</span> = hdfs://hadoop001:<span class="number">8020</span>/flume/TAILDIR/%Y%m%d/%H</span><br><span class="line"><span class="comment">#上传文件的前缀</span></span><br><span class="line"><span class="attr">a3.sinks.k3.hdfs.filePrefix</span> = upload-</span><br><span class="line"><span class="comment">#是否按照时间滚动文件夹</span></span><br><span class="line"><span class="attr">a3.sinks.k3.hdfs.round</span> = <span class="literal">true</span></span><br><span class="line"><span class="comment">#多少时间单位创建一个新的文件夹</span></span><br><span class="line"><span class="attr">a3.sinks.k3.hdfs.roundValue</span> = <span class="number">1</span></span><br><span class="line"><span class="comment">#重新定义时间单位</span></span><br><span class="line"><span class="attr">a3.sinks.k3.hdfs.roundUnit</span> = hour</span><br><span class="line"><span class="comment">#是否使用本地时间戳</span></span><br><span class="line"><span class="attr">a3.sinks.k3.hdfs.useLocalTimeStamp</span> = <span class="literal">true</span></span><br><span class="line"><span class="comment">#积攒多少个Event才flush到HDFS一次</span></span><br><span class="line"><span class="attr">a3.sinks.k3.hdfs.batchSize</span> = <span class="number">100</span></span><br><span class="line"><span class="comment">#设置文件类型，可支持压缩</span></span><br><span class="line"><span class="attr">a3.sinks.k3.hdfs.fileType</span> = DataStream</span><br><span class="line"><span class="comment">#多久生成一个新的文件</span></span><br><span class="line"><span class="attr">a3.sinks.k3.hdfs.rollInterval</span> = <span class="number">60</span></span><br><span class="line"><span class="comment">#设置每个文件的滚动大小大概是128M</span></span><br><span class="line"><span class="attr">a3.sinks.k3.hdfs.rollSize</span> = <span class="number">134217700</span></span><br><span class="line"><span class="comment">#文件的滚动与Event数量无关</span></span><br><span class="line"><span class="attr">a3.sinks.k3.hdfs.rollCount</span> = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Use a channel which buffers events in memory</span></span><br><span class="line"><span class="attr">a3.channels.c3.type</span> = memory</span><br><span class="line"><span class="attr">a3.channels.c3.capacity</span> = <span class="number">1000</span></span><br><span class="line"><span class="attr">a3.channels.c3.transactionCapacity</span> = <span class="number">100</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Bind the source and sink to the channel</span></span><br><span class="line"><span class="attr">a3.sources.r3.channels</span> = c3</span><br><span class="line"><span class="attr">a3.sinks.k3.channel</span> = c3</span><br></pre></td></tr></table></figure></li>
<li>运行Flume, 监控文件夹 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">flume-ng agent --conf conf/ --name a3 --conf-file job/flume-taildir-hdfs.conf</span><br></pre></td></tr></table></figure></li>
<li>查看HDFS上的数据 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 logs]$ hadoop fs -ls /flume/TAILDIR/20211101/18</span><br><span class="line">Found 1 items</span><br><span class="line">-rw-r--r--   3 atguigu supergroup   77439185 2021-11-01 18:05 /flume/TAILDIR/20211101/18/upload-.1635761078949</span><br><span class="line">[atguigu@hadoop001 logs]$</span><br></pre></td></tr></table></figure></li>
</ol>
</li>
<li>Taildir说明：<ul>
<li>Taildir Source维护了一个json格式的position File，其会定期的往position File中更新每个文件读取到的最新的位置，因此能够实现断点续传。Position File的格式如下：  <figure class="highlight json"><table><tr><td class="code"><pre><span class="line">&#123;<span class="attr">&quot;inode&quot;</span>:<span class="number">2496272</span>,<span class="attr">&quot;pos&quot;</span>:<span class="number">12</span>,<span class="attr">&quot;file&quot;</span>:<span class="string">&quot;/opt/module/flume/files/file1.txt&quot;</span>&#125;</span><br><span class="line">&#123;<span class="attr">&quot;inode&quot;</span>:<span class="number">2496275</span>,<span class="attr">&quot;pos&quot;</span>:<span class="number">12</span>,<span class="attr">&quot;file&quot;</span>:<span class="string">&quot;/opt/module/flume/files/file2.txt&quot;</span>&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
<li>注：Linux中储存文件元数据的区域就叫做inode，每个inode都有一个号码，操作系统用inode号码来识别不同的文件，Unix/Linux系统内部不使用文件名，而使用inode号码来识别文件。</li>
</ul>
</li>
</ol>
<h1 id="三、Flume进阶"><a href="#三、Flume进阶" class="headerlink" title="三、Flume进阶"></a>三、Flume进阶</h1><h2 id="3-1-Flume事务"><a href="#3-1-Flume事务" class="headerlink" title="3.1 Flume事务"></a>3.1 Flume事务</h2><p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16358198414212.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
<ol>
<li>Put事务流程<ul>
<li>doPut:将批数据先写入临时缓冲区putList</li>
<li>doCommit:检查channel内存队列是否足够合并。</li>
<li>doRollback:channel内存队列空间不足，回滚数据</li>
</ul>
</li>
<li>Take事务<ul>
<li>doTake:将数据取到临时缓冲区takeList，并将数据发送到HDFS</li>
<li>doCommit:如果数据全部发送成功，则清除临时缓冲区takeList</li>
<li>doRollback:数据发送过程中如果出现异常，rollback将临时缓冲区takeList中的数据归还给channel内存队列。</li>
</ul>
</li>
</ol>
<h2 id="3-2-Flume-Agent内部原理"><a href="#3-2-Flume-Agent内部原理" class="headerlink" title="3.2 Flume Agent内部原理"></a>3.2 Flume Agent内部原理</h2><p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16358207311935.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
<ol>
<li>ChannelSelector<ul>
<li>ChannelSelector的作用就是选出Event将要被发往哪个Channel。其共有两种类型，分别是Replicating（复制）和Multiplexing（多路复用）。</li>
<li>ReplicatingSelector会将同一个Event发往所有的Channel，Multiplexing会根据相应的原则，将不同的Event发往不同的Channel。</li>
</ul>
</li>
<li>SinkProcessor<ul>
<li>SinkProcessor共有三种类型，分别是DefaultSinkProcessor、LoadBalancingSinkProcessor和FailoverSinkProcessor</li>
<li>DefaultSinkProcessor对应的是单个的Sink，LoadBalancingSinkProcessor和FailoverSinkProcessor对应的是Sink Group，LoadBalancingSinkProcessor可以实现负载均衡的功能，FailoverSinkProcessor可以错误恢复的功能。</li>
</ul>
</li>
</ol>
<h2 id="3-3-Flume拓扑结构"><a href="#3-3-Flume拓扑结构" class="headerlink" title="3.3 Flume拓扑结构"></a>3.3 Flume拓扑结构</h2><h3 id="3-3-1-简单串联"><a href="#3-3-1-简单串联" class="headerlink" title="3.3.1 简单串联"></a>3.3.1 简单串联</h3><p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16359417802480.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"><br>这种模式是将多个flume顺序连接起来了，从最初的source开始到最终sink传送的目的存储系统。此模式不建议桥接过多的flume数量， flume数量过多不仅会影响传输速率，而且一旦传输过程中某个节点flume宕机，会影响整个传输系统。</p>
<h3 id="3-3-2-复制和多路复用"><a href="#3-3-2-复制和多路复用" class="headerlink" title="3.3.2 复制和多路复用"></a>3.3.2 复制和多路复用</h3><p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16359418050492.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"><br>Flume支持将事件流向一个或者多个目的地。这种模式可以将相同数据复制到多个channel中，或者将不同数据分发到不同的channel中，sink可以选择传送到不同的目的地。</p>
<h3 id="3-3-3-负载均衡和故障转移"><a href="#3-3-3-负载均衡和故障转移" class="headerlink" title="3.3.3 负载均衡和故障转移"></a>3.3.3 负载均衡和故障转移</h3><p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16359418259918.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"><br>Flume支持使用将多个sink逻辑上分到一个sink组，sink组配合不同的SinkProcessor可以实现负载均衡和错误恢复的功能。</p>
<h3 id="3-3-4-聚合"><a href="#3-3-4-聚合" class="headerlink" title="3.3.4 聚合"></a>3.3.4 聚合</h3><p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16359418605656.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"><br>这种模式是我们最常见的，也非常实用，日常web应用通常分布在上百个服务器，大者甚至上千个、上万个服务器。产生的日志，处理起来也非常麻烦。用flume的这种组合方式能很好的解决这一问题，每台服务器部署一个flume采集日志，传送到一个集中收集日志的flume，再由此flume上传到hdfs、hive、hbase等，进行日志分析。</p>
<h2 id="3-4-Flume企业开发案例"><a href="#3-4-Flume企业开发案例" class="headerlink" title="3.4 Flume企业开发案例"></a>3.4 Flume企业开发案例</h2><h3 id="3-4-1-复制案例"><a href="#3-4-1-复制案例" class="headerlink" title="3.4.1 复制案例"></a>3.4.1 复制案例</h3><p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16359420042208.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
<ul>
<li>需求描述：<ul>
<li>使用Flume1监控文件变动，Flume1将变动内容传递给Flume2，</li>
<li>Flume2负责存储到HDFS。同时Flume1将变动内容传递给Flume3，</li>
<li>Flume3负责输出到控制台</li>
</ul>
</li>
<li>功能实现<ol>
<li>创建Flume1 的核心配置文件 a1.conf（<font color ='red' >channel selector指定replicating</font>） 具体配置信息如下： <figure class="highlight ini"><table><tr><td class="code"><pre><span class="line"><span class="comment">#Named(agent,source,channel,sink)</span></span><br><span class="line"><span class="attr">a1.sources</span> = r1</span><br><span class="line"><span class="attr">a1.channels</span> = c1 c2</span><br><span class="line"><span class="attr">a1.sinks</span> = k1 k2</span><br><span class="line"><span class="comment">#Source</span></span><br><span class="line"><span class="attr">a1.sources.r1.type</span> = TAILDIR</span><br><span class="line"><span class="attr">a1.sources.r1.filegroups</span> = f1 f2</span><br><span class="line"><span class="attr">a1.sources.r1.filegroups.f1</span> = /opt/module/flume-<span class="number">1.9</span>.<span class="number">0</span>/jobs/taildir/.*file.*</span><br><span class="line"><span class="attr">a1.sources.r1.filegroups.f2</span> = /opt/module/flume-<span class="number">1.9</span>.<span class="number">0</span>/jobs/taildir/.*log.*</span><br><span class="line"><span class="attr">a1.sources.r1.positionFile</span> = /opt/module/flume-<span class="number">1.9</span>.<span class="number">0</span>/jobs/taildir/position/position.json</span><br><span class="line"></span><br><span class="line"><span class="comment">#channel selector</span></span><br><span class="line"><span class="attr">a1.sources.r1.selector.type</span> = replicating</span><br><span class="line"></span><br><span class="line"><span class="comment">#Channel</span></span><br><span class="line"><span class="attr">a1.channels.c1.type</span> = memory</span><br><span class="line"><span class="attr">a1.channels.c1.capacity</span> = <span class="number">10000</span></span><br><span class="line"><span class="attr">a1.channels.c1.transactionCapacity</span> = <span class="number">100</span></span><br><span class="line"></span><br><span class="line"><span class="attr">a1.channels.c2.type</span> = memory</span><br><span class="line"><span class="attr">a1.channels.c2.capacity</span> = <span class="number">10000</span></span><br><span class="line"><span class="attr">a1.channels.c2.transactionCapacity</span> = <span class="number">100</span></span><br><span class="line"><span class="comment">#Sink </span></span><br><span class="line"><span class="attr">a1.sinks.k1.type</span> = avro</span><br><span class="line"><span class="attr">a1.sinks.k1.hostname</span> = localhost</span><br><span class="line"><span class="attr">a1.sinks.k1.port</span> = <span class="number">7777</span></span><br><span class="line"></span><br><span class="line"><span class="attr">a1.sinks.k2.type</span> = avro</span><br><span class="line"><span class="attr">a1.sinks.k2.hostname</span> = localhost</span><br><span class="line"><span class="attr">a1.sinks.k2.port</span> = <span class="number">8888</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#Bind</span></span><br><span class="line"><span class="attr">a1.sources.r1.channels</span> = c1 c2</span><br><span class="line"><span class="attr">a1.sinks.k1.channel</span> = c1</span><br><span class="line"><span class="attr">a1.sinks.k2.channel</span> = c2</span><br></pre></td></tr></table></figure></li>
<li>创建Flume2 的核心配置文件 a2.conf 具体配置信息如下： <figure class="highlight ini"><table><tr><td class="code"><pre><span class="line"><span class="comment">#Named(agent,source,channel,sink)</span></span><br><span class="line"><span class="attr">a2.sources</span> = r1</span><br><span class="line"><span class="attr">a2.channels</span> = c1 </span><br><span class="line"><span class="attr">a2.sinks</span> = k1 </span><br><span class="line"></span><br><span class="line"><span class="comment">#Source</span></span><br><span class="line"><span class="attr">a2.sources.r1.type</span> = avro</span><br><span class="line"><span class="attr">a2.sources.r1.bind</span> = localhost</span><br><span class="line"><span class="attr">a2.sources.r1.port</span> = <span class="number">7777</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#Channel</span></span><br><span class="line"><span class="attr">a2.channels.c1.type</span> = memory</span><br><span class="line"><span class="attr">a2.channels.c1.capacity</span> = <span class="number">10000</span></span><br><span class="line"><span class="attr">a2.channels.c1.transactionCapacity</span> = <span class="number">100</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#Sink </span></span><br><span class="line"><span class="attr">a2.sinks.k1.type</span> = hdfs</span><br><span class="line"><span class="attr">a2.sinks.k1.hdfs.path</span> = hdfs://mycluster/flume/%Y%m%d/%H</span><br><span class="line"><span class="comment">#上传文件的前缀</span></span><br><span class="line"><span class="attr">a2.sinks.k1.hdfs.filePrefix</span> = logs-</span><br><span class="line"><span class="comment">#是否按照时间滚动文件夹</span></span><br><span class="line"><span class="attr">a2.sinks.k1.hdfs.round</span> = <span class="literal">true</span></span><br><span class="line"><span class="comment">#多少时间单位创建一个新的文件夹</span></span><br><span class="line"><span class="attr">a2.sinks.k1.hdfs.roundValue</span> = <span class="number">1</span></span><br><span class="line"><span class="comment">#重新定义时间单位</span></span><br><span class="line"><span class="attr">a2.sinks.k1.hdfs.roundUnit</span> = hour</span><br><span class="line"><span class="comment">#是否使用本地时间戳</span></span><br><span class="line"><span class="attr">a2.sinks.k1.hdfs.useLocalTimeStamp</span> = <span class="literal">true</span></span><br><span class="line"><span class="comment">#积攒多少个Event才flush到HDFS一次</span></span><br><span class="line"><span class="attr">a2.sinks.k1.hdfs.batchSize</span> = <span class="number">100</span></span><br><span class="line"><span class="comment">#设置文件类型，可支持压缩</span></span><br><span class="line"><span class="attr">a2.sinks.k1.hdfs.fileType</span> = DataStream</span><br><span class="line"><span class="comment">#多久生成一个新的文件</span></span><br><span class="line"><span class="attr">a2.sinks.k1.hdfs.rollInterval</span> = <span class="number">60</span></span><br><span class="line"><span class="comment">#设置每个文件的滚动大小</span></span><br><span class="line"><span class="attr">a2.sinks.k1.hdfs.rollSize</span> = <span class="number">134217700</span></span><br><span class="line"><span class="comment">#文件的滚动与Event数量无关</span></span><br><span class="line"><span class="attr">a2.sinks.k1.hdfs.rollCount</span> = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#Bind</span></span><br><span class="line"><span class="attr">a2.sources.r1.channels</span> = c1 </span><br><span class="line"><span class="attr">a2.sinks.k1.channel</span> = c1</span><br></pre></td></tr></table></figure></li>
<li>创建Flume3 的核心配置文件 a3.conf 具体配置信息如下： <figure class="highlight ini"><table><tr><td class="code"><pre><span class="line"><span class="comment">#Named(agent,source,channel,sink)</span></span><br><span class="line"><span class="attr">a3.sources</span> = r1</span><br><span class="line"><span class="attr">a3.channels</span> = c1 </span><br><span class="line"><span class="attr">a3.sinks</span> = k1 </span><br><span class="line"></span><br><span class="line"><span class="comment">#Source</span></span><br><span class="line"><span class="attr">a3.sources.r1.type</span> = avro</span><br><span class="line"><span class="attr">a3.sources.r1.bind</span> = localhost</span><br><span class="line"><span class="attr">a3.sources.r1.port</span> = <span class="number">8888</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#Channel</span></span><br><span class="line"><span class="attr">a3.channels.c1.type</span> = memory</span><br><span class="line"><span class="attr">a3.channels.c1.capacity</span> = <span class="number">10000</span></span><br><span class="line"><span class="attr">a3.channels.c1.transactionCapacity</span> = <span class="number">100</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#Sink </span></span><br><span class="line"><span class="attr">a3.sinks.k1.type</span> = logger</span><br><span class="line"></span><br><span class="line"><span class="comment">#Bind</span></span><br><span class="line"><span class="attr">a3.sources.r1.channels</span> = c1 </span><br><span class="line"><span class="attr">a3.sinks.k1.channel</span> = c1</span><br></pre></td></tr></table></figure></li>
<li>分别启动 Flume2和Flume3 然后在启动Flume1 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 启动Flume2</span></span><br><span class="line">flume-ng agent -c <span class="variable">$FLUME_HOME</span>/conf -f <span class="variable">$FLUME_HOME</span>/<span class="built_in">jobs</span>/replicating/a2.conf -n a2 -Dflume.root.logger=INFO,console</span><br><span class="line"><span class="comment"># 启动Flume3</span></span><br><span class="line">flume-ng agent -c <span class="variable">$FLUME_HOME</span>/conf -f <span class="variable">$FLUME_HOME</span>/<span class="built_in">jobs</span>/replicating/a3.conf -n a3 -Dflume.root.logger=INFO,console</span><br><span class="line"><span class="comment"># 启动Flume1</span></span><br><span class="line">flume-ng agent -c <span class="variable">$FLUME_HOME</span>/conf -f <span class="variable">$FLUME_HOME</span>/<span class="built_in">jobs</span>/replicating/a1.conf -n a1 -Dflume.root.logger=INFO,console </span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
<li>测试效果：往待监控的文件中追加内容，观察hdfs和控制台的变化！ <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">#flume1日志</span></span><br><span class="line">2021-11-03 21:05:48,539 (PollableSourceRunner-TaildirSource-r1) [INFO - org.apache.flume.source.taildir.TaildirSource.closeTailFiles(TaildirSource.java:307)] Closed file: /opt/module/flume-1.9.0/<span class="built_in">jobs</span>/taildir/replicating.log, inode: 104723556, pos: 43</span><br><span class="line"></span><br><span class="line"><span class="comment">#flume2日志</span></span><br><span class="line">2021-11-03 21:11:56,526 (SinkRunner-PollingRunner-DefaultSinkProcessor) [INFO - org.apache.flume.sink.hdfs.HDFSDataStream.configure(HDFSDataStream.java:57)] Serializer = TEXT, UseRawLocalFileSystem = <span class="literal">false</span></span><br><span class="line">2021-11-03 21:11:56,538 (SinkRunner-PollingRunner-DefaultSinkProcessor) [INFO - org.apache.flume.sink.hdfs.BucketWriter.open(BucketWriter.java:246)] Creating hdfs://mycluster/flume/20211103/21/logs-.1635945116527.tmp</span><br><span class="line"></span><br><span class="line"><span class="comment">#flume3日志</span></span><br><span class="line">2021-11-03 21:12:10,471 (SinkRunner-PollingRunner-DefaultSinkProcessor) [INFO - org.apache.flume.sink.LoggerSink.process(LoggerSink.java:95)] Event: &#123; headers:&#123;&#125; body: 32 30 32 31 E5 B9 B4 20 31 31 E6 9C 88 20 30 33 2021... 11... 03 &#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
</ol>
</li>
</ul>
<h3 id="3-4-2-负载均衡案例"><a href="#3-4-2-负载均衡案例" class="headerlink" title="3.4.2 负载均衡案例"></a>3.4.2 负载均衡案例</h3><p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/02qi-ye-an-lifu-zai-jun-hengan-li-shi-li-tu.png?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10" alt="02_企业案例-负载均衡-案例实例图"></p>
<ul>
<li>需求描述：<ul>
<li>使用Flume1监控端口数据，将监控的数据按照指定规则（轮训、随机） </li>
<li>传递给Flume2和Flume3,最后Flume2和Flume3将数据打印到控制台</li>
</ul>
</li>
<li>功能实现：<ol>
<li>创建Flume1 的核心配置文件 a1.conf（<font color ='red' >processor.type = load_balance processorprocessor.selector = round_robin</font>） 具体配置信息如下： <figure class="highlight ini"><table><tr><td class="code"><pre><span class="line"><span class="comment">#Named(agent,source,channel,sink)</span></span><br><span class="line"><span class="attr">a1.sources</span> = r1</span><br><span class="line"><span class="attr">a1.channels</span> = c1</span><br><span class="line"><span class="attr">a1.sinks</span> = k1 k2</span><br><span class="line"></span><br><span class="line"><span class="comment">#Source</span></span><br><span class="line"><span class="attr">a1.sources.r1.type</span> = netcat</span><br><span class="line"><span class="attr">a1.sources.r1.bind</span> = localhost</span><br><span class="line"><span class="attr">a1.sources.r1.port</span> = <span class="number">6666</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#Channel</span></span><br><span class="line"><span class="attr">a1.channels.c1.type</span> = memory</span><br><span class="line"><span class="attr">a1.channels.c1.capacity</span> = <span class="number">10000</span></span><br><span class="line"><span class="attr">a1.channels.c1.transactionCapacity</span> = <span class="number">100</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#Sink </span></span><br><span class="line"><span class="attr">a1.sinks.k1.type</span> = avro</span><br><span class="line"><span class="attr">a1.sinks.k1.hostname</span> = localhost</span><br><span class="line"><span class="attr">a1.sinks.k1.port</span> = <span class="number">7777</span></span><br><span class="line"></span><br><span class="line"><span class="attr">a1.sinks.k2.type</span> = avro</span><br><span class="line"><span class="attr">a1.sinks.k2.hostname</span> = localhost</span><br><span class="line"><span class="attr">a1.sinks.k2.port</span> = <span class="number">8888</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#Sink Processor</span></span><br><span class="line"><span class="attr">a1.sinkgroups</span> = g1</span><br><span class="line"><span class="attr">a1.sinkgroups.g1.sinks</span> = k1 k2</span><br><span class="line"><span class="attr">a1.sinkgroups.g1.processor.type</span> = load_balance</span><br><span class="line"><span class="attr">a1.sinkgroups.g1.processor.selector</span> = round_robin</span><br><span class="line"></span><br><span class="line"><span class="comment">#Bind</span></span><br><span class="line"><span class="attr">a1.sources.r1.channels</span> = c1</span><br><span class="line"><span class="attr">a1.sinks.k1.channel</span> = c1</span><br><span class="line"><span class="attr">a1.sinks.k2.channel</span> = c1</span><br></pre></td></tr></table></figure></li>
<li>创建Flume2 的核心配置文件 a2.conf 具体配置信息如下： <figure class="highlight ini"><table><tr><td class="code"><pre><span class="line"><span class="comment">#Named(agent,source,channel,sink)</span></span><br><span class="line"><span class="attr">a2.sources</span> = r1</span><br><span class="line"><span class="attr">a2.channels</span> = c1 </span><br><span class="line"><span class="attr">a2.sinks</span> = k1 </span><br><span class="line"></span><br><span class="line"><span class="comment">#Source</span></span><br><span class="line"><span class="attr">a2.sources.r1.type</span> = avro</span><br><span class="line"><span class="attr">a2.sources.r1.bind</span> = localhost</span><br><span class="line"><span class="attr">a2.sources.r1.port</span> = <span class="number">7777</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#Channel</span></span><br><span class="line"><span class="attr">a2.channels.c1.type</span> = memory</span><br><span class="line"><span class="attr">a2.channels.c1.capacity</span> = <span class="number">10000</span></span><br><span class="line"><span class="attr">a2.channels.c1.transactionCapacity</span> = <span class="number">100</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#Sink </span></span><br><span class="line"><span class="attr">a2.sinks.k1.type</span> = logger</span><br><span class="line"></span><br><span class="line"><span class="comment">#Bind</span></span><br><span class="line"><span class="attr">a2.sources.r1.channels</span> = c1 </span><br><span class="line"><span class="attr">a2.sinks.k1.channel</span> = c1</span><br></pre></td></tr></table></figure></li>
<li>创建Flume3 的核心配置文件 a3.conf 具体配置信息如下： <figure class="highlight ini"><table><tr><td class="code"><pre><span class="line"><span class="comment">#Named(agent,source,channel,sink)</span></span><br><span class="line"><span class="attr">a3.sources</span> = r1</span><br><span class="line"><span class="attr">a3.channels</span> = c1 </span><br><span class="line"><span class="attr">a3.sinks</span> = k1 </span><br><span class="line"></span><br><span class="line"><span class="comment">#Source</span></span><br><span class="line"><span class="attr">a3.sources.r1.type</span> = avro</span><br><span class="line"><span class="attr">a3.sources.r1.bind</span> = localhost</span><br><span class="line"><span class="attr">a3.sources.r1.port</span> = <span class="number">8888</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#Channel</span></span><br><span class="line"><span class="attr">a3.channels.c1.type</span> = memory</span><br><span class="line"><span class="attr">a3.channels.c1.capacity</span> = <span class="number">10000</span></span><br><span class="line"><span class="attr">a3.channels.c1.transactionCapacity</span> = <span class="number">100</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#Sink </span></span><br><span class="line"><span class="attr">a3.sinks.k1.type</span> = logger</span><br><span class="line"></span><br><span class="line"><span class="comment">#Bind</span></span><br><span class="line"><span class="attr">a3.sources.r1.channels</span> = c1 </span><br><span class="line"><span class="attr">a3.sinks.k1.channel</span> = c1</span><br></pre></td></tr></table></figure></li>
<li>分别启动 Flume2和Flume3 然后在启动Flume1 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 启动Flume2</span></span><br><span class="line">flume-ng agent -c <span class="variable">$FLUME_HOME</span>/conf -f <span class="variable">$FLUME_HOME</span>/<span class="built_in">jobs</span>/loadbalnace/a2.conf -n a2 -Dflume.root.logger=INFO,console</span><br><span class="line"><span class="comment"># 启动Flume3</span></span><br><span class="line">flume-ng agent -c <span class="variable">$FLUME_HOME</span>/conf -f <span class="variable">$FLUME_HOME</span>/<span class="built_in">jobs</span>/loadbalnace/a3.conf -n a3 -Dflume.root.logger=INFO,console</span><br><span class="line"><span class="comment"># 启动Flume1</span></span><br><span class="line">flume-ng agent -c <span class="variable">$FLUME_HOME</span>/conf -f <span class="variable">$FLUME_HOME</span>/<span class="built_in">jobs</span>/loadbalnace/a1.conf -n a1 -Dflume.root.logger=INFO,console </span><br></pre></td></tr></table></figure></li>
<li>测试效果：启动nc客户端 发消息，Flume1负责监听nc发来消息，然后以负载均衡的方式发送给Flume2和Flume3 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">#发送TCP消息到flume1</span></span><br><span class="line">[atguigu@hadoop001 loadbalnace]$ nc localhost 6666</span><br><span class="line">a</span><br><span class="line">OK</span><br><span class="line">b</span><br><span class="line">OK</span><br><span class="line">c</span><br><span class="line">OK</span><br><span class="line">d</span><br><span class="line">OK</span><br><span class="line"></span><br><span class="line"><span class="comment"># flume2日志    </span></span><br><span class="line">2021-11-03 21:23:39,584 (SinkRunner-PollingRunner-DefaultSinkProcessor) [INFO - org.apache.flume.sink.LoggerSink.process(LoggerSink.java:95)] Event: &#123; headers:&#123;&#125; body: 61                                              a &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># flume3日志    </span></span><br><span class="line">2021-11-03 21:23:42,237 (SinkRunner-PollingRunner-DefaultSinkProcessor) [INFO - org.apache.flume.sink.LoggerSink.process(LoggerSink.java:95)] Event: &#123; headers:&#123;&#125; body: 62                                              b &#125;</span><br><span class="line">2021-11-03 21:23:42,237 (SinkRunner-PollingRunner-DefaultSinkProcessor) [INFO - org.apache.flume.sink.LoggerSink.process(LoggerSink.java:95)] Event: &#123; headers:&#123;&#125; body: 63                                              c &#125;</span><br><span class="line">2021-11-03 21:23:42,237 (SinkRunner-PollingRunner-DefaultSinkProcessor) [INFO - org.apache.flume.sink.LoggerSink.process(LoggerSink.java:95)] Event: &#123; headers:&#123;&#125; body: 64                                              d &#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
</ol>
</li>
</ul>
<h3 id="3-4-3-故障转移案例"><a href="#3-4-3-故障转移案例" class="headerlink" title="3.4.3 故障转移案例"></a>3.4.3 故障转移案例</h3><p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/03qi-ye-an-ligu-zhang-zhuan-yian-li-shi-li-tu.png?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10" alt="03企业案例-故障转移-案例实例图"></p>
<ul>
<li><p>需求描述：</p>
<ul>
<li>使用Flume1监控端口数据，将监控到的数据按照故障转移的方式</li>
<li>传递给Flume2或者Flume3, Flume2或者Flume3将数据打印控制台</li>
</ul>
</li>
<li><p>功能实现：</p>
<ol>
<li>创建Flume1 的核心配置文件 a1.conf 具体配置信息如下：  <figure class="highlight ini"><table><tr><td class="code"><pre><span class="line"><span class="comment">#Named(agent,source,channel,sink)</span></span><br><span class="line"><span class="attr">a1.sources</span> = r1</span><br><span class="line"><span class="attr">a1.channels</span> = c1</span><br><span class="line"><span class="attr">a1.sinks</span> = k1 k2 </span><br><span class="line"><span class="comment">#Source</span></span><br><span class="line"><span class="attr">a1.sources.r1.type</span> = netcat</span><br><span class="line"><span class="attr">a1.sources.r1.bind</span> = localhost</span><br><span class="line"><span class="attr">a1.sources.r1.port</span> = <span class="number">6666</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#Channel</span></span><br><span class="line"><span class="attr">a1.channels.c1.type</span> = memory</span><br><span class="line"><span class="attr">a1.channels.c1.capacity</span> = <span class="number">10000</span></span><br><span class="line"><span class="attr">a1.channels.c1.transactionCapacity</span> = <span class="number">100</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#Sink </span></span><br><span class="line"><span class="attr">a1.sinks.k1.type</span> = avro</span><br><span class="line"><span class="attr">a1.sinks.k1.hostname</span> = localhost</span><br><span class="line"><span class="attr">a1.sinks.k1.port</span> = <span class="number">7777</span></span><br><span class="line"></span><br><span class="line"><span class="attr">a1.sinks.k2.type</span> = avro</span><br><span class="line"><span class="attr">a1.sinks.k2.hostname</span> = localhost</span><br><span class="line"><span class="attr">a1.sinks.k2.port</span> = <span class="number">8888</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#Sink Processor</span></span><br><span class="line"><span class="attr">a1.sinkgroups</span> = g1</span><br><span class="line"><span class="attr">a1.sinkgroups.g1.sinks</span> = k1 k2</span><br><span class="line"><span class="attr">a1.sinkgroups.g1.processor.type</span> = failover</span><br><span class="line"><span class="attr">a1.sinkgroups.g1.processor.priority.k1</span> = <span class="number">5</span></span><br><span class="line"><span class="attr">a1.sinkgroups.g1.processor.priority.k2</span> = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#Bind</span></span><br><span class="line"><span class="attr">a1.sources.r1.channels</span> = c1</span><br><span class="line"><span class="attr">a1.sinks.k1.channel</span> = c1</span><br><span class="line"><span class="attr">a1.sinks.k2.channel</span> = c1   </span><br></pre></td></tr></table></figure></li>
<li>创建Flume2 的核心配置文件 a2.conf 具体配置信息如下： <figure class="highlight ini"><table><tr><td class="code"><pre><span class="line"><span class="comment">#Named(agent,source,channel,sink)</span></span><br><span class="line"><span class="attr">a2.sources</span> = r1</span><br><span class="line"><span class="attr">a2.channels</span> = c1 </span><br><span class="line"><span class="attr">a2.sinks</span> = k1 </span><br><span class="line"></span><br><span class="line"><span class="comment">#Source</span></span><br><span class="line"><span class="attr">a2.sources.r1.type</span> = avro</span><br><span class="line"><span class="attr">a2.sources.r1.bind</span> = localhost</span><br><span class="line"><span class="attr">a2.sources.r1.port</span> = <span class="number">7777</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#Channel</span></span><br><span class="line"><span class="attr">a2.channels.c1.type</span> = memory</span><br><span class="line"><span class="attr">a2.channels.c1.capacity</span> = <span class="number">10000</span></span><br><span class="line"><span class="attr">a2.channels.c1.transactionCapacity</span> = <span class="number">100</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#Sink </span></span><br><span class="line"><span class="attr">a2.sinks.k1.type</span> = logger</span><br><span class="line"></span><br><span class="line"><span class="comment">#Bind</span></span><br><span class="line"><span class="attr">a2.sources.r1.channels</span> = c1 </span><br><span class="line"><span class="attr">a2.sinks.k1.channel</span> = c1</span><br></pre></td></tr></table></figure></li>
<li>创建Flume3 的核心配置文件 a3.conf 具体配置信息如下： <figure class="highlight ini"><table><tr><td class="code"><pre><span class="line"><span class="comment">#Named(agent,source,channel,sink)</span></span><br><span class="line"><span class="attr">a3.sources</span> = r1</span><br><span class="line"><span class="attr">a3.channels</span> = c1 </span><br><span class="line"><span class="attr">a3.sinks</span> = k1 </span><br><span class="line"><span class="comment">#Source</span></span><br><span class="line"><span class="attr">a3.sources.r1.type</span> = avro</span><br><span class="line"><span class="attr">a3.sources.r1.bind</span> = localhost</span><br><span class="line"><span class="attr">a3.sources.r1.port</span> = <span class="number">8888</span></span><br><span class="line"><span class="comment">#Channel</span></span><br><span class="line"><span class="attr">a3.channels.c1.type</span> = memory</span><br><span class="line"><span class="attr">a3.channels.c1.capacity</span> = <span class="number">10000</span></span><br><span class="line"><span class="attr">a3.channels.c1.transactionCapacity</span> = <span class="number">100</span></span><br><span class="line"><span class="comment">#Sink </span></span><br><span class="line"><span class="attr">a3.sinks.k1.type</span> = logger</span><br><span class="line"><span class="comment">#Bind</span></span><br><span class="line"><span class="attr">a3.sources.r1.channels</span> = c1 </span><br><span class="line"><span class="attr">a3.sinks.k1.channel</span> = c1</span><br></pre></td></tr></table></figure></li>
<li>分别启动 Flume2和Flume3 然后在启动Flume1 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 启动Flume2</span></span><br><span class="line">flume-ng agent -c <span class="variable">$FLUME_HOME</span>/conf -f <span class="variable">$FLUME_HOME</span>/<span class="built_in">jobs</span>/failover/a2.conf -n a2 -Dflume.root.logger=INFO,console</span><br><span class="line"><span class="comment"># 启动Flume3</span></span><br><span class="line">flume-ng agent -c <span class="variable">$FLUME_HOME</span>/conf -f <span class="variable">$FLUME_HOME</span>/<span class="built_in">jobs</span>/failover/a3.conf -n a3 -Dflume.root.logger=INFO,console</span><br><span class="line"><span class="comment"># 启动Flume1</span></span><br><span class="line">flume-ng agent -c <span class="variable">$FLUME_HOME</span>/conf -f <span class="variable">$FLUME_HOME</span>/<span class="built_in">jobs</span>/failover/a1.conf -n a1 -Dflume.root.logger=INFO,console </span><br></pre></td></tr></table></figure></li>
<li>启动nc客户端，发送消息，Flume1或者Flume2监听到消息打印到控制台，然后在模拟退出一台Flume 再发消息，查看结果。 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 failover]$ nc localhost 6666</span><br><span class="line">a</span><br><span class="line">OK</span><br><span class="line">b</span><br><span class="line">OK</span><br><span class="line">c</span><br><span class="line">OK</span><br><span class="line">d</span><br><span class="line">OK</span><br><span class="line"><span class="comment"># 查看flume2收到消息</span></span><br><span class="line"><span class="comment"># 停掉flume2继续发送消息</span></span><br><span class="line">1</span><br><span class="line">OK</span><br><span class="line">2</span><br><span class="line">OK</span><br><span class="line">3</span><br><span class="line">OK</span><br><span class="line">4</span><br><span class="line">OK</span><br><span class="line"><span class="comment"># 查看flume3收到消息，故障转移完成</span></span><br><span class="line"><span class="comment"># 重新启动flume2继续发送消息</span></span><br><span class="line">q</span><br><span class="line">OK</span><br><span class="line">w</span><br><span class="line">OK</span><br><span class="line">e</span><br><span class="line">OK</span><br><span class="line">r</span><br><span class="line">OK</span><br><span class="line"><span class="comment"># 查看flume2收到消息</span></span><br></pre></td></tr></table></figure></li>
<li>注意事项：模拟a3 故障后，a1会将数据发送给a2,但是如果把a3重新启动后，不会在第一时间将a3作为Active的sink,而是<br>遵循一个默认的规避原则，从2秒开始，a3故障一次重新启动后就对其规避使用累加上一次的两倍时间，默认30秒是上限。</li>
</ol>
</li>
</ul>
<h3 id="3-4-4-聚合案例"><a href="#3-4-4-聚合案例" class="headerlink" title="3.4.4 聚合案例"></a>3.4.4 聚合案例</h3><p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/04qi-ye-an-liju-hean-li-shi-li-tu.png?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10" alt="04_企业案例-聚合-案例实例图"></p>
<ul>
<li>需求描述：<ul>
<li>使用Flume1（hadoop102）监听端口数据，Flume2（hadoop103）监控追加文件的数据，</li>
<li>然后Flume1和Flume2将结果传递给Flume3,最后由Flume3将结果打印到控制台</li>
</ul>
</li>
<li>实现功能：<ol>
<li>准备工作<ul>
<li>将Flume软件发送给hadoop002和hadoop003</li>
<li>同步环境变量</li>
</ul>
</li>
<li>在hadoop102创建Flume1 的核心配置文件 a1.conf 具体配置信息如下（监控端口数据） <figure class="highlight ini"><table><tr><td class="code"><pre><span class="line"><span class="comment">#Named(agent,source,channel,sink)</span></span><br><span class="line"><span class="attr">a1.sources</span> = r1</span><br><span class="line"><span class="attr">a1.channels</span> = c1</span><br><span class="line"><span class="attr">a1.sinks</span> = k1</span><br><span class="line"></span><br><span class="line"><span class="comment">#Source</span></span><br><span class="line"><span class="attr">a1.sources.r1.type</span> = netcat</span><br><span class="line"><span class="attr">a1.sources.r1.bind</span> = localhost</span><br><span class="line"><span class="attr">a1.sources.r1.port</span> = <span class="number">6666</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#Channel</span></span><br><span class="line"><span class="attr">a1.channels.c1.type</span> = memory</span><br><span class="line"><span class="attr">a1.channels.c1.capacity</span> = <span class="number">10000</span></span><br><span class="line"><span class="attr">a1.channels.c1.transactionCapacity</span> = <span class="number">100</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#Sink </span></span><br><span class="line"><span class="attr">a1.sinks.k1.type</span> = avro</span><br><span class="line"><span class="attr">a1.sinks.k1.hostname</span> = hadoop003</span><br><span class="line"><span class="attr">a1.sinks.k1.port</span> = <span class="number">8888</span></span><br><span class="line"><span class="comment">#Bind</span></span><br><span class="line"><span class="attr">a1.sources.r1.channels</span> = c1</span><br><span class="line"><span class="attr">a1.sinks.k1.channel</span> = c1</span><br></pre></td></tr></table></figure></li>
<li>在hadoop103创建Flume2 的核心配置文件 a2.conf 具体配置信息如下（监控文件追加的数据）  <figure class="highlight ini"><table><tr><td class="code"><pre><span class="line"><span class="comment">#Named(agent,source,channel,sink)</span></span><br><span class="line"><span class="attr">a2.sources</span> = r1</span><br><span class="line"><span class="attr">a2.channels</span> = c1</span><br><span class="line"><span class="attr">a2.sinks</span> = k1</span><br><span class="line"></span><br><span class="line"><span class="comment">#Source</span></span><br><span class="line"><span class="attr">a2.sources.r1.type</span> = TAILDIR</span><br><span class="line"><span class="attr">a2.sources.r1.filegroups</span> = f1 f2</span><br><span class="line"><span class="attr">a2.sources.r1.filegroups.f1</span> = /opt/module/flume-<span class="number">1.9</span>.<span class="number">0</span>/jobs/taildir/.*file.*</span><br><span class="line"><span class="attr">a2.sources.r1.filegroups.f2</span> = /opt/module/flume-<span class="number">1.9</span>.<span class="number">0</span>/jobs/taildir/.*log.*</span><br><span class="line"><span class="attr">a2.sources.r1.positionFile</span> = /opt/module/flume-<span class="number">1.9</span>.<span class="number">0</span>/jobs/taildir/position/position.json</span><br><span class="line"></span><br><span class="line"><span class="comment">#Channel</span></span><br><span class="line"><span class="attr">a2.channels.c1.type</span> = memory</span><br><span class="line"><span class="attr">a2.channels.c1.capacity</span> = <span class="number">10000</span></span><br><span class="line"><span class="attr">a2.channels.c1.transactionCapacity</span> = <span class="number">100</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#Sink </span></span><br><span class="line"><span class="attr">a2.sinks.k1.type</span> = avro</span><br><span class="line"><span class="attr">a2.sinks.k1.hostname</span> = hadoop003</span><br><span class="line"><span class="attr">a2.sinks.k1.port</span> = <span class="number">8888</span></span><br><span class="line"><span class="comment">#Bind</span></span><br><span class="line"><span class="attr">a2.sources.r1.channels</span> = c1</span><br><span class="line"><span class="attr">a2.sinks.k1.channel</span> = c1</span><br></pre></td></tr></table></figure></li>
<li>在hadoop104创建Flume3 的核心配置文件 a2.conf 具体配置信息如下(监控 Flume1和Flume2发送的数据) <figure class="highlight ini"><table><tr><td class="code"><pre><span class="line"><span class="comment">#Named(agent,source,channel,sink)</span></span><br><span class="line"><span class="attr">a3.sources</span> = r1</span><br><span class="line"><span class="attr">a3.channels</span> = c1</span><br><span class="line"><span class="attr">a3.sinks</span> = k1</span><br><span class="line"></span><br><span class="line"><span class="comment">#Source</span></span><br><span class="line"><span class="attr">a3.sources.r1.type</span> = avro</span><br><span class="line"><span class="attr">a3.sources.r1.bind</span> = <span class="number">0.0</span>.<span class="number">0.0</span></span><br><span class="line"><span class="attr">a3.sources.r1.port</span> = <span class="number">8888</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#Channel</span></span><br><span class="line"><span class="attr">a3.channels.c1.type</span> = memory</span><br><span class="line"><span class="attr">a3.channels.c1.capacity</span> = <span class="number">10000</span></span><br><span class="line"><span class="attr">a3.channels.c1.transactionCapacity</span> = <span class="number">100</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#Sink </span></span><br><span class="line"><span class="attr">a3.sinks.k1.type</span> = logger</span><br><span class="line"><span class="comment">#Bind</span></span><br><span class="line"><span class="attr">a3.sources.r1.channels</span> = c1</span><br><span class="line"><span class="attr">a3.sinks.k1.channel</span> = c1</span><br></pre></td></tr></table></figure></li>
<li>分别启动 Flume3 然后再启动Flume1和Flume2 <figure class="highlight ini"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 启动Flume3（hadoop104）</span></span><br><span class="line">flume-ng agent -c $FLUME_HOME/conf -f $FLUME_HOME/jobs/aggre/a3.conf -n a3 <span class="attr">-Dflume.root.logger</span>=INFO,console</span><br><span class="line"><span class="comment"># 启动Flume2（hadoop103）</span></span><br><span class="line">flume-ng agent -c $FLUME_HOME/conf -f $FLUME_HOME/jobs/aggre/a2.conf -n a2 <span class="attr">-Dflume.root.logger</span>=INFO,console</span><br><span class="line"><span class="comment"># 启动Flume1 (hadoop102)</span></span><br><span class="line">flume-ng agent -c $FLUME_HOME/conf -f $FLUME_HOME/jobs/aggre/a1.conf -n a1 <span class="attr">-Dflume.root.logger</span>=INFO,console </span><br></pre></td></tr></table></figure></li>
<li>启动nc客户端，发送消息到Flume1，在Flume2监听监听的文件追加内容，查看flume3结果。 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 发送数据到Flume1</span></span><br><span class="line">[atguigu@hadoop002 ~]$ nc hadoop001 6666</span><br><span class="line">1</span><br><span class="line">OK</span><br><span class="line">2</span><br><span class="line">OK</span><br><span class="line">3</span><br><span class="line">OK</span><br><span class="line">4</span><br><span class="line">OK</span><br><span class="line"><span class="comment"># 追加内容到Flume2监控的文件       </span></span><br><span class="line">[atguigu@hadoop002 ~]$ <span class="built_in">echo</span> `date`</span><br><span class="line">2021年 11月 03日 星期三 22:00:52 CST</span><br><span class="line">[atguigu@hadoop002 ~]$ <span class="built_in">echo</span> `date` &gt;&gt; /opt/module/flume-1.9.0/<span class="built_in">jobs</span>/taildir/aggre.log</span><br><span class="line">[atguigu@hadoop002 ~]$ <span class="built_in">echo</span> `date` &gt;&gt; /opt/module/flume-1.9.0/<span class="built_in">jobs</span>/taildir/aggre.log</span><br><span class="line">[atguigu@hadoop002 ~]$ <span class="built_in">echo</span> `date` &gt;&gt; /opt/module/flume-1.9.0/<span class="built_in">jobs</span>/taildir/aggre.log</span><br><span class="line"></span><br><span class="line"><span class="comment">#Flume3日志</span></span><br><span class="line">2021-11-03 22:00:23,272 (SinkRunner-PollingRunner-DefaultSinkProcessor) [INFO - org.apache.flume.sink.LoggerSink.process(LoggerSink.java:95)] Event: &#123; headers:&#123;&#125; body: 31                                              1 &#125;</span><br><span class="line">2021-11-03 22:00:23,272 (SinkRunner-PollingRunner-DefaultSinkProcessor) [INFO - org.apache.flume.sink.LoggerSink.process(LoggerSink.java:95)] Event: &#123; headers:&#123;&#125; body: 32                                              2 &#125;</span><br><span class="line">2021-11-03 22:00:23,272 (SinkRunner-PollingRunner-DefaultSinkProcessor) [INFO - org.apache.flume.sink.LoggerSink.process(LoggerSink.java:95)] Event: &#123; headers:&#123;&#125; body: 33                                              3 &#125;</span><br><span class="line">2021-11-03 22:00:23,272 (SinkRunner-PollingRunner-DefaultSinkProcessor) [INFO - org.apache.flume.sink.LoggerSink.process(LoggerSink.java:95)] Event: &#123; headers:&#123;&#125; body: 34                                              4 &#125;</span><br><span class="line">2021-11-03 22:01:33,278 (SinkRunner-PollingRunner-DefaultSinkProcessor) [INFO - org.apache.flume.sink.LoggerSink.process(LoggerSink.java:95)] Event: &#123; headers:&#123;&#125; body: 32 30 32 31 E5 B9 B4 20 31 31 E6 9C 88 20 30 33 2021... 11... 03 &#125;</span><br><span class="line">2021-11-03 22:01:37,280 (SinkRunner-PollingRunner-DefaultSinkProcessor) [INFO - org.apache.flume.sink.LoggerSink.process(LoggerSink.java:95)] Event: &#123; headers:&#123;&#125; body: 32 30 32 31 E5 B9 B4 20 31 31 E6 9C 88 20 30 33 2021... 11... 03 &#125;</span><br><span class="line">2021-11-03 22:01:37,280 (SinkRunner-PollingRunner-DefaultSinkProcessor) [INFO - org.apache.flume.sink.LoggerSink.process(LoggerSink.java:95)] Event: &#123; headers:&#123;&#125; body: 32 30 32 31 E5 B9 B4 20 31 31 E6 9C 88 20 30 33 2021... 11... 03 &#125;</span><br></pre></td></tr></table></figure></li>
</ol>
</li>
</ul>
<h3 id="3-4-4-多路-拦截器-案例"><a href="#3-4-4-多路-拦截器-案例" class="headerlink" title="3.4.4 多路+拦截器 案例"></a>3.4.4 多路+拦截器 案例</h3><p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16359488070644.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"> </p>
<ul>
<li>需求描述：<ul>
<li>使用Flume1监控端口数据 </li>
<li>添加拦截器功能 对数据进行分类拦截，然后通过多路组件将不同规则的数据分别放入不同的channel中 </li>
<li>每一个channel对应一个 avro sink  </li>
<li>flume2 flume3 flume4 分别根据规则接受来自不同 avro sink 的数据，最后打印控制台</li>
</ul>
</li>
<li>完成功能：<ol>
<li>a1.conf (Flume1) <figure class="highlight ini"><table><tr><td class="code"><pre><span class="line">    <span class="comment">#Named(agent,source,channel,sink)</span></span><br><span class="line">    <span class="attr">a1.sources</span> = r1</span><br><span class="line">    <span class="attr">a1.channels</span> = c1 c2 c3 </span><br><span class="line">    <span class="attr">a1.sinks</span> = k1 k2 k3</span><br><span class="line">    <span class="comment">#Source</span></span><br><span class="line">    <span class="attr">a1.sources.r1.type</span> = netcat</span><br><span class="line">    <span class="attr">a1.sources.r1.bind</span> = <span class="number">0.0</span>.<span class="number">0.0</span></span><br><span class="line">    <span class="attr">a1.sources.r1.port</span> = <span class="number">6666</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">#channel selector</span></span><br><span class="line">    <span class="attr">a1.sources.r1.selector.type</span> = multiplexing</span><br><span class="line">    <span class="attr">a1.sources.r1.selector.header</span> = title</span><br><span class="line">    <span class="attr">a1.sources.r1.selector.mapping.at</span> = c1</span><br><span class="line">    <span class="attr">a1.sources.r1.selector.mapping.sg</span> = c2</span><br><span class="line">    <span class="attr">a1.sources.r1.selector.mapping.ot</span> = c3</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#interceptor</span></span><br><span class="line">    <span class="attr">a1.sources.r1.interceptors</span> = i1</span><br><span class="line">    <span class="attr">a1.sources.r1.interceptors.i1.type</span> = com.atguigu.flume.interecptor.MyInterceptor<span class="variable">$MyBuilder</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">#Channel</span></span><br><span class="line">    <span class="attr">a1.channels.c1.type</span> = memory</span><br><span class="line">    <span class="attr">a1.channels.c1.capacity</span> = <span class="number">10000</span></span><br><span class="line">    <span class="attr">a1.channels.c1.transactionCapacity</span> = <span class="number">100</span></span><br><span class="line">    </span><br><span class="line">    <span class="attr">a1.channels.c2.type</span> = memory</span><br><span class="line">    <span class="attr">a1.channels.c2.capacity</span> = <span class="number">10000</span></span><br><span class="line">    <span class="attr">a1.channels.c2.transactionCapacity</span> = <span class="number">100</span></span><br><span class="line">    </span><br><span class="line">    <span class="attr">a1.channels.c3.type</span> = memory</span><br><span class="line">    <span class="attr">a1.channels.c3.capacity</span> = <span class="number">10000</span></span><br><span class="line">    <span class="attr">a1.channels.c3.transactionCapacity</span> = <span class="number">100</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">#Sink </span></span><br><span class="line">    <span class="attr">a1.sinks.k1.type</span> = avro</span><br><span class="line">    <span class="attr">a1.sinks.k1.hostname</span> = <span class="number">0.0</span>.<span class="number">0.0</span></span><br><span class="line">    <span class="attr">a1.sinks.k1.port</span> = <span class="number">7777</span></span><br><span class="line">    </span><br><span class="line">    <span class="attr">a1.sinks.k2.type</span> = avro</span><br><span class="line">    <span class="attr">a1.sinks.k2.hostname</span> = <span class="number">0.0</span>.<span class="number">0.0</span></span><br><span class="line">    <span class="attr">a1.sinks.k2.port</span> = <span class="number">8888</span></span><br><span class="line">    </span><br><span class="line">    <span class="attr">a1.sinks.k3.type</span> = avro</span><br><span class="line">    <span class="attr">a1.sinks.k3.hostname</span> = <span class="number">0.0</span>.<span class="number">0.0</span></span><br><span class="line">    <span class="attr">a1.sinks.k3.port</span> = <span class="number">9999</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">#Bind</span></span><br><span class="line">    <span class="attr">a1.sources.r1.channels</span> = c1 c2 c3 </span><br><span class="line">    <span class="attr">a1.sinks.k1.channel</span> = c1</span><br><span class="line">    <span class="attr">a1.sinks.k2.channel</span> = c2</span><br><span class="line">    <span class="attr">a1.sinks.k3.channel</span> = c3</span><br><span class="line">    ```   </span><br><span class="line">1. a2.conf (Flume2)</span><br><span class="line">    ```ini</span><br><span class="line">    <span class="comment">#Named(agent,source,channel,sink)</span></span><br><span class="line">    <span class="attr">a2.sources</span> = r1</span><br><span class="line">    <span class="attr">a2.channels</span> = c1 </span><br><span class="line">    <span class="attr">a2.sinks</span> = k1 </span><br><span class="line">    </span><br><span class="line">    <span class="comment">#Source</span></span><br><span class="line">    <span class="attr">a2.sources.r1.type</span> = avro</span><br><span class="line">    <span class="attr">a2.sources.r1.bind</span> = <span class="number">0.0</span>.<span class="number">0.0</span></span><br><span class="line">    <span class="attr">a2.sources.r1.port</span> = <span class="number">7777</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">#Channel</span></span><br><span class="line">    <span class="attr">a2.channels.c1.type</span> = memory</span><br><span class="line">    <span class="attr">a2.channels.c1.capacity</span> = <span class="number">10000</span></span><br><span class="line">    <span class="attr">a2.channels.c1.transactionCapacity</span> = <span class="number">100</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">#Sink </span></span><br><span class="line">    <span class="attr">a2.sinks.k1.type</span> = logger</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#Bind</span></span><br><span class="line">    <span class="attr">a2.sources.r1.channels</span> = c1 </span><br><span class="line">    <span class="attr">a2.sinks.k1.channel</span> = c1</span><br></pre></td></tr></table></figure></li>
<li>a3.conf (Flume3) <figure class="highlight ini"><table><tr><td class="code"><pre><span class="line"><span class="comment">#Named(agent,source,channel,sink)</span></span><br><span class="line"><span class="attr">a3.sources</span> = r1</span><br><span class="line"><span class="attr">a3.channels</span> = c1 </span><br><span class="line"><span class="attr">a3.sinks</span> = k1 </span><br><span class="line"></span><br><span class="line"><span class="comment">#Source</span></span><br><span class="line"><span class="attr">a3.sources.r1.type</span> = avro</span><br><span class="line"><span class="attr">a3.sources.r1.bind</span> = <span class="number">0.0</span>.<span class="number">0.0</span></span><br><span class="line"><span class="attr">a3.sources.r1.port</span> = <span class="number">8888</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#Channel</span></span><br><span class="line"><span class="attr">a3.channels.c1.type</span> = memory</span><br><span class="line"><span class="attr">a3.channels.c1.capacity</span> = <span class="number">10000</span></span><br><span class="line"><span class="attr">a3.channels.c1.transactionCapacity</span> = <span class="number">100</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#Sink </span></span><br><span class="line"><span class="attr">a3.sinks.k1.type</span> = logger</span><br><span class="line"></span><br><span class="line"><span class="comment">#Bind</span></span><br><span class="line"><span class="attr">a3.sources.r1.channels</span> = c1 </span><br><span class="line"><span class="attr">a3.sinks.k1.channel</span> = c1</span><br></pre></td></tr></table></figure></li>
<li>a4.conf  (Flume4) <figure class="highlight ini"><table><tr><td class="code"><pre><span class="line"><span class="comment">#Named(agent,source,channel,sink)</span></span><br><span class="line"><span class="attr">a4.sources</span> = r1</span><br><span class="line"><span class="attr">a4.channels</span> = c1 </span><br><span class="line"><span class="attr">a4.sinks</span> = k1 </span><br><span class="line"></span><br><span class="line"><span class="comment">#Source</span></span><br><span class="line"><span class="attr">a4.sources.r1.type</span> = avro</span><br><span class="line"><span class="attr">a4.sources.r1.bind</span> = <span class="number">0.0</span>.<span class="number">0.0</span></span><br><span class="line"><span class="attr">a4.sources.r1.port</span> = <span class="number">9999</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#Channel</span></span><br><span class="line"><span class="attr">a4.channels.c1.type</span> = memory</span><br><span class="line"><span class="attr">a4.channels.c1.capacity</span> = <span class="number">10000</span></span><br><span class="line"><span class="attr">a4.channels.c1.transactionCapacity</span> = <span class="number">100</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#Sink </span></span><br><span class="line"><span class="attr">a4.sinks.k1.type</span> = logger</span><br><span class="line"></span><br><span class="line"><span class="comment">#Bind</span></span><br><span class="line"><span class="attr">a4.sources.r1.channels</span> = c1 </span><br><span class="line"><span class="attr">a4.sinks.k1.channel</span> = c1</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
</ol>
</li>
</ul>
<ol start="5">
<li>测试：<ol>
<li>将自定义拦截器打包上传至Linux 中 <ul>
<li>注意：修改Flume1的 a1.conf 文件中的  <figure class="highlight ini"><table><tr><td class="code"><pre><span class="line"><span class="attr">a1.sources.r1.interceptors.i1.type</span> = com.atguigu.flume.interceptor.DataTypeInterceptor<span class="variable">$MyBuilder</span></span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>执行以下运行命令<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">flume-ng agent -c <span class="variable">$FLUME_HOME</span>/conf -f <span class="variable">$FLUME_HOME</span>/<span class="built_in">jobs</span>/multiplexing/a1.conf -n a1 -Dflume.root.logger=INFO,console</span><br><span class="line"></span><br><span class="line">flume-ng agent -c <span class="variable">$FLUME_HOME</span>/conf -f <span class="variable">$FLUME_HOME</span>/<span class="built_in">jobs</span>/multiplexing/a2.conf -n a2 -Dflume.root.logger=INFO,console</span><br><span class="line"></span><br><span class="line">flume-ng agent -c <span class="variable">$FLUME_HOME</span>/conf -f <span class="variable">$FLUME_HOME</span>/<span class="built_in">jobs</span>/multiplexing/a3.conf -n a3 -Dflume.root.logger=INFO,console</span><br><span class="line"></span><br><span class="line">flume-ng agent -c <span class="variable">$FLUME_HOME</span>/conf -f <span class="variable">$FLUME_HOME</span>/<span class="built_in">jobs</span>/multiplexing/a4.conf -n a4 -Dflume.root.logger=INFO,console</span><br><span class="line"> </span><br></pre></td></tr></table></figure></li>
<li>nc模拟发送消息<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop001 ~]$ nc hadoop001 6666</span><br><span class="line"><span class="comment"># 发送atguiguaaa路由到flueme2</span></span><br><span class="line">atguiguaaa</span><br><span class="line">OK</span><br><span class="line"><span class="comment"># 发送shangguiguasdf路由到flueme3</span></span><br><span class="line">shangguiguasdf</span><br><span class="line">OK</span><br><span class="line"><span class="comment"># 发送asdasdf路由到flueme4</span></span><br><span class="line">asdasdf</span><br><span class="line">OK</span><br></pre></td></tr></table></figure></li>
</ol>
</li>
<li>自定义拦截器 <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MyInterceptor</span> <span class="keyword">implements</span> <span class="title">Interceptor</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 初始方法</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">initialize</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 拦截器的核心逻辑方法</span></span><br><span class="line"><span class="comment">     * 需求：</span></span><br><span class="line"><span class="comment">     *    区别采集数据的内容，包含 atguigu 或者 包含 sangguigu 再或者包含 其他</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> event</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Event <span class="title">intercept</span><span class="params">(Event event)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 获取event中的header</span></span><br><span class="line">        Map&lt;String, String&gt; headers = event.getHeaders();</span><br><span class="line">        <span class="comment">// 获取event 中 body</span></span><br><span class="line">        <span class="keyword">byte</span>[] body = event.getBody();</span><br><span class="line">        String data = <span class="keyword">new</span> String(body);</span><br><span class="line">        <span class="keyword">if</span>(data.contains(<span class="string">&quot;atguigu&quot;</span>))&#123;</span><br><span class="line">            headers.put(<span class="string">&quot;title&quot;</span>,<span class="string">&quot;at&quot;</span>);</span><br><span class="line">        &#125;<span class="keyword">else</span> <span class="keyword">if</span>(data.contains(<span class="string">&quot;shangguigu&quot;</span>))&#123;</span><br><span class="line">            headers.put(<span class="string">&quot;title&quot;</span>,<span class="string">&quot;sg&quot;</span>);</span><br><span class="line">        &#125;<span class="keyword">else</span> &#123;</span><br><span class="line">            headers.put(<span class="string">&quot;title&quot;</span>,<span class="string">&quot;ot&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> event;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 拦截器的核心逻辑方法，当来的多个event会循环调用 上面的 intercept</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> list</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> List&lt;Event&gt; <span class="title">intercept</span><span class="params">(List&lt;Event&gt; list)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">for</span> (Event event : list) &#123;</span><br><span class="line">            intercept(event);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> list;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 收尾工作</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 声明一个内部类</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyBuilder</span> <span class="keyword">implements</span> <span class="title">Builder</span></span>&#123;</span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * 实例化当前拦截器对象</span></span><br><span class="line"><span class="comment">         * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> Interceptor <span class="title">build</span><span class="params">()</span> </span>&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">new</span> MyInterceptor();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * 读取flume的配置信息</span></span><br><span class="line"><span class="comment">         * <span class="doctag">@param</span> context</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">configure</span><span class="params">(Context context)</span> </span>&#123;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ol>
<h1 id="第4章-企业真实面试题"><a href="#第4章-企业真实面试题" class="headerlink" title="第4章 企业真实面试题"></a>第4章 企业真实面试题</h1><h2 id="4-1-你是如何实现Flume数据传输的监控的"><a href="#4-1-你是如何实现Flume数据传输的监控的" class="headerlink" title="4.1 你是如何实现Flume数据传输的监控的"></a>4.1 你是如何实现Flume数据传输的监控的</h2><ul>
<li>第三方框架Prometheus+Grafana 或者Ganglia</li>
</ul>
<h2 id="4-2-Flume的Source，Sink，Channel的作用？你们Source是什么类型？"><a href="#4-2-Flume的Source，Sink，Channel的作用？你们Source是什么类型？" class="headerlink" title="4.2 Flume的Source，Sink，Channel的作用？你们Source是什么类型？"></a>4.2 Flume的Source，Sink，Channel的作用？你们Source是什么类型？</h2><ul>
<li>作用<ol>
<li>Source组件是专门用来收集数据的，可以处理各种类型、各种格式的日志数据，包括avro、thrift、exec、jms、spooling directory、netcat、sequence generator、syslog、http、legacy</li>
<li>Channel组件对采集到的数据进行缓存，可以存放在Memory或File中。</li>
<li>Sink组件是用于把数据发送到目的地的组件，目的地包括Hdfs、Logger、avro、thrift、ipc、file、Hbase、solr、自定义。</li>
</ol>
</li>
<li>采用的Source类型为：<ul>
<li>监控后台日志：exec</li>
<li>监控后台产生日志的端口：netcat</li>
</ul>
</li>
</ul>
<h2 id="4-3-Flume的Channel-Selectors"><a href="#4-3-Flume的Channel-Selectors" class="headerlink" title="4.3 Flume的Channel Selectors"></a>4.3 Flume的Channel Selectors</h2><p><img src="https://anzhen-tech-imges.oss-cn-beijing.aliyuncs.com/2021/11/07/16360037790920.jpg?x-oss-process=image/auto-orient,1/quality,q_90/watermark,text_YW56aGVuLnRlY2g,size_40,x_10,y_10"></p>
<h2 id="4-4-Flume参数调优"><a href="#4-4-Flume参数调优" class="headerlink" title="4.4 Flume参数调优"></a>4.4 Flume参数调优</h2><ol>
<li>Source<ul>
<li>增加Source个数（使用Tair Dir Source时可增加FileGroups个数）可以增大Source的读取数据的能力。例如：当某一个目录产生的文件过多时需要将这个文件目录拆分成多个文件目录，同时配置好多个Source 以保证Source有足够的能力获取到新产生的数据。</li>
<li>batchSize参数决定Source一次批量运输到Channel的event条数，适当调大这个参数可以提高Source搬运Event到Channel时的性能。</li>
</ul>
</li>
<li>Channel <ul>
<li>type：选择memory时Channel的性能最好，但是如果Flume进程意外挂掉可能会丢失数据。type选择file时Channel的容错性更好，但是性能上会比memory channel差。使用file Channel时dataDirs配置多个不同盘下的目录可以提高性能。</li>
<li>Capacity：参数决定Channel可容纳最大的event条数。transactionCapacity 参数决定每次Source往channel里面写的最大event条数和每次Sink从channel里面读的最大event条数。transactionCapacity需要大于Source和Sink的batchSize参数。</li>
</ul>
</li>
<li>Sink <ul>
<li>增加Sink的个数可以增加Sink消费event的能力。Sink也不是越多越好够用就行，过多的Sink会占用系统资源，造成系统资源不必要的浪费。</li>
<li>batchSize参数决定Sink一次批量从Channel读取的event条数，适当调大这个参数可以提高Sink从Channel搬出event的性能。</li>
</ul>
</li>
</ol>
<h2 id="4-5-Flume的事务机制"><a href="#4-5-Flume的事务机制" class="headerlink" title="4.5 Flume的事务机制"></a>4.5 Flume的事务机制</h2><p>Flume的事务机制（类似数据库的事务机制）：Flume使用两个独立的事务分别负责从Soucrce到Channel，以及从Channel到Sink的事件传递。比如spooling directory source 为文件的每一行创建一个事件，一旦事务中所有的事件全部传递到Channel且提交成功，那么Soucrce就将该文件标记为完成。同理，事务以类似的方式处理从Channel到Sink的传递过程，如果因为某种原因使得事件无法记录，那么事务将会回滚。且所有的事件都会保持到Channel中，等待重新传递。</p>
<h2 id="4-6-Flume采集数据会丢失吗"><a href="#4-6-Flume采集数据会丢失吗" class="headerlink" title="4.6 Flume采集数据会丢失吗?"></a>4.6 Flume采集数据会丢失吗?</h2><p>根据Flume的架构原理，Flume是不可能丢失数据的，其内部有完善的事务机制，Source到Channel是事务性的，Channel到Sink是事务性的，因此这两个环节不会出现数据的丢失，唯一可能丢失数据的情况是Channel采用memoryChannel，agent宕机导致数据丢失，或者Channel存储数据已满，导致Source不再写入，未写入的数据丢失。<br>Flume不会丢失数据，但是有可能造成数据的重复，例如数据已经成功由Sink发出，但是没有接收到响应，Sink会再次发送数据，此时可能会导致数据的重复。</p>
]]></content>
      <categories>
        <category>数据采集</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>数据采集</tag>
        <tag>Flume</tag>
        <tag>数仓</tag>
      </tags>
  </entry>
  <entry>
    <title>Hello World</title>
    <url>/2021/11/06/hello-world/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>
]]></content>
  </entry>
  <entry>
    <title>about_me</title>
    <url>/2019/11/07/about-me/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>Hello world</p>
]]></content>
  </entry>
</search>
