<!DOCTYPE html>


<html lang="zh-CN">
  

    <head>
      <meta charset="utf-8" />
        
      <meta
        name="viewport"
        content="width=device-width, initial-scale=1, maximum-scale=1"
      />
      <title>Kafka |  anzhen.tech</title>
  <meta name="generator" content="hexo-theme-ayer">
      
      <link rel="shortcut icon" href="/favicon.ico" />
       
<link rel="stylesheet" href="/dist/main.css">

      <link
        rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/Shen-Yu/cdn/css/remixicon.min.css"
      />
      
<link rel="stylesheet" href="/css/custom.css">
 
      <script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script>
       
 
<script>
var _hmt = _hmt || [];
(function() {
	var hm = document.createElement("script");
	hm.src = "https://hm.baidu.com/hm.js?20878462c8c8a6915b11b2d93a956d26";
	var s = document.getElementsByTagName("script")[0]; 
	s.parentNode.insertBefore(hm, s);
})();
</script>


      <link
        rel="stylesheet"
        href="https://cdn.jsdelivr.net/npm/@sweetalert2/theme-bulma@5.0.1/bulma.min.css"
      />
      <script src="https://cdn.jsdelivr.net/npm/sweetalert2@11.0.19/dist/sweetalert2.min.js"></script>

      <!-- mermaid -->
      
      <style>
        .swal2-styled.swal2-confirm {
          font-size: 1.6rem;
        }
      </style>
      <meta name="baidu-site-verification" content="code-mBRwXRLQqk" />
      <meta name="google-site-verification" content="bqavzWFaou2XjWPiLJI2ZoQwSGDVv_wZFPMkWKjEAz0" />
      
    <link rel="alternate" href="/atom.xml" title="anzhen.tech" type="application/atom+xml">
</head>
  </html>
</html>


<body>
  <div id="app">
    
      
    <main class="content on">
      <section class="outer">
  <article
  id="post-Kafka"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h1 class="article-title sea-center" style="border-left:0" itemprop="name">
  Kafka
</h1>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2021/11/19/Kafka/" class="article-date">
  <time datetime="2021-11-19T13:29:12.000Z" itemprop="datePublished">2021-11-19</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/categories/Kafka/">Kafka</a>
  </div>
  
<div class="word_count">
    <span class="post-time">
        <span class="post-meta-item-icon">
            <i class="ri-quill-pen-line"></i>
            <span class="post-meta-item-text"> 字数统计:</span>
            <span class="post-count">17.7k</span>
        </span>
    </span>

    <span class="post-time">
        &nbsp; | &nbsp;
        <span class="post-meta-item-icon">
            <i class="ri-book-open-line"></i>
            <span class="post-meta-item-text"> 阅读时长≈</span>
            <span class="post-count">79 分钟</span>
        </span>
    </span>
</div>
 
    </div>
      
    <div class="tocbot"></div>




  
    <div class="article-entry" itemprop="articleBody">
       
  <h1 id="Kafka"><a href="#Kafka" class="headerlink" title="Kafka"></a>Kafka</h1><h1 id="第1章-Kafka概述"><a href="#第1章-Kafka概述" class="headerlink" title="第1章 Kafka概述"></a>第1章 Kafka概述</h1><h2 id="1-1-定义"><a href="#1-1-定义" class="headerlink" title="1.1 定义"></a>1.1 定义</h2><ul>
<li>传统定义：Kafka是一个<font color ='red' >分布式</font>的基于<font color ='red' >发布/订阅模式</font>的<font color ='red' >消息队列（Message Queue）</font>，主要应用于大数据实时处理领域。</li>
<li>最新定义：Kafka是一个开源的分布式事件流平台（Event Streaming Platform），被数千家公司用于高性能数据管道、流分析、数据集成和关键任务应用。<br><img src="https://i.loli.net/2021/11/19/vEKlrIhO9zG5eJX.jpg"></li>
</ul>
<h2 id="1-2-消息队列"><a href="#1-2-消息队列" class="headerlink" title="1.2 消息队列"></a>1.2 消息队列</h2><h3 id="1-2-1-传统消息队列的应用场景"><a href="#1-2-1-传统消息队列的应用场景" class="headerlink" title="1.2.1 传统消息队列的应用场景"></a>1.2.1 传统消息队列的应用场景</h3><ol>
<li><p>缓冲/消峰：有助于控制和优化数据流经过系统的速度，解决生产消息和消费消息的处理速度不一致的情况。<br> <img src="https://i.loli.net/2021/11/19/U5cl9Sw48B3pqGX.jpg"></p>
</li>
<li><p>解耦：允许你独立的扩展或修改两边的处理过程，只要确保它们遵守同样的接口约束。<br> <img src="https://i.loli.net/2021/11/19/18xKnaykRSQ2gEX.jpg"></p>
</li>
<li><p>异步通信：很多时候，用户不想也不需要立即处理消息。消息队列提供了异步处理机制，允许用户把一个消息放入队列，但并不立即处理它。想向队列中放入多少消息就放多少，然后在需要的时候再去处理它们<br> <img src="https://i.loli.net/2021/11/19/i5bymXpZ1EINc2J.jpg"></p>
</li>
</ol>
<h3 id="1-2-2-消息队列的两种模式"><a href="#1-2-2-消息队列的两种模式" class="headerlink" title="1.2.2 消息队列的两种模式"></a>1.2.2 消息队列的两种模式</h3><ol>
<li>点对点模式<ul>
<li>消费者主动拉取数据，消息收到后消息清除<br><img src="https://i.loli.net/2021/11/19/etVkIU9p6TOqjoz.jpg"></li>
</ul>
</li>
<li>发布/订阅模式<ul>
<li>可以有多个topic主题 </li>
<li>消费者消费数据之后，不删除数据</li>
<li>每个消费者相互独立，都可以消费到数据<br><img src="https://i.loli.net/2021/11/19/WwGPAJs58LiDMVe.jpg"></li>
</ul>
</li>
</ol>
<h2 id="1-3-Kafka基础架构"><a href="#1-3-Kafka基础架构" class="headerlink" title="1.3 Kafka基础架构"></a>1.3 Kafka基础架构</h2><p><img src="https://i.loli.net/2021/11/19/UQ93WOvw6DN4onj.jpg"></p>
<ol>
<li>设计思路<ul>
<li>为方便扩展，并提高吞吐量，一个topic分为多个partition</li>
<li> 配合分区的设计，提出消费者组的概念，组内每个消费者并行消费</li>
<li> 为提高可用性，为每个partition增加若干副本，类似NameNode HA</li>
<li> ZK中记录谁是leader，Kafka2.8.0以后也可以配置不采用ZK</li>
</ul>
</li>
<li>组件<ul>
<li> Producer：消息生产者，就是向Kafka broker发消息的客户端；</li>
<li> Consumer：消息消费者，向Kafka broker取消息的客户端；</li>
<li> Consumer Group（CG）：消费者组，由多个consumer组成。消费者组内每个消费者负责消费不同分区的数据，一个分区只能由一个组内消费者消费；消费者组之间互不影响。所有的消费者都属于某个消费者组，即消费者组是逻辑上的一个订阅者。</li>
<li> Broker：一台Kafka服务器就是一个broker。一个集群由多个broker组成。一个broker可以容纳多个topic。</li>
<li> Topic：可以理解为一个队列，生产者和消费者面向的都是一个topic；</li>
<li> Partition：为了实现扩展性，一个非常大的topic可以分布到多个broker（即服务器）上，一个topic可以分为多个partition，每个partition是一个有序的队列；</li>
<li> Replica：副本，为保证集群中的某个节点发生故障时，该节点上的partition数据不丢失，且Kafka仍然能够继续工作，Kafka提供了副本机制，一个topic的每个分区都有若干个副本，一个Leader和若干个Follower。</li>
<li> Leader：每个分区多个副本的“主”，生产者发送数据的对象，以及消费者消费数据的对象都是Leader。</li>
<li> Follower：每个分区多个副本中的“从”，实时从Leader中同步数据，保持和Leader数据的同步。Leader发生故障时，某个Follower会成为新的Leader。</li>
</ul>
</li>
</ol>
<h1 id="第2章-Kafka快速入门"><a href="#第2章-Kafka快速入门" class="headerlink" title="第2章 Kafka快速入门"></a>第2章 Kafka快速入门</h1><h2 id="2-1-安装部署"><a href="#2-1-安装部署" class="headerlink" title="2.1 安装部署"></a>2.1 安装部署</h2><h3 id="2-1-1-集群规划"><a href="#2-1-1-集群规划" class="headerlink" title="2.1.1 集群规划"></a>2.1.1 集群规划</h3><table>
<thead>
<tr>
<th>hadoop001</th>
<th>hadoop103</th>
<th>hadoop104</th>
</tr>
</thead>
<tbody><tr>
<td>zk</td>
<td>zk</td>
<td>zk</td>
</tr>
<tr>
<td>kafka</td>
<td>kafka</td>
<td>kafka</td>
</tr>
</tbody></table>
<h3 id="2-1-2-集群部署"><a href="#2-1-2-集群部署" class="headerlink" title="2.1.2 集群部署"></a>2.1.2 集群部署</h3><ol>
<li>官方下载地址：<a target="_blank" rel="noopener" href="http://kafka.apache.org/downloads.html">http://kafka.apache.org/downloads.html</a></li>
<li>解压安装包 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf kafka_2.12-3.0.0.tgz -C /opt/module/</span><br></pre></td></tr></table></figure></li>
<li>配置环境变量  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop001 config]$ sudo vim /etc/profile.d/set_env.sh</span><br><span class="line"></span><br><span class="line"><span class="comment">#KAFKA_HOME</span></span><br><span class="line"><span class="built_in">export</span> KAFKA_HOME=/opt/module/kafka_2.12-3.0.0</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$KAFKA_HOME</span>/bin</span><br><span class="line"></span><br><span class="line"><span class="comment"># 分发环境变量文件</span></span><br><span class="line"> xsync /etc/profile.d/set_env.sh</span><br></pre></td></tr></table></figure></li>
<li>进入到$KAFKA_HOME/config目录，修改配置文件server.properties输入以下内容 <figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#broker的全局唯一编号，不能重复</span></span><br><span class="line"><span class="meta">broker.id</span>=<span class="string">0</span></span><br><span class="line"><span class="comment">#处理网络请求的线程数量</span></span><br><span class="line"><span class="meta">num.network.threads</span>=<span class="string">3</span></span><br><span class="line"><span class="comment">#用来处理磁盘IO的线程数量</span></span><br><span class="line"><span class="meta">num.io.threads</span>=<span class="string">8</span></span><br><span class="line"><span class="comment">#发送套接字的缓冲区大小</span></span><br><span class="line"><span class="meta">socket.send.buffer.bytes</span>=<span class="string">102400</span></span><br><span class="line"><span class="comment">#接收套接字的缓冲区大小</span></span><br><span class="line"><span class="meta">socket.receive.buffer.bytes</span>=<span class="string">102400</span></span><br><span class="line"><span class="comment">#请求套接字的缓冲区大小</span></span><br><span class="line"><span class="meta">socket.request.max.bytes</span>=<span class="string">104857600</span></span><br><span class="line"><span class="comment">#kafka运行日志(数据)存放的路径，路径不需要提前创建，kafka自动帮你创建</span></span><br><span class="line"><span class="meta">log.dirs</span>=<span class="string">/opt/module/kafka/datas</span></span><br><span class="line"><span class="comment">#topic在当前broker上的分区个数</span></span><br><span class="line"><span class="meta">num.partitions</span>=<span class="string">1</span></span><br><span class="line"><span class="comment">#用来恢复和清理data下数据的线程数量</span></span><br><span class="line"><span class="meta">num.recovery.threads.per.data.dir</span>=<span class="string">1</span></span><br><span class="line"><span class="comment">#segment文件保留的最长时间，超时将被删除</span></span><br><span class="line"><span class="meta">log.retention.hours</span>=<span class="string">168</span></span><br><span class="line"><span class="comment">#配置连接Zookeeper集群地址（在zk根目录下创建/kafka，方便管理）</span></span><br><span class="line"><span class="meta">zookeeper.connect</span>=<span class="string">hadoop001:2181,hadoop002:2181,hadoop003:2181/kafka</span></span><br></pre></td></tr></table></figure></li>
<li>分发安装包 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">xsync /opt/module/kafka_2.12-3.0.0/</span><br></pre></td></tr></table></figure></li>
<li>在各个节点上修改配置文件$KAFKA_HOME/config/server.properties中的 <figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># hadoop002</span></span><br><span class="line"><span class="meta">broker.id</span>=<span class="string">1</span></span><br><span class="line"><span class="comment"># hadoop003</span></span><br><span class="line"><span class="meta">broker.id</span>=<span class="string">2</span></span><br><span class="line"><span class="comment"># hadoop004</span></span><br><span class="line"><span class="meta">broker.id</span>=<span class="string">3</span></span><br><span class="line"><span class="comment"># hadoop005</span></span><br><span class="line"><span class="meta">broker.id</span>=<span class="string">4</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>注：broker.id不得重复</p>
</blockquote>
</li>
<li>启动集群<ul>
<li>先启动Zookeeper集群，然后启动Kafka  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">zookeeper_cluster.sh start</span><br></pre></td></tr></table></figure>
<blockquote>
<p>zookeeper_cluster.sh 参考<a href="mweblib://16343691008130">Zookeeper</a>篇《编写启动/停止Zookeeper集群脚本》</p>
</blockquote>
</li>
<li>依次在hadoop002、hadoop003、hadoop004,hadoop005节点上启动Kafka  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop001 ~]$ kafka-server-start.sh -daemon <span class="variable">$KAFKA_HOME</span>/config/server.properties</span><br><span class="line">[atguigu@hadoop001 ~]$ ssh hadoop002 kafka-server-start.sh -daemon <span class="variable">$KAFKA_HOME</span>/config/server.properties</span><br><span class="line">[atguigu@hadoop001 ~]$ ssh hadoop003 kafka-server-start.sh -daemon <span class="variable">$KAFKA_HOME</span>/config/server.properties</span><br><span class="line">[atguigu@hadoop001 ~]$ ssh hadoop004 kafka-server-start.sh -daemon <span class="variable">$KAFKA_HOME</span>/config/server.properties</span><br><span class="line">[atguigu@hadoop001 ~]$ ssh hadoop005 kafka-server-start.sh -daemon <span class="variable">$KAFKA_HOME</span>/config/server.properties</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>关闭集群 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop001 ~]$ kafka-server-stop.sh -daemon <span class="variable">$KAFKA_HOME</span>/config/server.properties</span><br><span class="line">[atguigu@hadoop001 ~]$ ssh hadoop002 kafka-server-stop.sh -daemon <span class="variable">$KAFKA_HOME</span>/config/server.properties</span><br><span class="line">[atguigu@hadoop001 ~]$ ssh hadoop003 kafka-server-stop.sh -daemon <span class="variable">$KAFKA_HOME</span>/config/server.properties</span><br><span class="line">[atguigu@hadoop001 ~]$ ssh hadoop004 kafka-server-stop.sh -daemon <span class="variable">$KAFKA_HOME</span>/config/server.properties</span><br><span class="line">[atguigu@hadoop001 ~]$ ssh hadoop005 kafka-server-stop.sh -daemon <span class="variable">$KAFKA_HOME</span>/config/server.properties</span><br></pre></td></tr></table></figure></li>
<li>Kafka启动失败的话可以暴力重置一下<ol>
<li>rm -rf /opt/module/kafka_2.12-3.0.0/logs/</li>
<li>删掉zookeeper /kafka</li>
<li>尝试重新启动</li>
</ol>
</li>
</ol>
<h3 id="2-1-3-集群启停脚本"><a href="#2-1-3-集群启停脚本" class="headerlink" title="2.1.3 集群启停脚本"></a>2.1.3 集群启停脚本</h3><ol>
<li>在/home/atguigu/bin目录下创建文件kf.sh脚本文件，脚本如下： <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#! /bin/bash</span></span><br><span class="line"><span class="keyword">if</span> [ <span class="variable">$#</span> -lt 1 ]</span><br><span class="line"><span class="keyword">then</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="string">&quot;No Args Input...&quot;</span></span><br><span class="line">    <span class="built_in">exit</span> ;</span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"><span class="keyword">case</span> <span class="variable">$1</span> <span class="keyword">in</span></span><br><span class="line"><span class="string">&quot;start&quot;</span>)&#123;</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> hadoop001 hadoop002 hadoop003 hadoop004 hadoop005</span><br><span class="line">    <span class="keyword">do</span></span><br><span class="line">        <span class="built_in">echo</span> <span class="string">&quot; --------启动 <span class="variable">$i</span> Kafka-------&quot;</span></span><br><span class="line">        ssh <span class="variable">$i</span> <span class="string">&quot;kafka-server-start.sh -daemon <span class="variable">$KAFKA_HOME</span>/config/server.properties&quot;</span></span><br><span class="line">    <span class="keyword">done</span></span><br><span class="line">&#125;;;</span><br><span class="line"><span class="string">&quot;stop&quot;</span>)&#123;</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> hadoop001 hadoop002 hadoop003 hadoop004 hadoop005</span><br><span class="line">    <span class="keyword">do</span></span><br><span class="line">        <span class="built_in">echo</span> <span class="string">&quot; --------停止 <span class="variable">$i</span> Kafka-------&quot;</span></span><br><span class="line">        ssh <span class="variable">$i</span> <span class="string">&quot;kafka-server-stop.sh &quot;</span></span><br><span class="line">    <span class="keyword">done</span></span><br><span class="line">&#125;;;</span><br><span class="line">*)</span><br><span class="line">    <span class="built_in">echo</span> <span class="string">&quot;Input Args Error...&quot;</span></span><br><span class="line">;;</span><br><span class="line"><span class="keyword">esac</span></span><br></pre></td></tr></table></figure></li>
<li>添加执行权限 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop001 bin]$ chmod +x kf.sh</span><br></pre></td></tr></table></figure></li>
<li>启动集群命令 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop001 ~]$ kf.sh start</span><br></pre></td></tr></table></figure></li>
<li>停止集群命令 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop001 ~]$ kf.sh stop</span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="2-2-Kafka命令行操作"><a href="#2-2-Kafka命令行操作" class="headerlink" title="2.2 Kafka命令行操作"></a>2.2 Kafka命令行操作</h2><p><img src="https://i.loli.net/2021/11/19/CJ8xFmjYL6tkrMz.jpg"></p>
<h3 id="2-2-1-topic命令行操作"><a href="#2-2-1-topic命令行操作" class="headerlink" title="2.2.1 topic命令行操作"></a>2.2.1 topic命令行操作</h3><ol>
<li><p>查看操作topic命令参数kafka-topics.sh</p>
<table>
<thead>
<tr>
<th>参数</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>–bootstrap-server<a href="String:servertoconnectto">String:servertoconnectto</a></td>
<td>连接的KafkaBroker主机名称和端口号</td>
</tr>
<tr>
<td>–topic<a href="String:topic">String:topic</a></td>
<td>操作的topic名称</td>
</tr>
<tr>
<td>–create</td>
<td>创建主题</td>
</tr>
<tr>
<td>–delete</td>
<td>删除主题</td>
</tr>
<tr>
<td>–alter</td>
<td>修改主题</td>
</tr>
<tr>
<td>–list</td>
<td>查看所有主题</td>
</tr>
<tr>
<td>–describe</td>
<td>查看主题详细描述</td>
</tr>
<tr>
<td>–partitions<a href="Integer:#ofpartitions">Integer:#ofpartitions</a></td>
<td>设置分区数</td>
</tr>
<tr>
<td>–replication-factor<a href="Integer:replicationfactor">Integer:replicationfactor</a></td>
<td>设置分区副本</td>
</tr>
<tr>
<td>–config<a href="String:name=value">String:name=value</a></td>
<td>更新系统默认的配置</td>
</tr>
</tbody></table>
</li>
<li><p>查看当前服务器中的所有topic</p>
 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kafka-topics.sh --bootstrap-server hadoop001:9092 --list</span><br></pre></td></tr></table></figure></li>
<li><p>创建topic</p>
 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop001 bin]$ kafka-topics.sh --bootstrap-server hadoop001:9092 --create --replication-factor 2 --partitions 1 --topic first</span><br><span class="line">Created topic first.</span><br><span class="line">[atguigu@hadoop001 bin]$ kafka-topics.sh --bootstrap-server hadoop001:9092 --list</span><br><span class="line">first</span><br><span class="line">[atguigu@hadoop001 bin]$</span><br></pre></td></tr></table></figure>
<ul>
<li>选项说明：<ul>
<li>–topic 定义topic名</li>
<li>–replication-factor  定义副本数</li>
<li>–partitions  定义分区数</li>
</ul>
</li>
</ul>
</li>
<li><p>查看某个Topic的详情</p>
 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">   [atguigu@hadoop001 bin]$ kafka-topics.sh --bootstrap-server hadoop001:9092 --describe --topic first</span><br><span class="line">   Topic: first	TopicId: wF2_VwRXR1q7l6MMamjZyg	PartitionCount: 1	ReplicationFactor: 2	Configs: segment.bytes=1073741824</span><br><span class="line">Topic: first	Partition: 0	Leader: 4	Replicas: 4,2	Isr: 4,2</span><br></pre></td></tr></table></figure></li>
<li><p>修改分区数</p>
 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop001 bin]$ kafka-topics.sh --bootstrap-server hadoop001:9092 --describe --topic first</span><br><span class="line">Topic: first	TopicId: wF2_VwRXR1q7l6MMamjZyg	PartitionCount: 3	ReplicationFactor: 2	Configs: segment.bytes=1073741824</span><br><span class="line">	Topic: first	Partition: 0	Leader: 4	Replicas: 4,2	Isr: 4,2</span><br><span class="line">	Topic: first	Partition: 1	Leader: 0	Replicas: 0,2	Isr: 0,2</span><br><span class="line">	Topic: first	Partition: 2	Leader: 2	Replicas: 2,4	Isr: 2,4</span><br><span class="line">[atguigu@hadoop001 bin]$</span><br></pre></td></tr></table></figure></li>
<li><p>删除topic</p>
 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop001 bin]$ kafka-topics.sh --bootstrap-server hadoop001:9092 --delete --topic first</span><br><span class="line">[atguigu@hadoop001 bin]$ kafka-topics.sh --bootstrap-server hadoop001:9092 --list</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop001 bin]$</span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="2-2-2-生产者命令行操作"><a href="#2-2-2-生产者命令行操作" class="headerlink" title="2.2.2 生产者命令行操作"></a>2.2.2 生产者命令行操作</h3><ol>
<li>查看操作生产者命令参数<code>kafka-console-producer.sh</code><table>
<thead>
<tr>
<th>参数</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>–bootstrap-server &lt;String: server toconnect to&gt;</td>
<td>连接的Kafka Broker主机名称和端口号</td>
</tr>
<tr>
<td>–topic &lt;String: topic&gt;</td>
<td>操作的topic名称</td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
</tbody></table>
</li>
<li>发送消息 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop001 bin]$ kafka-console-producer.sh --bootstrap-server hadoop001:9092 --topic first</span><br><span class="line">&gt;hello world</span><br><span class="line">&gt;atguigu atguigu</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="2-2-3-针对消费者相关命令大全"><a href="#2-2-3-针对消费者相关命令大全" class="headerlink" title="2.2.3 针对消费者相关命令大全"></a>2.2.3 针对消费者相关命令大全</h3><ol>
<li>查看操作消费者命令参数 kafka-console-consumer.sh<table>
<thead>
<tr>
<th>参数</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>–bootstrap-server &lt;String: server toconnect to&gt;</td>
<td>连接的Kafka Broker主机名称和端口号</td>
</tr>
<tr>
<td>–topic &lt;String: topic&gt;</td>
<td>操作的topic名称</td>
</tr>
<tr>
<td>–group &lt;String: consumer group id&gt;</td>
<td>指定消费者组名称</td>
</tr>
<tr>
<td>–partition &lt;Integer: partition&gt;</td>
<td>指定消费哪个分区数据</td>
</tr>
<tr>
<td>–offset &lt;String: consume offset&gt;</td>
<td>指定从什么位置消费</td>
</tr>
<tr>
<td>–from-beginning</td>
<td>从头开始消费</td>
</tr>
</tbody></table>
</li>
<li>消费消息 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-console-consumer.sh --bootstrap-server hadoop001:9092 --topic first</span><br><span class="line"></span><br><span class="line">kafka-console-consumer.sh --bootstrap-server hadoop001:9092 --from-beginning --topic first</span><br></pre></td></tr></table></figure>
<ul>
<li>–from-beginning：会把主题中现有的所有的数据都读取出来。</li>
</ul>
</li>
</ol>
<h1 id="第3章-Kafka生产者"><a href="#第3章-Kafka生产者" class="headerlink" title="第3章 Kafka生产者"></a>第3章 Kafka生产者</h1><h2 id="3-1-消息发送流程"><a href="#3-1-消息发送流程" class="headerlink" title="3.1 消息发送流程"></a>3.1 消息发送流程</h2><p><img src="https://i.loli.net/2021/11/19/NphlWrxSzD6UsJC.jpg"></p>
<ul>
<li><p>在消息发送过程中，涉及到两个线程，以及一个中间队列-RecordAccumulator.</p>
<ul>
<li>一是主线程，负责将消息进行封装和加工发送给消息中间件（RecordAccumulator）</li>
<li>二是send线程，负责从消息中间件中拉取数据发送到主题（Topic）的对应分区（Partition）</li>
</ul>
</li>
<li><p>流程描述</p>
<ul>
<li>main线程<ol>
<li>生产将要发送的数据封装成ProducerRecord对象，目的是发送到消息中间件</li>
<li>中间要经过拦截器列表、序列化器和分区器将消息发送到消息中间件</li>
<li>RecordAccumulator中有多个队列，与topic的分区相对应。消息发送时直接发送到分区对应的RecordAccumulator队列中</li>
</ol>
</li>
<li>sender线程<ol>
<li>当RecordAccumulator中攒够一批数据后，即达到指定量的数据之后，Sender线程将这一批数据拉取并发送给Topic。<ul>
<li>batch.size：只有数据积累到batch.size之后，sender才会发送数据。默认16k</li>
</ul>
</li>
<li>同时，如果RecordAccumulator中队列迟迟到不到指定量的数据时，会等到一定时长时发送。<ul>
<li>linger.ms：如果数据迟迟未达到batch.size，sender等待linger.time之后就会发送数据。单位ms，默认值是0ms，表示没有延迟。</li>
</ul>
</li>
</ol>
</li>
</ul>
</li>
<li><p>生产者相关调优参数</p>
<table>
<thead>
<tr>
<th>参数名称</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>batch.size</td>
<td>缓冲区一批数据最大值，默认16k</td>
</tr>
<tr>
<td>linger.ms</td>
<td>如果数据迟迟未达到batch.size，sender等待linger.time之后就会发送数据。单位ms，默认值是0ms，表示没有延迟。</td>
</tr>
<tr>
<td>buffer.memory    RecordAccumulator</td>
<td>缓冲区总大小，默认32m</td>
</tr>
<tr>
<td>retries</td>
<td>当消息发送出现错误的时候，系统会重发消息。retries表示重试次数。</td>
</tr>
<tr>
<td>compression.type</td>
<td>生产者发送的所有数据的压缩方式。默认是none，也就是不压缩。 支持的值：none、gzip、snappy、lz4和zstd。</td>
</tr>
</tbody></table>
</li>
</ul>
<h2 id="3-2-异步发送API"><a href="#3-2-异步发送API" class="headerlink" title="3.2 异步发送API"></a>3.2 异步发送API</h2><h3 id="3-2-1-普通异步发送"><a href="#3-2-1-普通异步发送" class="headerlink" title="3.2.1 普通异步发送"></a>3.2.1 普通异步发送</h3><p><img src="https://i.loli.net/2021/11/19/z6wBqsD5gpbU3rh.jpg"></p>
<ol>
<li>需求：创建Kafka生产者，采用异步的方式发送到Kafka Broker</li>
<li>编码实现<ol>
<li>maven依赖 <figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.kafka<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>kafka-clients<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.0.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br></pre></td></tr></table></figure></li>
<li>编码 <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CustomProducer</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">final</span> Properties properties = <span class="keyword">new</span> Properties();</span><br><span class="line">        <span class="comment">// 给kafka配置对象添加配置信息：bootstrap.servers</span></span><br><span class="line">        properties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">&quot;hadoop001:9092,hadoop002:9092,hadoop003:9092,hadoop004:9092,hadoop005:9092&quot;</span>);</span><br><span class="line">        <span class="comment">// key,value序列化（必须）：key.serializer，value.serializer</span></span><br><span class="line">        properties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());</span><br><span class="line">        properties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 批次大小 默认16K：batch.size</span></span><br><span class="line">        properties.put(ProducerConfig.BATCH_SIZE_CONFIG, <span class="number">16384</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 等待时间：linger.ms</span></span><br><span class="line">        properties.put(ProducerConfig.LINGER_MS_CONFIG, <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// RecordAccumulator缓冲区大小 默认32M：buffer.memory</span></span><br><span class="line">        properties.put(ProducerConfig.BUFFER_MEMORY_CONFIG, <span class="number">33554432</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3. 创建kafka生产者对象</span></span><br><span class="line">        KafkaProducer&lt;String, String&gt; kafkaProducer = <span class="keyword">new</span> KafkaProducer&lt;&gt;(properties);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 4. 调用send方法,发送消息</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">10</span>; i++) &#123;</span><br><span class="line">            kafkaProducer.send(<span class="keyword">new</span> ProducerRecord&lt;&gt;(<span class="string">&quot;first&quot;</span>,<span class="string">&quot;i am producer &quot;</span> + i));</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 5. 关闭资源</span></span><br><span class="line">        kafkaProducer.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
<li>测试：<ol>
<li>在hadoop001上开启Kafka消费者。 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop001 kafka]$ bin/kafka-console-consumer.sh --bootstrap-server hadoop001:9092 --topic first</span><br></pre></td></tr></table></figure></li>
<li>在IDEA中执行代码，观察hadoop001控制台中是否接收到消息。 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop001 kafka]$  bin/kafka-console-consumer.sh --bootstrap-server hadoop001:9092 --topic first</span><br><span class="line"></span><br><span class="line">atguigu 0</span><br><span class="line">atguigu 1</span><br><span class="line">atguigu 2</span><br><span class="line">atguigu 3</span><br><span class="line">atguigu 4</span><br></pre></td></tr></table></figure></li>
</ol>
</li>
</ol>
</li>
</ol>
<h3 id="3-2-2-带回调函数的异步发送"><a href="#3-2-2-带回调函数的异步发送" class="headerlink" title="3.2.2 带回调函数的异步发送"></a>3.2.2 带回调函数的异步发送</h3><p>回调函数会在producer收到ack时调用，为异步调用，该方法有两个参数，分别是元数据信息（RecordMetadata）和异常信息（Exception），如果Exception为null，说明消息发送成功，如果Exception不为null，说明消息发送失败。<br><img src="https://i.loli.net/2021/11/19/f7Y9QgpN8LCdiGs.jpg"></p>
<blockquote>
<p>注意：消息发送失败会自动重试，不需要我们在回调函数中手动重试。</p>
</blockquote>
<ol>
<li>编码 <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CustomProducerCallback</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1. 创建kafka生产者的配置对象</span></span><br><span class="line">        Properties properties = <span class="keyword">new</span> Properties();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 给kafka配置对象添加配置信息：bootstrap.servers</span></span><br><span class="line">        properties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">&quot;hadoop001:9092,hadoop002:9092,hadoop003:9092,hadoop004:9092,hadoop005:9092&quot;</span>);</span><br><span class="line">        <span class="comment">// key,value序列化（必须）：key.serializer，value.serializer</span></span><br><span class="line">        properties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());</span><br><span class="line">        properties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 批次大小 默认16K：batch.size</span></span><br><span class="line">        properties.put(ProducerConfig.BATCH_SIZE_CONFIG, <span class="number">16384</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 等待时间：linger.ms</span></span><br><span class="line">        properties.put(ProducerConfig.LINGER_MS_CONFIG, <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// RecordAccumulator缓冲区大小 默认32M：buffer.memory</span></span><br><span class="line">        properties.put(ProducerConfig.BUFFER_MEMORY_CONFIG, <span class="number">33554432</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3. 创建kafka生产者对象</span></span><br><span class="line">        KafkaProducer&lt;String, String&gt; kafkaProducer = <span class="keyword">new</span> KafkaProducer&lt;String, String&gt;(properties);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 4. 调用send方法,发送消息</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">10</span>; i++) &#123;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 添加回调</span></span><br><span class="line">            kafkaProducer.send(<span class="keyword">new</span> ProducerRecord&lt;&gt;(<span class="string">&quot;first&quot;</span>, <span class="string">&quot;atguigu &quot;</span> + i), <span class="keyword">new</span> Callback() &#123;</span><br><span class="line"></span><br><span class="line">                <span class="comment">// 该方法在Producer收到ack时调用，为异步调用</span></span><br><span class="line">                <span class="meta">@Override</span></span><br><span class="line">                <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onCompletion</span><span class="params">(RecordMetadata metadata, Exception exception)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">                    <span class="keyword">if</span> (exception == <span class="keyword">null</span>) &#123;</span><br><span class="line">                        <span class="comment">// 没有异常,输出信息到控制台</span></span><br><span class="line">                        System.out.println(<span class="string">&quot;onCompletion 主题：&quot;</span> + metadata.topic() + <span class="string">&quot;-&gt;&quot;</span> + <span class="string">&quot;分区：&quot;</span> + metadata.partition());</span><br><span class="line">                    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                        <span class="comment">// 出现异常打印</span></span><br><span class="line">                        exception.printStackTrace();</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;);</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 延迟一会会看到数据发往不同分区</span></span><br><span class="line">            Thread.sleep(<span class="number">2</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 5. 关闭资源</span></span><br><span class="line">        kafkaProducer.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>测试   <ol>
<li>在hadoop001上开启Kafka消费者。 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop001 kafka]$ bin/kafka-console-consumer.sh --bootstrap-server hadoop001:9092 --topic first</span><br></pre></td></tr></table></figure></li>
<li>在IDEA中执行代码，观察hadoop001控制台中是否接收到消息。 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop001 kafka]$  bin/kafka-console-consumer.sh --bootstrap-server hadoop001:9092 --topic first</span><br><span class="line">atguigu 0</span><br><span class="line">atguigu 1</span><br><span class="line">atguigu 2</span><br><span class="line">atguigu 3</span><br><span class="line">atguigu 4</span><br></pre></td></tr></table></figure></li>
<li>在IDEA控制台观察回调信息 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">onCompletion 主题：first-&gt;分区：2</span><br><span class="line">onCompletion 主题：first-&gt;分区：1</span><br><span class="line">onCompletion 主题：first-&gt;分区：1</span><br><span class="line">onCompletion 主题：first-&gt;分区：0</span><br><span class="line">onCompletion 主题：first-&gt;分区：2</span><br></pre></td></tr></table></figure></li>
</ol>
</li>
</ol>
<h2 id="3-3-同步发送API"><a href="#3-3-同步发送API" class="headerlink" title="3.3 同步发送API"></a>3.3 同步发送API</h2><p><img src="https://i.loli.net/2021/11/19/f1SAOroePk75mbG.jpg"><br>由于send方法返回的是一个Future对象，根据Futrue对象的特点，我们也可以实现同步发送的效果，只需在调用Future对象的get方法即可。</p>
<ol>
<li>编码 <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CustomProducerSync</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> InterruptedException, ExecutionException </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1. 创建kafka生产者的配置对象</span></span><br><span class="line">        Properties properties = <span class="keyword">new</span> Properties();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2. 给kafka配置对象添加配置信息</span></span><br><span class="line">        <span class="comment">// properties.put(&quot;bootstrap.servers&quot;,&quot;hadoop001:9092&quot;);</span></span><br><span class="line">        properties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">&quot;hadoop001:9092,hadoop002:9092,hadoop003:9092,hadoop004:9092,hadoop005:9092&quot;</span>);</span><br><span class="line">        <span class="comment">// key,value序列化（必须）：key.serializer，value.serializer</span></span><br><span class="line">        properties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());</span><br><span class="line">        properties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 批次大小 默认16K：batch.size</span></span><br><span class="line">        properties.put(ProducerConfig.BATCH_SIZE_CONFIG, <span class="number">16384</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 等待时间：linger.ms</span></span><br><span class="line">        properties.put(ProducerConfig.LINGER_MS_CONFIG, <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// RecordAccumulator缓冲区大小 默认32M：buffer.memory</span></span><br><span class="line">        properties.put(ProducerConfig.BUFFER_MEMORY_CONFIG, <span class="number">33554432</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置ack</span></span><br><span class="line">        <span class="comment">// properties.put(&quot;acks&quot;, &quot;all&quot;);</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 重试次数</span></span><br><span class="line">        <span class="comment">// properties.put(&quot;retries&quot;, 3);</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3. 创建kafka生产者对象</span></span><br><span class="line">        KafkaProducer&lt;String, String&gt; kafkaProducer = <span class="keyword">new</span> KafkaProducer&lt;&gt;(properties);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 4. 调用send方法,发送消息</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">10</span>; i++) &#123;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 异步发送 默认</span></span><br><span class="line"><span class="comment">//            kafkaProducer.send(new ProducerRecord&lt;&gt;(&quot;first&quot;,&quot;kafka&quot; + i));</span></span><br><span class="line">            <span class="comment">// 同步发送</span></span><br><span class="line">            <span class="keyword">final</span> Future&lt;RecordMetadata&gt; future = kafkaProducer.send(<span class="keyword">new</span> ProducerRecord&lt;&gt;(<span class="string">&quot;first&quot;</span>, <span class="string">&quot;kafka&quot;</span> + i));</span><br><span class="line">            <span class="keyword">final</span> RecordMetadata recordMetadata = future.get();</span><br><span class="line">            System.out.println(<span class="string">&quot;topic:&quot;</span> + recordMetadata.topic() + <span class="string">&quot;--&gt;&quot;</span> + <span class="string">&quot;partition&quot;</span> + recordMetadata.partition());</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 5. 关闭资源</span></span><br><span class="line">        kafkaProducer.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>测试<ol>
<li>在hadoop001上开启Kafka消费者。 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop001 kafka]$ bin/kafka-console-consumer.sh --bootstrap-server hadoop001:9092 --topic first</span><br></pre></td></tr></table></figure></li>
<li>在IDEA中执行代码，观察hadoop001控制台中是否接收到消息。 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop001 kafka]$  bin/kafka-console-consumer.sh --bootstrap-server hadoop001:9092 --topic first</span><br><span class="line">kafka0</span><br><span class="line">kafka1</span><br><span class="line">kafka2</span><br><span class="line">kafka3</span><br><span class="line">kafka4</span><br><span class="line">kafka5</span><br><span class="line">kafka6</span><br><span class="line">kafka7</span><br><span class="line">kafka8</span><br><span class="line">kafka9</span><br></pre></td></tr></table></figure></li>
<li>在IDEA控制台观察同步返回结果 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">topic:first--&gt;partition0</span><br><span class="line">topic:first--&gt;partition2</span><br><span class="line">topic:first--&gt;partition1</span><br><span class="line">topic:first--&gt;partition0</span><br><span class="line">topic:first--&gt;partition1</span><br><span class="line">topic:first--&gt;partition2</span><br><span class="line">topic:first--&gt;partition0</span><br><span class="line">topic:first--&gt;partition1</span><br><span class="line">topic:first--&gt;partition2</span><br><span class="line">topic:first--&gt;partition0</span><br></pre></td></tr></table></figure></li>
</ol>
</li>
</ol>
<h2 id="3-4-生产者分区"><a href="#3-4-生产者分区" class="headerlink" title="3.4 生产者分区"></a>3.4 生产者分区</h2><h3 id="3-4-1-分区的好处"><a href="#3-4-1-分区的好处" class="headerlink" title="3.4.1 分区的好处"></a>3.4.1 分区的好处</h3><p><img src="https://i.loli.net/2021/11/19/e8UvXuCBkH92pN6.jpg"></p>
<ol>
<li><font color ='red' >便于合理使用存储资源</font>，每个Partition在一个Broker上存储一部分数据，如果Broker足够多，那么就可以设置足够多的分区，因此整个集群就可以存储任意大小的数据了；</li>
<li><font color ='red' >提高并发度</font>，消费者可以以分区为单位进行消费。</li>
</ol>
<h3 id="3-4-2-生产者发送消息的分区策略"><a href="#3-4-2-生产者发送消息的分区策略" class="headerlink" title="3.4.2 生产者发送消息的分区策略"></a>3.4.2 生产者发送消息的分区策略</h3><ol>
<li><p>默认的分区器 DefaultPartitioner</p>
 <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">DefaultPartitioner</span> <span class="keyword">implements</span> <span class="title">Partitioner</span> </span>&#123;</span><br><span class="line">    ···</span><br><span class="line">    ···</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">partition</span><span class="params">(String topic, Object key, <span class="keyword">byte</span>[] keyBytes, Object value, <span class="keyword">byte</span>[] valueBytes, Cluster cluster,</span></span></span><br><span class="line"><span class="params"><span class="function">                         <span class="keyword">int</span> numPartitions)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (keyBytes == <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> stickyPartitionCache.partition(topic, cluster);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// hash the keyBytes to choose a partition</span></span><br><span class="line">        <span class="keyword">return</span> Utils.toPositive(Utils.murmur2(keyBytes)) % numPartitions;</span><br><span class="line">    &#125;</span><br><span class="line">    ···</span><br><span class="line">    ···</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li><p>ProducerRecord</p>
 <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ProducerRecord</span>&lt;<span class="title">K</span>, <span class="title">V</span>&gt; </span>&#123;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * 指明partition的情况下，直接将指明的值作为partiton值。</span></span><br><span class="line"><span class="comment">    * 例如partition=0，所有数据写入分区0</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">ProducerRecord</span><span class="params">(String topic, Integer partition, Long timestamp, K key, V value, Iterable&lt;Header&gt; headers)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (topic == <span class="keyword">null</span>)</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException(<span class="string">&quot;Topic cannot be null.&quot;</span>);</span><br><span class="line">        <span class="keyword">if</span> (timestamp != <span class="keyword">null</span> &amp;&amp; timestamp &lt; <span class="number">0</span>)</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException(</span><br><span class="line">                    String.format(<span class="string">&quot;Invalid timestamp: %d. Timestamp should always be non-negative or null.&quot;</span>, timestamp));</span><br><span class="line">        <span class="keyword">if</span> (partition != <span class="keyword">null</span> &amp;&amp; partition &lt; <span class="number">0</span>)</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException(</span><br><span class="line">                    String.format(<span class="string">&quot;Invalid partition: %d. Partition number should always be non-negative or null.&quot;</span>, partition));</span><br><span class="line">        <span class="keyword">this</span>.topic = topic;</span><br><span class="line">        <span class="keyword">this</span>.partition = partition;</span><br><span class="line">        <span class="keyword">this</span>.key = key;</span><br><span class="line">        <span class="keyword">this</span>.value = value;</span><br><span class="line">        <span class="keyword">this</span>.timestamp = timestamp;</span><br><span class="line">        <span class="keyword">this</span>.headers = <span class="keyword">new</span> RecordHeaders(headers);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">ProducerRecord</span><span class="params">(String topic, Integer partition, Long timestamp, K key, V value)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>(topic, partition, timestamp, key, value, <span class="keyword">null</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">   </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">ProducerRecord</span><span class="params">(String topic, Integer partition, K key, V value, Iterable&lt;Header&gt; headers)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>(topic, partition, <span class="keyword">null</span>, key, value, headers);</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">ProducerRecord</span><span class="params">(String topic, Integer partition, K key, V value)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>(topic, partition, <span class="keyword">null</span>, key, value, <span class="keyword">null</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">    * 没有指明partition值但有key的情况下，将key的hash值与topic的partition数进行取余得到partition值；</span></span><br><span class="line"><span class="comment">    * 例如：key1的hash值=5， key2的hash值=6 ，topic的partition数=2，那么key1 对应的value1写入1号分区，key2对应的value2写入0号分区。</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">ProducerRecord</span><span class="params">(String topic, K key, V value)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>(topic, <span class="keyword">null</span>, <span class="keyword">null</span>, key, value, <span class="keyword">null</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">    * 既没有partition值又没有key值的情况下，Kafka采用Sticky Partition（黏性分区器），会随机选择一个分区，并尽可能一直使用该分区，待该分区的batch已满或者已完成，Kafka再随机一个分区进行使用（和上一次的分区不同）。</span></span><br><span class="line"><span class="comment">    * 例如：第一次随机选择0号分区，等0号分区当前批次满了（16k）或者linger.ms设置的时间到， Kafka再随机一个分区进行使用（如果还是0会继续随机）。</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">ProducerRecord</span><span class="params">(String topic, V value)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>(topic, <span class="keyword">null</span>, <span class="keyword">null</span>, <span class="keyword">null</span>, value, <span class="keyword">null</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li><p>源码跟踪</p>
<ul>
<li>KafkaProducer#send(ProducerRecord&lt;K,V&gt;)</li>
<li>KafkaProducer#doSend</li>
<li>KafkaProducer#partition  <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">int</span> <span class="title">partition</span><span class="params">(ProducerRecord&lt;K, V&gt; record, <span class="keyword">byte</span>[] serializedKey, <span class="keyword">byte</span>[] serializedValue, Cluster cluster)</span> </span>&#123;</span><br><span class="line">    Integer partition = record.partition();</span><br><span class="line">    <span class="keyword">return</span> partition != <span class="keyword">null</span> ?</span><br><span class="line">            partition :</span><br><span class="line">            partitioner.partition(</span><br><span class="line">                    record.topic(), record.key(), serializedKey, record.value(), serializedValue, cluster);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>将数据发往指定partition的情况下，例如，将所有数据发往分区1中。</p>
<ol>
<li>编码 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">public class CustomProducerCallbackPartitions &#123;</span><br><span class="line">    public static void main(String[] args) &#123;</span><br><span class="line"></span><br><span class="line">        // 1. 创建kafka生产者的配置对象</span><br><span class="line">        Properties properties = new Properties();</span><br><span class="line"></span><br><span class="line">        // 2. 给kafka配置对象添加配置信息</span><br><span class="line">        // properties.put(<span class="string">&quot;bootstrap.servers&quot;</span>,<span class="string">&quot;hadoop001:9092&quot;</span>);</span><br><span class="line">        properties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">&quot;hadoop001:9092,hadoop002:9092,hadoop003:9092,hadoop004:9092,hadoop005:9092&quot;</span>);</span><br><span class="line">        // key,value序列化（必须）：key.serializer，value.serializer</span><br><span class="line">        properties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());</span><br><span class="line">        properties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        KafkaProducer&lt;String, String&gt; kafkaProducer = new KafkaProducer&lt;&gt;(properties);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (int i = 0; i &lt; 5; i++) &#123;</span><br><span class="line">            // 指定数据发送到1号分区，key为空（IDEA中ctrl + p查看参数）</span><br><span class="line">            kafkaProducer.send(new ProducerRecord&lt;&gt;(<span class="string">&quot;first&quot;</span>, 1,<span class="string">&quot;&quot;</span>,<span class="string">&quot;kafka  &quot;</span> + i), new <span class="function"><span class="title">Callback</span></span>() &#123;</span><br><span class="line">                @Override</span><br><span class="line">                public void onCompletion(RecordMetadata recordMetadata, Exception e) &#123;</span><br><span class="line">                    <span class="keyword">if</span> (e == null)&#123;</span><br><span class="line">                        System.out.println(<span class="string">&quot;onCompletion 主题：&quot;</span> + recordMetadata.topic() + <span class="string">&quot;-&gt;&quot;</span> + <span class="string">&quot;分区：&quot;</span> + recordMetadata.partition()</span><br><span class="line">                        );</span><br><span class="line">                    &#125;<span class="keyword">else</span> &#123;</span><br><span class="line">                        e.printStackTrace();</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        kafkaProducer.close();</span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>测试<ol>
<li>在hadoop001上开启Kafka消费者。 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop001 kafka]$ bin/kafka-console-consumer.sh --bootstrap-server hadoop001:9092 --topic first</span><br></pre></td></tr></table></figure></li>
<li>在IDEA中执行代码，观察hadoop001控制台中是否接收到消息。 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop001 kafka]$  bin/kafka-console-consumer.sh --bootstrap-server hadoop001:9092 --topic first</span><br><span class="line">atguigu 0</span><br><span class="line">atguigu 1</span><br><span class="line">atguigu 2</span><br><span class="line">atguigu 3</span><br><span class="line">atguigu 4</span><br></pre></td></tr></table></figure></li>
<li>在IDEA控制台观察回调信息 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">onCompletion 主题：first-&gt;分区：1</span><br><span class="line">onCompletion 主题：first-&gt;分区：1</span><br><span class="line">onCompletion 主题：first-&gt;分区：1</span><br><span class="line">onCompletion 主题：first-&gt;分区：1</span><br><span class="line">onCompletion 主题：first-&gt;分区：1</span><br></pre></td></tr></table></figure></li>
</ol>
</li>
</ol>
</li>
<li><p>没有指明partition值但有key的情况下，将key的字节数组hash值与topic的partition数进行取余得到partition值；</p>
<ol>
<li>编码 <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CustomProducerCallbackWithoutPartition</span> </span>&#123;</span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">    </span><br><span class="line">            <span class="comment">// 1. 创建kafka生产者的配置对象</span></span><br><span class="line">            Properties properties = <span class="keyword">new</span> Properties();</span><br><span class="line">    </span><br><span class="line">            <span class="comment">// 2. 给kafka配置对象添加配置信息</span></span><br><span class="line">            <span class="comment">// properties.put(&quot;bootstrap.servers&quot;,&quot;hadoop001:9092&quot;);</span></span><br><span class="line">            properties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">&quot;hadoop001:9092,hadoop002:9092,hadoop003:9092,hadoop004:9092,hadoop005:9092&quot;</span>);</span><br><span class="line">            <span class="comment">// key,value序列化（必须）：key.serializer，value.serializer</span></span><br><span class="line">            properties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());</span><br><span class="line">            properties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">            KafkaProducer&lt;String, String&gt; kafkaProducer = <span class="keyword">new</span> KafkaProducer&lt;&gt;(properties);</span><br><span class="line">    </span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">10</span>; i++) &#123;</span><br><span class="line">                <span class="comment">// 依次指定key值为i ，数据key的hash值与3个分区求余，分别发往1、2、0</span></span><br><span class="line">                <span class="keyword">int</span> finalI = i;</span><br><span class="line">                kafkaProducer.send(<span class="keyword">new</span> ProducerRecord&lt;&gt;(<span class="string">&quot;first&quot;</span>, i + <span class="string">&quot;&quot;</span>, <span class="string">&quot;atguigu &quot;</span> + i), <span class="keyword">new</span> Callback() &#123;</span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onCompletion</span><span class="params">(RecordMetadata recordMetadata, Exception e)</span> </span>&#123;</span><br><span class="line">                        <span class="keyword">if</span> (e == <span class="keyword">null</span>) &#123;</span><br><span class="line">                            System.out.println(<span class="string">&quot;key:&quot;</span> + finalI + <span class="string">&quot; 主题：&quot;</span> + recordMetadata.topic() + <span class="string">&quot;-&gt;&quot;</span></span><br><span class="line">                                    + <span class="string">&quot;分区：&quot;</span> + recordMetadata.partition()</span><br><span class="line">                            );</span><br><span class="line">                        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                            e.printStackTrace();</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;);</span><br><span class="line">            &#125;</span><br><span class="line">    </span><br><span class="line">            kafkaProducer.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>测试<ol>
<li>在hadoop001上开启Kafka消费者。 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop001 kafka]$ bin/kafka-console-consumer.sh --bootstrap-server hadoop001:9092 --topic first</span><br></pre></td></tr></table></figure></li>
<li>在IDEA中执行代码，观察hadoop001控制台中是否接收到消息。 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop001 kafka]$  bin/kafka-console-consumer.sh --bootstrap-server hadoop001:9092 --topic first</span><br><span class="line">atguigu 0</span><br><span class="line">atguigu 2</span><br><span class="line">atguigu 3</span><br><span class="line">atguigu 9</span><br><span class="line">atguigu 1</span><br><span class="line">atguigu 5</span><br><span class="line">atguigu 7</span><br><span class="line">atguigu 8</span><br><span class="line">atguigu 4</span><br><span class="line">atguigu 6</span><br></pre></td></tr></table></figure></li>
<li>在IDEA控制台观察回调信息 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">key:0 主题：first-&gt;分区：2</span><br><span class="line">key:2 主题：first-&gt;分区：2</span><br><span class="line">key:3 主题：first-&gt;分区：2</span><br><span class="line">key:9 主题：first-&gt;分区：2</span><br><span class="line">key:1 主题：first-&gt;分区：0</span><br><span class="line">key:5 主题：first-&gt;分区：0</span><br><span class="line">key:7 主题：first-&gt;分区：0</span><br><span class="line">key:8 主题：first-&gt;分区：0</span><br><span class="line">key:4 主题：first-&gt;分区：1</span><br><span class="line">key:6 主题：first-&gt;分区：1</span><br></pre></td></tr></table></figure>
<h3 id="3-4-3-自定义分区器"><a href="#3-4-3-自定义分区器" class="headerlink" title="3.4.3 自定义分区器"></a>3.4.3 自定义分区器</h3>研发人员可以根据企业需求，自己重新实现分区器。</li>
</ol>
</li>
</ol>
</li>
<li><p>需求<br> 实现一个分区器，发送过来的数据中如果包含atguigu，就发往0号分区，不包含atguigu，就发往1号分区</p>
</li>
<li><p>实现步骤</p>
<ol>
<li>定义类实现Partitioner接口</li>
<li>重写partition()方法</li>
</ol>
</li>
<li><p>自定义分区器编码</p>
 <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CustomProducerCallbackUsingMyPartition</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        Properties properties = <span class="keyword">new</span> Properties();</span><br><span class="line">        properties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">&quot;hadoop001:9092,hadoop002:9092,hadoop003:9092,hadoop004:9092,hadoop005:9092&quot;</span>);</span><br><span class="line">        properties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());</span><br><span class="line">        properties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());</span><br><span class="line"></span><br><span class="line">        properties.put(<span class="string">&quot;batch.size&quot;</span>, <span class="number">16384</span>);</span><br><span class="line">        properties.put(<span class="string">&quot;linger.ms&quot;</span>, <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        properties.put(<span class="string">&quot;buffer.memory&quot;</span>,<span class="number">33554432</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 添加自定义分区器</span></span><br><span class="line">        properties.put(ProducerConfig.PARTITIONER_CLASS_CONFIG, MyPartitioner.class.getName());</span><br><span class="line"></span><br><span class="line">        KafkaProducer&lt;String, String&gt; kafkaProducer = <span class="keyword">new</span> KafkaProducer&lt;&gt;(properties);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">5</span>; i++) &#123;</span><br><span class="line"></span><br><span class="line">            kafkaProducer.send(<span class="keyword">new</span> ProducerRecord&lt;&gt;(<span class="string">&quot;first&quot;</span>, i % <span class="number">2</span> == <span class="number">0</span> ? <span class="string">&quot;atguigu &quot;</span> + i : i + <span class="string">&quot;&quot;</span>), <span class="keyword">new</span> Callback() &#123;</span><br><span class="line">                <span class="meta">@Override</span></span><br><span class="line">                <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onCompletion</span><span class="params">(RecordMetadata recordMetadata, Exception e)</span> </span>&#123;</span><br><span class="line">                    <span class="keyword">if</span> (e == <span class="keyword">null</span>) &#123;</span><br><span class="line">                        System.out.println(<span class="string">&quot;主题：&quot;</span> + recordMetadata.topic() + <span class="string">&quot;-&gt;&quot;</span></span><br><span class="line">                                + <span class="string">&quot;分区：&quot;</span> + recordMetadata.partition()</span><br><span class="line">                        );</span><br><span class="line">                    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                        e.printStackTrace();</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        kafkaProducer.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li><p>使用分区器的方法，在生产者的配置中添加分区器参数</p>
 <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CustomProducerCallbackUsingMyPartition</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        Properties properties = <span class="keyword">new</span> Properties();</span><br><span class="line">        properties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">&quot;hadoop001:9092,hadoop002:9092,hadoop003:9092,hadoop004:9092,hadoop005:9092&quot;</span>);</span><br><span class="line">        properties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG);</span><br><span class="line">        properties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());</span><br><span class="line"></span><br><span class="line">        properties.put(<span class="string">&quot;batch.size&quot;</span>, <span class="number">16384</span>);</span><br><span class="line">        properties.put(<span class="string">&quot;linger.ms&quot;</span>, <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        properties.put(<span class="string">&quot;buffer.memory&quot;</span>,<span class="number">33554432</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 添加自定义分区器</span></span><br><span class="line">        properties.put(ProducerConfig.PARTITIONER_CLASS_CONFIG, MyPartitioner.class.getName());</span><br><span class="line"></span><br><span class="line">        KafkaProducer&lt;String, String&gt; kafkaProducer = <span class="keyword">new</span> KafkaProducer&lt;&gt;(properties);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">5</span>; i++) &#123;</span><br><span class="line"></span><br><span class="line">            kafkaProducer.send(<span class="keyword">new</span> ProducerRecord&lt;&gt;(<span class="string">&quot;first&quot;</span>, i % <span class="number">2</span> == <span class="number">0</span> ? <span class="string">&quot;atguigu &quot;</span> + i : i + <span class="string">&quot;&quot;</span>), <span class="keyword">new</span> Callback() &#123;</span><br><span class="line">                <span class="meta">@Override</span></span><br><span class="line">                <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onCompletion</span><span class="params">(RecordMetadata recordMetadata, Exception e)</span> </span>&#123;</span><br><span class="line">                    <span class="keyword">if</span> (e == <span class="keyword">null</span>) &#123;</span><br><span class="line">                        System.out.println(<span class="string">&quot;主题：&quot;</span> + recordMetadata.topic() + <span class="string">&quot;-&gt;&quot;</span></span><br><span class="line">                                + <span class="string">&quot;分区：&quot;</span> + recordMetadata.partition()</span><br><span class="line">                        );</span><br><span class="line">                    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                        e.printStackTrace();</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        kafkaProducer.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li><p>测试</p>
<ol>
<li>在hadoop001上开启Kafka消费者。 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">kafka-console-consumer.sh --bootstrap-server hadoop001:9092 --topic first</span><br><span class="line">atguigu 0</span><br><span class="line">atguigu 2</span><br><span class="line">atguigu 4</span><br><span class="line">1</span><br><span class="line">3</span><br></pre></td></tr></table></figure></li>
<li>在IDEA控制台观察回调信息 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">主题：first-&gt;分区：0</span><br><span class="line">主题：first-&gt;分区：0</span><br><span class="line">主题：first-&gt;分区：0</span><br><span class="line">主题：first-&gt;分区：1</span><br><span class="line">主题：first-&gt;分区：1</span><br></pre></td></tr></table></figure>
<h1 id="第4章-Kafka-Broker"><a href="#第4章-Kafka-Broker" class="headerlink" title="第4章 Kafka Broker"></a>第4章 Kafka Broker</h1><h2 id="4-1-Kafka-Broker工作流程"><a href="#4-1-Kafka-Broker工作流程" class="headerlink" title="4.1 Kafka Broker工作流程"></a>4.1 Kafka Broker工作流程</h2><h3 id="4-1-1-Zookeeper中存储的Kafka信息"><a href="#4-1-1-Zookeeper中存储的Kafka信息" class="headerlink" title="4.1.1 Zookeeper中存储的Kafka信息"></a>4.1.1 Zookeeper中存储的Kafka信息</h3><img src="https://i.loli.net/2021/11/19/h7ixP2akuVMmpQT.jpg"></li>
</ol>
</li>
<li><p>在zookeeper的服务端存储的Kafka相关信息：</p>
<ul>
<li><code>/kafka/admin</code>: 存储管理信息。主要为删除主题事件，分区迁移事件，优先副本选举信息 (一般为临时节点)</li>
<li><code>/kafka/brokers</code>: 存储 Broker 相关信息。broker 节点以及节点上的主题相关信息<ul>
<li><code>/kafka/brokers/topics/[topic]</code>: 存储某个topic的partitions所有分配信息  <figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;removing_replicas&quot;</span>: &#123;&#125;,</span><br><span class="line">  <span class="attr">&quot;partitions&quot;</span>: &#123; </span><br><span class="line">      <span class="attr">&quot;2&quot;</span>: [<span class="number">3</span>, <span class="number">4</span>], # 同步副本组brokerId列表</span><br><span class="line">      <span class="attr">&quot;1&quot;</span>: [<span class="number">2</span>, <span class="number">3</span>], </span><br><span class="line">      <span class="attr">&quot;0&quot;</span>: [<span class="number">1</span>, <span class="number">2</span>] </span><br><span class="line">    &#125;,</span><br><span class="line">  <span class="attr">&quot;topic_id&quot;</span>: <span class="string">&quot;wuuibxosTj2iZnI-8pknXw&quot;</span>,</span><br><span class="line">  <span class="attr">&quot;adding_replicas&quot;</span>: &#123;&#125;,</span><br><span class="line">  <span class="attr">&quot;version&quot;</span>: <span class="number">3</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li><code>/kafka/brokers/topics/[topic]/partitions/[broker.id]/state</code>: partition状态信息,记录谁是Leader，有哪些服务器可用  <figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;controller_epoch&quot;</span>: <span class="number">1</span>, #表示kafka集群中的中央控制器选举次数</span><br><span class="line">  <span class="attr">&quot;leader&quot;</span>: <span class="number">3</span>, #表示该partition选举leader的brokerId,</span><br><span class="line">  <span class="attr">&quot;version&quot;</span>: <span class="number">1</span>,</span><br><span class="line">  <span class="attr">&quot;leader_epoch&quot;</span>: <span class="number">0</span>, #该partition leader选举次数</span><br><span class="line">  <span class="attr">&quot;isr&quot;</span>: [<span class="number">3</span>, <span class="number">4</span>] #[同步副本组brokerId列表]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li><code>/kafka/brokers/ids[0...N]</code>: 记录有哪些服务器, 每个broker的配置文件中都需要指定一个数字类型的id(全局不可重复),此节点为临时znode(EPHEMERAL)  <figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;listener_security_protocol_map&quot;</span>: &#123; <span class="attr">&quot;PLAINTEXT&quot;</span>: <span class="string">&quot;PLAINTEXT&quot;</span> &#125;,</span><br><span class="line">  <span class="attr">&quot;endpoints&quot;</span>: [<span class="string">&quot;PLAINTEXT://hadoop001:9092&quot;</span>],</span><br><span class="line">  <span class="attr">&quot;jmx_port&quot;</span>: <span class="number">-1</span>, # jmx端口号</span><br><span class="line">  <span class="attr">&quot;features&quot;</span>: &#123;&#125;,</span><br><span class="line">  <span class="attr">&quot;host&quot;</span>: <span class="string">&quot;hadoop001&quot;</span>, #主机名或ip地址</span><br><span class="line">  <span class="attr">&quot;timestamp&quot;</span>: <span class="string">&quot;1636338340201&quot;</span>, # kafka broker初始启动时的时间戳</span><br><span class="line">  <span class="attr">&quot;port&quot;</span>: <span class="number">9092</span>, # kafka broker的服务端端口号,由server.properties中参数port确定</span><br><span class="line">  <span class="attr">&quot;version&quot;</span>: <span class="number">5</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><code>/kafka/cluster</code>: 存储 kafka 集群信息</li>
<li><code>/kafka/controller</code>: 存储控制器节点信息,辅助选举Leader  <figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">     <span class="attr">&quot;version&quot;</span>: <span class="number">1</span>, </span><br><span class="line">     <span class="attr">&quot;brokerid&quot;</span>: <span class="number">1</span>, </span><br><span class="line">     <span class="attr">&quot;timestamp&quot;</span>: <span class="string">&quot;1636338340300&quot;</span> </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li><code>/kafka/controller_epoch</code><ul>
<li>此值为一个数字,kafka集群中第一个broker第一次启动时为1，以后只要集群中center controller中央控制器所在broker变更或挂掉，就会重新选举新的center controller，每次center controller变更controller_epoch值就会 + 1; <h3 id="4-1-2-Leader选举流程"><a href="#4-1-2-Leader选举流程" class="headerlink" title="4.1.2 Leader选举流程"></a>4.1.2 Leader选举流程</h3>kafka集群中有2个种leader，一种是broker的leader即controller leader，还有一种就是partition的leader<br><img src="https://i.loli.net/2021/11/19/xu9tYf7TRJ1hN2m.jpg"></li>
</ul>
</li>
</ul>
</li>
</ol>
<h4 id="4-1-2-1-Controller-Leader"><a href="#4-1-2-1-Controller-Leader" class="headerlink" title="4.1.2.1 Controller Leader"></a>4.1.2.1 Controller Leader</h4><p>控制器（Controller），是 Apache Kafka 的核心组件。它的主要作用是在 ZooKeeper 的帮助下管理和协调整个 Kafka 集群。控制器其实就是一个 Broker，只不过它除了具有一般 Broker 的功能以外，还负责 Leader 的选举。</p>
<h5 id="4-1-2-1-1-如何选举控制器"><a href="#4-1-2-1-1-如何选举控制器" class="headerlink" title="4.1.2.1.1 如何选举控制器"></a>4.1.2.1.1 如何选举控制器</h5><p>集群中任意一台 Broker 都能充当控制器的角色，但是，在运行过程中，只能有一个 Broker 成为控制器，行使其管理和协调的职责。实际上，Broker 在启动时，会尝试去 ZooKeeper 中创建 /controller 节点。Kafka 当前选举控制器的规则是：第一个在 ZooKeeper 成功创建 /controller 临时节点的 Broker 会被指定为控制器。<br><img src="https://i.loli.net/2021/11/19/VD6FaWlZjnR2Ey8.jpg"></p>
<ol>
<li>第一个在 ZooKeeper 中成功创建 /controller 临时节点的 Broker 会被指定为控制器。</li>
<li>其他 Broker 在控制器节点上创建 Zookeeper watch 对象。</li>
<li>如果控制器被关闭或者与 Zookeeper 断开连接，Zookeeper 临时节点就会消失。集群中的其他 Broker 通过 watch 对象得到状态变化的通知，它们会尝试让自己成为新的控制器。</li>
<li>第一个在 Zookeeper 里创建一个临时节点 /controller 的 Broker 成为新控制器。其他 Broker 在新控制器节点上创建 Zookeeper watch 对象。</li>
<li>每个新选出的控制器通过 Zookeeper 的条件递增操作获得一个全新的、数值更大的 controller epoch。其他节点会忽略旧的 epoch 的消息。</li>
<li>当控制器发现一个 Broker 已离开集群，并且这个 Broker 是某些 Partition 的 Leader。此时，控制器会遍历这些 Partition，并用轮询方式确定谁应该成为新 Leader，随后，新 Leader 开始处理生产者和消费者的请求，而 Follower 开始从 Leader 那里复制消息。<blockquote>
<p>总结：<br>Kafka 使用 Zookeeper 的临时节点来选举控制器，并在节点加入集群或退出集群时通知控制器。控制器负责在节点加入或离开集群时进行 Partition Leader 选举。控制器使用 epoch 来避免“脑裂”，“脑裂”是指两个节点同时被认为自己是当前的控制器</p>
</blockquote>
</li>
</ol>
<h4 id="4-1-2-2-Partition-leader"><a href="#4-1-2-2-Partition-leader" class="headerlink" title="4.1.2.2 Partition leader"></a>4.1.2.2 Partition leader</h4><ol>
<li>Controller Leader监听brokers节点变化,并决定分区、副本分配和分区Leader选举</li>
<li>Controller Leader 将节点信息上传到ZK <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">/brokers/topics/[topic]/partitions/[broker.id]/state</span><br><span class="line">&#123;&quot;controller_epoch&quot;:1,&quot;leader&quot;:1,&quot;version&quot;:1,&quot;leader_epoch&quot;:0,&quot;isr&quot;:[1,2]&#125;</span><br></pre></td></tr></table></figure></li>
<li>如果当前Partition Leader的Broker挂了</li>
<li>Controller Leader监听到节点变化</li>
<li>Controller Leader获取ISR</li>
<li>选举新的Leader（在ISR中排在前面的优先）</li>
<li>更新Leader及ISR完成Partition Leader 选举</li>
<li>如果 ISR 为空了，就说明 Leader 副本也“挂掉”了，Kafka 需要重新选举一个新的 Leader。<ul>
<li>Kafka 把所有不在 ISR 中的存活副本都称为非同步副本。通常来说，非同步副本落后 Leader 太多，因此，如果选择这些副本作为新 Leader，就可能出现数据的丢失。毕竟，这些副本中保存的消息远远落后于老 Leader 中的消息。在 Kafka 中，选举这种副本的过程称为 Unclean 领导者选举。Broker 端参数 unclean.leader.election.enable 控制是否允许 Unclean 领导者选举。</li>
<li>开启 Unclean 领导者选举可能会造成数据丢失，但好处是：它使得 Partition Leader 副本一直存在，不至于停止对外提供服务，因此提升了高可用性。反之，禁止 Unclean 领导者选举的好处在于维护了数据的一致性，避免了消息丢失，但牺牲了高可用性。</li>
</ul>
</li>
</ol>
<h3 id="4-1-3-Kafka-Broker启动流程"><a href="#4-1-3-Kafka-Broker启动流程" class="headerlink" title="4.1.3 Kafka Broker启动流程"></a>4.1.3 Kafka Broker启动流程</h3><p><img src="https://i.loli.net/2021/11/19/FZSaxJR1PQIpXts.jpg"></p>
<table>
<thead>
<tr>
<th>参数名称</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>acks</td>
<td>0：生产者发送过来的数据，不需要等数据落盘应答。<br>1：生产者发送过来的数据，Leader收到数据后应答。<br>-1（all）：生产者发送过来的数据，Leader+和isr队列里面的所有节点收齐数据后应答。默认值是-1。</td>
</tr>
<tr>
<td>enable.idempotence</td>
<td>是否开启事务，默认true，开启事务</td>
</tr>
<tr>
<td>max.in.flight.requests.per.connection</td>
<td>允许最多没有返回ack的次数，默认为5，开启幂等性要保证该值是 1-5的数字。</td>
</tr>
<tr>
<td>log.segment.bytes</td>
<td>Kafka中log日志是分成一块块存储的，此配置是指log日志划分 成块的大小，默认值1G。</td>
</tr>
<tr>
<td>log.retention.hours</td>
<td>Kafka中数据保存的时间，默认7天</td>
</tr>
</tbody></table>
<h2 id="4-2-数据可靠性保证"><a href="#4-2-数据可靠性保证" class="headerlink" title="4.2 数据可靠性保证"></a>4.2 数据可靠性保证</h2><h3 id="4-2-1-ack应答级别"><a href="#4-2-1-ack应答级别" class="headerlink" title="4.2.1 ack应答级别"></a>4.2.1 ack应答级别</h3><p>当producer向leader发送数据时，可以通过request.required.acks参数来设置数据可靠性的级别</p>
<ul>
<li>0：生产者发送过来的数据，不需要等数据落盘应答。这种情况下数据传输效率最高，但是数据可靠性确是最低的<br>  <img src="https://i.loli.net/2021/11/19/TVCWoOZ5A3NuR8D.jpg"></li>
<li>1：生产者发送过来的数据，Leader收到数据后应答。<ul>
<li>数据丢失分析：producer发送数据到leader，leader写本地日志成功，返回客户端成功；此时ISR中的副本还没有来得及拉取该消息，leader就宕机了，那么此次发送的消息就会丢失<br><img src="https://i.loli.net/2021/11/19/SEUKrXFvO6ex5Ms.jpg"></li>
</ul>
</li>
<li>-1（all）：生产者发送过来的数据，Leader和ISR队列里面的所有节点收齐数据后应答。<ul>
<li>acks=-1的情况下，数据发送到leader, ISR的follower全部完成数据同步后，leader此时挂掉，那么会选举出新的leader，数据不会丢失<br>  <img src="https://i.loli.net/2021/11/19/yQGx8LTE4cYfXPV.jpg"></li>
<li>思考：Leader收到数据，所有Follower都开始同步数据，但有一个Follower，因为某种故障，迟迟不能与Leader进行同步，那这个问题怎么解决呢？</li>
<li>Leader维护了一个动态的in-sync replica set（ISR），意为和Leader保持同步的Follower集合。当ISR中的Follower完成数据的同步之后，Leader就会给producer发送ack。如果Follower长时间未向Leader同步数据，则该Follower将被踢出ISR，该时间阈值由replica.lag.time.max.ms参数设定，默认30s。Leader发生故障之后，就会从ISR中选举新的Leader。</li>
<li>数据重复分析：acks=-1的情况下，数据发送到leader后 ，部分ISR的副本同步，leader此时挂掉。比如follower1和follower2都有可能变成新的leader, producer端会得到返回异常，producer端会重新发送数据，数据可能会重复<br>  <img src="https://i.loli.net/2021/11/19/5XCGBIZmNsOjQ1F.jpg"><br>  <img src="https://i.loli.net/2021/11/19/mHgn63Gj2yaLs7Z.jpg"></li>
</ul>
</li>
</ul>
<h3 id="4-2-2-Leader和Follower故障处理细节"><a href="#4-2-2-Leader和Follower故障处理细节" class="headerlink" title="4.2.2 Leader和Follower故障处理细节"></a>4.2.2 Leader和Follower故障处理细节</h3><ul>
<li><p>LEO（Log End Offset）：每个副本的最后一个offset</p>
</li>
<li><p>HW（High Watermark）：所有副本中最小的LEO，也是消费者能见到的最大的offset</p>
</li>
<li><p>Leader故障</p>
<ul>
<li>leader 发生故障之后，会从 ISR 中选出一个新的 leader</li>
<li>为保证多个副本之间的数据一致性，其余的 follower 会先将各自的 log 文件高于 HW 的部分截掉，然后从新的 leader同步数据<blockquote>
<p>注意：这只能保证副本之间的数据一致性，并不能保证数据不丢失或者不重复。</p>
</blockquote>
</li>
</ul>
</li>
<li><p>Follower故障</p>
<ul>
<li>follower 发生故障后会被临时踢出 ISR</li>
<li>待该 follower 恢复后，follower 会读取本地磁盘记录的上次的 HW，并将 log 文件高于 HW 的部分截取掉，从 HW 开始向 leader 进行同步。</li>
<li>等该 follower 的 LEO 大于等于该 Partition 的 HW，即 follower 追上 leader 之后，就可以重新加入 ISR 了。</li>
</ul>
</li>
</ul>
<h3 id="4-2-3-精确一次（Exactly-Once）"><a href="#4-2-3-精确一次（Exactly-Once）" class="headerlink" title="4.2.3 精确一次（Exactly Once）"></a>4.2.3 精确一次（Exactly Once）</h3><ul>
<li>至少一次（At Least Once）：例如，将服务器的ACK级别设置为-1（ISR完全应答），且min.insync.replicas设置大于等于2.可以保证Producer到Broker之间不会丢失数据。<ul>
<li><code>min.insync.replicas</code>: ack为-1时生效，表示ISR里应答的最小follower数量。默认为1（leader本身也算一个！），所以当ISR里除了leader本身，没有其他的follower，即使ack设为-1，运行过程中也只会保存leader一份数据，相当于1（leader应答）的效果，不能保证不丢数据。</li>
<li>所以需要将min.insync.replicas设置大于等于2，才能保证有其他副本同步到数据</li>
</ul>
</li>
<li>最多一次（At Most Once）：例如，将服务器ACK级别设置为0（无需应答），可以保证生产者每条消息只会被发送一次。<ul>
<li>At Least Once可以保证数据不丢失，但是不能保证数据不重复</li>
<li>At Most Once可以保证数据不重复，但是不能保证数据不丢失。</li>
</ul>
</li>
<li>精确一次（Exactly Once）：对于一些非常重要的信息，比如和钱相关的数据，要求数据既不能重复也不丢失。每条消息肯定会被传输一次且仅传输一次。<ul>
<li>Kafka通过 幂等性（Idempotence）和事务（Transaction）这两种机制实现了 精确一次（exactly once）语义</li>
</ul>
</li>
</ul>
<h3 id="4-2-4-幂等性"><a href="#4-2-4-幂等性" class="headerlink" title="4.2.4 幂等性"></a>4.2.4 幂等性</h3><ol>
<li>幂等性原理<ul>
<li>幂等性就是指Producer不论向Broker发送多少次重复数据，Broker端都只会持久化一条。幂等性结合至少一次（At Least Once），就构成了Kafka的精确一次（Exactly Once）。</li>
<li>即：精确一次（Exactly Once） = 至少一次（At Least Once） + 幂等性 。</li>
<li>开启幂等性的Producer在初始化的时候会被分配一个ProducerID，发往同一Partition的消息会附带Sequence Number。而Broker端会对&lt;ProducerID, Partition, SeqNumber&gt;主键做缓存，当具有相同主键的消息提交时，Broker只会持久化一条。</li>
<li>注意：Producer重启PID就会变化，即&lt;PID, Partition, SeqNumber&gt; 主键变化，所以<font color ='red' >幂等性无法保证跨分区跨会话的Exactly Once</font>。<br>  <img src="https://i.loli.net/2021/11/19/pba75dhHBmuX6TP.jpg"></li>
</ul>
</li>
<li>如何使用幂等性<ul>
<li>开启参数enable.idempotence 默认为true，false关闭</li>
<li>相关参数max.in.flight.requests.per.connection 该参数指定了生产者在收到服务器响应之前可以发送多少个消息。它的值越高，就会占用越多的内存，不过也会提升吞吐量。把它设为 1 可以保证消息是按照发送的顺序写入服务器的，即使发生了重试</li>
<li><font color ='red' >幂等性保证的是单分区单会话exactly once</font></li>
</ul>
</li>
</ol>
<h3 id="4-2-5-生产者事务"><a href="#4-2-5-生产者事务" class="headerlink" title="4.2.5 生产者事务"></a>4.2.5 生产者事务</h3><ul>
<li>保证多次提交到不同主题和不同分区的消息的原子性，即要么全部发送成功，要么全部发送失败</li>
<li>保证conumser-transform-produce 应用模式中，消息能被原子性转换。</li>
</ul>
<ol>
<li><p>Kafka事务原理<br> <img src="https://i.loli.net/2021/11/19/i8TlgIzKCsAGyqH.jpg"></p>
</li>
<li><p>流程描述</p>
<ul>
<li><p>Producer准备TransactionId</p>
<ul>
<li>Kafka事务为了实现Producer的主从功能，提出了TransactionId的概念，同一个TransactionId只能有一个在运行，后面启动的Producer会使得前面的Producer立即抛出异常。</li>
</ul>
</li>
<li><p>初始化事务</p>
<ul>
<li>Producer会首先向任意一个broker发送查找自己对应事务协调器的请求，获取请求后，Producer会向事务协调器请求PID,同时在这个过程中，如果发现对应TransactionId有之前未完成的任务，它还会做以下两件事：<ul>
<li>恢复Producer对应TransactionId之前未完成的事务（Commit/Abort）</li>
<li>对PID对应epoch进行递增，防止脑裂问题。</li>
</ul>
</li>
</ul>
</li>
<li><p>开始事务：</p>
<ul>
<li>本地记录事务状态为开始，但是协调器只有在接受到事务第一条消息后，才会标记为事务真正的开始。</li>
</ul>
</li>
<li><p>进行事务：</p>
<ul>
<li>在kafka事务中，会原子的支持Consumer-Process-Producer过程，因此在这个过程中还提供了一个API</li>
<li>producer.sendOffsetsToTransaction();这个过程会将消费的offset暂存在协调器中，等事务提交时统一提交</li>
</ul>
</li>
<li><p>提交/回滚：</p>
<ul>
<li>当提交或回滚的时候，协调器会进行一个两段提交<ul>
<li>第一阶段，将事务日志，将此事务设置为PREPARE_COMMIT或PREPARE_ABORT</li>
<li>第二阶段，发送Transaction Marker给事务涉及到的Leader发送标记信息，标记此条信息为已提交或已放弃</li>
</ul>
</li>
<li>当完成第二阶段后，协调器最终会将此事务标记为COMPLETE_COMMIT或COMPLETE_ABORT</li>
</ul>
</li>
<li><p>事务故障恢复</p>
<ul>
<li>__transaction_state-分区-Leader存储事务信息的特殊主题，负责传递和持久化事务状态，通过持久化状态，可以使得协调器即使崩溃，也能选举新的Leader继续补全事务</li>
<li>在提交阶段，为了防止其他Leader崩溃而没有收到commit消息，协调器会先保存事务状态，再发送Transaction Marker消息</li>
<li>Producer 在发送 beginTransaction() 时，如果出现 timeout 或者错误：Producer 只需要重试即可；</li>
<li>Producer 在发送数据时出现错误：Producer 应该 abort 这个事务，如果 Produce 没有 abort（比如设置了重试无限次，并且 batch 超时设置得非常大），TransactionCoordinator 将会在这个事务超时之后 abort 这个事务操作；</li>
<li>Producer 发送 commitTransaction() 时出现 timeout 或者错误：Producer 应该重试这个请求（幂等保证）；</li>
<li>Coordinator Failure：如果 Transaction Coordinator 发生切换（事务 topic leader 切换），Coordinator 可以从日志中恢复。如果发送事务有处于 PREPARE_COMMIT 或 PREPARE_ABORT 状态，那么直接执行 commit 或者 abort 操作，如果是一个正在进行的事务，Coordinator 的失败并不需要 abort 事务，producer 只需要向新的 Coordinator 发送请求即可。</li>
</ul>
</li>
</ul>
</li>
<li><p>事务模板</p>
 <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 1初始化事务</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">initTransactions</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 2开启事务</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">beginTransaction</span><span class="params">()</span> <span class="keyword">throws</span> ProducerFencedException</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 3在事务内提交已经消费的偏移量</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">sendOffsetsToTransaction</span><span class="params">(Map&lt;TopicPartition, OffsetAndMetadata&gt; offsets,</span></span></span><br><span class="line"><span class="params"><span class="function">                              String consumerGroupId)</span> <span class="keyword">throws</span> ProducerFencedException</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 4提交事务</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">commitTransaction</span><span class="params">()</span> <span class="keyword">throws</span> ProducerFencedException</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 5放弃事务（类似于回滚事务的操作）</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">abortTransaction</span><span class="params">()</span> <span class="keyword">throws</span> ProducerFencedException</span>;</span><br></pre></td></tr></table></figure></li>
<li><p>编码测试</p>
 <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CustomProducerTransaction</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">final</span> Properties properties = <span class="keyword">new</span> Properties();</span><br><span class="line">        <span class="comment">// 给kafka配置对象添加配置信息：bootstrap.servers</span></span><br><span class="line">        properties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">&quot;hadoop001:9092,hadoop002:9092,hadoop003:9092,hadoop004:9092,hadoop005:9092&quot;</span>);</span><br><span class="line">        <span class="comment">// key,value序列化（必须）：key.serializer，value.serializer</span></span><br><span class="line">        properties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());</span><br><span class="line">        properties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 批次大小 默认16K：batch.size</span></span><br><span class="line">        properties.put(ProducerConfig.BATCH_SIZE_CONFIG, <span class="number">16384</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 等待时间：linger.ms</span></span><br><span class="line">        properties.put(ProducerConfig.LINGER_MS_CONFIG, <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// RecordAccumulator缓冲区大小 默认32M：buffer.memory</span></span><br><span class="line">        properties.put(ProducerConfig.BUFFER_MEMORY_CONFIG, <span class="number">33554432</span>);</span><br><span class="line"></span><br><span class="line">        properties.put(ProducerConfig.TRANSACTIONAL_ID_CONFIG, <span class="string">&quot;transaction_id_0&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3. 创建kafka生产者对象</span></span><br><span class="line">        KafkaProducer&lt;String, String&gt; kafkaProducer = <span class="keyword">new</span> KafkaProducer&lt;&gt;(properties);</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 1初始化事务</span></span><br><span class="line">        kafkaProducer.initTransactions();</span><br><span class="line">        <span class="comment">// 2开启事务</span></span><br><span class="line">        kafkaProducer.beginTransaction();</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="comment">// 4. 调用send方法,发送消息</span></span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">1090000000</span>; i++) &#123;</span><br><span class="line">                kafkaProducer.send(<span class="keyword">new</span> ProducerRecord&lt;&gt;(<span class="string">&quot;first&quot;</span>,<span class="string">&quot;i am producer &quot;</span> + i));</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">// 4提交事务</span></span><br><span class="line">            kafkaProducer.commitTransaction();</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">             <span class="comment">// 5放弃事务（类似于回滚事务的操作）</span></span><br><span class="line">            kafkaProducer.abortTransaction();</span><br><span class="line">        &#125;<span class="keyword">finally</span> &#123;</span><br><span class="line">            <span class="comment">// 5. 关闭资源</span></span><br><span class="line">            kafkaProducer.close();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="4-3-文件存储"><a href="#4-3-文件存储" class="headerlink" title="4.3 文件存储"></a>4.3 文件存储</h2><h3 id="4-3-1-文件存储机制"><a href="#4-3-1-文件存储机制" class="headerlink" title="4.3.1 文件存储机制"></a>4.3.1 文件存储机制</h3><blockquote>
<p>思考：Topic数据到底是怎么存储的呢？</p>
</blockquote>
<ol>
<li>启动生产者，并发送消息 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop001 ~]$ kafka-console-producer.sh --bootstrap-server hadoop001:9092 --topic first</span><br><span class="line">&gt;hello world</span><br></pre></td></tr></table></figure></li>
<li>查看hadoop001（或者其他任意broker）的/opt/module/kafka_2.12-3.0.0/logs/first-0/路径上的文件 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop001 bin]$ kafka-topics.sh --bootstrap-server hadoop001:9092 --describe -topic first</span><br><span class="line">Topic: first	TopicId: LtoXnmphRHmMEVuj_o5yyw	PartitionCount: 3	ReplicationFactor: 2	Configs: segment.bytes=1073741824</span><br><span class="line">	Topic: first	Partition: 0	Leader: 3	Replicas: 3,5	Isr: 3,5</span><br><span class="line">	Topic: first	Partition: 1	Leader: 5	Replicas: 5,1	Isr: 5,1</span><br><span class="line">	Topic: first	Partition: 2	Leader: 1	Replicas: 1,4	Isr: 1,4</span><br><span class="line">[atguigu@dw-server-001 bin]$</span><br><span class="line">[atguigu@hadoop001 ~]$ ll /opt/module/kafka_2.12-3.0.0/logs/first-0/</span><br><span class="line">总用量 60</span><br><span class="line">-rw-rw-r--. 1 atguigu atguigu 10485760 11月  9 21:16 00000000000000000000.index</span><br><span class="line">-rw-rw-r--. 1 atguigu atguigu    41536 11月  9 21:16 00000000000000000000.log</span><br><span class="line">-rw-rw-r--. 1 atguigu atguigu 10485756 11月  9 21:16 00000000000000000000.timeindex</span><br><span class="line">-rw-rw-r--. 1 atguigu atguigu        8 11月  9 11:08 leader-epoch-checkpoint</span><br><span class="line">-rw-rw-r--. 1 atguigu atguigu       43 11月  9 10:56 partition.metadata</span><br><span class="line">[atguigu@hadoop001 ~]$</span><br></pre></td></tr></table></figure></li>
<li>查看log日志 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop001 kafka]$ bin/kafka-console-producer.sh --bootstrap-server hadoop001:9092 --topic first</span><br><span class="line">&gt;hello world</span><br></pre></td></tr></table></figure></li>
<li>通过工具查看index和log信息 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop001 kafka]$ kafka-run-class.sh kafka.tools.DumpLogSegments --files datas/first-0/00000000000000000000.index </span><br><span class="line"></span><br><span class="line">Dumping datas/first-0/00000000000000000000.index</span><br><span class="line">offset: 3 position: 152</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop001 kafka]$ bin/kafka-run-class.sh kafka.tools.DumpLogSegments --files datas/first-0/00000000000000000000.log</span><br><span class="line"></span><br><span class="line">Dumping datas/first-0/00000000000000000000.log</span><br><span class="line">Starting offset: 0</span><br><span class="line">baseOffset: 0 lastOffset: 1 count: 2 baseSequence: -1 lastSequence: -1 producerId: -1 producerEpoch: -1 partitionLeaderEpoch: 0 isTransactional: <span class="literal">false</span> isControl: <span class="literal">false</span> position: 0 CreateTime: 1636338440962 size: 75 magic: 2 compresscodec: none crc: 2745337109 isvalid: <span class="literal">true</span></span><br><span class="line">baseOffset: 2 lastOffset: 2 count: 1 baseSequence: -1 lastSequence: -1 producerId: -1 producerEpoch: -1 partitionLeaderEpoch: 0 isTransactional: <span class="literal">false</span> isControl: <span class="literal">false</span> position: 75 CreateTime: 1636351749089 size: 77 magic: 2 compresscodec: none crc: 273943004 isvalid: <span class="literal">true</span></span><br><span class="line">baseOffset: 3 lastOffset: 3 count: 1 baseSequence: -1 lastSequence: -1 producerId: -1 producerEpoch: -1 partitionLeaderEpoch: 0 isTransactional: <span class="literal">false</span> isControl: <span class="literal">false</span> position: 152 CreateTime: 1636351749119 size: 77 magic: 2 compresscodec: none crc: 106207379 isvalid: <span class="literal">true</span></span><br><span class="line">baseOffset: 4 lastOffset: 8 count: 5 baseSequence: -1 lastSequence: -1 producerId: -1 producerEpoch: -1 partitionLeaderEpoch: 0 isTransactional: <span class="literal">false</span> isControl: <span class="literal">false</span> position: 229 CreateTime: 1636353061435 size: 141 magic: 2 compresscodec: none crc: 157376877 isvalid: <span class="literal">true</span></span><br><span class="line">baseOffset: 9 lastOffset: 13 count: 5 baseSequence: -1 lastSequence: -1 producerId: -1 producerEpoch: -1 partitionLeaderEpoch: 0 isTransactional: <span class="literal">false</span> isControl: <span class="literal">false</span> position: 370 CreateTime: 1636353204051 size: 146 magic: 2 compresscodec: none crc: 4058582827 isvalid: <span class="literal">true</span></span><br></pre></td></tr></table></figure></li>
<li>Topic数据的存储机制<br> topic是逻辑上的概念，而partition是物理上的概念，<font color ='red' >每个partition对应于一个log文件</font>，该log文件中存储的就是Producer生产的数据。<font color ='red' >Producer生产的数据会被不断追加到该log文件末端</font>，为防止log文件过大导致数据定位效率低下，<font color ='red' >Kafka采取了分片和索引机制</font>，将每个partition分为多个segment。<font color ='red' >每个segment对应两个文件：“.index”文件和“.log”文件</font>。这些文件位于一个文件夹下，该文件夹的命名规则为：topic名称+分区序号，例如：first-0。<br> <img src="https://i.loli.net/2021/11/19/Xtf8CTPNYw1bv63.jpg"></li>
<li>index文件和log文件详解<ul>
<li><code>.index</code>文件存储大量的索引信息，索引信息按照数组的逻辑排列。<ul>
<li>Index存储数据采用的是稀疏索引存储</li>
<li>log.index.interval.bytes: 引项字节间隔密度，会影响索引文件中的区间密度和查询效率默认4kb</li>
</ul>
</li>
<li><code>.log</code>文件存储大量的数据，数据直接紧密排列，索引文件中的元数据指向对应数据文件中message的物理偏移地址。<ul>
<li><code>log.segment.bytes</code>：Kafka中log日志是分成一块块存储的，此配置是指log日志划分 成块的大小，默认值1G。</li>
</ul>
</li>
</ul>
</li>
<li>日志切分文件逻辑<ul>
<li>日志文件和索引文件都会存在多个文件，组成多个 SegmentLog</li>
<li>当前日志分段文件的大小超过了 broker 端参数 log.segment.bytes 配置的值。log.segment.bytes 参数的默认值为 1073741824，即 1GB。</li>
<li>当前日志分段中消息的最大时间戳与当前系统的时间戳的差值大于 log.roll.ms 或 log.roll.hours 参数配置的值。如果同时配置了 log.roll.ms 和 log.roll.hours 参数，那么 log.roll.ms 的优先级高。默认情况下，只配置了 log.roll.hours 参数，其值为168，即 7 天</li>
<li>偏移量索引文件或时间戳索引文件的大小达到 broker 端参数 log.index.size.max.bytes 配置的值。log.index.size.max.bytes 的默认值为 10485760，即 10MB</li>
<li>追加的消息的偏移量与当前日志分段的偏移量之间的差值大于 Integer.MAX_VALUE，即要追加的消息的偏移量不能转变为相对偏移量<ul>
<li>为什么是 Integer.MAX_VALUE ？<ul>
<li>在偏移量索引文件中，每个索引项共占用 8 个字节，并分为两部分。相对偏移量和物理地址。</li>
<li>相对偏移量：表示消息相对与基准偏移量的偏移量，占 4 个字节</li>
<li>物理地址：消息在日志分段文件中对应的物理位置，也占 4 个字节</li>
<li>4 个字节刚好对应 Integer.MAX_VALUE ，如果大于 Integer.MAX_VALUE ，则不能用 4 个字节进行表示了。</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>索引文件切分过程<ul>
<li>索引文件会根据 log.index.size.max.bytes 值进行预先分配空间，即文件创建的时候就是最大值，当真正的进行索引文件切分的时候，才会将其裁剪到实际数据大小的文件。这一点是跟日志文件有所区别的地方。其意义降低了代码逻辑的复杂性。</li>
</ul>
</li>
<li>查找消息<ol>
<li>offset 查询<br> <img src="https://i.loli.net/2021/11/19/MIv3NqtbdhCcRDw.png"><ol>
<li>偏移量索引由相对偏移量和物理地址组成。</li>
<li>可以通过kafka-dump-log.sh命令解析.index 文件 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">kafka-dump-log.sh --files ./00000000000000000000.index</span><br><span class="line">offset:0 position:0</span><br><span class="line">offset:20 position:320</span><br><span class="line">offset:43 position:1220</span><br></pre></td></tr></table></figure>
<blockquote>
<p>注意：offset 与 position 没有直接关系，由于存在数据删除和日志清理。</p>
</blockquote>
</li>
<li>如何查看 偏移量为 23 的消息？<ul>
<li>Kafka 中存在一个 ConcurrentSkipListMap 来保存在每个日志分段，通过跳跃表方式，定位到在 00000000000000000000.index ，通过二分法在偏移量索引文件中找到不大于 23 的最大索引项，即 offset 20 那栏，然后从日志分段文件中的物理位置为320 开始顺序查找偏移量为 23 的消息。</li>
</ul>
</li>
</ol>
</li>
<li>时间戳方式查询<ol>
<li>通过时间戳方式进行查找消息，需要通过查找时间戳索引和偏移量索引两个文件。</li>
<li>时间戳索引索引格式<br> <img src="https://i.loli.net/2021/11/19/XBykJIseCf87pMm.jpg"><br> <img src="https://i.loli.net/2021/11/19/Bh6sgKwkiZFS82d.jpg"></li>
<li>查找时间戳为 1557554753430 开始的消息？<ul>
<li>将 1557554753430 和每个日志分段中最大时间戳 largestTimeStamp 逐一对比，直到找到不小于 1557554753430 所对应的日志分段。日志分段中的 largestTimeStamp 的计算是先查询该日志分段所对应时间戳索引文件，找到最后一条索引项，若最后一条索引项的时间戳字段值大于 0 ，则取该值，否则去该日志分段的最近修改时间。</li>
<li>找到相应日志分段之后，使用二分法进行定位，与偏移量索引方式类似，找到不大于 1557554753430 最大索引项，也就是 [1557554753420 430]。</li>
<li>拿着偏移量为 430 到偏移量索引文件中使用二分法找到不大于 430 最大索引项，即 [20，320] 。</li>
<li>日志文件中从 320 的物理位置开始查找不小于 1557554753430 数据。<blockquote>
<p>注意：timestamp文件中的 offset 与 index 文件中的 relativeOffset 不是一一对应的哦。因为数据的写入是各自追加。</p>
</blockquote>
</li>
</ul>
</li>
</ol>
</li>
</ol>
</li>
</ol>
<h3 id="4-3-2-文件清理策略"><a href="#4-3-2-文件清理策略" class="headerlink" title="4.3.2 文件清理策略"></a>4.3.2 文件清理策略</h3><ul>
<li>Kafka中默认的日志保存时间为7天，可以通过调整log.retention.hours参数，修改保存时间。</li>
<li>那么日志一旦超过了设置的时间，怎么处理呢？<ul>
<li>Kafka中提供的日志清理策略有delete和compact两种。<ul>
<li>delete日志删除：将过期数据删除。<ul>
<li>log.cleanup.policy = delete    所有数据启用删除策略</li>
<li>cleanup.policy= delete     单个主题数据启用删除策略</li>
<li>思考：如果一个segment中有一部分数据过期，一部分没有过期，怎么处理？<br>  <img src="https://i.loli.net/2021/11/19/4c5bpKFfz3H8R1Q.jpg"></li>
</ul>
</li>
<li>compact日志压缩：对于相同key的不同value值，只保留最后一个版本。<ul>
<li>log.cleanup.policy = compact  所有数据启用压缩策略</li>
<li>cleanup.policy = compact单个主题数据启用删除策略<h2 id="4-4-Kafka-高效读写数据"><a href="#4-4-Kafka-高效读写数据" class="headerlink" title="4.4 Kafka 高效读写数据"></a>4.4 Kafka 高效读写数据</h2></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<ol>
<li>Kafka本身是分布式集群，可以采用分区技术，并发度高。</li>
<li>顺序写磁盘 <ul>
<li>Kafka的producer生产数据，要写入到log文件中，写的过程是一直追加到文件末端，为顺序写。官网有数据表明，同样的磁盘，顺序写能到600M/s，而随机写只有100K/s。这与磁盘的机械机构有关，顺序写之所以快，是因为其省去了大量磁头寻址的时间。<br>  <img src="https://i.loli.net/2021/11/19/8lStAREKQoIFXwT.jpg"></li>
</ul>
</li>
<li>零复制技术<ul>
<li>Kafka的数据加工处理操作交由Kafka生产者和Kafka消费者处理。</li>
<li>Kafka Broker应用层不关心存储的数据，所以就不用走应用层，传输效率高。<br>  <img src="https://i.loli.net/2021/11/19/epTbkQq1EvDM3ul.jpg"></li>
</ul>
</li>
</ol>
<h1 id="第5章-Kafka消费者"><a href="#第5章-Kafka消费者" class="headerlink" title="第5章 Kafka消费者"></a>第5章 Kafka消费者</h1><h2 id="5-1-Kafka消费者工作流程"><a href="#5-1-Kafka消费者工作流程" class="headerlink" title="5.1 Kafka消费者工作流程"></a>5.1 Kafka消费者工作流程</h2><p><img src="https://i.loli.net/2021/11/19/OuTZbBEeAcFd7gU.jpg"></p>
<ul>
<li>Kafka中消息是以topic进行分类的，生产者生产消息，消费者消费消息，都是面向topic的。</li>
<li>一个topic下的每一个分区都单独维护一个offset，所以分发到不同分区中的数据是不同的数据。消费者的分区维护是一个消费者组一个主题的一个分区维护一个offset。<br>消费者相关调优参数</li>
<li>消费逻辑<ol>
<li>单个消费者，可以消费Kafka中所有主题、所有分区</li>
<li>单个消费者与消费者之间相互独立。都能同时消费所有主题、所有分区。</li>
<li>消费者组，可以消费Kafka中所有主题、所有分区。只不过在消费的过程中，考虑到并发，每个消费者组中的消费者，只能消费指定分区。</li>
<li>消费者组和消费者组之间一点关系都没有。都能同时消费所有主题、所有分区。</li>
</ol>
</li>
<li>消费者相关调优参数</li>
</ul>
<table>
<thead>
<tr>
<th>参数名称</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>bootstrap.servers</td>
<td>向Kafka集群建立初始连接用到的host/port列表。</td>
</tr>
<tr>
<td>group.id</td>
<td>标记消费者所属的消费者组</td>
</tr>
<tr>
<td>enable.auto.commit</td>
<td>默认值为true，消费者会自动周期性地向服务器提交偏移量。</td>
</tr>
<tr>
<td>auto.commit.interval.ms</td>
<td>如果设置了 enable.auto.commit 的值为true， 则该值定义了消费者偏移量向Kafka提交的频率，默认5s</td>
</tr>
<tr>
<td>auto.offset.reset</td>
<td>当Kafka中没有初始偏移量或当前偏移量在服务器中不存在（如，数据被 删除了），该如何处理？ <br>earliest：自动重置偏移量到最早的偏移量 <br>latest：自动重置偏移量为最新的偏移量 <br>none：如果消费组原来的（previous）偏移量不存在，则向消费者抛异常 <br>anything：向消费者抛异常</td>
</tr>
</tbody></table>
<h2 id="5-2-消费方式"><a href="#5-2-消费方式" class="headerlink" title="5.2 消费方式"></a>5.2 消费方式</h2><p><img src="https://i.loli.net/2021/11/19/q54B7gmylYAhF8i.jpg"></p>
<ul>
<li>pull（拉）模式：consumer采用从broker中主动拉取数据。Kafka采用这种方式。</li>
<li>push（推）模式：Kafka没有采用这种方式，因为由broker决定消息发送速率，很难适应所有消费者的消费速率。例如推送的速度是50m/s，Consumer1、Consumer2就来不及处理消息。</li>
<li>pull模式不足之处是，如果Kafka没有数据，消费者可能会陷入循环中，一直返回空数据。针对这一点，Kafka的消费者在消费数据时会传入一个时长参数timeout，如果当前没有数据可供消费，consumer会等待一段时间之后再返回，这段时长即为timeout。</li>
</ul>
<h3 id="5-2-1-独立消费者"><a href="#5-2-1-独立消费者" class="headerlink" title="5.2.1 独立消费者"></a>5.2.1 独立消费者</h3><blockquote>
<p>注意：在消费者API代码中必须配置消费者组，命令行启动消费者不填写消费者组会被自动填写随机的消费者组。</p>
</blockquote>
<ol>
<li>需求：创建一个独立消费者，消费first主题中数据。</li>
<li>编码 <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.atguigu.kafka.consumer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerConfig;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecord;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecords;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.KafkaConsumer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.time.Duration;</span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CustomConsumer</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1.创建消费者的配置对象</span></span><br><span class="line">        Properties properties = <span class="keyword">new</span> Properties();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2.给消费者配置对象添加参数</span></span><br><span class="line">        properties.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">&quot;hadoop102:9092&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 配置序列化 必须</span></span><br><span class="line">properties.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, <span class="string">&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span>);</span><br><span class="line">        properties.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, <span class="string">&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 配置消费者组（组名任意起名） 必须</span></span><br><span class="line">        properties.put(ConsumerConfig.GROUP_ID_CONFIG, <span class="string">&quot;test&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 创建消费者对象</span></span><br><span class="line">        KafkaConsumer&lt;String, String&gt; kafkaConsumer = <span class="keyword">new</span> KafkaConsumer&lt;String, String&gt;(properties);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 注册要消费的主题（可以消费多个主题）</span></span><br><span class="line">        ArrayList&lt;String&gt; topics = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">        topics.add(<span class="string">&quot;first&quot;</span>);</span><br><span class="line">        kafkaConsumer.subscribe(topics);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 拉取数据打印</span></span><br><span class="line">        <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">            <span class="comment">// 设置1s中消费一批数据</span></span><br><span class="line">            ConsumerRecords&lt;String, String&gt; consumerRecords = kafkaConsumer.poll(Duration.ofSeconds(<span class="number">1</span>));</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 打印消费到的数据</span></span><br><span class="line">            <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; consumerRecord : consumerRecords) &#123;</span><br><span class="line">                System.out.println(consumerRecord);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>测试<ol>
<li>在IDEA中执行消费者程序</li>
<li>在Kafka集群控制台，创建Kafka生产者，并输入数据 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop001 kafka]$ kafka-console-producer.sh --bootstrap-server hadoop102:9092 --topic first</span><br><span class="line">&gt;hello</span><br></pre></td></tr></table></figure></li>
<li>在IDEA控制台观察接收到的数据 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ConsumerRecord(topic = first, partition = 1, leaderEpoch = 3, offset = 0, CreateTime = 1629160841112, serialized key size = -1, serialized value size = 5, headers = RecordHeaders(headers = [], isReadOnly = <span class="literal">false</span>), key = null, value = hello)</span><br></pre></td></tr></table></figure></li>
</ol>
</li>
</ol>
<h3 id="5-2-2-消费者组案例"><a href="#5-2-2-消费者组案例" class="headerlink" title="5.2.2 消费者组案例"></a>5.2.2 消费者组案例</h3><ul>
<li>Consumer Group（CG）：消费者组，由多个consumer组成。<ul>
<li>消费者组内每个消费者负责消费不同分区的数据，一个分区只能由一个组内消费者消费；</li>
<li>消费者组之间互不影响。所有的消费者都属于某个消费者组，即消费者组是逻辑上的一个订阅者。</li>
</ul>
</li>
</ul>
<h2 id="5-3-分区分配策略"><a href="#5-3-分区分配策略" class="headerlink" title="5.3 分区分配策略"></a>5.3 分区分配策略</h2><ul>
<li>一个consumer group中有多个consumer，一个 topic有多个partition，所以必然会涉及到partition的分配问题，即确定哪个partition由哪个consumer来消费。</li>
<li>Kafka有三种主流的分区分配策略： Range、RoundRobin、Sticky。</li>
<li>可以通过配置参数partition.assignment.strategy，修改分区的分配策略。默认策略是Range +  Sticky。Kafka可以同时使用多个分区分配策略。</li>
</ul>
<h3 id="5-3-1-Range"><a href="#5-3-1-Range" class="headerlink" title="5.3.1 Range"></a>5.3.1 Range</h3><p><img src="https://i.loli.net/2021/11/19/s4QuYAL2UBqRfm8.jpg"></p>
<ol>
<li>Range分区策略原理<ul>
<li>按照消费者总数和分区总数进行整除运算来获得一个跨度，然后将分区按照跨度进行平均分配，以保证分区尽可能均匀地分配给所有的消费者。对于每一个topic，RangeAssignor策略会将消费组内所有订阅这个topic的消费者按照名称的字典序排序，然后为每个消费者划分固定的分区范围，如果不够平均分配，那么字典序靠前的消费者会被多分配一个分区。</li>
<li>假如现在有 7 个分区，3 个消费者，排序后的分区将会是0,1,2,3,4,5,6；消费者排序完之后将会是C0-0,C1-0,C2-0。</li>
<li>通过 partitions数/consumer数 来决定每个消费者应该消费几个分区。如果除不尽，那么前面几个消费者将会多消费 1 个分区。</li>
<li>例如，7/3 = 3 余 1 ，除不尽，那么 消费者 C0-0 便会多消费 1 个分区</li>
</ul>
<blockquote>
<p>注意：如果只是针对 1 个 topic 而言，C0-0消费者多消费1个分区影响不是很大。但是如果有 N 多个 topic，那么针对每个 topic，消费者 C0-0 都将多消费 1 个分区，topic越多，C0-0 消费的分区会比其他消费者明显多消费 N 个分区。<br> 容易产生数据倾斜！</p>
</blockquote>
</li>
<li>Range分区分配策略案例<ul>
<li>修改主题first为7个分区  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop001 kafka]$ bin/kafka-topics.sh --bootstrap-server hadoop102:9092 --alter --topic first --partitions 7</span><br></pre></td></tr></table></figure>
<ul>
<li>注意：分区数可以增加，但是不能减少。</li>
</ul>
</li>
<li>创建三个线程启动三个消费者CustomConsumer组成消费者组，组名都为“test”  <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> tech.anzhen.kafka.consumer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerConfig;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecord;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecords;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.KafkaConsumer;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.serialization.StringDeserializer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.time.Duration;</span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CustomConsumerGroup</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">final</span> Properties properties = <span class="keyword">new</span> Properties();</span><br><span class="line"></span><br><span class="line">        properties.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">&quot;hadoop001:9092,hadoop002:9092,hadoop003:9092,hadoop004:9092,hadoop005:9092&quot;</span>);</span><br><span class="line"></span><br><span class="line">        properties.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());</span><br><span class="line">        properties.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());</span><br><span class="line"></span><br><span class="line">        properties.put(ConsumerConfig.GROUP_ID_CONFIG, <span class="string">&quot;123&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">new</span> Thread(() -&gt; consumerThread(properties)).start();</span><br><span class="line">        <span class="keyword">new</span> Thread(() -&gt; consumerThread(properties)).start();</span><br><span class="line">        <span class="keyword">new</span> Thread(() -&gt; consumerThread(properties)).start();</span><br><span class="line">        <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">consumerThread</span><span class="params">(Properties properties)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">final</span> KafkaConsumer&lt;Object, Object&gt; kafkaConsumer = <span class="keyword">new</span> KafkaConsumer&lt;&gt;(properties);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">final</span> ArrayList&lt;String&gt; topicList = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">        topicList.add(<span class="string">&quot;first&quot;</span>);</span><br><span class="line">        kafkaConsumer.subscribe(topicList);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">            <span class="keyword">final</span> ConsumerRecords&lt;Object, Object&gt; poll = kafkaConsumer.poll(Duration.ofSeconds(<span class="number">1</span>));</span><br><span class="line">            <span class="keyword">if</span> (poll != <span class="keyword">null</span>) &#123;</span><br><span class="line">                <span class="keyword">for</span> (ConsumerRecord&lt;Object, Object&gt; consumerRecord : poll) &#123;</span><br><span class="line">                    System.out.println(Thread.currentThread().getName() + <span class="string">&quot; &quot;</span> + consumerRecord);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>启动CustomProducer生产者，发送500条消息，随机发送到不同的分区：<ul>
<li>说明：Kafka默认的分区分配策略就是Range + Sticky，所有不需要修改策略。<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> tech.anzhen.kafka.producer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.KafkaProducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.ProducerConfig;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.ProducerRecord;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CustomProducer</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">        Properties properties = <span class="keyword">new</span> Properties();</span><br><span class="line"></span><br><span class="line">        properties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">&quot;hadoop001:9092,hadoop002:9092,hadoop003:9092,hadoop004:9092,hadoop005:9092&quot;</span>);</span><br><span class="line"></span><br><span class="line">        properties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, <span class="string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>);</span><br><span class="line">        properties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, <span class="string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>);</span><br><span class="line"></span><br><span class="line">        KafkaProducer&lt;String, String&gt; kafkaProducer = <span class="keyword">new</span> KafkaProducer&lt;&gt;(properties);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">500</span>; i++) &#123;</span><br><span class="line"></span><br><span class="line">            kafkaProducer.send(<span class="keyword">new</span> ProducerRecord&lt;&gt;(<span class="string">&quot;first&quot;</span>, <span class="string">&quot;atguigu &quot;</span> + i));</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 避免发送到同一个分区</span></span><br><span class="line">            Thread.sleep(<span class="number">2</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        kafkaProducer.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>观察数据分区<br>  <img src="https://i.loli.net/2021/11/19/o3FS1q8waTPmdCe.jpg"></li>
</ul>
</li>
</ol>
<h3 id="5-3-2-RoundRobin"><a href="#5-3-2-RoundRobin" class="headerlink" title="5.3.2 RoundRobin"></a>5.3.2 RoundRobin</h3><p><img src="https://i.loli.net/2021/11/19/IjrJBkqwRslfc1m.jpg"></p>
<ol>
<li>RoundRobin分区策略原理<ul>
<li>RoundRobin 轮询分区策略，是把所有的 partition 和所有的 consumer 都列出来，然后按照 hascode 进行排序，最后通过轮询算法来分配 partition 给到各个消费者。</li>
</ul>
</li>
<li>RoundRobin分区分配策略案例<ol>
<li>修改CustomConsumer消费者代码中分区分配策略为RoundRobin。 <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> tech.anzhen.kafka.consumer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerConfig;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecord;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecords;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.KafkaConsumer;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.RoundRobinAssignor;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.serialization.StringDeserializer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.time.Duration;</span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.ConcurrentHashMap;</span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.CountDownLatch;</span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.atomic.AtomicInteger;</span><br><span class="line"><span class="keyword">import</span> java.util.function.BiConsumer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CustomConsumerRoundRobin</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">static</span> ConcurrentHashMap&lt;String, AtomicInteger&gt; map = <span class="keyword">new</span> ConcurrentHashMap&lt;&gt;();</span><br><span class="line">    <span class="keyword">static</span> CountDownLatch countDownLatch = <span class="keyword">new</span> CountDownLatch(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">        <span class="keyword">final</span> Properties properties = <span class="keyword">new</span> Properties();</span><br><span class="line"></span><br><span class="line">        properties.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">&quot;bd-server-001:9092,bd-server-002:9092,bd-server-003:9092,bd-server-004:9092,bd-server-005:9092&quot;</span>);</span><br><span class="line"></span><br><span class="line">        properties.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());</span><br><span class="line">        properties.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 修改分区分配策略</span></span><br><span class="line">        properties.put(ConsumerConfig.PARTITION_ASSIGNMENT_STRATEGY_CONFIG, RoundRobinAssignor.class.getName());</span><br><span class="line"></span><br><span class="line">        properties.put(ConsumerConfig.GROUP_ID_CONFIG, <span class="string">&quot;123&quot;</span>);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="keyword">new</span> Thread(() -&gt; consumerThread(properties)).start();</span><br><span class="line">        <span class="keyword">new</span> Thread(() -&gt; consumerThread(properties)).start();</span><br><span class="line">        <span class="keyword">new</span> Thread(() -&gt; consumerThread(properties)).start();</span><br><span class="line"></span><br><span class="line">        countDownLatch.await();</span><br><span class="line">        map.forEach((s, atomicInteger) -&gt; &#123;</span><br><span class="line">            System.out.println(<span class="string">&quot;线程: &quot;</span> + s.split(<span class="string">&quot;,&quot;</span>)[<span class="number">0</span>] + <span class="string">&quot; 分区: &quot;</span> + s.split(<span class="string">&quot;,&quot;</span>)[<span class="number">1</span>] + <span class="string">&quot; 消费数量：&quot;</span> + atomicInteger.get());</span><br><span class="line">        &#125;);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">consumerThread</span><span class="params">(Properties properties)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">final</span> KafkaConsumer&lt;Object, Object&gt; kafkaConsumer = <span class="keyword">new</span> KafkaConsumer&lt;&gt;(properties);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">final</span> ArrayList&lt;String&gt; topicList = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">        topicList.add(<span class="string">&quot;first&quot;</span>);</span><br><span class="line">        kafkaConsumer.subscribe(topicList);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">            <span class="keyword">final</span> ConsumerRecords&lt;Object, Object&gt; poll = kafkaConsumer.poll(Duration.ofSeconds(<span class="number">1</span>));</span><br><span class="line">            <span class="keyword">if</span> (poll != <span class="keyword">null</span>) &#123;</span><br><span class="line">                <span class="keyword">for</span> (ConsumerRecord&lt;Object, Object&gt; consumerRecord : poll) &#123;</span><br><span class="line">                    <span class="keyword">final</span> AtomicInteger atomicInteger = map.putIfAbsent(Thread.currentThread().getName() + <span class="string">&quot;,&quot;</span> + consumerRecord.partition(), <span class="keyword">new</span> AtomicInteger(<span class="number">1</span>));</span><br><span class="line">                    <span class="keyword">if</span> ( atomicInteger != <span class="keyword">null</span>) &#123;</span><br><span class="line">                        atomicInteger.incrementAndGet();</span><br><span class="line">                    &#125;</span><br><span class="line">                    <span class="keyword">if</span> (consumerRecord.value().toString().equals(<span class="string">&quot;over&quot;</span>)) &#123;</span><br><span class="line">                        countDownLatch.countDown();</span><br><span class="line">                        <span class="keyword">return</span>;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ol>
<ul>
<li>观察分区分布<br>  <img src="https://i.loli.net/2021/11/19/bB4OuDdJseglSIf.jpg"></li>
</ul>
</li>
</ol>
<h3 id="5-3-3-Sticky"><a href="#5-3-3-Sticky" class="headerlink" title="5.3.3 Sticky"></a>5.3.3 Sticky</h3><p>特殊的分配策略StickyAssignor，Kafka从0.11.x版本开始引入这种分配策略，首先会尽量均衡的放置分区到消费者上面，在出现同一消费者组内消费者出现问题的时候，会尽量保持原有分配的分区不变化。</p>
<ul>
<li>修改分区分配策略为粘性  <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> tech.anzhen.kafka.consumer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerConfig;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecord;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecords;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.KafkaConsumer;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.RoundRobinAssignor;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.StickyAssignor;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.serialization.StringDeserializer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.time.Duration;</span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.ConcurrentHashMap;</span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.CountDownLatch;</span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.atomic.AtomicInteger;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CustomConsumerStickyAssignor</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">static</span> ConcurrentHashMap&lt;String, AtomicInteger&gt; map = <span class="keyword">new</span> ConcurrentHashMap&lt;&gt;();</span><br><span class="line">    <span class="keyword">static</span> CountDownLatch countDownLatch = <span class="keyword">new</span> CountDownLatch(<span class="number">1</span>);</span><br><span class="line">    <span class="keyword">static</span> AtomicInteger total = <span class="keyword">new</span> AtomicInteger(<span class="number">5001</span>);</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">        <span class="keyword">final</span> Properties properties = <span class="keyword">new</span> Properties();</span><br><span class="line"></span><br><span class="line">        properties.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">&quot;bd-server-001:9092,bd-server-002:9092,bd-server-003:9092,bd-server-004:9092,bd-server-005:9092&quot;</span>);</span><br><span class="line"></span><br><span class="line">        properties.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());</span><br><span class="line">        properties.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 修改分区分配策略</span></span><br><span class="line">        properties.put(ConsumerConfig.PARTITION_ASSIGNMENT_STRATEGY_CONFIG, StickyAssignor.class.getName());</span><br><span class="line"></span><br><span class="line">        properties.put(ConsumerConfig.GROUP_ID_CONFIG, <span class="string">&quot;1234&quot;</span>);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="keyword">new</span> Thread(() -&gt; consumerThread(properties,<span class="number">0</span>)).start();</span><br><span class="line">        <span class="keyword">new</span> Thread(() -&gt; consumerThread(properties,<span class="number">1</span>)).start();</span><br><span class="line">        <span class="keyword">new</span> Thread(() -&gt; consumerThread(properties,<span class="number">2</span>)).start();</span><br><span class="line"></span><br><span class="line">        countDownLatch.await();</span><br><span class="line">        map.forEach((s, atomicInteger) -&gt; &#123;</span><br><span class="line">            System.out.println(<span class="string">&quot;线程: &quot;</span> + s.split(<span class="string">&quot;,&quot;</span>)[<span class="number">0</span>] + <span class="string">&quot; 分区: &quot;</span> + s.split(<span class="string">&quot;,&quot;</span>)[<span class="number">1</span>] + <span class="string">&quot; 消费数量：&quot;</span> + atomicInteger.get());</span><br><span class="line">        &#125;);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">consumerThread</span><span class="params">(Properties properties, <span class="keyword">int</span> index)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">final</span> KafkaConsumer&lt;Object, Object&gt; kafkaConsumer = <span class="keyword">new</span> KafkaConsumer&lt;&gt;(properties);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">final</span> ArrayList&lt;String&gt; topicList = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">        topicList.add(<span class="string">&quot;first&quot;</span>);</span><br><span class="line">        kafkaConsumer.subscribe(topicList);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">            <span class="keyword">final</span> ConsumerRecords&lt;Object, Object&gt; poll = kafkaConsumer.poll(Duration.ofSeconds(<span class="number">1</span>));</span><br><span class="line">            <span class="keyword">if</span> (poll != <span class="keyword">null</span>) &#123;</span><br><span class="line">                <span class="keyword">for</span> (ConsumerRecord&lt;Object, Object&gt; consumerRecord : poll) &#123;</span><br><span class="line">                    <span class="keyword">final</span> AtomicInteger atomicInteger = map.putIfAbsent(Thread.currentThread().getName() + <span class="string">&quot;,&quot;</span> + consumerRecord.partition(), <span class="keyword">new</span> AtomicInteger(<span class="number">1</span>));</span><br><span class="line">                    <span class="keyword">if</span> ( atomicInteger != <span class="keyword">null</span>) &#123;</span><br><span class="line">                        atomicInteger.incrementAndGet();</span><br><span class="line">                    &#125;</span><br><span class="line">                    <span class="keyword">if</span> (total.decrementAndGet() == <span class="number">0</span>) &#123;</span><br><span class="line">                        countDownLatch.countDown();</span><br><span class="line">                        <span class="keyword">return</span>;</span><br><span class="line">                    &#125;</span><br><span class="line">                    <span class="keyword">if</span> (index == <span class="number">2</span> &amp;&amp; total.get() &lt; <span class="number">4900</span>) &#123;</span><br><span class="line">                        System.out.println(<span class="string">&quot;线程2挂掉了&quot;</span>);</span><br><span class="line">                        <span class="keyword">return</span>;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
<li>观察消息分布<br>  <img src="https://i.loli.net/2021/11/19/FY1Hnhak7oQjPuq.jpg"></li>
</ul>
<h2 id="5-4-offset相关"><a href="#5-4-offset相关" class="headerlink" title="5.4 offset相关"></a>5.4 offset相关</h2><h3 id="5-4-1-offset的维护"><a href="#5-4-1-offset的维护" class="headerlink" title="5.4.1 offset的维护"></a>5.4.1 offset的维护</h3><ul>
<li>由于consumer在消费过程中可能会出现断电宕机等故障，consumer恢复后，需要从故障前的位置的继续消费，所以consumer需要实时记录自己消费到了哪个offset，以便故障恢复后继续消费。</li>
<li>Kafka0.9版本之前，consumer默认将offset保存在Zookeeper中，</li>
<li>从0.9版本开始，consumer默认将offset保存在Kafka一个内置的topic中，该topic为<code>__consumer_offsets</code>。</li>
</ul>
<ol>
<li>消费offset案例<ul>
<li>思想：__consumer_offsets为Kafka中的topic，那就可以通过消费者进行消费。</li>
<li>采用命令行方式，创建一个新的topic  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@bd-server-001 ~]<span class="comment"># kafka-topics.sh --bootstrap-server bd-server-001:9092 --create --topic test-consumer-offset --partitions 2 --replication-factor 2</span></span><br><span class="line">Created topic test-consumer-offset.</span><br><span class="line">[root@bd-server-001 ~]<span class="comment"># kafka-topics.sh --bootstrap-server bd-server-001:9092 --list</span></span><br><span class="line">__consumer_offsets</span><br><span class="line">first</span><br><span class="line">test-consumer-offset</span><br></pre></td></tr></table></figure></li>
<li>在配置文件$KAFKA_HOME/config/consumer.properties中添加配置exclude.internal.topics=false，默认是true，表示不能消费系统主题。为了查看该系统主题数据，所以该参数修改为false。</li>
<li>启动生产者往atguigu生产数据  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@bd-server-001 ~]<span class="comment"># kafka-console-producer.sh --topic test-consumer-offset --bootstrap-server  bd-server-001:9092</span></span><br><span class="line">&gt;123</span><br><span class="line">&gt;321</span><br><span class="line">&gt;1234567</span><br></pre></td></tr></table></figure></li>
<li>启动消费者消费atguigu数据  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@bd-server-001 ~]<span class="comment"># kafka-console-consumer.sh --bootstrap-server bd-server-001:9092 --topic test-consumer-offset --group test</span></span><br><span class="line">123</span><br><span class="line">321</span><br><span class="line">1234567</span><br></pre></td></tr></table></figure>
<ul>
<li>注意：必须指定消费者组。</li>
</ul>
</li>
<li>查看消费者消费主题__consumer_offsets  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@bd-server-001 ~]<span class="comment"># kafka-console-consumer.sh --topic __consumer_offsets --bootstrap-server  bd-server-001:9092 --consumer.config $KAFKA_HOME/config/consumer.properties  --formatter &quot;kafka.coordinator.group.GroupMetadataManager\$OffsetsMessageFormatter&quot; --from-beginning</span></span><br><span class="line">···</span><br><span class="line">[test-consumer-group,__consumer_offsets,26]::OffsetAndMetadata(offset=0, leaderEpoch=Optional.empty, metadata=, commitTimestamp=1637325778217, expireTimestamp=None)</span><br><span class="line">[test-consumer-group,__consumer_offsets,29]::OffsetAndMetadata(offset=0, leaderEpoch=Optional.empty, metadata=, commitTimestamp=1637325778217, expireTimestamp=None)</span><br><span class="line">[test-consumer-group,__consumer_offsets,34]::OffsetAndMetadata(offset=0, leaderEpoch=Optional.empty, metadata=, commitTimestamp=1637325778217, expireTimestamp=None)</span><br><span class="line">[test-consumer-group,__consumer_offsets,10]::OffsetAndMetadata(offset=0, leaderEpoch=Optional.empty, metadata=, commitTimestamp=1637325778217, expireTimestamp=None)</span><br><span class="line">[test-consumer-group,__consumer_offsets,32]::OffsetAndMetadata(offset=0, leaderEpoch=Optional.empty, metadata=, commitTimestamp=1637325778217, expireTimestamp=None)</span><br><span class="line">[test-consumer-group,__consumer_offsets,40]::OffsetAndMetadata(offset=8792, leaderEpoch=Optional[3], metadata=, commitTimestamp=1637325778217, expireTimestamp=None)</span><br><span class="line">···</span><br></pre></td></tr></table></figure>
<h3 id="5-4-2-自动提交offset"><a href="#5-4-2-自动提交offset" class="headerlink" title="5.4.2 自动提交offset"></a>5.4.2 自动提交offset</h3>为了使我们能够专注于自己的业务逻辑，Kafka提供了自动提交offset的功能。<br>自动提交offset的相关参数：</li>
</ul>
</li>
</ol>
<ul>
<li>enable.auto.commit：是否开启自动提交offset功能，默认是true</li>
<li>auto.commit.interval.ms：自动提交offset的时间间隔，默认是5s  <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> tech.anzhen.kafka.producer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.KafkaProducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.ProducerConfig;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.ProducerRecord;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.serialization.StringSerializer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.TimeUnit;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CustomProducer</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">        <span class="keyword">final</span> Properties properties = <span class="keyword">new</span> Properties();</span><br><span class="line">        <span class="comment">// 给kafka配置对象添加配置信息：bootstrap.servers</span></span><br><span class="line">        properties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">&quot;bd-server-001:9092,bd-server-002:9092,bd-server-003:9092,bd-server-004:9092,bd-server-005:9092&quot;</span>);</span><br><span class="line">        <span class="comment">// key,value序列化（必须）：key.serializer，value.serializer</span></span><br><span class="line">        properties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());</span><br><span class="line">        properties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 批次大小 默认16K：batch.size</span></span><br><span class="line">        properties.put(ProducerConfig.BATCH_SIZE_CONFIG, <span class="number">16384</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 等待时间：linger.ms</span></span><br><span class="line">        properties.put(ProducerConfig.LINGER_MS_CONFIG, <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// RecordAccumulator缓冲区大小 默认32M：buffer.memory</span></span><br><span class="line">        properties.put(ProducerConfig.BUFFER_MEMORY_CONFIG, <span class="number">33554432</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3. 创建kafka生产者对象</span></span><br><span class="line">        KafkaProducer&lt;String, String&gt; kafkaProducer = <span class="keyword">new</span> KafkaProducer&lt;&gt;(properties);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 4. 调用send方法,发送消息</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">5000</span>; i++) &#123;</span><br><span class="line">            kafkaProducer.send(<span class="keyword">new</span> ProducerRecord&lt;&gt;(<span class="string">&quot;first&quot;</span>, i + <span class="string">&quot;&quot;</span>, <span class="string">&quot;i am producer &quot;</span> + i));</span><br><span class="line">        &#125;</span><br><span class="line">        TimeUnit.SECONDS.sleep(<span class="number">10</span>);</span><br><span class="line">        kafkaProducer.send(<span class="keyword">new</span> ProducerRecord&lt;&gt;(<span class="string">&quot;first&quot;</span>, <span class="string">&quot;over&quot;</span>));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 5. 关闭资源</span></span><br><span class="line">        kafkaProducer.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="5-4-3-自动重置Offset"><a href="#5-4-3-自动重置Offset" class="headerlink" title="5.4.3 自动重置Offset"></a>5.4.3 自动重置Offset</h3><ul>
<li><code>auto.offset.reset = earliest | latest | none</code>, 默认是latest</li>
<li>当Kafka中没有初始偏移量（消费者组第一次消费）或服务器上不再存在当前偏移量时（例如该数据已被删除），该怎么办：<ul>
<li>earliest：自动将偏移量重置为最早的偏移量</li>
<li>latest（默认值）：自动将偏移量重置为最新偏移量</li>
<li>none：如果未找到消费者组的先前偏移量，则向消费者抛出异常<br><img src="https://i.loli.net/2021/11/19/koWfR193XKUJgMe.jpg"></li>
</ul>
</li>
</ul>
<h3 id="5-4-4-手动提交offset"><a href="#5-4-4-手动提交offset" class="headerlink" title="5.4.4 手动提交offset"></a>5.4.4 手动提交offset</h3><ul>
<li>虽然自动提交offset十分简单便利，但由于其是基于时间提交的，开发人员难以把握offset提交的时机。因此Kafka还提供了手动提交offset的API。</li>
<li>手动提交offset的方法有两种：分别是commitSync（同步提交）和commitAsync（异步提交）。<ul>
<li>两者的相同点是，都会将本次poll的一批数据最高的偏移量提交；</li>
<li>不同点是，同步提交阻塞当前线程，一直到提交成功，并且会自动失败重试（由不可控因素导致，也会出现提交失败）；而异步提交则没有失败重试机制，故有可能提交失败。</li>
</ul>
</li>
<li>同步提交offset<ul>
<li>由于同步提交offset有失败重试机制，故更加可靠，以下为同步提交offset的示例。  <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> tech.anzhen.kafka.consumer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerConfig;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecord;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecords;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.KafkaConsumer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.time.Duration;</span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span>: Anzhen</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@description</span>:</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@PROJECT</span>_NAME: sgg-big-data</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@create</span>: 2021-11-19 20:49</span></span><br><span class="line"><span class="comment"> **/</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CustomConsumerByHand</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 1. 创建kafka消费者配置类</span></span><br><span class="line">        Properties properties = <span class="keyword">new</span> Properties();</span><br><span class="line">        <span class="comment">// 2. 添加配置参数</span></span><br><span class="line">        <span class="comment">// 添加连接</span></span><br><span class="line">        properties.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">&quot;bd-server-001:9092,bd-server-002:9092,bd-server-003:9092,bd-server-004:9092,bd-server-005:9092&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 配置序列化 必须</span></span><br><span class="line">        properties.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, <span class="string">&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span>);</span><br><span class="line">        properties.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, <span class="string">&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 配置消费者组</span></span><br><span class="line">        properties.put(ConsumerConfig.GROUP_ID_CONFIG, <span class="string">&quot;test&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 是否自动提交offset</span></span><br><span class="line">        properties.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, <span class="string">&quot;false&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//3. 创建kafka消费者</span></span><br><span class="line">        KafkaConsumer&lt;String, String&gt; consumer = <span class="keyword">new</span> KafkaConsumer&lt;&gt;(properties);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//4. 设置消费主题  形参是列表</span></span><br><span class="line">        consumer.subscribe(Arrays.asList(<span class="string">&quot;first&quot;</span>));</span><br><span class="line"></span><br><span class="line">        <span class="comment">//5. 消费数据</span></span><br><span class="line">        <span class="keyword">while</span> (<span class="keyword">true</span>)&#123;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 读取消息</span></span><br><span class="line">            ConsumerRecords&lt;String, String&gt; consumerRecords = consumer.poll(Duration.ofSeconds(<span class="number">1</span>));</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 输出消息</span></span><br><span class="line">            <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; consumerRecord : consumerRecords) &#123;</span><br><span class="line">                System.out.println(consumerRecord.value());</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 同步提交offset</span></span><br><span class="line">            consumer.commitSync();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>异步提交offset<ul>
<li>虽然同步提交offset更可靠一些，但是由于其会阻塞当前线程，直到提交成功。因此吞吐量会受到很大的影响。因此更多的情况下，会选用异步提交offset的方式。</li>
<li>以下为异步提交offset的示例：  <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1. 创建kafka消费者配置类</span></span><br><span class="line">        Properties properties = <span class="keyword">new</span> Properties();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2. 添加配置参数</span></span><br><span class="line">        <span class="comment">// 添加连接</span></span><br><span class="line">        properties.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">&quot;hadoop102:9092&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 配置序列化 必须</span></span><br><span class="line">properties.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, <span class="string">&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span>);</span><br><span class="line">        properties.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, <span class="string">&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 配置消费者组</span></span><br><span class="line">        properties.put(ConsumerConfig.GROUP_ID_CONFIG, <span class="string">&quot;test&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 是否自动提交offset</span></span><br><span class="line">        properties.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, <span class="string">&quot;false&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//3. 创建Kafka消费者</span></span><br><span class="line">        KafkaConsumer&lt;String, String&gt; consumer = <span class="keyword">new</span> KafkaConsumer&lt;&gt;(properties);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//4. 设置消费主题  形参是列表</span></span><br><span class="line">        consumer.subscribe(Arrays.asList(<span class="string">&quot;first&quot;</span>));</span><br><span class="line"></span><br><span class="line">        <span class="comment">//5. 消费数据</span></span><br><span class="line">        <span class="keyword">while</span> (<span class="keyword">true</span>)&#123;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 读取消息</span></span><br><span class="line">            ConsumerRecords&lt;String, String&gt; consumerRecords = consumer.poll(Duration.ofSeconds(<span class="number">1</span>));</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 输出消息</span></span><br><span class="line">            <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; consumerRecord : consumerRecords) &#123;</span><br><span class="line">                System.out.println(consumerRecord.value());</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 异步提交offset</span></span><br><span class="line">            consumer.commitAsync(<span class="keyword">new</span> OffsetCommitCallback() &#123;</span><br><span class="line">                <span class="comment">/**</span></span><br><span class="line"><span class="comment">                 * 回调函数输出</span></span><br><span class="line"><span class="comment">                 * <span class="doctag">@param</span> offsets   offset信息</span></span><br><span class="line"><span class="comment">                 * <span class="doctag">@param</span> exception 异常</span></span><br><span class="line"><span class="comment">                 */</span></span><br><span class="line">                <span class="meta">@Override</span></span><br><span class="line">                <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onComplete</span><span class="params">(Map&lt;TopicPartition, OffsetAndMetadata&gt; offsets, Exception exception)</span> </span>&#123;</span><br><span class="line">                    <span class="comment">// 如果出现异常打印</span></span><br><span class="line">                    <span class="keyword">if</span> (exception != <span class="keyword">null</span> )&#123;</span><br><span class="line">                        System.err.println(<span class="string">&quot;Commit failed for &quot;</span> + offsets);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>
<h3 id="5-4-5-数据漏消费和重复消费分析"><a href="#5-4-5-数据漏消费和重复消费分析" class="headerlink" title="5.4.5 数据漏消费和重复消费分析"></a>5.4.5 数据漏消费和重复消费分析</h3><p>无论是同步提交还是异步提交offset，都有可能会造成数据的漏消费或者重复消费。先提交offset后消费，有可能造成数据的漏消费；而先消费后提交offset，有可能会造成数据的重复消费。所以需要Consumer事务保证<font color ='red' >精准一次性消费</font></p>
<h2 id="5-5-Consumer事务（精准一次性消费）"><a href="#5-5-Consumer事务（精准一次性消费）" class="headerlink" title="5.5 Consumer事务（精准一次性消费）"></a>5.5 Consumer事务（精准一次性消费）</h2><p><img src="https://i.loli.net/2021/11/19/mlHzxE5U2XwLYyd.jpg"><br>如果想完成Consumer端的精准一次性消费，那么需要Kafka消费端将消费过程和提交offset过程做原子绑定。此时我们需要将Kafka的offset保存到支持事务的自定义介质（比如MySQL）</p>
<h1 id="第6章-Kafka-Eagle监控"><a href="#第6章-Kafka-Eagle监控" class="headerlink" title="第6章 Kafka-Eagle监控"></a>第6章 Kafka-Eagle监控</h1><ul>
<li>官网：<a target="_blank" rel="noopener" href="https://www.kafka-eagle.org/">https://www.kafka-eagle.org/</a></li>
</ul>
<ol>
<li>关闭Kafka集群 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop001 kafka]$ kf.sh stop</span><br></pre></td></tr></table></figure></li>
<li>修改$KAFAK_HOME/bin/kafka-server-start.sh命令修改如下参数值： <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> [ <span class="string">&quot;x<span class="variable">$KAFKA_HEAP_OPTS</span>&quot;</span> = <span class="string">&quot;x&quot;</span> ]; <span class="keyword">then</span></span><br><span class="line">    <span class="built_in">export</span> KAFKA_HEAP_OPTS=<span class="string">&quot;-server -Xms2G -Xmx2G -XX:PermSize=128m -XX:+UseG1GC -XX:MaxGCPauseMillis=200 -XX:ParallelGCThreads=8 -XX:ConcGCThreads=5 -XX:InitiatingHeapOccupancyPercent=70&quot;</span></span><br><span class="line">    <span class="built_in">export</span> JMX_PORT=<span class="string">&quot;9999&quot;</span></span><br><span class="line">    <span class="comment">#export KAFKA_HEAP_OPTS=&quot;-Xmx1G -Xms1G&quot;</span></span><br><span class="line"><span class="keyword">fi</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>注意：修改之后在启动Kafka之前要分发之其他节点<br>xsync kafka-server-start.sh</p>
</blockquote>
</li>
<li>上传压缩包kafka-eagle-bin-2.0.8.tar.gz到集群/opt/software目录</li>
<li>解压到本地 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop001 software]$ tar -zxvf kafka-eagle-bin-2.0.8.tar.gz </span><br></pre></td></tr></table></figure></li>
<li>将efak-web-2.0.8-bin.tar.gz解压至/opt/module <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop001 kafka-eagle-bin-2.0.8]$ tar -zxvf efak-web-2.0.8-bin.tar.gz -C /opt/module/</span><br></pre></td></tr></table></figure></li>
<li>修改配置文件 /opt/module/efak-web-2.0.8/conf/system-config.properties <figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">######################################</span></span><br><span class="line"><span class="comment"># multi zookeeper &amp; kafka cluster list</span></span><br><span class="line"><span class="comment"># Settings prefixed with &#x27;kafka.eagle.&#x27; will be deprecated, use &#x27;efak.&#x27; instead</span></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line"><span class="meta">efak.zk.cluster.alias</span>=<span class="string">cluster1</span></span><br><span class="line"><span class="meta">cluster1.zk.list</span>=<span class="string">hadoop102:2181,hadoop103:2181,hadoop104:2181/kafka</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line"><span class="comment"># zookeeper enable acl</span></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line"><span class="meta">cluster1.zk.acl.enable</span>=<span class="string">false</span></span><br><span class="line"><span class="meta">cluster1.zk.acl.schema</span>=<span class="string">digest</span></span><br><span class="line"><span class="meta">cluster1.zk.acl.username</span>=<span class="string">test</span></span><br><span class="line"><span class="meta">cluster1.zk.acl.password</span>=<span class="string">test123</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line"><span class="comment"># broker size online list</span></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line"><span class="meta">cluster1.efak.broker.size</span>=<span class="string">20</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line"><span class="comment"># zk client thread limit</span></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line"><span class="meta">kafka.zk.limit.size</span>=<span class="string">32</span></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line"><span class="comment"># EFAK webui port</span></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line"><span class="meta">efak.webui.port</span>=<span class="string">8048</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line"><span class="comment"># kafka jmx acl and ssl authenticate</span></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line"><span class="meta">cluster1.efak.jmx.acl</span>=<span class="string">false</span></span><br><span class="line"><span class="meta">cluster1.efak.jmx.user</span>=<span class="string">keadmin</span></span><br><span class="line"><span class="meta">cluster1.efak.jmx.password</span>=<span class="string">keadmin123</span></span><br><span class="line"><span class="meta">cluster1.efak.jmx.ssl</span>=<span class="string">false</span></span><br><span class="line"><span class="meta">cluster1.efak.jmx.truststore.location</span>=<span class="string">/data/ssl/certificates/kafka.truststore</span></span><br><span class="line"><span class="meta">cluster1.efak.jmx.truststore.password</span>=<span class="string">ke123456</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line"><span class="comment"># kafka offset storage</span></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line"><span class="comment"># offset保存在kafka</span></span><br><span class="line"><span class="meta">cluster1.efak.offset.storage</span>=<span class="string">kafka</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line"><span class="comment"># kafka jmx uri</span></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line"><span class="meta">cluster1.efak.jmx.uri</span>=<span class="string">service:jmx:rmi:///jndi/rmi://%s/jmxrmi</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line"><span class="comment"># kafka metrics, 15 days by default</span></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line"><span class="meta">efak.metrics.charts</span>=<span class="string">true</span></span><br><span class="line"><span class="meta">efak.metrics.retain</span>=<span class="string">15</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line"><span class="comment"># kafka sql topic records max</span></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line"><span class="meta">efak.sql.topic.records.max</span>=<span class="string">5000</span></span><br><span class="line"><span class="meta">efak.sql.topic.preview.records.max</span>=<span class="string">10</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line"><span class="comment"># delete kafka topic token</span></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line"><span class="meta">efak.topic.token</span>=<span class="string">keadmin</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line"><span class="comment"># kafka sasl authenticate</span></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line"><span class="meta">cluster1.efak.sasl.enable</span>=<span class="string">false</span></span><br><span class="line"><span class="meta">cluster1.efak.sasl.protocol</span>=<span class="string">SASL_PLAINTEXT</span></span><br><span class="line"><span class="meta">cluster1.efak.sasl.mechanism</span>=<span class="string">SCRAM-SHA-256</span></span><br><span class="line"><span class="meta">cluster1.efak.sasl.jaas.config</span>=<span class="string">org.apache.kafka.common.security.scram.ScramLoginModule required username=&quot;kafka&quot; password=&quot;kafka-eagle&quot;;</span></span><br><span class="line"><span class="meta">cluster1.efak.sasl.client.id</span>=<span class="string"></span></span><br><span class="line"><span class="meta">cluster1.efak.blacklist.topics</span>=<span class="string"></span></span><br><span class="line"><span class="meta">cluster1.efak.sasl.cgroup.enable</span>=<span class="string">false</span></span><br><span class="line"><span class="meta">cluster1.efak.sasl.cgroup.topics</span>=<span class="string"></span></span><br><span class="line"><span class="meta">cluster2.efak.sasl.enable</span>=<span class="string">false</span></span><br><span class="line"><span class="meta">cluster2.efak.sasl.protocol</span>=<span class="string">SASL_PLAINTEXT</span></span><br><span class="line"><span class="meta">cluster2.efak.sasl.mechanism</span>=<span class="string">PLAIN</span></span><br><span class="line"><span class="meta">cluster2.efak.sasl.jaas.config</span>=<span class="string">org.apache.kafka.common.security.plain.PlainLoginModule required username=&quot;kafka&quot; password=&quot;kafka-eagle&quot;;</span></span><br><span class="line"><span class="meta">cluster2.efak.sasl.client.id</span>=<span class="string"></span></span><br><span class="line"><span class="meta">cluster2.efak.blacklist.topics</span>=<span class="string"></span></span><br><span class="line"><span class="meta">cluster2.efak.sasl.cgroup.enable</span>=<span class="string">false</span></span><br><span class="line"><span class="meta">cluster2.efak.sasl.cgroup.topics</span>=<span class="string"></span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line"><span class="comment"># kafka ssl authenticate</span></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line"><span class="meta">cluster3.efak.ssl.enable</span>=<span class="string">false</span></span><br><span class="line"><span class="meta">cluster3.efak.ssl.protocol</span>=<span class="string">SSL</span></span><br><span class="line"><span class="meta">cluster3.efak.ssl.truststore.location</span>=<span class="string"></span></span><br><span class="line"><span class="meta">cluster3.efak.ssl.truststore.password</span>=<span class="string"></span></span><br><span class="line"><span class="meta">cluster3.efak.ssl.keystore.location</span>=<span class="string"></span></span><br><span class="line"><span class="meta">cluster3.efak.ssl.keystore.password</span>=<span class="string"></span></span><br><span class="line"><span class="meta">cluster3.efak.ssl.key.password</span>=<span class="string"></span></span><br><span class="line"><span class="meta">cluster3.efak.ssl.endpoint.identification.algorithm</span>=<span class="string">https</span></span><br><span class="line"><span class="meta">cluster3.efak.blacklist.topics</span>=<span class="string"></span></span><br><span class="line"><span class="meta">cluster3.efak.ssl.cgroup.enable</span>=<span class="string">false</span></span><br><span class="line"><span class="meta">cluster3.efak.ssl.cgroup.topics</span>=<span class="string"></span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line"><span class="comment"># kafka sqlite jdbc driver address</span></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line"><span class="comment"># 配置mysql连接</span></span><br><span class="line"><span class="meta">efak.driver</span>=<span class="string">com.mysql.jdbc.Driver</span></span><br><span class="line"><span class="meta">efak.url</span>=<span class="string">jdbc:mysql://hadoop102:3306/ke?useUnicode=true&amp;characterEncoding=UTF-8&amp;zeroDateTimeBehavior=convertToNull</span></span><br><span class="line"><span class="meta">efak.username</span>=<span class="string">root</span></span><br><span class="line"><span class="meta">efak.password</span>=<span class="string">000000</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line"><span class="comment"># kafka mysql jdbc driver address</span></span><br><span class="line"><span class="comment">######################################</span></span><br><span class="line"><span class="comment">#efak.driver=com.mysql.cj.jdbc.Driver</span></span><br><span class="line"><span class="comment">#efak.url=jdbc:mysql://127.0.0.1:3306/ke?useUnicode=true&amp;characterEncoding=UTF-8&amp;zeroDateTimeBehavior=convertToNull</span></span><br><span class="line"><span class="comment">#efak.username=root</span></span><br><span class="line"><span class="comment">#efak.password=123456</span></span><br></pre></td></tr></table></figure></li>
<li>添加环境变量 /etc/profile.d/my_env.sh <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># kafkaEFAK</span></span><br><span class="line"><span class="built_in">export</span> KE_HOME=/opt/module/efak</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$KE_HOME</span>/bin</span><br></pre></td></tr></table></figure></li>
<li>启动<ol>
<li>启动ZK以及KAFKA</li>
<li>启动efak <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop001 efak]$ bin/ke.sh start</span><br><span class="line">Version 2.0.8 -- Copyright 2016-2021</span><br><span class="line">*****************************************************************</span><br><span class="line">* EFAK Service has started success.</span><br><span class="line">* Welcome, Now you can visit <span class="string">&#x27;http://192.168.10.102:8048&#x27;</span></span><br><span class="line">* Account:admin ,Password:123456</span><br><span class="line">*****************************************************************</span><br><span class="line">* &lt;Usage&gt; ke.sh [start|status|stop|restart|stats] &lt;/Usage&gt;</span><br><span class="line">* &lt;Usage&gt; https://www.kafka-eagle.org/ &lt;/Usage&gt;</span><br><span class="line">*****************************************************************</span><br></pre></td></tr></table></figure>
<blockquote>
<p>说明：如果停止efak，执行命令 ke.sh stop</p>
</blockquote>
</li>
</ol>
</li>
<li>登录页面查看监控数据<br> <a target="_blank" rel="noopener" href="http://192.168.10.102:8048/">http://192.168.10.102:8048/</a></li>
</ol>
 
      <!-- reward -->
      
      <div id="reword-out">
        <div id="reward-btn">
          打赏
        </div>
      </div>
      
    </div>
    

    <!-- copyright -->
    
    <div class="declare">
      <ul class="post-copyright">
        <li>
          <i class="ri-copyright-line"></i>
          <strong>版权声明： </strong>
          
          本博客所有文章除特别声明外，著作权归作者所有。转载请注明出处！
          
        </li>
      </ul>
    </div>
    
    <footer class="article-footer">
       
<div class="share-btn">
      <span class="share-sns share-outer">
        <i class="ri-share-forward-line"></i>
        分享
      </span>
      <div class="share-wrap">
        <i class="arrow"></i>
        <div class="share-icons">
          
          <a class="weibo share-sns" href="javascript:;" data-type="weibo">
            <i class="ri-weibo-fill"></i>
          </a>
          <a class="weixin share-sns wxFab" href="javascript:;" data-type="weixin">
            <i class="ri-wechat-fill"></i>
          </a>
          <a class="qq share-sns" href="javascript:;" data-type="qq">
            <i class="ri-qq-fill"></i>
          </a>
          <a class="douban share-sns" href="javascript:;" data-type="douban">
            <i class="ri-douban-line"></i>
          </a>
          <!-- <a class="qzone share-sns" href="javascript:;" data-type="qzone">
            <i class="icon icon-qzone"></i>
          </a> -->
          
          <a class="facebook share-sns" href="javascript:;" data-type="facebook">
            <i class="ri-facebook-circle-fill"></i>
          </a>
          <a class="twitter share-sns" href="javascript:;" data-type="twitter">
            <i class="ri-twitter-fill"></i>
          </a>
          <a class="google share-sns" href="javascript:;" data-type="google">
            <i class="ri-google-fill"></i>
          </a>
        </div>
      </div>
</div>

<div class="wx-share-modal">
    <a class="modal-close" href="javascript:;"><i class="ri-close-circle-line"></i></a>
    <p>扫一扫，分享到微信</p>
    <div class="wx-qrcode">
      <img src="//api.qrserver.com/v1/create-qr-code/?size=150x150&data=https://anzhen-tech.gitee.io/2021/11/19/Kafka/" alt="微信分享二维码">
    </div>
</div>

<div id="share-mask"></div>  
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Kafka/" rel="tag">Kafka</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/interview/" rel="tag">interview</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/" rel="tag">大数据</a></li></ul>

    </footer>
  </div>

   
  <nav class="article-nav">
    
      <a href="/2021/12/04/Scala%E5%85%A5%E9%97%A8/" class="article-nav-link">
        <strong class="article-nav-caption">上一篇</strong>
        <div class="article-nav-title">
          
            Scala入门
          
        </div>
      </a>
    
    
      <a href="/2021/11/07/Java%E5%B9%B6%E5%8F%91%E4%B8%8E%E5%A4%9A%E7%BA%BF%E7%A8%8B/" class="article-nav-link">
        <strong class="article-nav-caption">下一篇</strong>
        <div class="article-nav-title">Java并发与多线程</div>
      </a>
    
  </nav>

   
<!-- valine评论 -->
<div id="vcomments-box">
  <div id="vcomments"></div>
</div>
<script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/valine@1.4.14/dist/Valine.min.js"></script>
<script>
  new Valine({
    el: "#vcomments",
    app_id: "XCKHv09pYxF5EmF2ezNgFfLS-gzGzoHsz",
    app_key: "gyCHBp787fNNfXDiHGIcj7Am",
    path: window.location.pathname,
    avatar: "monsterid",
    placeholder: "给我的文章加点评论吧~",
    recordIP: true,
  });
  const infoEle = document.querySelector("#vcomments .info");
  if (infoEle && infoEle.childNodes && infoEle.childNodes.length > 0) {
    infoEle.childNodes.forEach(function (item) {
      item.parentNode.removeChild(item);
    });
  }
</script>
<style>
  #vcomments-box {
    padding: 5px 30px;
  }

  @media screen and (max-width: 800px) {
    #vcomments-box {
      padding: 5px 0px;
    }
  }

  #vcomments-box #vcomments {
    background-color: #fff;
  }

  .v .vlist .vcard .vh {
    padding-right: 20px;
  }

  .v .vlist .vcard {
    padding-left: 10px;
  }
</style>

 
   
     
</article>

</section>
      <footer class="footer">
  <div class="outer">
    <ul>
      <li>
        Copyrights &copy;
        2019-2022
        <i class="ri-heart-fill heart_icon"></i> Anzhen
      </li>
    </ul>
    <ul>
      <li>
        
      </li>
    </ul>
    <ul>
      <li>
        
        
        <span>
  <span><i class="ri-user-3-fill"></i>访问人数:<span id="busuanzi_value_site_uv"></span></span>
  <span class="division">|</span>
  <span><i class="ri-eye-fill"></i>浏览次数:<span id="busuanzi_value_page_pv"></span></span>
</span>
        
      </li>
    </ul>
    <ul>
      
    </ul>
    <ul>
      
    </ul>
    <ul>
      <li>
        <!-- cnzz统计 -->
        
        <script type="text/javascript" src=''></script>
        
      </li>
    </ul>
  </div>
</footer>    
    </main>
    <div class="float_btns">
      <div class="totop" id="totop">
  <i class="ri-arrow-up-line"></i>
</div>

<div class="todark" id="todark">
  <i class="ri-moon-line"></i>
</div>

    </div>
    <aside class="sidebar on">
      <button class="navbar-toggle"></button>
<nav class="navbar">
  
  <div class="logo">
    <a href="/"><img src="/images/ayer-side.svg" alt="anzhen.tech"></a>
  </div>
  
  <ul class="nav nav-main">
    
    <li class="nav-item">
      <a class="nav-item-link" href="/">主页</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/archives">归档</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/categories">分类</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags">标签</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags/HDFS">HDFS</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags/Yarn">Yarn</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags/MR">MR</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags/Hive">Hive</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags/%E6%95%B0%E6%8D%AE%E9%87%87%E9%9B%86">数据采集</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags/HBase">HBase</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags/Kafka">Kafka</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags/Spark">Spark</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags/Flink">Flink</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags/MySQL">MySQL</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags/Java">Java</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags/interview">面试宝典</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/2019/11/07/about-me">关于我</a>
    </li>
    
  </ul>
</nav>
<nav class="navbar navbar-bottom">
  <ul class="nav">
    <li class="nav-item">
      
      <a class="nav-item-link nav-item-search"  title="搜索">
        <i class="ri-search-line"></i>
      </a>
      
      
      <a class="nav-item-link" target="_blank" href="/atom.xml" title="RSS Feed">
        <i class="ri-rss-line"></i>
      </a>
      
    </li>
  </ul>
</nav>
<div class="search-form-wrap">
  <div class="local-search local-search-plugin">
  <input type="search" id="local-search-input" class="local-search-input" placeholder="Search...">
  <div id="local-search-result" class="local-search-result"></div>
</div>
</div>
    </aside>
    <div id="mask"></div>

<!-- #reward -->
<div id="reward">
  <span class="close"><i class="ri-close-line"></i></span>
  <p class="reward-p"><i class="ri-cup-line"></i>请我喝杯咖啡吧~</p>
  <div class="reward-box">
    
    <div class="reward-item">
      <img class="reward-img" src="/images/alipay.png">
      <span class="reward-type">支付宝</span>
    </div>
    
    
    <div class="reward-item">
      <img class="reward-img" src="/images/wechat.png">
      <span class="reward-type">微信</span>
    </div>
    
  </div>
</div>
    
<script src="/js/jquery-3.6.0.min.js"></script>
 
<script src="/js/lazyload.min.js"></script>

<!-- Tocbot -->
 
<script src="/js/tocbot.min.js"></script>

<script>
  tocbot.init({
    tocSelector: ".tocbot",
    contentSelector: ".article-entry",
    headingSelector: "h1, h2, h3, h4, h5, h6",
    hasInnerContainers: true,
    scrollSmooth: true,
    scrollContainer: "main",
    positionFixedSelector: ".tocbot",
    positionFixedClass: "is-position-fixed",
    fixedSidebarOffset: "auto",
  });
</script>

<script src="https://cdn.jsdelivr.net/npm/jquery-modal@0.9.2/jquery.modal.min.js"></script>
<link
  rel="stylesheet"
  href="https://cdn.jsdelivr.net/npm/jquery-modal@0.9.2/jquery.modal.min.css"
/>
<script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js"></script>

<script src="/dist/main.js"></script>

<!-- ImageViewer -->
 <!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                <!--  Controls are self-explanatory. Order can be changed. -->

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" style="display:none" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css">
<script src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js"></script>

<script>
    function viewer_init() {
        let pswpElement = document.querySelectorAll('.pswp')[0];
        let $imgArr = document.querySelectorAll(('.article-entry img:not(.reward-img)'))

        $imgArr.forEach(($em, i) => {
            $em.onclick = () => {
                // slider展开状态
                // todo: 这样不好，后面改成状态
                if (document.querySelector('.left-col.show')) return
                let items = []
                $imgArr.forEach(($em2, i2) => {
                    let img = $em2.getAttribute('data-idx', i2)
                    let src = $em2.getAttribute('data-target') || $em2.getAttribute('src')
                    let title = $em2.getAttribute('alt')
                    // 获得原图尺寸
                    const image = new Image()
                    image.src = src
                    items.push({
                        src: src,
                        w: image.width || $em2.width,
                        h: image.height || $em2.height,
                        title: title
                    })
                })
                var gallery = new PhotoSwipe(pswpElement, PhotoSwipeUI_Default, items, {
                    index: parseInt(i)
                });
                gallery.init()
            }
        })
    }
    viewer_init()
</script> 
<!-- MathJax -->
 <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
      tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
  });

  MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
      for(i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
      }
  });
</script>

<script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.6/unpacked/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script>
  var ayerConfig = {
    mathjax: true,
  };
</script>

<!-- Katex -->

<!-- busuanzi  -->
 
<script src="/js/busuanzi-2.3.pure.min.js"></script>
 
<!-- ClickLove -->

<!-- ClickBoom1 -->

<!-- ClickBoom2 -->

<!-- CodeCopy -->
 
<link rel="stylesheet" href="/css/clipboard.css">
 <script src="https://cdn.jsdelivr.net/npm/clipboard@2/dist/clipboard.min.js"></script>
<script>
  function wait(callback, seconds) {
    var timelag = null;
    timelag = window.setTimeout(callback, seconds);
  }
  !function (e, t, a) {
    var initCopyCode = function(){
      var copyHtml = '';
      copyHtml += '<button class="btn-copy" data-clipboard-snippet="">';
      copyHtml += '<i class="ri-file-copy-2-line"></i><span>COPY</span>';
      copyHtml += '</button>';
      $(".highlight .code pre").before(copyHtml);
      $(".article pre code").before(copyHtml);
      var clipboard = new ClipboardJS('.btn-copy', {
        target: function(trigger) {
          return trigger.nextElementSibling;
        }
      });
      clipboard.on('success', function(e) {
        let $btn = $(e.trigger);
        $btn.addClass('copied');
        let $icon = $($btn.find('i'));
        $icon.removeClass('ri-file-copy-2-line');
        $icon.addClass('ri-checkbox-circle-line');
        let $span = $($btn.find('span'));
        $span[0].innerText = 'COPIED';
        
        wait(function () { // 等待两秒钟后恢复
          $icon.removeClass('ri-checkbox-circle-line');
          $icon.addClass('ri-file-copy-2-line');
          $span[0].innerText = 'COPY';
        }, 2000);
      });
      clipboard.on('error', function(e) {
        e.clearSelection();
        let $btn = $(e.trigger);
        $btn.addClass('copy-failed');
        let $icon = $($btn.find('i'));
        $icon.removeClass('ri-file-copy-2-line');
        $icon.addClass('ri-time-line');
        let $span = $($btn.find('span'));
        $span[0].innerText = 'COPY FAILED';
        
        wait(function () { // 等待两秒钟后恢复
          $icon.removeClass('ri-time-line');
          $icon.addClass('ri-file-copy-2-line');
          $span[0].innerText = 'COPY';
        }, 2000);
      });
    }
    initCopyCode();
  }(window, document);
</script>
 
<!-- CanvasBackground -->

<script>
  if (window.mermaid) {
    mermaid.initialize({ theme: "forest" });
  }
</script>


    
    <div id="music">
    
    
    
    <iframe frameborder="no" border="1" marginwidth="0" marginheight="0" width="200" height="52"
        src="//music.163.com/outchain/player?type=2&id=318916815&auto=1&height=32"></iframe>
</div>

<style>
    #music {
        position: fixed;
        right: 15px;
        bottom: 0;
        z-index: 998;
    }
</style>
    
    

  </div>
<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>