<!DOCTYPE html>


<html lang="zh-CN">
  

    <head>
      <meta charset="utf-8" />
        
      <meta
        name="viewport"
        content="width=device-width, initial-scale=1, maximum-scale=1"
      />
      <title>SparkCore |  anzhen.tech</title>
  <meta name="generator" content="hexo-theme-ayer">
      
      <link rel="shortcut icon" href="/favicon.ico" />
       
<link rel="stylesheet" href="/dist/main.css">

      <link
        rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/Shen-Yu/cdn/css/remixicon.min.css"
      />
      
<link rel="stylesheet" href="/css/custom.css">
 
      <script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script>
       
 
<script>
var _hmt = _hmt || [];
(function() {
	var hm = document.createElement("script");
	hm.src = "https://hm.baidu.com/hm.js?20878462c8c8a6915b11b2d93a956d26";
	var s = document.getElementsByTagName("script")[0]; 
	s.parentNode.insertBefore(hm, s);
})();
</script>


      <link
        rel="stylesheet"
        href="https://cdn.jsdelivr.net/npm/@sweetalert2/theme-bulma@5.0.1/bulma.min.css"
      />
      <script src="https://cdn.jsdelivr.net/npm/sweetalert2@11.0.19/dist/sweetalert2.min.js"></script>

      <!-- mermaid -->
      
      <style>
        .swal2-styled.swal2-confirm {
          font-size: 1.6rem;
        }
      </style>
      <meta name="baidu-site-verification" content="code-mBRwXRLQqk" />
      <meta name="google-site-verification" content="bqavzWFaou2XjWPiLJI2ZoQwSGDVv_wZFPMkWKjEAz0" />
      
    <link rel="alternate" href="/atom.xml" title="anzhen.tech" type="application/atom+xml">
</head>
  </html>
</html>


<body>
  <div id="app">
    
      
    <main class="content on">
      <section class="outer">
  <article
  id="post-SparkCore"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h1 class="article-title sea-center" style="border-left:0" itemprop="name">
  SparkCore
</h1>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2021/12/11/SparkCore/" class="article-date">
  <time datetime="2021-12-11T12:44:25.000Z" itemprop="datePublished">2021-12-11</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/categories/Spark/">Spark</a>
  </div>
  
<div class="word_count">
    <span class="post-time">
        <span class="post-meta-item-icon">
            <i class="ri-quill-pen-line"></i>
            <span class="post-meta-item-text"> 字数统计:</span>
            <span class="post-count">27.6k</span>
        </span>
    </span>

    <span class="post-time">
        &nbsp; | &nbsp;
        <span class="post-meta-item-icon">
            <i class="ri-book-open-line"></i>
            <span class="post-meta-item-text"> 阅读时长≈</span>
            <span class="post-count">126 分钟</span>
        </span>
    </span>
</div>
 
    </div>
      
    <div class="tocbot"></div>




  
    <div class="article-entry" itemprop="articleBody">
       
  <h1 id="Spark-Core"><a href="#Spark-Core" class="headerlink" title="Spark Core"></a>Spark Core</h1><h1 id="第1章-Spark概述"><a href="#第1章-Spark概述" class="headerlink" title="第1章 Spark概述"></a>第1章 Spark概述</h1><h2 id="1-1-Spark是什么"><a href="#1-1-Spark是什么" class="headerlink" title="1.1 Spark是什么"></a>1.1 Spark是什么</h2><p><img src="https://s2.loli.net/2021/12/11/PRpgQ1WYFbI9StM.jpg"><br><img src="https://s2.loli.net/2021/12/11/WqDcxak3HC26V8Y.jpg"><br>Spark是一种基于内存的快速、通用、可扩展的大数据分析计算引擎。</p>
<h2 id="1-2-Spark-and-Hadoop"><a href="#1-2-Spark-and-Hadoop" class="headerlink" title="1.2 Spark and Hadoop"></a>1.2 Spark and Hadoop</h2><p>在之前的学习中，Hadoop的MapReduce是大家广为熟知的计算框架，那为什么咱们还要学习新的计算框架Spark呢，这里就不得不提到Spark和Hadoop的关系。<br>首先从时间节点上来看:</p>
<ul>
<li>Hadoop<ul>
<li>2006年1月，Doug Cutting加入Yahoo，领导Hadoop的开发</li>
<li>2008年1月，Hadoop成为Apache顶级项目</li>
<li>2011年1.0正式发布</li>
<li>2012年3月稳定版发布</li>
<li>2013年10月发布2.X (Yarn)版本</li>
</ul>
</li>
<li>Spark<ul>
<li>2009年，Spark诞生于伯克利大学的AMPLab实验室</li>
<li>2010年，伯克利大学正式开源了Spark项目</li>
<li>2013年6月，Spark成为了Apache基金会下的项目</li>
<li>2014年2月，Spark以飞快的速度成为了Apache的顶级项目</li>
<li>2015年至今，Spark变得愈发火爆，大量的国内公司开始重点部署或者使用Spark<br>然后我们再从功能上来看:</li>
</ul>
</li>
<li>Hadoop<ul>
<li>Hadoop是由java语言编写的，在分布式服务器集群上存储海量数据并运行分布式分析应用的开源框架</li>
<li>作为Hadoop分布式文件系统，HDFS处于Hadoop生态圈的最下层，存储着所有的数据，支持着Hadoop的所有服务。它的理论基础源于Google的TheGoogleFileSystem这篇论文，它是GFS的开源实现。</li>
<li>MapReduce是一种编程模型，Hadoop根据Google的MapReduce论文将其实现，作为Hadoop的分布式计算模型，是Hadoop的核心。基于这个框架，分布式并行程序的编写变得异常简单。综合了HDFS的分布式存储和MapReduce的分布式计算，Hadoop在处理海量数据时，性能横向扩展变得非常容易。</li>
<li>HBase是对Google的Bigtable的开源实现，但又和Bigtable存在许多不同之处。HBase是一个基于HDFS的分布式数据库，擅长实时地随机读/写超大规模数据集。它也是Hadoop非常重要的组件。</li>
</ul>
</li>
<li>Spark<ul>
<li>Spark是一种由Scala语言开发的快速、通用、可扩展的大数据分析引擎</li>
<li>Spark Core中提供了Spark最基础与最核心的功能</li>
<li>Spark SQL是Spark用来操作结构化数据的组件。通过Spark SQL，用户可以使用SQL或者Apache Hive版本的SQL方言（HQL）来查询数据。</li>
<li>Spark Streaming是Spark平台上针对实时数据进行流式计算的组件，提供了丰富的处理数据流的API。</li>
<li>由上面的信息可以获知，Spark出现的时间相对较晚，并且主要功能主要是用于数据计算，所以其实Spark一直被认为是Hadoop 框架的升级版。</li>
</ul>
</li>
</ul>
<h2 id="1-3-Spark-or-Hadoop"><a href="#1-3-Spark-or-Hadoop" class="headerlink" title="1.3 Spark or Hadoop"></a>1.3 Spark or Hadoop</h2><p>Hadoop的MR框架和Spark框架都是数据处理框架，那么我们在使用时如何选择呢？</p>
<ul>
<li>Hadoop MapReduce由于其设计初衷并不是为了满足循环迭代式数据流处理，因此在多并行运行的数据可复用场景（如：机器学习、图挖掘算法、交互式数据挖掘算法）中存在诸多计算效率等问题。所以Spark应运而生，Spark就是在传统的MapReduce 计算框架的基础上，利用其计算过程的优化，从而大大加快了数据分析、挖掘的运行和读写速度，并将计算单元缩小到更适合并行计算和重复使用的RDD计算模型。</li>
<li>机器学习中ALS、凸优化梯度下降等。这些都需要基于数据集或者数据集的衍生数据反复查询反复操作。MR这种模式不太合适，即使多MR串行处理，性能和时间也是一个问题。数据的共享依赖于磁盘。另外一种是交互式数据挖掘，MR显然不擅长。而Spark所基于的scala语言恰恰擅长函数的处理。</li>
<li>Spark是一个分布式数据快速分析项目。它的核心技术是弹性分布式数据集（Resilient Distributed Datasets），提供了比MapReduce丰富的模型，可以快速在内存中对数据集进行多次迭代，来支持复杂的数据挖掘算法和图形计算算法。</li>
<li>Spark和Hadoop的根本差异是多个作业之间的数据通信问题 : Spark多个作业之间数据通信是基于内存，而Hadoop是基于磁盘。</li>
<li>Spark Task的启动时间快。Spark采用fork线程的方式，而Hadoop采用创建新的进程的方式。</li>
<li>Spark只有在shuffle的时候将数据写入磁盘，而Hadoop中多个MR作业之间的数据交互都要依赖于磁盘交互</li>
<li>Spark的缓存机制比HDFS的缓存机制高效。</li>
</ul>
<p>经过上面的比较，我们可以看出在绝大多数的数据计算场景中，Spark确实会比MapReduce更有优势。但是Spark是基于内存的，所以在实际的生产环境中，由于内存的限制，可能会由于内存资源不够导致Job执行失败，此时，MapReduce其实是一个更好的选择，所以Spark并不能完全替代MR。</p>
<h2 id="1-4-Spark-核心模块"><a href="#1-4-Spark-核心模块" class="headerlink" title="1.4 Spark 核心模块"></a>1.4 Spark 核心模块</h2><p><img src="https://s2.loli.net/2021/12/11/AdkW8MaIlSPyJxh.jpg"></p>
<ul>
<li>Spark Core<br>Spark Core中提供了Spark最基础与最核心的功能，Spark其他的功能如：Spark SQL，Spark Streaming，GraphX, MLlib都是在Spark Core的基础上进行扩展的</li>
<li>Spark SQL<br>Spark SQL是Spark用来操作结构化数据的组件。通过Spark SQL，用户可以使用SQL或者Apache Hive版本的SQL方言（HQL）来查询数据。</li>
<li>Spark Streaming<br>Spark Streaming是Spark平台上针对实时数据进行流式计算的组件，提供了丰富的处理数据流的API。</li>
<li>Spark MLlib<br>MLlib是Spark提供的一个机器学习算法库。MLlib不仅提供了模型评估、数据导入等额外的功能，还提供了一些更底层的机器学习原语。</li>
<li>Spark GraphX<br>GraphX是Spark面向图计算提供的框架与算法库。</li>
</ul>
<h1 id="第3章-Spark运行环境"><a href="#第3章-Spark运行环境" class="headerlink" title="第3章 Spark运行环境"></a>第3章 Spark运行环境</h1><p>Spark作为一个数据处理框架和计算引擎，被设计在所有常见的集群环境中运行, 在国内工作中主流的环境为Yarn，不过逐渐容器式环境也慢慢流行起来。接下来，我们就分别看看不同环境下Spark的运行<br><img src="https://s2.loli.net/2021/12/11/bnH9TZVF6WaSkjt.jpg"></p>
<h2 id="3-1-Local模式"><a href="#3-1-Local模式" class="headerlink" title="3.1  Local模式"></a>3.1  Local模式</h2><p>想啥呢，你之前一直在使用的模式可不是Local模式哟。所谓的Local模式，就是不需要其他任何节点资源就可以在本地执行Spark代码的环境，一般用于教学，调试，演示等，之前在IDEA中运行代码的环境我们称之为开发环境，不太一样。</p>
<h3 id="3-1-1-解压缩文件"><a href="#3-1-1-解压缩文件" class="headerlink" title="3.1.1 解压缩文件"></a>3.1.1 解压缩文件</h3><p>将spark-3.0.0-bin-hadoop3.2.tgz文件上传到Linux并解压缩，放置在指定位置，路径中不要包含中文或空格，课件后续如果涉及到解压缩操作，不再强调。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf spark-3.0.0-bin-hadoop3.2.tgz -C /opt/module</span><br><span class="line"><span class="built_in">cd</span> /opt/module </span><br><span class="line">mv spark-3.0.0-bin-hadoop3.2 spark-local</span><br></pre></td></tr></table></figure>

<h3 id="3-1-2-启动Local环境"><a href="#3-1-2-启动Local环境" class="headerlink" title="3.1.2 启动Local环境"></a>3.1.2 启动Local环境</h3><ol>
<li>进入解压缩后的路径，执行如下指令 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@bd-server-001 spark-local]$ bin/spark-shell</span><br><span class="line">21/12/01 08:26:41 WARN NativeCodeLoader: Unable to load native-hadoop library <span class="keyword">for</span> your platform... using builtin-java classes <span class="built_in">where</span> applicable</span><br><span class="line">Using Spark<span class="string">&#x27;s default log4j profile: org/apache/spark/log4j-defaults.properties</span></span><br><span class="line"><span class="string">Setting default log level to &quot;WARN&quot;.</span></span><br><span class="line"><span class="string">To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).</span></span><br><span class="line"><span class="string">Spark context Web UI available at http://bd-server-001:4040</span></span><br><span class="line"><span class="string">Spark context available as &#x27;</span>sc<span class="string">&#x27; (master = local[*], app id = local-1638318406888).</span></span><br><span class="line"><span class="string">Spark session available as &#x27;</span>spark<span class="string">&#x27;.</span></span><br><span class="line"><span class="string">Welcome to</span></span><br><span class="line"><span class="string">      ____              __</span></span><br><span class="line"><span class="string">     / __/__  ___ _____/ /__</span></span><br><span class="line"><span class="string">    _\ \/ _ \/ _ `/ __/  &#x27;</span>_/</span><br><span class="line">   /___/ .__/\_,_/_/ /_/\_\   version 3.0.0</span><br><span class="line">      /_/</span><br><span class="line"></span><br><span class="line">Using Scala version 2.12.10 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_212)</span><br><span class="line">Type <span class="keyword">in</span> expressions to have them evaluated.</span><br><span class="line">Type :<span class="built_in">help</span> <span class="keyword">for</span> more information.</span><br><span class="line"></span><br><span class="line">scala&gt;</span><br></pre></td></tr></table></figure></li>
<li>启动成功后，可以输入网址进行Web UI监控页面访问<br> <img src="https://s2.loli.net/2021/12/11/rqa1YJvW2wfQKBP.jpg"></li>
</ol>
<h3 id="3-1-3-命令行工具"><a href="#3-1-3-命令行工具" class="headerlink" title="3.1.3 命令行工具"></a>3.1.3 命令行工具</h3><p>在解压缩文件夹下的data目录中，添加word.txt文件。在命令行工具中执行如下代码指令（和IDEA中代码简化版一致）</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">sc</span><br><span class="line">    .textFile(<span class="string">&quot;data/word.txt&quot;</span>)</span><br><span class="line">    .flatMap(_.split(<span class="string">&quot; &quot;</span>))</span><br><span class="line">    .map((_,<span class="number">1</span>))</span><br><span class="line">    .reduceByKey(_+_)</span><br><span class="line">    .collect</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; sc.textFile(<span class="string">&quot;data/word.txt&quot;</span>).flatMap(_.split(<span class="string">&quot; &quot;</span>)).map((_,1)).reduceByKey(_+_).collect</span><br><span class="line">res0: Array[(String, Int)] = Array((scala,1), (flink,1), (hello,3), (spark,1))</span><br><span class="line"></span><br><span class="line">scala&gt;</span><br></pre></td></tr></table></figure>

<h3 id="3-1-4-退出本地模式"><a href="#3-1-4-退出本地模式" class="headerlink" title="3.1.4 退出本地模式"></a>3.1.4 退出本地模式</h3><p>按键Ctrl+C或输入Scala指令<code>:quit</code></p>
<h3 id="3-1-5-提交应用"><a href="#3-1-5-提交应用" class="headerlink" title="3.1.5 提交应用"></a>3.1.5 提交应用</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">bin/spark-submit \</span><br><span class="line">--class org.apache.spark.examples.SparkPi \</span><br><span class="line">--master <span class="built_in">local</span>[2] \</span><br><span class="line">./examples/jars/spark-examples_2.12-3.0.0.jar \</span><br><span class="line">10</span><br></pre></td></tr></table></figure>
<ol>
<li>–class表示要执行程序的主类，此处可以更换为咱们自己写的应用程序</li>
<li>–master local[2] 部署模式，默认为本地模式，数字表示分配的虚拟CPU核数量</li>
<li>spark-examples_2.12-3.0.0.jar 运行的应用类所在的jar包，实际使用时，可以设定为咱们自己打的jar包</li>
<li>数字10表示程序的入口参数，用于设定当前应用的任务数量</li>
</ol>
<p><img src="https://s2.loli.net/2021/12/11/UleCBVSxPaX8HkA.jpg"></p>
<h2 id="3-2-Standalone模式"><a href="#3-2-Standalone模式" class="headerlink" title="3.2  Standalone模式"></a>3.2  Standalone模式</h2><p>local本地模式毕竟只是用来进行练习演示的，真实工作中还是要将应用提交到对应的集群中去执行，这里我们来看看只使用Spark自身节点运行的集群模式，也就是我们所谓的独立部署（Standalone）模式。Spark的Standalone模式体现了经典的master-slave模式。<br>Spark有自己的资源调度框架：<br><img src="https://s2.loli.net/2021/12/11/9FdAabfSstQUqv4.jpg"></p>
<p>集群规划:<br>|Linux1    |Linux2    |Linux3 |<br>|——–|———|——|<br>|Spark    |Worker Master|Worker    |Worker    |</p>
<h3 id="3-2-1-解压缩文件"><a href="#3-2-1-解压缩文件" class="headerlink" title="3.2.1 解压缩文件"></a>3.2.1 解压缩文件</h3><p>将spark-3.0.0-bin-hadoop3.2.tgz文件上传到Linux并解压缩在指定位置</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf spark-3.0.0-bin-hadoop3.2.tgz -C /opt/module</span><br><span class="line"><span class="built_in">cd</span> /opt/module </span><br><span class="line">mv spark-3.0.0-bin-hadoop3.2 spark-standalone</span><br></pre></td></tr></table></figure>

<h3 id="3-2-2-修改配置文件"><a href="#3-2-2-修改配置文件" class="headerlink" title="3.2.2 修改配置文件"></a>3.2.2 修改配置文件</h3><ol>
<li>进入解压缩后路径的conf目录，修改slaves.template文件名为slaves <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mv slaves.template slaves</span><br></pre></td></tr></table></figure></li>
<li>修改slaves文件，添加worker节点 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">bd-server-001</span><br><span class="line">bd-server-002</span><br><span class="line">bd-server-003</span><br><span class="line">bd-server-004</span><br><span class="line">bd-server-005</span><br></pre></td></tr></table></figure></li>
<li>修改spark-env.sh.template文件名为spark-env.sh <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mv spark-env.sh.template spark-env.sh</span><br></pre></td></tr></table></figure></li>
<li>修改spark-env.sh文件，添加JAVA_HOME环境变量和集群对应的master节点 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> JAVA_HOME=/opt/module/jdk1.8.0_212</span><br><span class="line">SPARK_MASTER_HOST=bd-server-001</span><br><span class="line">SPARK_MASTER_PORT=7077</span><br></pre></td></tr></table></figure></li>
<li>分发spark-standalone目录 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">xsync spark-standalone</span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="3-2-3-启动集群"><a href="#3-2-3-启动集群" class="headerlink" title="3.2.3 启动集群"></a>3.2.3 启动集群</h3><ol>
<li>执行脚本命令：<code>sbin/start-all.sh</code> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@bd-server-001 spark-standalone]$ sbin/start-all.sh</span><br><span class="line">starting org.apache.spark.deploy.master.Master, logging to /opt/module/spark-standalone/logs/spark-atguigu-org.apache.spark.deploy.master.Master-1-bd-server-001.out</span><br><span class="line">bd-server-002: starting org.apache.spark.deploy.worker.Worker, logging to /opt/module/spark-standalone/logs/spark-atguigu-org.apache.spark.deploy.worker.Worker-1-bd-server-002.out</span><br><span class="line">bd-server-001: starting org.apache.spark.deploy.worker.Worker, logging to /opt/module/spark-standalone/logs/spark-atguigu-org.apache.spark.deploy.worker.Worker-1-bd-server-001.out</span><br><span class="line">bd-server-003: starting org.apache.spark.deploy.worker.Worker, logging to /opt/module/spark-standalone/logs/spark-atguigu-org.apache.spark.deploy.worker.Worker-1-bd-server-003.out</span><br><span class="line">bd-server-004: starting org.apache.spark.deploy.worker.Worker, logging to /opt/module/spark-standalone/logs/spark-atguigu-org.apache.spark.deploy.worker.Worker-1-bd-server-004.out</span><br><span class="line">bd-server-005: starting org.apache.spark.deploy.worker.Worker, logging to /opt/module/spark-standalone/logs/spark-atguigu-org.apache.spark.deploy.worker.Worker-1-bd-server-005.out</span><br><span class="line">[atguigu@bd-server-001 spark-standalone]$</span><br></pre></td></tr></table></figure>
</li>
<li>查看三台服务器运行进程 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@bd-server-001 spark-standalone]$ jpsall</span><br><span class="line">&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;  bd-server-001  &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;</span><br><span class="line"> <span class="string">37979 Master</span></span><br><span class="line"><span class="string"> 38070 Worker</span></span><br><span class="line"><span class="string">&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;  bd-server-002  &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;</span></span><br><span class="line"><span class="string"> 27011 Worker</span></span><br><span class="line"><span class="string">&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;  bd-server-003  &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;</span></span><br><span class="line"><span class="string"> 19210 Worker</span></span><br><span class="line"><span class="string">&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;  bd-server-004  &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;</span></span><br><span class="line"><span class="string"> 11267 Worker</span></span><br><span class="line"><span class="string">&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;  bd-server-005  &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;</span></span><br><span class="line"><span class="string"> 10646 Worker</span></span><br><span class="line"><span class="string">[atguigu@bd-server-001 spark-standalone]$</span></span><br></pre></td></tr></table></figure></li>
<li>查看Master资源监控Web UI界面: <a target="_blank" rel="noopener" href="http://bd-server-001:8080/">http://bd-server-001:8080/</a><br> <img src="https://s2.loli.net/2021/12/11/GM6EUpD5tJ7FckB.jpg"></li>
</ol>
<h3 id="3-2-4-提交应用"><a href="#3-2-4-提交应用" class="headerlink" title="3.2.4 提交应用"></a>3.2.4 提交应用</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">bin/spark-submit \</span><br><span class="line">--class org.apache.spark.examples.SparkPi \</span><br><span class="line">--master spark://bd-server-001:7077 \</span><br><span class="line">./examples/jars/spark-examples_2.12-3.0.0.jar \</span><br><span class="line">10</span><br></pre></td></tr></table></figure>
<ol>
<li>–class表示要执行程序的主类</li>
<li>–master spark://linux1:7077 独立部署模式，连接到Spark集群</li>
<li>spark-examples_2.12-3.0.0.jar 运行类所在的jar包</li>
<li>数字10表示程序的入口参数，用于设定当前应用的任务数量<br><img src="https://s2.loli.net/2021/12/11/vBFDSG5PC6TrlcR.jpg"></li>
<li>执行任务时，会产生多个Java进程 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@bd-server-001 ~]$ jpsall</span><br><span class="line">&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;  bd-server-001  &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;</span><br><span class="line"> <span class="string">41170 Master</span></span><br><span class="line"><span class="string"> 41262 Worker</span></span><br><span class="line"><span class="string"> 41561 SparkSubmit</span></span><br><span class="line"><span class="string"> 41674 CoarseGrainedExecutorBackend</span></span><br><span class="line"><span class="string">&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;  bd-server-002  &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;</span></span><br><span class="line"><span class="string"> 29924 Worker</span></span><br><span class="line"><span class="string"> 30101 CoarseGrainedExecutorBackend</span></span><br><span class="line"><span class="string">&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;  bd-server-003  &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;</span></span><br><span class="line"><span class="string"> 21171 Worker</span></span><br><span class="line"><span class="string"> 21348 CoarseGrainedExecutorBackend</span></span><br><span class="line"><span class="string">&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;  bd-server-004  &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;</span></span><br><span class="line"><span class="string"> 12192 Worker</span></span><br><span class="line"><span class="string"> 12370 CoarseGrainedExecutorBackend</span></span><br><span class="line"><span class="string">&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;  bd-server-005  &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;</span></span><br><span class="line"><span class="string"> 11571 Worker</span></span><br><span class="line"><span class="string"> 11748 CoarseGrainedExecutorBackend</span></span><br></pre></td></tr></table></figure>
<ul>
<li>SparkSubmit：任务提交进程</li>
<li>CoarseGrainedExecutorBackend：任务执行进程</li>
</ul>
</li>
<li>执行任务时，默认采用服务器集群节点的总核数，每个节点内存1024M。<br> <img src="https://s2.loli.net/2021/12/11/3RpYmK5CPNJ7vda.jpg"></li>
</ol>
<h3 id="3-2-5-提交参数说明"><a href="#3-2-5-提交参数说明" class="headerlink" title="3.2.5 提交参数说明"></a>3.2.5 提交参数说明</h3><p>在提交应用中，一般会同时一些提交参数</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">bin/spark-submit \</span><br><span class="line">--class &lt;main-class&gt;</span><br><span class="line">--master &lt;master-url&gt; \</span><br><span class="line">... <span class="comment"># other options</span></span><br><span class="line">&lt;application-jar&gt; \</span><br><span class="line">[application-arguments]</span><br></pre></td></tr></table></figure>
<table>
<thead>
<tr>
<th>参数</th>
<th>解释</th>
<th>可选值举例</th>
</tr>
</thead>
<tbody><tr>
<td>–class</td>
<td>Spark程序中包含主函数的类</td>
<td></td>
</tr>
<tr>
<td>–master</td>
<td>Spark程序运行的模式(环境)</td>
<td>模式：local[*]、spark://linux1:7077、Yarn</td>
</tr>
<tr>
<td>–executor-memory 1G</td>
<td>指定每个executor可用内存为1G</td>
<td>符合集群内存配置即可，具体情况具体分析。</td>
</tr>
<tr>
<td>–total-executor-cores 2</td>
<td>指定所有executor使用的cpu核数为2个</td>
<td></td>
</tr>
<tr>
<td>–executor-cores</td>
<td>指定每个executor使用的cpu核数</td>
<td></td>
</tr>
<tr>
<td>application-jar</td>
<td>打包好的应用jar，包含依赖。这个URL在集群中全局可见。 比如hdfs:// 共享存储系统，如果是file:// path，那么所有的节点的path都包含同样的jar</td>
<td></td>
</tr>
<tr>
<td>application-arguments</td>
<td>传给main()方法的参数</td>
<td></td>
</tr>
</tbody></table>
<h3 id="3-2-6-配置历史服务"><a href="#3-2-6-配置历史服务" class="headerlink" title="3.2.6 配置历史服务"></a>3.2.6 配置历史服务</h3><p>由于spark-shell停止掉后，集群监控linux1:4040页面就看不到历史任务的运行情况，所以开发时都配置历史服务器记录任务运行情况。</p>
<ol>
<li><p>修改spark-defaults.conf.template文件名为spark-defaults.conf</p>
 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mv spark-defaults.conf.template spark-defaults.conf</span><br></pre></td></tr></table></figure></li>
<li><p>修改spark-default.conf文件，配置日志存储路径</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">spark.eventLog.enabled          true</span><br><span class="line">spark.eventLog.dir              hdfs://bd-server-002:9092/directory</span><br></pre></td></tr></table></figure>
<ul>
<li>注意：需要启动hadoop集群，HDFS上的directory目录需要提前存在。</li>
</ul>
</li>
<li><p>修改spark-env.sh文件, 添加日志配置</p>
 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> SPARK_HISTORY_OPTS=<span class="string">&quot;</span></span><br><span class="line"><span class="string">-Dspark.history.ui.port=18080 </span></span><br><span class="line"><span class="string">-Dspark.history.fs.logDirectory=hdfs://bd-server-002:9092/directory </span></span><br><span class="line"><span class="string">-Dspark.history.retainedApplications=30&quot;</span></span><br></pre></td></tr></table></figure>
<ul>
<li>参数1含义：WEB UI访问的端口号为18080</li>
<li>参数2含义：指定历史服务器日志存储路径</li>
<li>参数3含义：指定保存Application历史记录的个数，如果超过这个值，旧的应用程序信息将被删除，这个是内存中的应用数，而不是页面上显示的应用数。</li>
</ul>
</li>
<li><p>分发配置文件</p>
 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">    xsync conf </span><br><span class="line">    ```    </span><br><span class="line">5. 重新启动集群和历史服务</span><br><span class="line">    ```bash</span><br><span class="line">    sbin/stop-all.sh</span><br><span class="line">    sbin/start-all.sh</span><br><span class="line">    sbin/start-history-server.sh</span><br></pre></td></tr></table></figure></li>
<li><p>重新执行任务</p>
 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">bin/spark-submit \</span><br><span class="line">--class org.apache.spark.examples.SparkPi \</span><br><span class="line">--master spark://bd-server-001:7077 \</span><br><span class="line">./examples/jars/spark-examples_2.12-3.0.0.jar \</span><br><span class="line">10</span><br></pre></td></tr></table></figure>
<p> <img src="https://s2.loli.net/2021/12/11/CO3ZQe7rWwEvb1R.jpg"></p>
</li>
<li><p>查看历史服务：<a target="_blank" rel="noopener" href="http://bd-server-001:18080/">http://bd-server-001:18080</a><br> <img src="https://s2.loli.net/2021/12/11/j1xLsnyQ6hGpIlt.jpg"></p>
</li>
</ol>
<h3 id="3-2-7-配置高可用（HA）"><a href="#3-2-7-配置高可用（HA）" class="headerlink" title="3.2.7 配置高可用（HA）"></a>3.2.7 配置高可用（HA）</h3><p>所谓的高可用是因为当前集群中的Master节点只有一个，所以会存在单点故障问题。所以为了解决单点故障问题，需要在集群中配置多个Master节点，一旦处于活动状态的Master发生故障时，由备用Master提供服务，保证作业可以继续执行。这里的高可用一般采用Zookeeper设置<br>集群规划:<br>|      |Linux1     |Linux2    |Linux3  |<br>|—-|———|——-|———|<br>|Spark|    <font color ='red' >Master</font><br><font color ='blue' >Zookeeper</font><br>Worker|<font color ='red' >Master</font><br><font color ='blue' >Zookeeper</font><br>Worker|<br><font color ='blue' >Zookeeper</font><br>Worker|</p>
<ol>
<li><p>停止集群</p>
 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sbin/stop-all.sh </span><br></pre></td></tr></table></figure></li>
<li><p>启动Zookeeper</p>
 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">zookeeper_cluster.sh status</span><br></pre></td></tr></table></figure></li>
<li><p>修改spark-env.sh文件添加如下配置</p>
 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">注释如下内容：</span><br><span class="line"><span class="comment">#SPARK_MASTER_HOST=linux1</span></span><br><span class="line"><span class="comment">#SPARK_MASTER_PORT=7077</span></span><br><span class="line"></span><br><span class="line">添加如下内容:</span><br><span class="line"><span class="comment">#Master监控页面默认访问端口为8080，但是可能会和Zookeeper冲突，所以改成8989，也可以自定义，访问UI监控页面时请注意</span></span><br><span class="line">SPARK_MASTER_WEBUI_PORT=8989</span><br><span class="line"></span><br><span class="line"><span class="built_in">export</span> SPARK_DAEMON_JAVA_OPTS=<span class="string">&quot;</span></span><br><span class="line"><span class="string">-Dspark.deploy.recoveryMode=ZOOKEEPER </span></span><br><span class="line"><span class="string">-Dspark.deploy.zookeeper.url=linux1,linux2,linux3 </span></span><br><span class="line"><span class="string">-Dspark.deploy.zookeeper.dir=/spark&quot;</span></span><br></pre></td></tr></table></figure></li>
<li><p>分发配置文件</p>
 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">xsync conf/ </span><br></pre></td></tr></table></figure></li>
<li><p>启动集群</p>
 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sbin/start-all.sh</span><br></pre></td></tr></table></figure>
<p> <img src="https://s2.loli.net/2021/12/11/JjywVYhLs78W3KT.jpg"></p>
</li>
<li><p>启动bd-server-002和bd-server-003的单独Master节点，这两个节点Master状态处于备用状态</p>
 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sbin/start-master.sh</span><br></pre></td></tr></table></figure>
<p> <img src="https://s2.loli.net/2021/12/11/h69zTQAvVymeWZd.jpg"><br> <img src="https://s2.loli.net/2021/12/11/cm9lpOiIdFLvUCQ.jpg"></p>
</li>
<li><p>提交应用到高可用集群</p>
 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">bin/spark-submit \</span><br><span class="line">--class org.apache.spark.examples.SparkPi \</span><br><span class="line">--master spark://bd-server-001:7077,bd-server-002:7077,bd-server-003:7077 \</span><br><span class="line">./examples/jars/spark-examples_2.12-3.0.0.jar \</span><br><span class="line">10</span><br></pre></td></tr></table></figure></li>
<li><p>停止linux1的Master资源监控进程</p>
 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@bd-server-001 spark-standalone]$ jps</span><br><span class="line">48129 JournalNode</span><br><span class="line">48466 ResourceManager</span><br><span class="line">47557 QuorumPeerMain</span><br><span class="line">49861 Worker</span><br><span class="line">49957 HistoryServer</span><br><span class="line">50405 Jps</span><br><span class="line">48569 NodeManager</span><br><span class="line">48378 DFSZKFailoverController</span><br><span class="line">47902 DataNode</span><br><span class="line">49758 Master</span><br><span class="line">47775 NameNode</span><br><span class="line">[atguigu@bd-server-001 spark-standalone]$ <span class="built_in">kill</span> -9 49758</span><br><span class="line">[atguigu@bd-server-001 spark-standalone]$</span><br></pre></td></tr></table></figure></li>
<li><p>查看linux2的Master 资源监控Web UI，稍等一段时间后，linux2节点的Master状态提升为活动状态<br> <img src="https://s2.loli.net/2021/12/11/kslxS6FLeTvny7d.jpg"></p>
</li>
</ol>
<h2 id="3-3-Yarn模式"><a href="#3-3-Yarn模式" class="headerlink" title="3.3  Yarn模式"></a>3.3  Yarn模式</h2><p>独立部署（Standalone）模式由Spark自身提供计算资源，无需其他框架提供资源。这种方式降低了和其他第三方资源框架的耦合性，独立性非常强。但是你也要记住，Spark主要是计算框架，而不是资源调度框架，所以本身提供的资源调度并不是它的强项，所以还是和其他专业的资源调度框架集成会更靠谱一些。所以接下来我们来学习在强大的Yarn环境下Spark是如何工作的（其实是因为在国内工作中，Yarn使用的非常多）。</p>
<h3 id="3-3-1-解压缩文件"><a href="#3-3-1-解压缩文件" class="headerlink" title="3.3.1 解压缩文件"></a>3.3.1 解压缩文件</h3><p>将spark-3.0.0-bin-hadoop3.2.tgz文件上传到linux并解压缩，放置在指定位置。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf spark-3.0.0-bin-hadoop3.2.tgz -C /opt/module</span><br><span class="line"><span class="built_in">cd</span> /opt/module </span><br><span class="line">mv spark-3.0.0-bin-hadoop3.2 spark-yarn</span><br></pre></td></tr></table></figure>


<h3 id="3-3-2-修改配置文件"><a href="#3-3-2-修改配置文件" class="headerlink" title="3.3.2 修改配置文件"></a>3.3.2 修改配置文件</h3><ol>
<li>修改hadoop配置文件/opt/module/hadoop/etc/hadoop/yarn-site.xml, 并分发 <figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!--是否启动一个线程检查每个任务正使用的物理内存量，如果任务超出分配值，则直接将其杀掉，默认是true --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.pmem-check-enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--是否启动一个线程检查每个任务正使用的虚拟内存量，如果任务超出分配值，则直接将其杀掉，默认是true --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.vmem-check-enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li>
</ol>
<ol start="2">
<li>修改conf/spark-env.sh，添加JAVA_HOME和YARN_CONF_DIR配置 <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">mv spark-env.sh.template spark-env.sh</span><br><span class="line"></span><br><span class="line">export <span class="type">JAVA_HOME</span>=/opt/module/jdk1<span class="number">.8</span><span class="number">.0</span>_144</span><br><span class="line"><span class="type">YARN_CONF_DIR</span>=/opt/module/hadoop/etc/hadoop</span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="3-3-3-启动HDFS以及YARN集群"><a href="#3-3-3-启动HDFS以及YARN集群" class="headerlink" title="3.3.3 启动HDFS以及YARN集群"></a>3.3.3 启动HDFS以及YARN集群</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">zookeeper_cluster.sh start</span><br><span class="line">hadoop_cluster.sh start</span><br></pre></td></tr></table></figure>


<h3 id="3-3-4-提交应用"><a href="#3-3-4-提交应用" class="headerlink" title="3.3.4 提交应用"></a>3.3.4 提交应用</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">bin/spark-submit \</span><br><span class="line">--<span class="class"><span class="keyword">class</span> <span class="title">org</span>.<span class="title">apache</span>.<span class="title">spark</span>.<span class="title">examples</span>.<span class="title">SparkPi</span> <span class="title">\</span></span></span><br><span class="line">--master yarn \</span><br><span class="line">--deploy-mode cluster \</span><br><span class="line">./examples/jars/spark-examples_2<span class="number">.12</span><span class="number">-3.0</span><span class="number">.0</span>.jar \</span><br><span class="line"><span class="number">10</span></span><br></pre></td></tr></table></figure>
<p><img src="https://s2.loli.net/2022/01/22/MkZTrcbmLfIdSj4.jpg"><br><img src="https://s2.loli.net/2022/01/22/4hMIgeWRdzQm3c6.jpg"></p>
<p>查看<a href="http://linux2:8088页面，点击History，查看历史页面">http://linux2:8088页面，点击History，查看历史页面</a><br><img src="https://s2.loli.net/2022/01/22/jesMriWz34o8StZ.jpg"><br><img src="https://s2.loli.net/2022/01/22/YfSAyeEh6XKcgrj.jpg"></p>
<h3 id="3-3-5-配置历史服务器"><a href="#3-3-5-配置历史服务器" class="headerlink" title="3.3.5 配置历史服务器"></a>3.3.5 配置历史服务器</h3><ol>
<li><p>修改spark-defaults.conf.template文件名为spark-defaults.conf</p>
 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mv spark-defaults.conf.template spark-defaults.conf</span><br></pre></td></tr></table></figure></li>
<li><p>修改spark-default.conf文件，配置日志存储路径</p>
 <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">spark.eventLog.enabled          <span class="literal">true</span></span><br><span class="line">spark.eventLog.dir               hdfs:<span class="comment">//linux1:8020/directory</span></span><br></pre></td></tr></table></figure>
<p> 注意：需要启动hadoop集群，HDFS上的目录需要提前存在。</p>
 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@linux1 hadoop]<span class="comment"># sbin/start-dfs.sh</span></span><br><span class="line">[root@linux1 hadoop]<span class="comment"># hadoop fs -mkdir /directory</span></span><br></pre></td></tr></table></figure></li>
<li><p>修改spark-env.sh文件, 添加日志配置</p>
 <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">export <span class="type">SPARK_HISTORY_OPTS</span>=<span class="string">&quot;</span></span><br><span class="line"><span class="string">-Dspark.history.ui.port=18080 </span></span><br><span class="line"><span class="string">-Dspark.history.fs.logDirectory=hdfs://linux1:8020/directory </span></span><br><span class="line"><span class="string">-Dspark.history.retainedApplications=30&quot;</span></span><br></pre></td></tr></table></figure>
<ul>
<li>参数1含义：WEB UI访问的端口号为18080</li>
<li>参数2含义：指定历史服务器日志存储路径</li>
<li>参数3含义：指定保存Application历史记录的个数，如果超过这个值，旧的应用程序信息将被删除，这个是内存中的应用数，而不是页面上显示的应用数。</li>
</ul>
</li>
<li><p>修改spark-defaults.conf</p>
 <figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">spark.yarn.historyServer.address</span>=linux1:<span class="number">18080</span></span><br><span class="line"><span class="attr">spark.history.ui.port</span>=<span class="number">18080</span></span><br></pre></td></tr></table></figure></li>
<li><p>启动历史服务</p>
 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sbin/start-history-server.sh </span><br></pre></td></tr></table></figure></li>
<li><p>重新提交应用</p>
 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">bin/spark-submit \</span><br><span class="line">--class org.apache.spark.examples.SparkPi \</span><br><span class="line">--master yarn \</span><br><span class="line">--deploy-mode client \</span><br><span class="line">./examples/jars/spark-examples_2.12-3.0.0.jar \</span><br><span class="line">10</span><br></pre></td></tr></table></figure>
<p> <img src="https://s2.loli.net/2022/01/22/YgUQu7FEVz9JTP2.jpg"><br> <img src="https://s2.loli.net/2022/01/22/bihJcluITaWKwA2.jpg"></p>
</li>
<li><p>Web页面查看日志：<a target="_blank" rel="noopener" href="http://linux2:8088/">http://linux2:8088</a><br> <img src="https://s2.loli.net/2022/01/22/BDFdKT2ocOu3x71.jpg"><br> <img src="https://s2.loli.net/2022/01/22/MdyXW6Kks5AJTEp.jpg"></p>
</li>
</ol>
<h2 id="3-4-K8S-amp-Mesos模式"><a href="#3-4-K8S-amp-Mesos模式" class="headerlink" title="3.4  K8S &amp; Mesos模式"></a>3.4  K8S &amp; Mesos模式</h2><p>Mesos是Apache下的开源分布式资源管理框架，它被称为是分布式系统的内核,在Twitter得到广泛使用,管理着Twitter超过30,0000台服务器上的应用部署，但是在国内，依然使用着传统的Hadoop大数据框架，所以国内使用Mesos框架的并不多，但是原理都差不多。</p>
<p>容器化部署是目前业界很流行的一项技术，基于Docker镜像运行能够让用户更加方便地对应用进行管理和运维。容器管理工具中最为流行的就是Kubernetes（k8s），而Spark也在最近的版本中支持了k8s部署模式。这里我们也不做过多的讲解。给个链接大家自己感受一下：<a target="_blank" rel="noopener" href="https://spark.apache.org/docs/latest/running-on-kubernetes.html">https://spark.apache.org/docs/latest/running-on-kubernetes.html</a></p>
<h2 id="3-5-本地模式"><a href="#3-5-本地模式" class="headerlink" title="3.5  本地模式"></a>3.5  本地模式</h2><p>在同学们自己学习时，每次都需要启动虚拟机，启动集群，这是一个比较繁琐的过程，并且会占大量的系统资源，导致系统执行变慢，不仅仅影响学习效果，也影响学习进度，Spark非常暖心地提供了可以在本地操作系统下启动本地集群的方式，这样，在不使用虚拟机的情况下，也能学习Spark的基本使用</p>
<h2 id="3-5-1-解压缩文件"><a href="#3-5-1-解压缩文件" class="headerlink" title="3.5.1 解压缩文件"></a>3.5.1 解压缩文件</h2><p>将文件spark-3.0.0-bin-hadoop3.2.tgz解压缩到无中文无空格的路径中</p>
<h3 id="3-5-2-启动本地环境"><a href="#3-5-2-启动本地环境" class="headerlink" title="3.5.2 启动本地环境"></a>3.5.2 启动本地环境</h3><ol>
<li>执行解压缩文件路径下bin目录中的spark-shell文件，启动Spark本地环境 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">&gt;bin  ./spark-shell</span><br><span class="line">CMD /Library/Java/JavaVirtualMachines/jdk1.8.0_291.jdk/Contents/Home//bin/java</span><br><span class="line">/Library/Java/JavaVirtualMachines/jdk1.8.0_291.jdk/Contents/Home//bin/java -cp /Users/zhenan/app/spark-3.0.0-bin-hadoop3.2/conf/:/Users/zhenan/app/spark-3.0.0-bin-hadoop3.2/jars/* -Dscala.usejavacp=<span class="literal">true</span> -Xmx1g org.apache.spark.deploy.SparkSubmit --class org.apache.spark.repl.Main --name Spark shell spark-shell</span><br><span class="line">21/12/18 20:44:00 WARN NativeCodeLoader: Unable to load native-hadoop library <span class="keyword">for</span> your platform... using builtin-java classes <span class="built_in">where</span> applicable</span><br><span class="line">Using Spark<span class="string">&#x27;s default log4j profile: org/apache/spark/log4j-defaults.properties</span></span><br><span class="line"><span class="string">Setting default log level to &quot;WARN&quot;.</span></span><br><span class="line"><span class="string">To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).</span></span><br><span class="line"><span class="string">Spark context Web UI available at http://bogon:4040</span></span><br><span class="line"><span class="string">Spark context available as &#x27;</span>sc<span class="string">&#x27; (master = local[*], app id = local-1639831450687).</span></span><br><span class="line"><span class="string">Spark session available as &#x27;</span>spark<span class="string">&#x27;.</span></span><br><span class="line"><span class="string">Welcome to</span></span><br><span class="line"><span class="string">      ____              __</span></span><br><span class="line"><span class="string">     / __/__  ___ _____/ /__</span></span><br><span class="line"><span class="string">    _\ \/ _ \/ _ `/ __/  &#x27;</span>_/</span><br><span class="line">   /___/ .__/\_,_/_/ /_/\_\   version 3.0.0</span><br><span class="line">      /_/</span><br><span class="line"></span><br><span class="line">Using Scala version 2.12.10 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_291)</span><br><span class="line">Type <span class="keyword">in</span> expressions to have them evaluated.</span><br><span class="line">Type :<span class="built_in">help</span> <span class="keyword">for</span> more information.</span><br><span class="line"></span><br><span class="line">scala</span><br></pre></td></tr></table></figure></li>
<li>在bin目录中创建input目录，并添加word.txt文件, 在命令行中输入脚本代码 <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; sc.makeRDD(<span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>)).map(_+<span class="number">1</span>).collect()</span><br><span class="line">res1: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">scala&gt;</span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="3-5-3-命令行提交应用"><a href="#3-5-3-命令行提交应用" class="headerlink" title="3.5.3 命令行提交应用"></a>3.5.3 命令行提交应用</h3><p>在命令行窗口中执行提交指令</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">☁  bin  ./spark-submit --<span class="class"><span class="keyword">class</span> <span class="title">org</span>.<span class="title">apache</span>.<span class="title">spark</span>.<span class="title">examples</span>.<span class="title">SparkPi</span> <span class="title">--master</span> &#x27;<span class="title">local</span>[2]&#x27; ..<span class="title">/examples/jars/spark-examples_2</span>.12<span class="title">-3</span>.0.0.<span class="title">jar</span> 10</span></span><br><span class="line"><span class="type">CMD</span> /<span class="type">Library</span>/<span class="type">Java</span>/<span class="type">JavaVirtualMachines</span>/jdk1<span class="number">.8</span><span class="number">.0</span>_291.jdk/<span class="type">Contents</span>/<span class="type">Home</span><span class="comment">//bin/java</span></span><br><span class="line">/<span class="type">Library</span>/<span class="type">Java</span>/<span class="type">JavaVirtualMachines</span>/jdk1<span class="number">.8</span><span class="number">.0</span>_291.jdk/<span class="type">Contents</span>/<span class="type">Home</span><span class="comment">//bin/java -cp /Users/zhenan/app/spark-3.0.0-bin-hadoop3.2/conf/:/Users/zhenan/app/spark-3.0.0-bin-hadoop3.2/jars/* -Xmx1g org.apache.spark.deploy.SparkSubmit --master local[2] --class org.apache.spark.examples.SparkPi ../examples/jars/spark-examples_2.12-3.0.0.jar 10</span></span><br><span class="line"><span class="number">21</span>/<span class="number">12</span>/<span class="number">18</span> <span class="number">20</span>:<span class="number">47</span>:<span class="number">19</span> <span class="type">WARN</span> <span class="type">NativeCodeLoader</span>: <span class="type">Unable</span> to load native-hadoop library <span class="keyword">for</span> your platform... using builtin-java classes where applicable</span><br><span class="line"><span class="type">Using</span> <span class="type">Spark</span><span class="symbol">&#x27;s</span> <span class="keyword">default</span> log4j profile: org/apache/spark/log4j-defaults.properties</span><br><span class="line"><span class="number">21</span>/<span class="number">12</span>/<span class="number">18</span> <span class="number">20</span>:<span class="number">47</span>:<span class="number">20</span> <span class="type">INFO</span> <span class="type">SparkContext</span>: <span class="type">Running</span> <span class="type">Spark</span> version <span class="number">3.0</span><span class="number">.0</span></span><br><span class="line"><span class="number">21</span>/<span class="number">12</span>/<span class="number">18</span> <span class="number">20</span>:<span class="number">47</span>:<span class="number">20</span> <span class="type">INFO</span> <span class="type">ResourceUtils</span>: ==============================================================</span><br><span class="line"><span class="number">21</span>/<span class="number">12</span>/<span class="number">18</span> <span class="number">20</span>:<span class="number">47</span>:<span class="number">20</span> <span class="type">INFO</span> <span class="type">ResourceUtils</span>: <span class="type">Resources</span> <span class="keyword">for</span> spark.driver:</span><br><span class="line">。。。</span><br><span class="line">。。。</span><br><span class="line">。。。</span><br><span class="line"><span class="type">Pi</span> is roughly <span class="number">3.140919140919141</span></span><br><span class="line"><span class="number">21</span>/<span class="number">12</span>/<span class="number">18</span> <span class="number">20</span>:<span class="number">47</span>:<span class="number">23</span> <span class="type">INFO</span> <span class="type">SparkUI</span>: <span class="type">Stopped</span> <span class="type">Spark</span> web <span class="type">UI</span> at http:<span class="comment">//bogon:4040</span></span><br><span class="line"><span class="number">21</span>/<span class="number">12</span>/<span class="number">18</span> <span class="number">20</span>:<span class="number">47</span>:<span class="number">23</span> <span class="type">INFO</span> <span class="type">MapOutputTrackerMasterEndpoint</span>: <span class="type">MapOutputTrackerMasterEndpoint</span> stopped!</span><br><span class="line"><span class="number">21</span>/<span class="number">12</span>/<span class="number">18</span> <span class="number">20</span>:<span class="number">47</span>:<span class="number">23</span> <span class="type">INFO</span> <span class="type">MemoryStore</span>: <span class="type">MemoryStore</span> cleared</span><br><span class="line"><span class="number">21</span>/<span class="number">12</span>/<span class="number">18</span> <span class="number">20</span>:<span class="number">47</span>:<span class="number">23</span> <span class="type">INFO</span> <span class="type">BlockManager</span>: <span class="type">BlockManager</span> stopped</span><br><span class="line"><span class="number">21</span>/<span class="number">12</span>/<span class="number">18</span> <span class="number">20</span>:<span class="number">47</span>:<span class="number">23</span> <span class="type">INFO</span> <span class="type">BlockManagerMaster</span>: <span class="type">BlockManagerMaster</span> stopped</span><br><span class="line"><span class="number">21</span>/<span class="number">12</span>/<span class="number">18</span> <span class="number">20</span>:<span class="number">47</span>:<span class="number">23</span> <span class="type">INFO</span> <span class="type">OutputCommitCoordinator</span>$<span class="type">OutputCommitCoordinatorEndpoint</span>: <span class="type">OutputCommitCoordinator</span> stopped!</span><br><span class="line"><span class="number">21</span>/<span class="number">12</span>/<span class="number">18</span> <span class="number">20</span>:<span class="number">47</span>:<span class="number">23</span> <span class="type">INFO</span> <span class="type">SparkContext</span>: <span class="type">Successfully</span> stopped <span class="type">SparkContext</span></span><br><span class="line"><span class="number">21</span>/<span class="number">12</span>/<span class="number">18</span> <span class="number">20</span>:<span class="number">47</span>:<span class="number">23</span> <span class="type">INFO</span> <span class="type">ShutdownHookManager</span>: <span class="type">Shutdown</span> hook called</span><br><span class="line"><span class="number">21</span>/<span class="number">12</span>/<span class="number">18</span> <span class="number">20</span>:<span class="number">47</span>:<span class="number">23</span> <span class="type">INFO</span> <span class="type">ShutdownHookManager</span>: <span class="type">Deleting</span> directory /<span class="keyword">private</span>/<span class="keyword">var</span>/folders/__/y015x_9s237711b3mcmf0t1c0000gn/<span class="type">T</span>/spark<span class="number">-49</span>b55761-b06e<span class="number">-43</span>a8<span class="number">-89</span>d3-f6ae77ed8e45</span><br><span class="line"><span class="number">21</span>/<span class="number">12</span>/<span class="number">18</span> <span class="number">20</span>:<span class="number">47</span>:<span class="number">23</span> <span class="type">INFO</span> <span class="type">ShutdownHookManager</span>: <span class="type">Deleting</span> directory /<span class="keyword">private</span>/<span class="keyword">var</span>/folders/__/y015x_9s237711b3mcmf0t1c0000gn/<span class="type">T</span>/spark<span class="number">-3</span>ec7edc0-eca2<span class="number">-4003</span>-ad13<span class="number">-6</span>febbc651e59</span><br></pre></td></tr></table></figure>

<h2 id="3-6-部署模式对比"><a href="#3-6-部署模式对比" class="headerlink" title="3.6  部署模式对比"></a>3.6  部署模式对比</h2><table>
<thead>
<tr>
<th>模式</th>
<th>Spark安装机器数</th>
<th>需启动的进程</th>
<th>所属者</th>
<th>应用场景</th>
</tr>
</thead>
<tbody><tr>
<td>Local</td>
<td>1</td>
<td>无</td>
<td>Spark</td>
<td>测试</td>
</tr>
<tr>
<td>Standalone</td>
<td>3</td>
<td>Master及Worker</td>
<td>Spark</td>
<td>单独部署</td>
</tr>
<tr>
<td>Yarn</td>
<td>1</td>
<td>Yarn及HDFS</td>
<td>Hadoop</td>
<td>混合部署</td>
</tr>
</tbody></table>
<h2 id="3-7-端口号"><a href="#3-7-端口号" class="headerlink" title="3.7  端口号"></a>3.7  端口号</h2><ul>
<li>Spark查看当前Spark-shell运行任务情况端口号：4040（计算）</li>
<li>Spark Master内部通信服务端口号：7077</li>
<li>Standalone模式下，Spark Master Web端口号：8080（资源）</li>
<li>Spark历史服务器端口号：18080</li>
<li>Hadoop YARN任务运行情况查看端口号：8088</li>
</ul>
<h1 id="第4章-Spark运行架构"><a href="#第4章-Spark运行架构" class="headerlink" title="第4章 Spark运行架构"></a>第4章 Spark运行架构</h1><h2 id="4-1-运行架构"><a href="#4-1-运行架构" class="headerlink" title="4.1 运行架构"></a>4.1 运行架构</h2><p>Spark框架的核心是一个计算引擎，整体来说，它采用了标准 master-slave 的结构。<br>如下图所示，它展示了一个 Spark执行时的基本结构。图形中的Driver表示master，负责管理整个集群中的作业任务调度。图形中的Executor 则是 slave，负责实际执行任务。<br><img src="https://s2.loli.net/2021/12/11/BjZFPrvakJHNEhI.jpg"></p>
<h2 id="4-2-核心组件"><a href="#4-2-核心组件" class="headerlink" title="4.2 核心组件"></a>4.2 核心组件</h2><p>由上图可以看出，对于Spark框架有两个核心组件：Driver和Executor</p>
<h3 id="4-2-1-Driver"><a href="#4-2-1-Driver" class="headerlink" title="4.2.1 Driver"></a>4.2.1 Driver</h3><p>Spark驱动器节点，用于执行Spark任务中的main方法，负责实际代码的执行工作。Driver在Spark作业执行时主要负责：</p>
<ul>
<li>将用户程序转化为作业（job）</li>
<li>在Executor之间调度任务(task)</li>
<li>跟踪Executor的执行情况</li>
<li>通过UI展示查询运行情况<br>实际上，我们无法准确地描述Driver的定义，因为在整个的编程过程中没有看到任何有关Driver的字眼。所以简单理解，所谓的Driver就是驱使整个应用运行起来的程序，也称之为Driver类。</li>
</ul>
<h3 id="4-2-2-Executor"><a href="#4-2-2-Executor" class="headerlink" title="4.2.2 Executor"></a>4.2.2 Executor</h3><p>Spark Executor是集群中工作节点（Worker）中的一个JVM进程，负责在 Spark 作业中运行具体任务（Task），任务彼此之间相互独立。Spark 应用启动时，Executor节点被同时启动，并且始终伴随着整个 Spark 应用的生命周期而存在。如果有Executor节点发生了故障或崩溃，Spark 应用也可以继续执行，会将出错节点上的任务调度到其他Executor节点上继续运行。<br>Executor有两个核心功能：</p>
<ul>
<li>负责运行组成Spark应用的任务，并将结果返回给驱动器进程</li>
<li>它们通过自身的块管理器（Block Manager）为用户程序中要求缓存的 RDD 提供内存式存储。RDD 是直接缓存在Executor进程内的，因此任务可以在运行时充分利用缓存数据加速运算。</li>
</ul>
<h3 id="4-2-3-Master-amp-Worker"><a href="#4-2-3-Master-amp-Worker" class="headerlink" title="4.2.3 Master &amp; Worker"></a>4.2.3 Master &amp; Worker</h3><p>Spark集群的独立部署环境中，不需要依赖其他的资源调度框架，自身就实现了资源调度的功能，所以环境中还有其他两个核心组件：Master和Worker，这里的Master是一个进程，主要负责资源的调度和分配，并进行集群的监控等职责，类似于Yarn环境中的RM, 而Worker呢，也是进程，一个Worker运行在集群中的一台服务器上，由Master分配资源对数据进行并行的处理和计算，类似于Yarn环境中NM。</p>
<h3 id="4-2-4-ApplicationMaster"><a href="#4-2-4-ApplicationMaster" class="headerlink" title="4.2.4 ApplicationMaster"></a>4.2.4 ApplicationMaster</h3><p>Hadoop用户向YARN集群提交应用程序时,提交程序中应该包含ApplicationMaster，用于向资源调度器申请执行任务的资源容器Container，运行用户自己的程序任务job，监控整个任务的执行，跟踪整个任务的状态，处理任务失败等异常情况。<br>说的简单点就是，ResourceManager（资源）和Driver（计算）之间的解耦合靠的就是ApplicationMaster。</p>
<h2 id="4-3-核心概念"><a href="#4-3-核心概念" class="headerlink" title="4.3 核心概念"></a>4.3 核心概念</h2><h3 id="4-3-1-Executor与Core（核）"><a href="#4-3-1-Executor与Core（核）" class="headerlink" title="4.3.1 Executor与Core（核）"></a>4.3.1 Executor与Core（核）</h3><p>Spark Executor是集群中运行在工作节点（Worker）中的一个JVM进程，是整个集群中的专门用于计算的节点。在提交应用中，可以提供参数指定计算节点的个数，以及对应的资源。这里的资源一般指的是工作节点Executor的内存大小和使用的虚拟CPU核（Core）数量。</p>
<p>应用程序相关启动参数如下：<br>|名称                  |   说明                      |<br>|——————-|—————————-|<br>|–num-executors      |配置Executor的数量                   |<br>|–executor-memory  |    配置每个Executor的内存大小         |<br>|–executor-cores      |配置每个Executor的虚拟CPU core数量      |</p>
<h3 id="4-3-2-并行度（Parallelism）"><a href="#4-3-2-并行度（Parallelism）" class="headerlink" title="4.3.2 并行度（Parallelism）"></a>4.3.2 并行度（Parallelism）</h3><p>在分布式计算框架中一般都是多个任务同时执行，由于任务分布在不同的计算节点进行计算，所以能够真正地实现多任务并行执行，记住，这里是并行，而不是并发。这里我们将整个集群并行执行任务的数量称之为并行度。那么一个作业到底并行度是多少呢？这个取决于框架的默认配置。应用程序也可以在运行过程中动态修改。</p>
<h3 id="4-3-3-有向无环图（DAG）"><a href="#4-3-3-有向无环图（DAG）" class="headerlink" title="4.3.3 有向无环图（DAG）"></a>4.3.3 有向无环图（DAG）</h3><p>大数据计算引擎框架我们根据使用方式的不同一般会分为四类，其中第一类就是Hadoop所承载的MapReduce,它将计算分为两个阶段，分别为 Map阶段 和 Reduce阶段。对于上层应用来说，就不得不想方设法去拆分算法，甚至于不得不在上层应用实现多个 Job 的串联，以完成一个完整的算法，例如迭代计算。 由于这样的弊端，催生了支持 DAG 框架的产生。因此，支持 DAG 的框架被划分为第二代计算引擎。如 Tez 以及更上层的 Oozie。这里我们不去细究各种 DAG 实现之间的区别，不过对于当时的 Tez 和 Oozie 来说，大多还是批处理的任务。接下来就是以 Spark 为代表的第三代的计算引擎。第三代计算引擎的特点主要是 Job 内部的 DAG 支持（不跨越 Job），以及实时计算。<br>这里所谓的有向无环图，并不是真正意义的图形，而是由Spark程序直接映射成的数据流的高级抽象模型。简单理解就是将整个程序计算的执行过程用图形表示出来,这样更直观，更便于理解，可以用于表示程序的拓扑结构。<br>DAG（Directed Acyclic Graph）有向无环图是由点和线组成的拓扑图形，该图形具有方向，不会闭环。</p>
<h3 id="4-4-提交流程"><a href="#4-4-提交流程" class="headerlink" title="4.4 提交流程"></a>4.4 提交流程</h3><p>所谓的提交流程，其实就是我们开发人员根据需求写的应用程序通过Spark客户端提交给Spark运行环境执行计算的流程。在不同的部署环境中，这个提交过程基本相同，但是又有细微的区别，我们这里不进行详细的比较，但是因为国内工作中，将Spark引用部署到Yarn环境中会更多一些，所以本课程中的提交流程是基于Yarn环境的。</p>
<p>Spark应用程序提交到Yarn环境中执行的时候，一般会有两种部署执行的方式：Client和Cluster。两种模式主要区别在于：Driver程序的运行节点位置。</p>
<h3 id="4-2-1-Yarn-Client模式"><a href="#4-2-1-Yarn-Client模式" class="headerlink" title="4.2.1 Yarn Client模式"></a>4.2.1 Yarn Client模式</h3><p>Client模式将用于监控和调度的Driver模块在客户端执行，而不是在Yarn中，所以一般用于测试。</p>
<ul>
<li>Driver在任务提交的本地机器上运行</li>
<li>Driver启动后会和ResourceManager通讯申请启动ApplicationMaster</li>
<li>ResourceManager分配container，在合适的NodeManager上启动ApplicationMaster，负责向ResourceManager申请Executor内存</li>
<li>ResourceManager接到ApplicationMaster的资源申请后会分配container，然后ApplicationMaster在资源分配指定的NodeManager上启动Executor进程</li>
<li>Executor进程启动后会向Driver反向注册，Executor全部注册完成后Driver开始执行main函数</li>
<li>之后执行到Action算子时，触发一个Job，并根据宽依赖开始划分stage，每个stage生成对应的TaskSet，之后将task分发到各个Executor上执行。</li>
</ul>
<h3 id="4-2-2-Yarn-Cluster模式"><a href="#4-2-2-Yarn-Cluster模式" class="headerlink" title="4.2.2 Yarn Cluster模式"></a>4.2.2 Yarn Cluster模式</h3><p>Cluster模式将用于监控和调度的Driver模块启动在Yarn集群资源中执行。一般应用于实际生产环境。</p>
<ul>
<li>在YARN Cluster模式下，任务提交后会和ResourceManager通讯申请启动ApplicationMaster，</li>
<li>随后ResourceManager分配container，在合适的NodeManager上启动ApplicationMaster，此时的ApplicationMaster就是Driver。</li>
<li>Driver启动后向ResourceManager申请Executor内存，ResourceManager接到ApplicationMaster的资源申请后会分配container，然后在合适的NodeManager上启动Executor进程</li>
<li>Executor进程启动后会向Driver反向注册，Executor全部注册完成后Driver开始执行main函数，</li>
<li>之后执行到Action算子时，触发一个Job，并根据宽依赖开始划分stage，每个stage生成对应的TaskSet，之后将task分发到各个Executor上执行。</li>
</ul>
<h1 id="第5章-Spark核心编程"><a href="#第5章-Spark核心编程" class="headerlink" title="第5章 Spark核心编程"></a>第5章 Spark核心编程</h1><p>Spark计算框架为了能够进行高并发和高吞吐的数据处理，封装了三大数据结构，用于处理不同的应用场景。三大数据结构分别是：</p>
<ul>
<li>RDD : 弹性分布式数据集</li>
<li>累加器：分布式共享只写变量</li>
<li>广播变量：分布式共享只读变量</li>
</ul>
<h2 id="5-1-RDD"><a href="#5-1-RDD" class="headerlink" title="5.1 RDD"></a>5.1 RDD</h2><h3 id="5-1-1-什么是RDD"><a href="#5-1-1-什么是RDD" class="headerlink" title="5.1.1 什么是RDD"></a>5.1.1 什么是RDD</h3><p>RDD（Resilient Distributed Dataset）叫做弹性分布式数据集，是Spark中最基本的数据处理模型。代码中是一个抽象类，它代表一个弹性的、不可变、可分区、里面的元素可并行计算的集合。</p>
<ul>
<li>弹性<ul>
<li>存储的弹性：内存与磁盘的自动切换；</li>
<li>容错的弹性：数据丢失可以自动恢复；</li>
<li>计算的弹性：计算出错重试机制；</li>
<li>分片的弹性：可根据需要重新分片。</li>
</ul>
</li>
<li>分布式：数据存储在大数据集群不同节点上</li>
<li>数据集：RDD封装了计算逻辑，只处理数据，并不保存数据</li>
<li>数据抽象：RDD是一个抽象类，需要子类具体实现</li>
<li>不可变：RDD封装了计算逻辑，是不可以改变的，想要改变，只能产生新的RDD，在新的RDD里面封装计算逻辑</li>
<li>可分区、并行计算</li>
</ul>
<h3 id="5-1-2-核心属性"><a href="#5-1-2-核心属性" class="headerlink" title="5.1.2 核心属性"></a>5.1.2 核心属性</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">* Internally, each RDD is characterized by five main properties:</span><br><span class="line">*</span><br><span class="line">*  分区列表 - A list of partitions</span><br><span class="line">*  分区计算函数 - A function for computing each split  </span><br><span class="line">*  RDD之间的依赖关系 - A list of dependencies on other RDDs  </span><br><span class="line">*  分区器（可选） - Optionally, a Partitioner for key-value RDDs (e.g. to say that the RDD is hash-partitioned)  </span><br><span class="line">*  首选位置（可选） - Optionally, a list of preferred locations to compute each split on (e.g. block locations for an HDFS file)           </span><br></pre></td></tr></table></figure>

<ul>
<li><p>分区列表<br>  RDD数据结构中存在分区列表，用于执行任务时并行计算，是实现分布式计算的重要属性。</p>
  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Implemented by subclasses to return the set of partitions in this RDD. This method will only</span></span><br><span class="line"><span class="comment"> * be called once, so it is safe to implement a time-consuming computation in it.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * The partitions in this array must satisfy the following property:</span></span><br><span class="line"><span class="comment"> *   `rdd.partitions.zipWithIndex.forall &#123; case (partition, index) =&gt; partition.index == index &#125;`</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">getPartitions</span></span>: <span class="type">Array</span>[<span class="type">Partition</span>]</span><br></pre></td></tr></table></figure></li>
<li><p>分区计算函数<br>  Spark在计算时，是使用分区函数对每一个分区进行计算</p>
  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * :: DeveloperApi ::</span></span><br><span class="line"><span class="comment"> * Implemented by subclasses to compute a given partition.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@DeveloperApi</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute</span></span>(split: <span class="type">Partition</span>, context: <span class="type">TaskContext</span>): <span class="type">Iterator</span>[<span class="type">T</span>]</span><br></pre></td></tr></table></figure></li>
<li><p>RDD之间的依赖关系<br>  RDD是计算模型的封装，当需求中需要将多个计算模型进行组合时，就需要将多个RDD建立依赖关系</p>
  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Implemented by subclasses to return how this RDD depends on parent RDDs. This method will only</span></span><br><span class="line"><span class="comment"> * be called once, so it is safe to implement a time-consuming computation in it.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">getDependencies</span></span>: <span class="type">Seq</span>[<span class="type">Dependency</span>[_]] = deps</span><br></pre></td></tr></table></figure></li>
<li><p>分区器（可选）<br>  当数据为KV类型数据时，可以通过设定分区器自定义数据的分区</p>
  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/** Optionally overridden by subclasses to specify how they are partitioned. */</span></span><br><span class="line"><span class="meta">@transient</span> <span class="keyword">val</span> partitioner: <span class="type">Option</span>[<span class="type">Partitioner</span>] = <span class="type">None</span></span><br></pre></td></tr></table></figure></li>
<li><p>首选位置（可选）</p>
<ul>
<li>计算数据时，可以根据计算节点的状态选择不同的节点位置进行计算  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Optionally overridden by subclasses to specify placement preferences.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">getPreferredLocations</span></span>(split: <span class="type">Partition</span>): <span class="type">Seq</span>[<span class="type">String</span>] = <span class="type">Nil</span></span><br></pre></td></tr></table></figure></li>
<li>数据计算本地化<ul>
<li>进程本地化：数据和计算在同一个进程中</li>
<li>节点本地化：数据和计算在同一个节点中</li>
<li>机架本地化：数据和计算在同一个机架中</li>
<li>任意：数据和计算在不同的任意节点</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="5-1-3-执行原理"><a href="#5-1-3-执行原理" class="headerlink" title="5.1.3 执行原理"></a>5.1.3 执行原理</h3><p>从计算的角度来讲，数据处理过程中需要计算资源（内存 &amp; CPU）和计算模型（逻辑）。执行时，需要将计算资源和计算模型进行协调和整合。<br>Spark框架在执行时，先申请资源，然后将应用程序的数据处理逻辑分解成一个一个的计算任务。然后将任务发到已经分配资源的计算节点上, 按照指定的计算模型进行数据计算。最后得到计算结果。<br>RDD是Spark框架中用于数据处理的核心模型，接下来我们看看，在Yarn环境中，RDD的工作原理:</p>
<ol>
<li>启动Yarn集群环境<pre><code> ![](https://s2.loli.net/2021/12/11/TRiUEOMQtA36CHn.jpg)
</code></pre>
</li>
<li>Spark通过申请资源创建调度节点和计算节点<br> <img src="https://s2.loli.net/2021/12/11/laJOMSj6m1BUeKG.jpg"></li>
<li>Spark框架根据需求将计算逻辑根据分区划分成不同的任务<br> <img src="https://s2.loli.net/2021/12/11/MRKL1gjPIN3ATo2.jpg"></li>
<li>调度节点将任务根据计算节点状态发送到对应的计算节点进行计算<br> <img src="https://s2.loli.net/2021/12/11/PC91pTKyi47RhzA.jpg"></li>
</ol>
<p>从以上流程可以看出RDD在整个流程中主要用于将逻辑进行封装，并生成Task发送给Executor节点执行计算，接下来我们就一起看看Spark框架中RDD是具体是如何进行数据处理的。</p>
<h3 id="5-1-4-基础编程"><a href="#5-1-4-基础编程" class="headerlink" title="5.1.4 基础编程"></a>5.1.4 基础编程</h3><h4 id="5-1-4-1-RDD创建"><a href="#5-1-4-1-RDD创建" class="headerlink" title="5.1.4.1 RDD创建"></a>5.1.4.1 RDD创建</h4><p>在Spark中创建RDD的创建方式可以分为四种：</p>
<ol>
<li>从集合（内存）中创建RDD<ul>
<li>从集合中创建RDD，Spark主要提供了两个方法：parallelize和makeRDD  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">    .setMaster(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">    .setAppName(<span class="string">&quot;spark&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"><span class="keyword">val</span> rdd1 = sparkContext.parallelize(</span><br><span class="line">    <span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line">)</span><br><span class="line"><span class="keyword">val</span> rdd2 = sparkContext.makeRDD(</span><br><span class="line">    <span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line">)</span><br><span class="line">rdd1.collect().foreach(println)</span><br><span class="line">rdd2.collect().foreach(println)</span><br><span class="line">sparkContext.stop()</span><br></pre></td></tr></table></figure></li>
<li>从底层代码实现来讲，makeRDD方法其实就是parallelize方法  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">makeRDD</span></span>[<span class="type">T</span>: <span class="type">ClassTag</span>](</span><br><span class="line">    seq: <span class="type">Seq</span>[<span class="type">T</span>],</span><br><span class="line">    numSlices: <span class="type">Int</span> = defaultParallelism): <span class="type">RDD</span>[<span class="type">T</span>] = withScope &#123;</span><br><span class="line">  parallelize(seq, numSlices)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>从外部存储（文件）创建RDD<ul>
<li>由外部存储系统的数据集创建RDD包括：本地的文件系统，所有Hadoop支持的数据集，比如HDFS、HBase等。  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sparkConf =<span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">    .setMaster(<span class="string">&quot;local[*]&quot;</span>).setAppName(<span class="string">&quot;spark&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"><span class="keyword">val</span> fileRDD: <span class="type">RDD</span>[<span class="type">String</span>] = sparkContext.textFile(<span class="string">&quot;input&quot;</span>)</span><br><span class="line">fileRDD.collect().foreach(println)</span><br><span class="line">sparkContext.stop()</span><br></pre></td></tr></table></figure></li>
<li>textFile 可以将文件作为数据源<ul>
<li>参数表示文件路径,可以为绝对路径也可以为相对路径<ul>
<li>绝对路径：不可改变的路径<ul>
<li>网络：网络协议://IP:Port//Path</li>
<li>文件系统：file://xx/xx/xx</li>
</ul>
</li>
<li>相对路径：可以改变的路径，一定存在一个基准路径</li>
</ul>
</li>
<li>返回结果为RDD数据模型，其中泛型标识文件中的每一行数据</li>
</ul>
</li>
</ul>
</li>
<li>从其他RDD创建<br> 主要是通过一个RDD运算完后，再产生新的RDD。详情请参考后续章节</li>
<li>直接创建RDD（new）<br> 使用new的方式直接构造RDD，一般由Spark框架自身使用。</li>
</ol>
<h4 id="5-1-4-2-RDD并行度与分区"><a href="#5-1-4-2-RDD并行度与分区" class="headerlink" title="5.1.4.2 RDD并行度与分区"></a>5.1.4.2 RDD并行度与分区</h4><p>默认情况下，Spark可以将一个作业切分多个任务后，发送给Executor节点并行计算，而能够并行计算的任务数量我们称之为并行度。这个数量可以在构建RDD时指定。记住，这里的并行执行的任务数量，并不是指的切分任务的数量，不要混淆了。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">&quot;local[*]&quot;</span>).setAppName(<span class="string">&quot;spark&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"><span class="keyword">val</span> dataRDD: <span class="type">RDD</span>[<span class="type">Int</span>] =</span><br><span class="line">    sparkContext.makeRDD(</span><br><span class="line">        <span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>),</span><br><span class="line">        <span class="number">4</span>)</span><br><span class="line"><span class="keyword">val</span> fileRDD: <span class="type">RDD</span>[<span class="type">String</span>] =</span><br><span class="line">    sparkContext.textFile(</span><br><span class="line">        <span class="string">&quot;input&quot;</span>,</span><br><span class="line">        <span class="number">2</span>)</span><br><span class="line">fileRDD.collect().foreach(println)</span><br><span class="line">sparkContext.stop()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<ul>
<li>读取内存数据时，数据可以按照并行度的设定进行数据的分区操作，数据分区规则的Spark核心源码如下：  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">positions</span></span>(length: <span class="type">Long</span>, numSlices: <span class="type">Int</span>): <span class="type">Iterator</span>[(<span class="type">Int</span>, <span class="type">Int</span>)] = &#123;</span><br><span class="line">  (<span class="number">0</span> until numSlices).iterator.map &#123; i =&gt;</span><br><span class="line">    <span class="keyword">val</span> start = ((i * length) / numSlices).toInt</span><br><span class="line">    <span class="keyword">val</span> end = (((i + <span class="number">1</span>) * length) / numSlices).toInt</span><br><span class="line">    (start, end)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>读取文件数据时，数据是按照Hadoop文件读取的规则进行切片分区，而切片规则和数据读取的规则有些差异，具体Spark核心源码如下  <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> InputSplit[] getSplits(JobConf job, <span class="keyword">int</span> numSplits)</span><br><span class="line">    <span class="keyword">throws</span> IOException &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">long</span> totalSize = <span class="number">0</span>;                           <span class="comment">// compute total size</span></span><br><span class="line">    <span class="keyword">for</span> (FileStatus file: files) &#123;                <span class="comment">// check we have valid files</span></span><br><span class="line">      <span class="keyword">if</span> (file.isDirectory()) &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> IOException(<span class="string">&quot;Not a file: &quot;</span>+ file.getPath());</span><br><span class="line">      &#125;</span><br><span class="line">      totalSize += file.getLen();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">long</span> goalSize = totalSize / (numSplits == <span class="number">0</span> ? <span class="number">1</span> : numSplits);</span><br><span class="line">    <span class="keyword">long</span> minSize = Math.max(job.getLong(org.apache.hadoop.mapreduce.lib.input.</span><br><span class="line">      FileInputFormat.SPLIT_MINSIZE, <span class="number">1</span>), minSplitSize);</span><br><span class="line">      </span><br><span class="line">    ...</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> (FileStatus file: files) &#123;</span><br><span class="line">    </span><br><span class="line">        ...</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> (isSplitable(fs, path)) &#123;</span><br><span class="line">          <span class="keyword">long</span> blockSize = file.getBlockSize();</span><br><span class="line">          <span class="keyword">long</span> splitSize = computeSplitSize(goalSize, minSize, blockSize);</span><br><span class="line"></span><br><span class="line">          ...</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">long</span> <span class="title">computeSplitSize</span><span class="params">(<span class="keyword">long</span> goalSize, <span class="keyword">long</span> minSize,</span></span></span><br><span class="line"><span class="params"><span class="function">                                       <span class="keyword">long</span> blockSize)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> Math.max(minSize, Math.min(goalSize, blockSize));</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>Hadoop计算分区和读取分区数据的方式不一样<ol>
<li>计算分区按照字节来计算</li>
<li>读取分区不是按照字节，是按照行来读取（单次不能跨行）<ul>
<li>读取数据时，按照偏移量来计算，相同的偏移量不能被重复读取</li>
</ul>
</li>
</ol>
</li>
</ul>
</li>
</ul>
<h4 id="5-1-4-3-RDD转换算子"><a href="#5-1-4-3-RDD转换算子" class="headerlink" title="5.1.4.3 RDD转换算子"></a>5.1.4.3 RDD转换算子</h4><p>RDD根据数据处理方式的不同将算子整体上分为Value类型、双Value类型和Key-Value类型</p>
<h5 id="5-1-4-3-1-Value类型"><a href="#5-1-4-3-1-Value类型" class="headerlink" title="5.1.4.3.1 Value类型"></a>5.1.4.3.1 Value类型</h5><ol>
<li><p>map</p>
<ul>
<li>函数签名  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">map</span></span>[<span class="type">U</span>: <span class="type">ClassTag</span>](f: <span class="type">T</span> =&gt; <span class="type">U</span>): <span class="type">RDD</span>[<span class="type">U</span>]</span><br></pre></td></tr></table></figure></li>
<li>函数说明<br>  将处理的数据逐条进行映射转换，这里的转换可以是类型的转换，也可以是值的转换。  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> dataRDD: <span class="type">RDD</span>[<span class="type">Int</span>] = sparkContext.makeRDD(<span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>))</span><br><span class="line"><span class="keyword">val</span> dataRDD1: <span class="type">RDD</span>[<span class="type">Int</span>] = dataRDD.map(</span><br><span class="line">    num =&gt; &#123;</span><br><span class="line">        num * <span class="number">2</span></span><br><span class="line">    &#125;</span><br><span class="line">)</span><br><span class="line"><span class="keyword">val</span> dataRDD2: <span class="type">RDD</span>[<span class="type">String</span>] = dataRDD1.map(</span><br><span class="line">    num =&gt; &#123;</span><br><span class="line">        <span class="string">&quot;&quot;</span> + num</span><br><span class="line">    &#125;</span><br><span class="line">)</span><br></pre></td></tr></table></figure></li>
<li>小功能：从服务器日志数据<a href="media/16382517322435/apache.log">apache.log</a>中获取用户请求URL资源路径  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">apache_log</span> </span>= &#123;</span><br><span class="line">    <span class="comment">//构建环境</span></span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">&quot;WordCount&quot;</span>).setMaster(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">    sparkContext.textFile(<span class="string">&quot;data/apache.log&quot;</span>)</span><br><span class="line">        .map(</span><br><span class="line">            line =&gt; &#123;</span><br><span class="line">                line.split(<span class="string">&quot; &quot;</span>)(<span class="number">6</span>)</span><br><span class="line">            &#125;</span><br><span class="line">        )</span><br><span class="line">        .collect()</span><br><span class="line">        .toList</span><br><span class="line">        .distinct</span><br><span class="line">        .foreach(println)</span><br><span class="line">    </span><br><span class="line">    sparkContext.stop()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>mapPartitions</p>
<ul>
<li>函数签名  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mapPartitions</span></span>[<span class="type">U</span>: <span class="type">ClassTag</span>](</span><br><span class="line">    f: <span class="type">Iterator</span>[<span class="type">T</span>] =&gt; <span class="type">Iterator</span>[<span class="type">U</span>],</span><br><span class="line">    preservesPartitioning: <span class="type">Boolean</span> = <span class="literal">false</span>): <span class="type">RDD</span>[<span class="type">U</span>]</span><br></pre></td></tr></table></figure></li>
<li>函数说明<br>  将待处理的数据以分区为单位发送到计算节点进行处理，这里的处理是指可以进行任意的处理，哪怕是过滤数据。  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> dataRDD1: <span class="type">RDD</span>[<span class="type">Int</span>] = dataRDD.mapPartitions(</span><br><span class="line">    datas =&gt; &#123;</span><br><span class="line">        datas.filter(_==<span class="number">2</span>)</span><br><span class="line">    &#125;</span><br><span class="line">)</span><br></pre></td></tr></table></figure></li>
</ul>
<blockquote>
<p>思考一个问题：map和mapPartitions的区别？</p>
<ul>
<li>数据处理角度<br>  Map算子是分区内一个数据一个数据的执行，类似于串行操作。而mapPartitions算子是以分区为单位进行批处理操作。</li>
<li>功能的角度<br>  Map算子主要目的将数据源中的数据进行转换和改变。但是不会减少或增多数据。MapPartitions算子需要传递一个迭代器，返回一个迭代器，没有要求的元素的个数保持不变，所以可以增加或减少数据</li>
<li>性能的角度<br>  Map算子因为类似于串行操作，所以性能比较低，而是mapPartitions算子类似于批处理，所以性能较高。但是mapPartitions算子会长时间占用内存，那么这样会导致内存可能不够用，出现内存溢出的错误。所以在内存有限的情况下，不推荐使用。使用map操作。</li>
</ul>
</blockquote>
</li>
</ol>
<ol start="3">
<li><p>mapPartitionsWithIndex</p>
<ul>
<li>函数签名  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mapPartitionsWithIndex</span></span>[<span class="type">U</span>: <span class="type">ClassTag</span>](</span><br><span class="line">  f: (<span class="type">Int</span>, <span class="type">Iterator</span>[<span class="type">T</span>]) =&gt; <span class="type">Iterator</span>[<span class="type">U</span>],</span><br><span class="line">  preservesPartitioning: <span class="type">Boolean</span> = <span class="literal">false</span>): <span class="type">RDD</span>[<span class="type">U</span>]</span><br></pre></td></tr></table></figure></li>
<li>函数说明<br>  将待处理的数据以分区为单位发送到计算节点进行处理，这里的处理是指可以进行任意的处理，哪怕是过滤数据，在处理时同时可以获取当前分区索引。  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> dataRDD1 = dataRDD.mapPartitionsWithIndex(</span><br><span class="line">    (index, datas) =&gt; &#123;</span><br><span class="line">         datas.map(index, _)</span><br><span class="line">    &#125;</span><br><span class="line">)</span><br></pre></td></tr></table></figure></li>
<li>小功能：<ul>
<li>获取第二个数据分区的数据</li>
<li>获取每个数据分区的最大值<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    println(<span class="string">&quot;************分区最大值****************&quot;</span>)</span><br><span class="line">    <span class="comment">//构建环境</span></span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">&quot;WordCount&quot;</span>).setMaster(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">    <span class="keyword">val</span> rdd = sparkContext.makeRDD(<span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>), <span class="number">2</span>)</span><br><span class="line">    <span class="keyword">val</span> max = rdd.mapPartitions(iter =&gt; &#123;</span><br><span class="line">        <span class="type">List</span>(iter.max).iterator</span><br><span class="line">    &#125;)</span><br><span class="line">    println(max.collect().toList)</span><br><span class="line">    </span><br><span class="line">    println(<span class="string">&quot;************错误姿势****************&quot;</span>)</span><br><span class="line">    <span class="comment">//分布式环境，无法保证第一个执行的分区就是第一个分区</span></span><br><span class="line">    <span class="comment">//且分布式中没有顺序</span></span><br><span class="line">    <span class="keyword">var</span> partNum = <span class="number">0</span></span><br><span class="line">    <span class="keyword">val</span> partitionMax = rdd.mapPartitions(iter =&gt; &#123;</span><br><span class="line">        <span class="keyword">if</span> (partNum == <span class="number">0</span>) &#123;</span><br><span class="line">            partNum += <span class="number">1</span></span><br><span class="line">            <span class="type">List</span>(iter.max).iterator</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            partNum += <span class="number">1</span></span><br><span class="line">            <span class="type">Nil</span>.iterator</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;)</span><br><span class="line">    println(partitionMax.collect().toList)</span><br><span class="line">    </span><br><span class="line">    println(<span class="string">&quot;************正确姿势****************&quot;</span>)</span><br><span class="line">    <span class="comment">//分布式环境，无法保证第一个执行的分区就是第一个分区</span></span><br><span class="line">    <span class="comment">//且分布式中没有顺序</span></span><br><span class="line">    <span class="keyword">val</span> partitionMaxWithIndex = rdd.mapPartitionsWithIndex(</span><br><span class="line">        (index, iter) =&gt; &#123;</span><br><span class="line">            <span class="keyword">if</span> (index == <span class="number">0</span>) &#123;</span><br><span class="line">                <span class="type">List</span>(iter.max).iterator</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="type">Nil</span>.iterator</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;)</span><br><span class="line">    println(partitionMaxWithIndex.collect().toList)</span><br><span class="line">    </span><br><span class="line">    println(<span class="string">&quot;************第二个分区数据****************&quot;</span>)</span><br><span class="line">    <span class="comment">//分布式环境，无法保证第一个执行的分区就是第一个分区</span></span><br><span class="line">    <span class="comment">//且分布式中没有顺序</span></span><br><span class="line">    <span class="keyword">val</span> secondPartitionMaxWithIndex = rdd.mapPartitionsWithIndex(</span><br><span class="line">        (index, iter) =&gt; &#123;</span><br><span class="line">            <span class="keyword">if</span> (index == <span class="number">1</span>) &#123;</span><br><span class="line">                iter</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="type">Nil</span>.iterator</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;)</span><br><span class="line">    println(secondPartitionMaxWithIndex.collect().toList)</span><br><span class="line">    </span><br><span class="line">    sparkContext.stop()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>
</li>
<li><p>flatMap</p>
<ul>
<li>函数签名  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">flatMap</span></span>[<span class="type">U</span>: <span class="type">ClassTag</span>](f: <span class="type">T</span> =&gt; <span class="type">TraversableOnce</span>[<span class="type">U</span>]): <span class="type">RDD</span>[<span class="type">U</span>]</span><br></pre></td></tr></table></figure></li>
<li>函数说明<br>  将处理的数据进行扁平化后再进行映射处理，所以算子也称之为扁平映射  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> dataRDD = sparkContext.makeRDD(<span class="type">List</span>(</span><br><span class="line">    <span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>),<span class="type">List</span>(<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line">),<span class="number">1</span>)</span><br><span class="line"><span class="keyword">val</span> dataRDD1 = dataRDD.flatMap(</span><br><span class="line">    list =&gt; list</span><br><span class="line">)</span><br></pre></td></tr></table></figure></li>
<li>小功能：将List(List(1,2),3,List(4,5))进行扁平化操作  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">flatMapTest2</span> </span>= &#123;</span><br><span class="line">    println(<span class="string">&quot;************flatMapTest****************&quot;</span>)</span><br><span class="line">    <span class="comment">//构建环境</span></span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">&quot;WordCount&quot;</span>).setMaster(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">    <span class="keyword">val</span> rdd = sparkContext.makeRDD(<span class="type">List</span>(<span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>), <span class="number">4</span>, <span class="type">List</span>(<span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>)), <span class="number">2</span>)</span><br><span class="line">    <span class="keyword">val</span> flatMap = rdd.flatMap &#123;</span><br><span class="line">        <span class="keyword">case</span> seq: <span class="type">List</span>[_] =&gt; seq</span><br><span class="line">        <span class="keyword">case</span> other =&gt; <span class="type">List</span>(other)</span><br><span class="line">    &#125;</span><br><span class="line">    println(flatMap.collect().toList)</span><br><span class="line">    </span><br><span class="line">    sparkContext.stop()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>glom</p>
<ul>
<li>函数签名  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">glom</span></span>(): <span class="type">RDD</span>[<span class="type">Array</span>[<span class="type">T</span>]]</span><br></pre></td></tr></table></figure></li>
<li>函数说明<br>  将同一个分区的数据直接转换为相同类型的内存数组进行处理，分区不变  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> dataRDD : <span class="type">RDD</span>[<span class="type">Int</span>] = sparkContext.makeRDD(<span class="type">List</span>(</span><br><span class="line">    <span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span></span><br><span class="line">),<span class="number">1</span>)</span><br><span class="line"><span class="keyword">val</span> dataRDD1:<span class="type">RDD</span>[<span class="type">Array</span>[<span class="type">Int</span>]] = dataRDD.glom()</span><br></pre></td></tr></table></figure></li>
<li>小功能：计算所有分区最大值求和（分区内取最大值，分区间最大值求和）  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">glomTest3</span> </span>= &#123;</span><br><span class="line">    println(<span class="string">&quot;************glom****************&quot;</span>)</span><br><span class="line">    <span class="comment">//构建环境</span></span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">&quot;WordCount&quot;</span>).setMaster(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">    <span class="keyword">val</span> summ: <span class="type">Int</span> = sparkContext</span><br><span class="line">        .makeRDD(<span class="type">List</span>(<span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>), <span class="type">List</span>(<span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>), <span class="type">List</span>(<span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>)), <span class="number">2</span>)</span><br><span class="line">        .glom()</span><br><span class="line">        .map(</span><br><span class="line">            array =&gt; &#123;</span><br><span class="line">                <span class="keyword">var</span> sum = <span class="number">0</span></span><br><span class="line">                array.foreach(</span><br><span class="line">                    list =&gt; &#123;</span><br><span class="line">                        list.foreach(</span><br><span class="line">                            num =&gt; &#123;</span><br><span class="line">                                sum += num</span><br><span class="line">                            &#125;</span><br><span class="line">                        )</span><br><span class="line">                    &#125;</span><br><span class="line">                )</span><br><span class="line">                sum</span><br><span class="line">            &#125;</span><br><span class="line">        )</span><br><span class="line">        .collect()</span><br><span class="line">        .sum</span><br><span class="line">    println(summ)</span><br><span class="line">    sparkContext.stop()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>groupBy</p>
<ul>
<li>函数签名  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">groupBy</span></span>[<span class="type">K</span>](f: <span class="type">T</span> =&gt; <span class="type">K</span>)(<span class="keyword">implicit</span> kt: <span class="type">ClassTag</span>[<span class="type">K</span>]): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">Iterable</span>[<span class="type">T</span>])]</span><br></pre></td></tr></table></figure></li>
<li>函数说明<br>  将数据根据指定的规则进行分组, 分区默认不变，但是数据会被打乱重新组合，我们将这样的操作称之为shuffle。极限情况下，数据可能被分在同一个分区中<br>  一个组的数据在一个分区中，但是并不是说一个分区中只有一个组  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> dataRDD = sparkContext.makeRDD(<span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>),<span class="number">1</span>)</span><br><span class="line"><span class="keyword">val</span> dataRDD1 = dataRDD.groupBy(</span><br><span class="line">    _%<span class="number">2</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure></li>
<li>小功能：<ul>
<li>将List(“Hello”, “hive”, “hbase”, “Hadoop”)根据单词首写字母进行分组。</li>
<li>从服务器日志数据apache.log中获取每个时间段访问量。</li>
<li>WordCount。<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">groupByTest2</span> </span>= &#123;</span><br><span class="line">    println(<span class="string">&quot;****************************&quot;</span>)</span><br><span class="line">    <span class="comment">//构建环境</span></span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">&quot;WordCount&quot;</span>).setMaster(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">    <span class="keyword">val</span> rdd = sparkContext.makeRDD(<span class="type">List</span>(<span class="string">&quot;Hello&quot;</span>, <span class="string">&quot;hive&quot;</span>, <span class="string">&quot;hbase&quot;</span>, <span class="string">&quot;Hadoop&quot;</span>))</span><br><span class="line">    <span class="keyword">val</span> groupBy: <span class="type">RDD</span>[(<span class="type">Char</span>, <span class="type">Iterable</span>[<span class="type">String</span>])] = rdd.groupBy(str =&gt; str.charAt(<span class="number">0</span>))</span><br><span class="line">    groupBy.collect().foreach(println)</span><br><span class="line">    sparkContext.stop()</span><br><span class="line">&#125;</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">groupByTest_apache_log</span> </span>= &#123;</span><br><span class="line">    println(<span class="string">&quot;****************************&quot;</span>)</span><br><span class="line">    <span class="comment">//构建环境</span></span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">&quot;WordCount&quot;</span>).setMaster(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">    </span><br><span class="line">    sparkContext</span><br><span class="line">        .textFile(<span class="string">&quot;data/apache.log&quot;</span>)</span><br><span class="line">        .map(</span><br><span class="line">            line=&gt;&#123;</span><br><span class="line">                line.split(<span class="string">&quot; &quot;</span>)(<span class="number">3</span>).split(<span class="string">&quot;:&quot;</span>)(<span class="number">1</span>)</span><br><span class="line">            &#125;</span><br><span class="line">        )</span><br><span class="line">        .groupBy(hour=&gt;hour)</span><br><span class="line">        .map&#123;</span><br><span class="line">            <span class="keyword">case</span>(hour,list)=&gt;&#123;</span><br><span class="line">                (hour,list.toList.size)</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        .collect()</span><br><span class="line">        .foreach(println)</span><br><span class="line">    sparkContext.stop()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>
</li>
<li><p>filter</p>
<ul>
<li>函数签名  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">filter</span></span>(f: <span class="type">T</span> =&gt; <span class="type">Boolean</span>): <span class="type">RDD</span>[<span class="type">T</span>]</span><br></pre></td></tr></table></figure></li>
<li>函数说明<br>  将数据根据指定的规则进行筛选过滤，符合规则的数据保留，不符合规则的数据丢弃。<br>  当数据进行筛选过滤后，分区不变，但是分区内的数据可能不均衡，生产环境下，可能会出现数据倾斜。  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> dataRDD = sparkContext.makeRDD(<span class="type">List</span>(</span><br><span class="line">    <span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span></span><br><span class="line">),<span class="number">1</span>)</span><br><span class="line"><span class="keyword">val</span> dataRDD1 = dataRDD.filter(_%<span class="number">2</span> == <span class="number">0</span>)</span><br></pre></td></tr></table></figure></li>
<li>小功能：从服务器日志数据apache.log中获取2015年5月17日的请求路径  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">filterTest_log</span> </span>= &#123;</span><br><span class="line">    println(<span class="string">&quot;****************************&quot;</span>)</span><br><span class="line">    <span class="comment">//构建环境</span></span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">&quot;WordCount&quot;</span>).setMaster(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">    sparkContext</span><br><span class="line">        .textFile(<span class="string">&quot;data/apache.log&quot;</span>)</span><br><span class="line">        .filter(line =&gt; &#123;</span><br><span class="line">            <span class="keyword">val</span> datas = line.split(<span class="string">&quot; &quot;</span>)</span><br><span class="line">            <span class="keyword">val</span> time = datas(<span class="number">3</span>)</span><br><span class="line">            time.startsWith(<span class="string">&quot;17/05/2015&quot;</span>)</span><br><span class="line">        &#125;)</span><br><span class="line">        .map(line =&gt; &#123;</span><br><span class="line">            line.split(<span class="string">&quot; &quot;</span>)(<span class="number">6</span>)</span><br><span class="line">        &#125;)</span><br><span class="line">        .collect()</span><br><span class="line">        .toList</span><br><span class="line">        .distinct</span><br><span class="line">        .foreach(println)</span><br><span class="line">    sparkContext.stop()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>sample</p>
<ul>
<li>函数签名  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sample</span></span>(</span><br><span class="line">  withReplacement: <span class="type">Boolean</span>,</span><br><span class="line">  fraction: <span class="type">Double</span>,</span><br><span class="line">  seed: <span class="type">Long</span> = <span class="type">Utils</span>.random.nextLong): <span class="type">RDD</span>[<span class="type">T</span>]</span><br></pre></td></tr></table></figure></li>
<li>函数说明<br>  根据指定的规则从数据集中抽取数据  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> dataRDD = sparkContext.makeRDD(<span class="type">List</span>(</span><br><span class="line">    <span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span></span><br><span class="line">),<span class="number">1</span>)</span><br><span class="line"><span class="comment">// 抽取数据不放回（伯努利算法）</span></span><br><span class="line"><span class="comment">// 伯努利算法：又叫0、1分布。例如扔硬币，要么正面，要么反面。</span></span><br><span class="line"><span class="comment">// 具体实现：根据种子和随机算法算出一个数和第二个参数设置几率比较，小于第二个参数要，大于不要</span></span><br><span class="line"><span class="comment">// 第一个参数：抽取的数据是否放回，false：不放回</span></span><br><span class="line"><span class="comment">// 第二个参数：抽取的几率，范围在[0,1]之间,0：全不取；1：全取；</span></span><br><span class="line"><span class="comment">// 第三个参数：随机数种子</span></span><br><span class="line"><span class="keyword">val</span> dataRDD1 = dataRDD.sample(<span class="literal">false</span>, <span class="number">0.5</span>)</span><br><span class="line"><span class="comment">// 抽取数据放回（泊松算法）</span></span><br><span class="line"><span class="comment">// 第一个参数：抽取的数据是否放回，true：放回；false：不放回</span></span><br><span class="line"><span class="comment">// 第二个参数：重复数据的几率，范围大于等于0.表示每一个元素被期望抽取到的次数</span></span><br><span class="line"><span class="comment">// 第三个参数：随机数种子</span></span><br><span class="line"><span class="keyword">val</span> dataRDD2 = dataRDD.sample(<span class="literal">true</span>, <span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>思考一个问题：有啥用，抽奖吗？</p>
<ul>
<li>抽样  </li>
</ul>
</blockquote>
</li>
</ul>
</li>
<li><p>distinct</p>
<ul>
<li>函数签名  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">distinct</span></span>()(<span class="keyword">implicit</span> ord: <span class="type">Ordering</span>[<span class="type">T</span>] = <span class="literal">null</span>): <span class="type">RDD</span>[<span class="type">T</span>]</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">distinct</span></span>(numPartitions: <span class="type">Int</span>)(<span class="keyword">implicit</span> ord: <span class="type">Ordering</span>[<span class="type">T</span>] = <span class="literal">null</span>): <span class="type">RDD</span>[<span class="type">T</span>]</span><br></pre></td></tr></table></figure></li>
<li>函数说明<br>  将数据集中重复的数据去重  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> dataRDD = sparkContext.makeRDD(<span class="type">List</span>(</span><br><span class="line">    <span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">1</span>,<span class="number">2</span></span><br><span class="line">),<span class="number">1</span>)</span><br><span class="line"><span class="keyword">val</span> dataRDD1 = dataRDD.distinct()</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> dataRDD2 = dataRDD.distinct(<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>思考一个问题：如果不用该算子，你有什么办法实现数据去重？</p>
<ul>
<li>map</li>
<li>set</li>
</ul>
</blockquote>
</li>
</ul>
</li>
<li><p>coalesce</p>
<ul>
<li>函数签名  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">coalesce</span></span>(numPartitions: <span class="type">Int</span>, shuffle: <span class="type">Boolean</span> = <span class="literal">false</span>,</span><br><span class="line">           partitionCoalescer: <span class="type">Option</span>[<span class="type">PartitionCoalescer</span>] = <span class="type">Option</span>.empty)</span><br><span class="line">          (<span class="keyword">implicit</span> ord: <span class="type">Ordering</span>[<span class="type">T</span>] = <span class="literal">null</span>)</span><br><span class="line">  : <span class="type">RDD</span>[<span class="type">T</span>]</span><br></pre></td></tr></table></figure></li>
<li>函数说明<br>  根据数据量缩减分区，用于大数据集过滤后，提高小数据集的执行效率<br>  当spark程序中，存在过多的小任务的时候，可以通过coalesce方法，收缩合并分区，减少分区的个数，减小任务调度成本  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> dataRDD = sparkContext.makeRDD(<span class="type">List</span>(</span><br><span class="line">    <span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">1</span>,<span class="number">2</span></span><br><span class="line">),<span class="number">6</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> dataRDD1 = dataRDD.coalesce(<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>思考一个问题：我想要扩大分区，怎么办？</p>
<ul>
<li>repartition</li>
</ul>
</blockquote>
</li>
</ul>
</li>
<li><p>repartition</p>
<ul>
<li>函数签名  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">repartition</span></span>(numPartitions: <span class="type">Int</span>)(<span class="keyword">implicit</span> ord: <span class="type">Ordering</span>[<span class="type">T</span>] = <span class="literal">null</span>): <span class="type">RDD</span>[<span class="type">T</span>]</span><br></pre></td></tr></table></figure></li>
<li>函数说明<br>  该操作内部其实执行的是coalesce操作，参数shuffle的默认值为true。无论是将分区数多的RDD转换为分区数少的RDD，还是将分区数少的RDD转换为分区数多的RDD，repartition操作都可以完成，因为无论如何都会经shuffle过程。  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> dataRDD = sparkContext.makeRDD(<span class="type">List</span>(</span><br><span class="line">    <span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">1</span>,<span class="number">2</span></span><br><span class="line">),<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> dataRDD1 = dataRDD.repartition(<span class="number">4</span>)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>思考一个问题：coalesce和repartition区别？</p>
</blockquote>
</li>
</ul>
</li>
<li><p>sortBy</p>
<ul>
<li>函数签名  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sortBy</span></span>[<span class="type">K</span>](</span><br><span class="line">  f: (<span class="type">T</span>) =&gt; <span class="type">K</span>,</span><br><span class="line">  ascending: <span class="type">Boolean</span> = <span class="literal">true</span>,</span><br><span class="line">  numPartitions: <span class="type">Int</span> = <span class="keyword">this</span>.partitions.length)</span><br><span class="line">  (<span class="keyword">implicit</span> ord: <span class="type">Ordering</span>[<span class="type">K</span>], ctag: <span class="type">ClassTag</span>[<span class="type">K</span>]): <span class="type">RDD</span>[<span class="type">T</span>]</span><br></pre></td></tr></table></figure></li>
<li>函数说明<br>  该操作用于排序数据。在排序之前，可以将数据通过f函数进行处理，之后按照f函数处理的结果进行排序，默认为升序排列。排序后新产生的RDD的分区数与原RDD的分区数一致。中间存在shuffle的过程  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> dataRDD = sparkContext.makeRDD(<span class="type">List</span>(</span><br><span class="line">    <span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">1</span>,<span class="number">2</span></span><br><span class="line">),<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> dataRDD1 = dataRDD.sortBy(num=&gt;num, <span class="literal">false</span>, <span class="number">4</span>)</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ol>
<h5 id="5-1-4-3-2-双Value类型"><a href="#5-1-4-3-2-双Value类型" class="headerlink" title="5.1.4.3.2 双Value类型"></a>5.1.4.3.2 双Value类型</h5><ol>
<li>intersection<ul>
<li>函数签名  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">intersection</span></span>(other: <span class="type">RDD</span>[<span class="type">T</span>]): <span class="type">RDD</span>[<span class="type">T</span>]</span><br></pre></td></tr></table></figure></li>
<li>函数说明  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 对源RDD和参数RDD求交集后返回一个新的RDD</span></span><br><span class="line"><span class="keyword">val</span> dataRDD1 = sparkContext.makeRDD(<span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>))</span><br><span class="line"><span class="keyword">val</span> dataRDD2 = sparkContext.makeRDD(<span class="type">List</span>(<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>))</span><br><span class="line"><span class="keyword">val</span> dataRDD = dataRDD1.intersection(dataRDD2)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>思考一个问题：如果两个RDD数据类型不一致怎么办？</p>
<ul>
<li>Cannot resolve overloaded method ‘intersection’</li>
<li>方法声明的泛型，必须和调用方数据类型一致</li>
</ul>
</blockquote>
</li>
</ul>
</li>
<li>union<ul>
<li>函数签名  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">union</span></span>(other: <span class="type">RDD</span>[<span class="type">T</span>]): <span class="type">RDD</span>[<span class="type">T</span>]</span><br></pre></td></tr></table></figure></li>
<li>函数说明  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//对源RDD和参数RDD求并集后返回一个新的RDD</span></span><br><span class="line"><span class="keyword">val</span> dataRDD1 = sparkContext.makeRDD(<span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>))</span><br><span class="line"><span class="keyword">val</span> dataRDD2 = sparkContext.makeRDD(<span class="type">List</span>(<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>))</span><br><span class="line"><span class="keyword">val</span> dataRDD = dataRDD1.union(dataRDD2)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>思考一个问题：如果两个RDD数据类型不一致怎么办？</p>
<ul>
<li>type mismatch;</li>
<li>方法声明的泛型，必须和调用方数据类型一致</li>
</ul>
</blockquote>
</li>
</ul>
</li>
<li>subtract<ul>
<li>函数签名  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">subtract</span></span>(other: <span class="type">RDD</span>[<span class="type">T</span>]): <span class="type">RDD</span>[<span class="type">T</span>]</span><br></pre></td></tr></table></figure></li>
<li>函数说明  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//以一个RDD元素为主，去除两个RDD中重复元素，将其他元素保留下来。求差集</span></span><br><span class="line"><span class="keyword">val</span> dataRDD1 = sparkContext.makeRDD(<span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>))</span><br><span class="line"><span class="keyword">val</span> dataRDD2 = sparkContext.makeRDD(<span class="type">List</span>(<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>))</span><br><span class="line"><span class="keyword">val</span> dataRDD = dataRDD1.subtract(dataRDD2)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>思考一个问题：如果两个RDD数据类型不一致怎么办？</p>
<ul>
<li>Cannot resolve overloaded method ‘subtract’</li>
<li>方法声明的泛型，必须和调用方数据类型一致</li>
</ul>
</blockquote>
</li>
</ul>
</li>
<li>zip<ul>
<li>函数签名  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">zip</span></span>[<span class="type">U</span>: <span class="type">ClassTag</span>](other: <span class="type">RDD</span>[<span class="type">U</span>]): <span class="type">RDD</span>[(<span class="type">T</span>, <span class="type">U</span>)]</span><br></pre></td></tr></table></figure></li>
<li>函数说明  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//将两个RDD中的元素，以键值对的形式进行合并。其中，键值对中的Key为第1个RDD中的元素，Value为第2个RDD中的相同位置的元素。</span></span><br><span class="line"><span class="keyword">val</span> dataRDD1 = sparkContext.makeRDD(<span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>))</span><br><span class="line"><span class="keyword">val</span> dataRDD2 = sparkContext.makeRDD(<span class="type">List</span>(<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>))</span><br><span class="line"><span class="keyword">val</span> dataRDD = dataRDD1.zip(dataRDD2)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>思考一个问题：</p>
<ul>
<li>如果两个RDD数据类型不一致怎么办？<ul>
<li>没影响</li>
</ul>
</li>
<li>如果两个RDD数据分区不一致怎么办？<ul>
<li> Can only zip RDDs with same number of elements in each partition</li>
</ul>
</li>
<li>如果两个RDD分区数据数量不一致怎么办？<ul>
<li>Can’t zip RDDs with unequal numbers of partitions: List(2, 3)</li>
</ul>
</li>
</ul>
</blockquote>
</li>
</ul>
</li>
</ol>
<h5 id="5-1-4-3-3-Key-Value类型"><a href="#5-1-4-3-3-Key-Value类型" class="headerlink" title="5.1.4.3.3 Key-Value类型"></a>5.1.4.3.3 Key-Value类型</h5><ol>
<li><p>partitionBy</p>
<ul>
<li>函数签名  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">partitionBy</span></span>(partitioner: <span class="type">Partitioner</span>): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">V</span>)]</span><br></pre></td></tr></table></figure></li>
<li>函数说明  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//将数据按照指定Partitioner重新进行分区。Spark默认的分区器是HashPartitioner</span></span><br><span class="line"><span class="keyword">val</span> rdd: <span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">String</span>)] =</span><br><span class="line">    sc.makeRDD(<span class="type">Array</span>((<span class="number">1</span>,<span class="string">&quot;aaa&quot;</span>),(<span class="number">2</span>,<span class="string">&quot;bbb&quot;</span>),(<span class="number">3</span>,<span class="string">&quot;ccc&quot;</span>)),<span class="number">3</span>)</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">HashPartitioner</span></span><br><span class="line"><span class="keyword">val</span> rdd2: <span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">String</span>)] =</span><br><span class="line">    rdd.partitionBy(<span class="keyword">new</span> <span class="type">HashPartitioner</span>(<span class="number">2</span>))</span><br></pre></td></tr></table></figure>
<blockquote>
<p>思考一个问题：</p>
<ul>
<li>如果重分区的分区器和当前RDD的分区器一样怎么办？<ul>
<li>不怎么办</li>
</ul>
</li>
<li>Spark还有其他分区器吗？<ul>
<li>Spark采用Hadoop的分区器</li>
</ul>
</li>
<li>如果想按照自己的方法进行数据分区怎么办？<ul>
<li>同Hadoop自定义分区器</li>
</ul>
</li>
</ul>
</blockquote>
</li>
</ul>
</li>
<li><p>reduceByKey </p>
<ul>
<li>函数签名  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reduceByKey</span></span>(func: (<span class="type">V</span>, <span class="type">V</span>) =&gt; <span class="type">V</span>): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">V</span>)]</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reduceByKey</span></span>(func: (<span class="type">V</span>, <span class="type">V</span>) =&gt; <span class="type">V</span>, numPartitions: <span class="type">Int</span>): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">V</span>)]</span><br></pre></td></tr></table></figure></li>
<li>函数说明<br>  可以将数据按照相同的Key对Value进行聚合  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> dataRDD1 = sparkContext.makeRDD(<span class="type">List</span>((<span class="string">&quot;a&quot;</span>,<span class="number">1</span>),(<span class="string">&quot;b&quot;</span>,<span class="number">2</span>),(<span class="string">&quot;c&quot;</span>,<span class="number">3</span>)))</span><br><span class="line"><span class="keyword">val</span> dataRDD2 = dataRDD1.reduceByKey(_+_)</span><br><span class="line"><span class="keyword">val</span> dataRDD3 = dataRDD1.reduceByKey(_+_, <span class="number">2</span>)</span><br></pre></td></tr></table></figure></li>
<li>小功能：WordCount  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reduceByKey</span> </span>= &#123;</span><br><span class="line">    println(<span class="string">&quot;*************reduceByKey***************&quot;</span>)</span><br><span class="line">    <span class="comment">//构建环境</span></span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">&quot;WordCount&quot;</span>).setMaster(<span class="string">&quot;local&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">val</span> rdd2: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = sparkContext.makeRDD(<span class="type">List</span>(</span><br><span class="line">        (<span class="string">&quot;a&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">2</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">3</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">4</span>)</span><br><span class="line">    ))</span><br><span class="line">    </span><br><span class="line">    rdd2</span><br><span class="line">        .reduceByKey(_ + _)</span><br><span class="line">        .collect()</span><br><span class="line">        .foreach(println)</span><br><span class="line">    sparkContext.stop()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>groupByKey </p>
<ul>
<li>函数签名  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">groupByKey</span></span>(): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">Iterable</span>[<span class="type">V</span>])]</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">groupByKey</span></span>(numPartitions: <span class="type">Int</span>): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">Iterable</span>[<span class="type">V</span>])]</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">groupByKey</span></span>(partitioner: <span class="type">Partitioner</span>): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">Iterable</span>[<span class="type">V</span>])]</span><br></pre></td></tr></table></figure></li>
<li>函数说明<br>  将数据源的数据根据key对value进行分组  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> dataRDD1 =</span><br><span class="line">    sparkContext.makeRDD(<span class="type">List</span>((<span class="string">&quot;a&quot;</span>,<span class="number">1</span>),(<span class="string">&quot;b&quot;</span>,<span class="number">2</span>),(<span class="string">&quot;c&quot;</span>,<span class="number">3</span>)))</span><br><span class="line"><span class="keyword">val</span> dataRDD2 = dataRDD1.groupByKey()</span><br><span class="line"><span class="keyword">val</span> dataRDD3 = dataRDD1.groupByKey(<span class="number">2</span>)</span><br><span class="line"><span class="keyword">val</span> dataRDD4 = dataRDD1.groupByKey(<span class="keyword">new</span> <span class="type">HashPartitioner</span>(<span class="number">2</span>))</span><br></pre></td></tr></table></figure></li>
<li>小功能：WordCount  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">groupByKey</span> </span>= &#123;</span><br><span class="line">    println(<span class="string">&quot;*************groupByKey***************&quot;</span>)</span><br><span class="line">    <span class="comment">//构建环境</span></span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">&quot;WordCount&quot;</span>).setMaster(<span class="string">&quot;local&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> rdd2: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = sparkContext.makeRDD(<span class="type">List</span>(</span><br><span class="line">        (<span class="string">&quot;a&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">2</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">3</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">4</span>)</span><br><span class="line">    ))</span><br><span class="line"></span><br><span class="line">    rdd2</span><br><span class="line">        .groupByKey()</span><br><span class="line">        .mapValues(iter =&gt; iter.sum)</span><br><span class="line">        .collect()</span><br><span class="line">        .foreach(println)</span><br><span class="line">    sparkContext.stop()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>思考一个问题：reduceByKey和groupByKey的区别？</p>
<ul>
<li>从shuffle的角度：reduceByKey和groupByKey都存在shuffle的操作，但是reduceByKey可以在shuffle前对分区内相同key的数据进行预聚合（combine）功能，这样会减少落盘的数据量，而groupByKey只是进行分组，不存在数据量减少的问题，所以reduceByKey性能比较高。</li>
<li>从功能的角度：reduceByKey其实包含分组和聚合的功能。groupByKey只能分组，不能聚合，所以在分组聚合的场合下，推荐使用reduceByKey，如果仅仅是分组而不需要聚合。那么还是只能使用groupByKey</li>
</ul>
</blockquote>
</li>
</ul>
</li>
<li><p>aggregateByKey </p>
<ul>
<li>函数签名  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">aggregateByKey</span></span>[<span class="type">U</span>: <span class="type">ClassTag</span>](zeroValue: <span class="type">U</span>)(seqOp: (<span class="type">U</span>, <span class="type">V</span>) =&gt; <span class="type">U</span>,</span><br><span class="line">    combOp: (<span class="type">U</span>, <span class="type">U</span>) =&gt; <span class="type">U</span>): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">U</span>)]</span><br></pre></td></tr></table></figure></li>
<li>函数说明<br>  将数据根据不同的规则进行分区内计算和分区间计算  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> dataRDD1 =</span><br><span class="line">    sparkContext.makeRDD(<span class="type">List</span>((<span class="string">&quot;a&quot;</span>,<span class="number">1</span>),(<span class="string">&quot;b&quot;</span>,<span class="number">2</span>),(<span class="string">&quot;c&quot;</span>,<span class="number">3</span>)))</span><br><span class="line"><span class="keyword">val</span> dataRDD2 =</span><br><span class="line">    dataRDD1.aggregateByKey(<span class="number">0</span>)(_+_,_+_)</span><br></pre></td></tr></table></figure>
  取出每个分区内相同key的最大值然后分区间相加  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// TODO : 取出每个分区内相同key的最大值然后分区间相加</span></span><br><span class="line"><span class="comment">// aggregateByKey算子是函数柯里化，存在两个参数列表</span></span><br><span class="line"><span class="comment">// 1. 第一个参数列表中的参数表示初始值</span></span><br><span class="line"><span class="comment">// 2. 第二个参数列表中含有两个参数</span></span><br><span class="line"><span class="comment">//    2.1 第一个参数表示分区内的计算规则</span></span><br><span class="line"><span class="comment">//    2.2 第二个参数表示分区间的计算规则</span></span><br><span class="line"><span class="keyword">val</span> rdd =</span><br><span class="line">    sc.makeRDD(<span class="type">List</span>(</span><br><span class="line">        (<span class="string">&quot;a&quot;</span>,<span class="number">1</span>),(<span class="string">&quot;a&quot;</span>,<span class="number">2</span>),(<span class="string">&quot;c&quot;</span>,<span class="number">3</span>),</span><br><span class="line">        (<span class="string">&quot;b&quot;</span>,<span class="number">4</span>),(<span class="string">&quot;c&quot;</span>,<span class="number">5</span>),(<span class="string">&quot;c&quot;</span>,<span class="number">6</span>)</span><br><span class="line">    ),<span class="number">2</span>)</span><br><span class="line"><span class="comment">// 0:(&quot;a&quot;,1),(&quot;a&quot;,2),(&quot;c&quot;,3) =&gt; (a,10)(c,10)</span></span><br><span class="line"><span class="comment">//                                         =&gt; (a,10)(b,10)(c,20)</span></span><br><span class="line"><span class="comment">// 1:(&quot;b&quot;,4),(&quot;c&quot;,5),(&quot;c&quot;,6) =&gt; (b,10)(c,10)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> resultRDD =</span><br><span class="line">    rdd.aggregateByKey(<span class="number">10</span>)(</span><br><span class="line">        (x, y) =&gt; math.max(x,y),</span><br><span class="line">        (x, y) =&gt; x + y</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">resultRDD.collect().foreach(println)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>思考一个问题：分区内计算规则和分区间计算规则相同怎么办？（WordCount）</p>
<ul>
<li>不怎么办，等同于foldByKey</li>
</ul>
</blockquote>
</li>
</ul>
</li>
<li><p>foldByKey </p>
<ul>
<li>函数签名  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">foldByKey</span></span>(zeroValue: <span class="type">V</span>)(func: (<span class="type">V</span>, <span class="type">V</span>) =&gt; <span class="type">V</span>): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">V</span>)]</span><br></pre></td></tr></table></figure></li>
<li>函数说明<br>  当分区内计算规则和分区间计算规则相同时，aggregateByKey就可以简化为foldByKey  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> dataRDD1 = sparkContext.makeRDD(<span class="type">List</span>((<span class="string">&quot;a&quot;</span>,<span class="number">1</span>),(<span class="string">&quot;b&quot;</span>,<span class="number">2</span>),(<span class="string">&quot;c&quot;</span>,<span class="number">3</span>)))</span><br><span class="line"><span class="keyword">val</span> dataRDD2 = dataRDD1.foldByKey(<span class="number">0</span>)(_+_)</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>combineByKey </p>
<ul>
<li>函数签名  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">combineByKey</span></span>[<span class="type">C</span>](</span><br><span class="line">  createCombiner: <span class="type">V</span> =&gt; <span class="type">C</span>,</span><br><span class="line">  mergeValue: (<span class="type">C</span>, <span class="type">V</span>) =&gt; <span class="type">C</span>,</span><br><span class="line">  mergeCombiners: (<span class="type">C</span>, <span class="type">C</span>) =&gt; <span class="type">C</span>): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">C</span>)]</span><br></pre></td></tr></table></figure></li>
<li>函数说明<br>  最通用的对key-value型rdd进行聚集操作的聚集函数（aggregation function）。类似于aggregate()，combineByKey()允许用户返回值的类型与输入不一致。</li>
<li>小功能：数据List((“a”, 88), (“b”, 95), (“a”, 91), (“b”, 93), (“a”, 95), (“b”, 98))求每个key的平均值  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> list: <span class="type">List</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">List</span>((<span class="string">&quot;a&quot;</span>, <span class="number">88</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">95</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">91</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">93</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">95</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">98</span>))</span><br><span class="line"><span class="keyword">val</span> input: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = sc.makeRDD(list, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> combineRdd: <span class="type">RDD</span>[(<span class="type">String</span>, (<span class="type">Int</span>, <span class="type">Int</span>))] = input.combineByKey(</span><br><span class="line">    (_, <span class="number">1</span>),</span><br><span class="line">    (acc: (<span class="type">Int</span>, <span class="type">Int</span>), v) =&gt; (acc._1 + v, acc._2 + <span class="number">1</span>),</span><br><span class="line">    (acc1: (<span class="type">Int</span>, <span class="type">Int</span>), acc2: (<span class="type">Int</span>, <span class="type">Int</span>)) =&gt; (acc1._1 + acc2._1, acc1._2 + acc2._2)</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>思考一个问题：reduceByKey、foldByKey、aggregateByKey、combineByKey的区别？</p>
<ul>
<li>reduceByKey: 相同key的第一个数据不进行任何计算，分区内和分区间计算规则相同</li>
<li>foldByKey: 相同key的第一个数据和初始值进行分区内计算，分区内和分区间计算规则相同</li>
<li>aggregateByKey：相同key的第一个数据和初始值进行分区内计算，分区内和分区间计算规则可以不相同</li>
<li>combineByKey:当计算时，发现数据结构不满足要求时，可以让第一个数据转换结构。分区内和分区间计算规则不相同。</li>
</ul>
</blockquote>
</li>
</ul>
</li>
<li><p>sortByKey</p>
<ul>
<li>函数签名  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sortByKey</span></span>(ascending: <span class="type">Boolean</span> = <span class="literal">true</span>, numPartitions: <span class="type">Int</span> = self.partitions.length)</span><br><span class="line">  : <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">V</span>)]</span><br></pre></td></tr></table></figure></li>
<li>函数说明<br>  在一个(K,V)的RDD上调用，K必须实现Ordered接口(特质)，返回一个按照key进行排序的  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> dataRDD1 = sparkContext.makeRDD(<span class="type">List</span>((<span class="string">&quot;a&quot;</span>,<span class="number">1</span>),(<span class="string">&quot;b&quot;</span>,<span class="number">2</span>),(<span class="string">&quot;c&quot;</span>,<span class="number">3</span>)))</span><br><span class="line"><span class="keyword">val</span> sortRDD1: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = dataRDD1.sortByKey(<span class="literal">true</span>)</span><br><span class="line"><span class="keyword">val</span> sortRDD1: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = dataRDD1.sortByKey(<span class="literal">false</span>)</span><br></pre></td></tr></table></figure></li>
<li>小功能：设置key为自定义类User  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sortByKey_user</span> </span>= &#123;</span><br><span class="line">    println(<span class="string">&quot;*************sortByKey***************&quot;</span>)</span><br><span class="line">    <span class="comment">//构建环境</span></span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">&quot;WordCount&quot;</span>).setMaster(<span class="string">&quot;local&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> rdd: <span class="type">RDD</span>[(<span class="type">User</span>, <span class="type">Int</span>)] = sparkContext.makeRDD(<span class="type">List</span>(</span><br><span class="line">        (<span class="keyword">new</span> <span class="type">User</span>(), <span class="number">1</span>), (<span class="keyword">new</span> <span class="type">User</span>(), <span class="number">2</span>), (<span class="keyword">new</span> <span class="type">User</span>(), <span class="number">3</span>), (<span class="keyword">new</span> <span class="type">User</span>(), <span class="number">4</span>)</span><br><span class="line">    ))</span><br><span class="line">    rdd</span><br><span class="line">        .sortByKey()</span><br><span class="line">        .collect()</span><br><span class="line">        .foreach(println)</span><br><span class="line">    sparkContext.stop()</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">User</span> <span class="keyword">extends</span> <span class="title">Ordered</span>[<span class="type">User</span>] </span>&#123;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">compare</span></span>(that: <span class="type">User</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">        <span class="number">0</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>join</p>
<ul>
<li>函数签名  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">join</span></span>[<span class="type">W</span>](other: <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">W</span>)]): <span class="type">RDD</span>[(<span class="type">K</span>, (<span class="type">V</span>, <span class="type">W</span>))]</span><br></pre></td></tr></table></figure></li>
<li>函数说明<br>  在类型为(K,V)和(K,W)的RDD上调用，返回一个相同key对应的所有元素连接在一起的(K,(V,W))的RDD  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd: <span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">String</span>)] = sc.makeRDD(<span class="type">Array</span>((<span class="number">1</span>, <span class="string">&quot;a&quot;</span>), (<span class="number">2</span>, <span class="string">&quot;b&quot;</span>), (<span class="number">3</span>, <span class="string">&quot;c&quot;</span>)))</span><br><span class="line"><span class="keyword">val</span> rdd1: <span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">Int</span>)] = sc.makeRDD(<span class="type">Array</span>((<span class="number">1</span>, <span class="number">4</span>), (<span class="number">2</span>, <span class="number">5</span>), (<span class="number">3</span>, <span class="number">6</span>)))</span><br><span class="line">rdd.join(rdd1).collect().foreach(println)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>思考一个问题：如果key存在不相等呢？</p>
<ul>
<li>不相等的关联不到</li>
</ul>
</blockquote>
</li>
</ul>
</li>
<li><p>leftOuterJoin</p>
<ul>
<li>函数签名  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">leftOuterJoin</span></span>[<span class="type">W</span>](other: <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">W</span>)]): <span class="type">RDD</span>[(<span class="type">K</span>, (<span class="type">V</span>, <span class="type">Option</span>[<span class="type">W</span>]))]</span><br></pre></td></tr></table></figure></li>
<li>函数说明<br>  类似于SQL语句的左外连接  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> dataRDD1 = sparkContext.makeRDD(<span class="type">List</span>((<span class="string">&quot;a&quot;</span>,<span class="number">1</span>),(<span class="string">&quot;b&quot;</span>,<span class="number">2</span>),(<span class="string">&quot;c&quot;</span>,<span class="number">3</span>)))</span><br><span class="line"><span class="keyword">val</span> dataRDD2 = sparkContext.makeRDD(<span class="type">List</span>((<span class="string">&quot;a&quot;</span>,<span class="number">1</span>),(<span class="string">&quot;b&quot;</span>,<span class="number">2</span>),(<span class="string">&quot;c&quot;</span>,<span class="number">3</span>)))</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> rdd: <span class="type">RDD</span>[(<span class="type">String</span>, (<span class="type">Int</span>, <span class="type">Option</span>[<span class="type">Int</span>]))] = dataRDD1.leftOuterJoin(dataRDD2)</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>cogroup</p>
<ul>
<li>函数签名  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cogroup</span></span>[<span class="type">W</span>](other: <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">W</span>)]): <span class="type">RDD</span>[(<span class="type">K</span>, (<span class="type">Iterable</span>[<span class="type">V</span>], <span class="type">Iterable</span>[<span class="type">W</span>]))]</span><br></pre></td></tr></table></figure></li>
<li>函数说明<br>  在类型为(K,V)和(K,W)的RDD上调用，返回一个(K,(Iterable<V>,Iterable<W>))类型的RDD  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> dataRDD1 = sparkContext.makeRDD(<span class="type">List</span>((<span class="string">&quot;a&quot;</span>,<span class="number">1</span>),(<span class="string">&quot;a&quot;</span>,<span class="number">2</span>),(<span class="string">&quot;c&quot;</span>,<span class="number">3</span>)))</span><br><span class="line"><span class="keyword">val</span> dataRDD2 = sparkContext.makeRDD(<span class="type">List</span>((<span class="string">&quot;a&quot;</span>,<span class="number">1</span>),(<span class="string">&quot;c&quot;</span>,<span class="number">2</span>),(<span class="string">&quot;c&quot;</span>,<span class="number">3</span>)))</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> value: <span class="type">RDD</span>[(<span class="type">String</span>, (<span class="type">Iterable</span>[<span class="type">Int</span>], <span class="type">Iterable</span>[<span class="type">Int</span>]))] = </span><br><span class="line"></span><br><span class="line">dataRDD1.cogroup(dataRDD2)</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ol>
<h4 id="5-1-4-4-案例实操"><a href="#5-1-4-4-案例实操" class="headerlink" title="5.1.4.4 案例实操"></a>5.1.4.4 案例实操</h4><ol>
<li>数据准备<ul>
<li><a href="media/16382517322435/agent.log">agent.log</a></li>
<li>时间戳，省份，城市，用户，广告，中间字段使用空格分隔。</li>
</ul>
</li>
<li>需求描述<ul>
<li>统计出每一个省份每个广告被点击数量排行的Top3</li>
</ul>
</li>
<li>需求分析<ul>
<li>转换数据结构line =&gt;（（省份，广告），1） </li>
<li>聚合数据（（省份，广告），1） =&gt; （（省份，广告），count）</li>
<li>转换数据结构（（省份，广告），count）=&gt;（省份，（广告，count））</li>
<li>分组（省份，（广告，count））=&gt;（省份，List（（广告，count），（广告，count），·····））</li>
<li>排序取前三</li>
</ul>
</li>
<li>功能实现 <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">agent_log</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    println(<span class="string">&quot;*************agent.log***************&quot;</span>)</span><br><span class="line">    <span class="comment">//构建环境</span></span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">&quot;WordCount&quot;</span>).setMaster(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"></span><br><span class="line">    sparkContext</span><br><span class="line">        .textFile(<span class="string">&quot;data/agent.log&quot;</span>)</span><br><span class="line">        .map(line =&gt; &#123;</span><br><span class="line">            <span class="keyword">val</span> datas: <span class="type">Array</span>[<span class="type">String</span>] = line.split(<span class="string">&quot; &quot;</span>)</span><br><span class="line">            ((datas(<span class="number">1</span>), datas(<span class="number">4</span>)), <span class="number">1</span>)</span><br><span class="line">        &#125;)</span><br><span class="line">        .reduceByKey(_ + _)</span><br><span class="line">        .map &#123;</span><br><span class="line">            <span class="keyword">case</span> ((p, a), s) =&gt; (p, (a, s))</span><br><span class="line">        &#125;</span><br><span class="line">        .groupByKey()</span><br><span class="line">        .mapValues(</span><br><span class="line">            iter =&gt; &#123;</span><br><span class="line">                iter.toList.sortBy(_._2)(<span class="type">Ordering</span>.<span class="type">Int</span>.reverse).take(<span class="number">3</span>)</span><br><span class="line">            &#125;</span><br><span class="line">        )</span><br><span class="line">        .collect()</span><br><span class="line">        .foreach(println)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    sparkContext.stop()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ol>
<h4 id="5-1-4-5-RDD行动算子"><a href="#5-1-4-5-RDD行动算子" class="headerlink" title="5.1.4.5 RDD行动算子"></a>5.1.4.5 RDD行动算子</h4><ul>
<li>所谓行动算子就是通过调用RDD的方法，让RDD的计算开始执行，执行时会让整个RDD模型的逻辑全部执行；如果不调用行动算子，那么逻辑不会启动</li>
<li>转换算子是将旧的RDD转换为新的RDD，返回结果一定为RDD</li>
<li>行动算子是让RDD逻辑开始执行，返回结果为具体的值<ul>
<li>一个Spark程序中，可以生成作业，然后通过算子执行作业（Job），触发行动算子，一定会产生一个新的作业（Job）</li>
</ul>
</li>
</ul>
<ol>
<li><p>reduce</p>
<ul>
<li>函数签名  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reduce</span></span>(f: (<span class="type">T</span>, <span class="type">T</span>) =&gt; <span class="type">T</span>): <span class="type">T</span></span><br></pre></td></tr></table></figure></li>
<li>函数说明<br>  聚集RDD中的所有元素，先聚合分区内数据，再聚合分区间数据<br>  简化，规约，聚合  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd: <span class="type">RDD</span>[<span class="type">Int</span>] = sc.makeRDD(<span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>))</span><br><span class="line">    </span><br><span class="line"><span class="comment">// 聚合数据</span></span><br><span class="line"><span class="keyword">val</span> reduceResult: <span class="type">Int</span> = rdd.reduce(_+_)</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>collect</p>
<ul>
<li>函数签名  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">collect</span></span>(): <span class="type">Array</span>[<span class="type">T</span>]</span><br></pre></td></tr></table></figure></li>
<li>函数说明<br>  在驱动程序（Driver）中，以数组Array的形式返回数据集的所有元素  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd: <span class="type">RDD</span>[<span class="type">Int</span>] = sc.makeRDD(<span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>))</span><br><span class="line">    </span><br><span class="line"><span class="comment">// 收集数据到Driver</span></span><br><span class="line">rdd.collect().foreach(println)</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>count</p>
<ul>
<li>函数签名  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">count</span></span>(): <span class="type">Long</span></span><br></pre></td></tr></table></figure></li>
<li>函数说明<br>  返回RDD中元素的个数  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd: <span class="type">RDD</span>[<span class="type">Int</span>] = sc.makeRDD(<span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>))</span><br><span class="line">    </span><br><span class="line"><span class="comment">// 返回RDD中元素的个数</span></span><br><span class="line"><span class="keyword">val</span> countResult: <span class="type">Long</span> = rdd.count()</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>first</p>
<ul>
<li>函数签名  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">first</span></span>(): <span class="type">T</span></span><br></pre></td></tr></table></figure></li>
<li>函数说明<br>  返回RDD中的第一个元素  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd: <span class="type">RDD</span>[<span class="type">Int</span>] = sc.makeRDD(<span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">// 返回RDD中元素的个数</span></span><br><span class="line"><span class="keyword">val</span> firstResult: <span class="type">Int</span> = rdd.first()</span><br><span class="line">println(firstResult)</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>take</p>
<ul>
<li>函数签名  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">take</span></span>(num: <span class="type">Int</span>): <span class="type">Array</span>[<span class="type">T</span>]</span><br></pre></td></tr></table></figure></li>
<li>函数说明<br>  返回一个由RDD的前n个元素组成的数组  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">vval rdd: <span class="type">RDD</span>[<span class="type">Int</span>] = sc.makeRDD(<span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">// 返回RDD中元素的个数</span></span><br><span class="line"><span class="keyword">val</span> takeResult: <span class="type">Array</span>[<span class="type">Int</span>] = rdd.take(<span class="number">2</span>)</span><br><span class="line">println(takeResult.mkString(<span class="string">&quot;,&quot;</span>))</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>takeOrdered</p>
<ul>
<li>函数签名  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">takeOrdered</span></span>(num: <span class="type">Int</span>)(<span class="keyword">implicit</span> ord: <span class="type">Ordering</span>[<span class="type">T</span>]): <span class="type">Array</span>[<span class="type">T</span>]</span><br></pre></td></tr></table></figure></li>
<li>函数说明<br>  返回该RDD排序后的前n个元素组成的数组  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd: <span class="type">RDD</span>[<span class="type">Int</span>] = sc.makeRDD(<span class="type">List</span>(<span class="number">1</span>,<span class="number">3</span>,<span class="number">2</span>,<span class="number">4</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">// 返回RDD中元素的个数</span></span><br><span class="line"><span class="keyword">val</span> result: <span class="type">Array</span>[<span class="type">Int</span>] = rdd.takeOrdered(<span class="number">2</span>)</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>aggregate</p>
<ul>
<li>函数签名  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">aggregate</span></span>[<span class="type">U</span>: <span class="type">ClassTag</span>](zeroValue: <span class="type">U</span>)(seqOp: (<span class="type">U</span>, <span class="type">T</span>) =&gt; <span class="type">U</span>, combOp: (<span class="type">U</span>, <span class="type">U</span>) =&gt; <span class="type">U</span>): <span class="type">U</span></span><br></pre></td></tr></table></figure></li>
<li>函数说明<br>  分区的数据通过初始值和分区内的数据进行聚合，然后再和初始值进行分区间的数据聚合  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd: <span class="type">RDD</span>[<span class="type">Int</span>] = sc.makeRDD(<span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>), <span class="number">8</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 将该RDD所有元素相加得到结果</span></span><br><span class="line"><span class="comment">//val result: Int = rdd.aggregate(0)(_ + _, _ + _)</span></span><br><span class="line"><span class="keyword">val</span> result: <span class="type">Int</span> = rdd.aggregate(<span class="number">10</span>)(_ + _, _ + _)</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>fold</p>
<ul>
<li>函数签名  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fold</span></span>(zeroValue: <span class="type">T</span>)(op: (<span class="type">T</span>, <span class="type">T</span>) =&gt; <span class="type">T</span>): <span class="type">T</span></span><br></pre></td></tr></table></figure></li>
<li>函数说明<br>  折叠操作，aggregate的简化版操作  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd: <span class="type">RDD</span>[<span class="type">Int</span>] = sc.makeRDD(<span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> foldResult: <span class="type">Int</span> = rdd.fold(<span class="number">0</span>)(_+_)</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>countByKey</p>
<ul>
<li>函数签名  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">countByKey</span></span>(): <span class="type">Map</span>[<span class="type">K</span>, <span class="type">Long</span>]</span><br></pre></td></tr></table></figure></li>
<li>函数说明<br>  统计每种key的个数  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd: <span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">String</span>)] = sc.makeRDD(<span class="type">List</span>((<span class="number">1</span>, <span class="string">&quot;a&quot;</span>), (<span class="number">1</span>, <span class="string">&quot;a&quot;</span>), (<span class="number">1</span>, <span class="string">&quot;a&quot;</span>), (<span class="number">2</span>, <span class="string">&quot;b&quot;</span>), (<span class="number">3</span>, <span class="string">&quot;c&quot;</span>), (<span class="number">3</span>, <span class="string">&quot;c&quot;</span>)))</span><br><span class="line"></span><br><span class="line"><span class="comment">// 统计每种key的个数</span></span><br><span class="line"><span class="keyword">val</span> result: collection.<span class="type">Map</span>[<span class="type">Int</span>, <span class="type">Long</span>] = rdd.countByKey()</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ol>
<ol start="10">
<li><p>save相关算子</p>
<ul>
<li>函数签名  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">saveAsTextFile</span></span>(path: <span class="type">String</span>): <span class="type">Unit</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">saveAsObjectFile</span></span>(path: <span class="type">String</span>): <span class="type">Unit</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">saveAsSequenceFile</span></span>(</span><br><span class="line">  path: <span class="type">String</span>,</span><br><span class="line">  codec: <span class="type">Option</span>[<span class="type">Class</span>[_ &lt;: <span class="type">CompressionCodec</span>]] = <span class="type">None</span>): <span class="type">Unit</span></span><br></pre></td></tr></table></figure></li>
<li>函数说明<br>  将数据保存到不同格式的文件中  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 保存成Text文件</span></span><br><span class="line">rdd.saveAsTextFile(<span class="string">&quot;output&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 序列化成对象保存到文件</span></span><br><span class="line">rdd.saveAsObjectFile(<span class="string">&quot;output1&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 保存成Sequencefile文件</span></span><br><span class="line">rdd.map((_,<span class="number">1</span>)).saveAsSequenceFile(<span class="string">&quot;output2&quot;</span>)</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>foreach</p>
<ul>
<li>函数签名  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">foreach</span></span>(f: <span class="type">T</span> =&gt; <span class="type">Unit</span>): <span class="type">Unit</span> = withScope &#123;</span><br><span class="line">    <span class="keyword">val</span> cleanF = sc.clean(f)</span><br><span class="line">    sc.runJob(<span class="keyword">this</span>, (iter: <span class="type">Iterator</span>[<span class="type">T</span>]) =&gt; iter.foreach(cleanF))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>函数说明<br>  分布式遍历RDD中的每一个元素，调用指定函数  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd: <span class="type">RDD</span>[<span class="type">Int</span>] = sc.makeRDD(<span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">// 收集后打印</span></span><br><span class="line">rdd.map(num=&gt;num).collect().foreach(println)</span><br><span class="line"></span><br><span class="line">println(<span class="string">&quot;****************&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 分布式打印</span></span><br><span class="line">rdd.foreach(println)</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ol>
<h4 id="5-1-4-6-RDD序列化"><a href="#5-1-4-6-RDD序列化" class="headerlink" title="5.1.4.6 RDD序列化"></a>5.1.4.6 RDD序列化</h4><ol>
<li>闭包检查<br> 从计算的角度, 算子以外的代码都是在Driver端执行, 算子里面的代码都是在Executor端执行。那么在scala的函数式编程中，就会导致算子内经常会用到算子外的数据，这样就形成了闭包的效果，如果使用的算子外的数据无法序列化，就意味着无法传值给Executor端执行，就会发生错误，所以需要在执行任务计算前，检测闭包内的对象是否可以进行序列化，这个操作我们称之为闭包检测。Scala2.12版本后闭包编译方式发生了改变</li>
<li>序列化方法和属性<br> 从计算的角度, 算子以外的代码都是在Driver端执行, 算子里面的代码都是在Executor端执行，看如下代码： <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">serializable02_function</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="comment">//1.创建SparkConf并设置App名称</span></span><br><span class="line">        <span class="keyword">val</span> conf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">&quot;SparkCoreTest&quot;</span>).setMaster(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">//2.创建SparkContext，该对象是提交Spark App的入口</span></span><br><span class="line">        <span class="keyword">val</span> sc: <span class="type">SparkContext</span> = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"></span><br><span class="line">        <span class="comment">//3.创建一个RDD</span></span><br><span class="line">        <span class="keyword">val</span> rdd: <span class="type">RDD</span>[<span class="type">String</span>] = sc.makeRDD(<span class="type">Array</span>(<span class="string">&quot;hello world&quot;</span>, <span class="string">&quot;hello spark&quot;</span>, <span class="string">&quot;hive&quot;</span>, <span class="string">&quot;atguigu&quot;</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment">//3.1创建一个Search对象</span></span><br><span class="line">        <span class="keyword">val</span> search = <span class="keyword">new</span> <span class="type">Search</span>(<span class="string">&quot;hello&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">//3.2 函数传递，打印：ERROR Task not serializable</span></span><br><span class="line">        search.getMatch1(rdd).collect().foreach(println)</span><br><span class="line"></span><br><span class="line">        <span class="comment">//3.3 属性传递，打印：ERROR Task not serializable</span></span><br><span class="line">        search.getMatch2(rdd).collect().foreach(println)</span><br><span class="line"></span><br><span class="line">        <span class="comment">//4.关闭连接</span></span><br><span class="line">        sc.stop()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Search</span>(<span class="params">query:<span class="type">String</span></span>) <span class="keyword">extends</span> <span class="title">Serializable</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">isMatch</span></span>(s: <span class="type">String</span>): <span class="type">Boolean</span> = &#123;</span><br><span class="line">        s.contains(query)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 函数序列化案例</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">getMatch1</span> </span>(rdd: <span class="type">RDD</span>[<span class="type">String</span>]): <span class="type">RDD</span>[<span class="type">String</span>] = &#123;</span><br><span class="line">        <span class="comment">//rdd.filter(this.isMatch)</span></span><br><span class="line">        rdd.filter(isMatch)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 属性序列化案例</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">getMatch2</span></span>(rdd: <span class="type">RDD</span>[<span class="type">String</span>]): <span class="type">RDD</span>[<span class="type">String</span>] = &#123;</span><br><span class="line">        <span class="comment">//rdd.filter(x =&gt; x.contains(this.query))</span></span><br><span class="line">        rdd.filter(x =&gt; x.contains(query))</span><br><span class="line">        <span class="comment">//val q = query</span></span><br><span class="line">        <span class="comment">//rdd.filter(x =&gt; x.contains(q))</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>Kryo序列化框架<br> 参考地址: <a target="_blank" rel="noopener" href="https://github.com/EsotericSoftware/kryo">https://github.com/EsotericSoftware/kryo</a><br> Java的序列化能够序列化任何的类。但是比较重（字节多），序列化后，对象的提交也比较大。Spark出于性能的考虑，Spark2.0开始支持另外一种Kryo序列化机制。Kryo速度是Serializable的10倍。当RDD在Shuffle数据的时候，简单数据类型、数组和字符串类型已经在Spark内部使用Kryo来序列化。<br> 注意：即使使用Kryo序列化，也要继承Serializable接口。 <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">serializable_Kryo</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> conf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">                .setAppName(<span class="string">&quot;SerDemo&quot;</span>)</span><br><span class="line">                .setMaster(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">                <span class="comment">// 替换默认的序列化机制</span></span><br><span class="line">                .set(<span class="string">&quot;spark.serializer&quot;</span>, <span class="string">&quot;org.apache.spark.serializer.KryoSerializer&quot;</span>)</span><br><span class="line">                <span class="comment">// 注册需要使用 kryo 序列化的自定义类</span></span><br><span class="line">                .registerKryoClasses(<span class="type">Array</span>(classOf[<span class="type">Searcher</span>]))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> rdd: <span class="type">RDD</span>[<span class="type">String</span>] = sc.makeRDD(<span class="type">Array</span>(<span class="string">&quot;hello world&quot;</span>, <span class="string">&quot;hello atguigu&quot;</span>, <span class="string">&quot;atguigu&quot;</span>, <span class="string">&quot;hahah&quot;</span>), <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> searcher = <span class="keyword">new</span> <span class="type">Searcher</span>(<span class="string">&quot;hello&quot;</span>)</span><br><span class="line">        <span class="keyword">val</span> result: <span class="type">RDD</span>[<span class="type">String</span>] = searcher.getMatchedRDD1(rdd)</span><br><span class="line"></span><br><span class="line">        result.collect.foreach(println)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Searcher</span>(<span class="params">val query: <span class="type">String</span></span>) </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">isMatch</span></span>(s: <span class="type">String</span>) = &#123;</span><br><span class="line">        s.contains(query)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">getMatchedRDD1</span></span>(rdd: <span class="type">RDD</span>[<span class="type">String</span>]) = &#123;</span><br><span class="line">        rdd.filter(isMatch) </span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">getMatchedRDD2</span></span>(rdd: <span class="type">RDD</span>[<span class="type">String</span>]) = &#123;</span><br><span class="line">        <span class="keyword">val</span> q = query</span><br><span class="line">        rdd.filter(_.contains(q))</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ol>
<h4 id="5-1-4-7-RDD依赖关系"><a href="#5-1-4-7-RDD依赖关系" class="headerlink" title="5.1.4.7 RDD依赖关系"></a>5.1.4.7 RDD依赖关系</h4><ol>
<li><p>RDD 血缘关系<br> RDD只支持粗粒度转换，即在大量记录上执行的单个操作。将创建RDD的一系列Lineage（血统）记录下来，以便恢复丢失的分区。RDD的Lineage会记录RDD的元数据信息和转换行为，当该RDD的部分分区数据丢失时，它可以根据这些信息来重新运算和恢复丢失的数据分区。</p>
 <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> fileRDD: <span class="type">RDD</span>[<span class="type">String</span>] = sc.textFile(<span class="string">&quot;input/1.txt&quot;</span>)</span><br><span class="line">println(fileRDD.toDebugString)</span><br><span class="line">println(<span class="string">&quot;----------------------&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> wordRDD: <span class="type">RDD</span>[<span class="type">String</span>] = fileRDD.flatMap(_.split(<span class="string">&quot; &quot;</span>))</span><br><span class="line">println(wordRDD.toDebugString)</span><br><span class="line">println(<span class="string">&quot;----------------------&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> mapRDD: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = wordRDD.map((_,<span class="number">1</span>))</span><br><span class="line">println(mapRDD.toDebugString)</span><br><span class="line">println(<span class="string">&quot;----------------------&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> resultRDD: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = mapRDD.reduceByKey(_+_)</span><br><span class="line">println(resultRDD.toDebugString)</span><br><span class="line"></span><br><span class="line">resultRDD.collect()</span><br></pre></td></tr></table></figure></li>
<li><p>RDD 依赖关系<br> 这里所谓的依赖关系，其实就是两个相邻RDD之间的关系</p>
 <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sc: <span class="type">SparkContext</span> = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> fileRDD: <span class="type">RDD</span>[<span class="type">String</span>] = sc.textFile(<span class="string">&quot;input/1.txt&quot;</span>)</span><br><span class="line">println(fileRDD.dependencies)</span><br><span class="line">println(<span class="string">&quot;----------------------&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> wordRDD: <span class="type">RDD</span>[<span class="type">String</span>] = fileRDD.flatMap(_.split(<span class="string">&quot; &quot;</span>))</span><br><span class="line">println(wordRDD.dependencies)</span><br><span class="line">println(<span class="string">&quot;----------------------&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> mapRDD: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = wordRDD.map((_,<span class="number">1</span>))</span><br><span class="line">println(mapRDD.dependencies)</span><br><span class="line">println(<span class="string">&quot;----------------------&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> resultRDD: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = mapRDD.reduceByKey(_+_)</span><br><span class="line">println(resultRDD.dependencies)</span><br><span class="line"></span><br><span class="line">resultRDD.collect()</span><br></pre></td></tr></table></figure></li>
<li><p>RDD 窄依赖<br> 窄依赖表示每一个父(上游)RDD的Partition最多被子（下游）RDD的一个Partition使用，窄依赖我们形象的比喻为独生子女。</p>
 <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">OneToOneDependency</span>[<span class="type">T</span>](<span class="params">rdd: <span class="type">RDD</span>[<span class="type">T</span>]</span>) <span class="keyword">extends</span> <span class="title">NarrowDependency</span>[<span class="type">T</span>](<span class="params">rdd</span>) </span></span><br></pre></td></tr></table></figure></li>
<li><p>RDD 宽依赖<br> 宽依赖表示同一个父（上游）RDD的Partition被多个子（下游）RDD的Partition依赖，会引起Shuffle，总结：宽依赖我们形象的比喻为多生。</p>
 <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ShuffleDependency</span>[<span class="type">K</span>: <span class="type">ClassTag</span>, <span class="type">V</span>: <span class="type">ClassTag</span>, <span class="type">C</span>: <span class="type">ClassTag</span>](<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="class">    @transient private val _rdd: <span class="type">RDD</span>[_ &lt;: <span class="type">Product2</span>[<span class="type">K</span>, <span class="type">V</span>]],</span></span></span><br><span class="line"><span class="params"><span class="class">    val partitioner: <span class="type">Partitioner</span>,</span></span></span><br><span class="line"><span class="params"><span class="class">    val serializer: <span class="type">Serializer</span> = <span class="type">SparkEnv</span>.get.serializer,</span></span></span><br><span class="line"><span class="params"><span class="class">    val keyOrdering: <span class="type">Option</span>[<span class="type">Ordering</span>[<span class="type">K</span>]] = <span class="type">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="class">    val aggregator: <span class="type">Option</span>[<span class="type">Aggregator</span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">C</span>]] = <span class="type">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="class">    val mapSideCombine: <span class="type">Boolean</span> = false</span>)</span></span><br><span class="line">  <span class="keyword">extends</span> <span class="type">Dependency</span>[<span class="type">Product2</span>[<span class="type">K</span>, <span class="type">V</span>]] </span><br></pre></td></tr></table></figure></li>
<li><p>RDD 阶段划分<br> DAG（Directed Acyclic Graph）有向无环图是由点和线组成的拓扑图形，该图形具有方向，不会闭环。例如，DAG记录了RDD的转换过程和任务的阶段。<br> <img src="https://s2.loli.net/2021/12/11/ki2LpfesH4xmbhW.jpg"></p>
</li>
<li><p>RDD 阶段划分源码</p>
<ul>
<li>SparkContext对象包含有一个私有属性DAGScheduler阶段调度器，主要用于阶段的划分。在一个应用程序中，任务的提交都是从行动算子触发的。行动算子的方法内部会调用一个runJob方法，其中就有DAG调度器发挥运行Job的作用：  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dagScheduler.runJob(rdd, cleanedFunc, partitions, callSite, resultHandler, localProperties.get)</span><br></pre></td></tr></table></figure></li>
<li>runJob方法中，会执行submitJob方法：  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> waiter = submitJob(rdd, func, partitions, callSite, resultHandler, properties)</span><br></pre></td></tr></table></figure></li>
<li>继续查看这个方法的源码，其内部的重点代码区域如下：  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> func2 = func.asInstanceOf[(<span class="type">TaskContext</span>, <span class="type">Iterator</span>[_]) =&gt; _]</span><br><span class="line"><span class="keyword">val</span> waiter = <span class="keyword">new</span> <span class="type">JobWaiter</span>[<span class="type">U</span>](<span class="keyword">this</span>, jobId, partitions.size, resultHandler)</span><br><span class="line">eventProcessLoop.post(<span class="type">JobSubmitted</span>(</span><br><span class="line">    jobId, rdd, func2, partitions.toArray, callSite, waiter,</span><br><span class="line">    <span class="type">Utils</span>.cloneProperties(properties)))</span><br><span class="line">waiter</span><br></pre></td></tr></table></figure></li>
<li>此处有一个JobSubmitted事件，这个事件作为post方法的参数，该post方法主要用于将事件放入到一个队列中，然后等待事件线程执行队列中的事件：  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">post</span></span>(event: <span class="type">E</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="keyword">if</span> (!stopped.get) &#123;</span><br><span class="line">    <span class="keyword">if</span> (eventThread.isAlive) &#123;</span><br><span class="line">      eventQueue.put(event)</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      onError(<span class="keyword">new</span> <span class="type">IllegalStateException</span>(<span class="string">s&quot;<span class="subst">$name</span> has already been stopped accidentally.&quot;</span>))</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>查看这个事件线程eventThread，当这个事件线程执行的时候，会运行run方法，在方法的内部会取出事件队列中的事件。  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span>[spark] <span class="keyword">val</span> eventThread = <span class="keyword">new</span> <span class="type">Thread</span>(name) &#123;</span><br><span class="line">  setDaemon(<span class="literal">true</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="keyword">while</span> (!stopped.get) &#123;</span><br><span class="line">        <span class="keyword">val</span> event = eventQueue.take()</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">          onReceive(event)</span><br><span class="line">        &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">          <span class="keyword">case</span> <span class="type">NonFatal</span>(e) =&gt;</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">              onError(e)</span><br><span class="line">            &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">              <span class="keyword">case</span> <span class="type">NonFatal</span>(e) =&gt; logError(<span class="string">&quot;Unexpected error in &quot;</span> + name, e)</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> ie: <span class="type">InterruptedException</span> =&gt; <span class="comment">// exit even if eventQueue is not empty</span></span><br><span class="line">      <span class="keyword">case</span> <span class="type">NonFatal</span>(e) =&gt; logError(<span class="string">&quot;Unexpected error in &quot;</span> + name, e)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>事件取出之后，会将事件传给一个onReceive方法，继续查看该方法的源码，直接点进去会看到显示的是一个抽象的方法，这个抽象方法是位于EventLoop这个抽象类中的，真正执行的onReceive方法是实现这个抽象类的DAGSchedulerEventProcessLoop类中的onReceive方法。  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onReceive</span></span>(event: <span class="type">DAGSchedulerEvent</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> timerContext = timer.time()</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    doOnReceive(event)</span><br><span class="line">  &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">    timerContext.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>这是阶段调度器的主要事件循环。该方法又将事件传给了doOnReceive方法，  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">doOnReceive</span></span>(event: <span class="type">DAGSchedulerEvent</span>): <span class="type">Unit</span> = event <span class="keyword">match</span> &#123;</span><br><span class="line">  <span class="keyword">case</span> <span class="type">JobSubmitted</span>(jobId, rdd, func, partitions, callSite, listener, properties) =&gt;</span><br><span class="line">    dagScheduler.handleJobSubmitted(jobId, rdd, func, partitions, callSite, listener, properties)</span><br><span class="line"></span><br><span class="line">	……</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>该方法中包含模式匹配，JobSubmitted事件正好可以匹配到第一项，说白了就是DAGScheduler类会向事件队列发送一个消息，消息中包含的是事件，然后事件线程收到消息之后会对消息进行匹配。此处会执行handleJobSubmitted方法，查看其源码，其中  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span>[scheduler] <span class="function"><span class="keyword">def</span> <span class="title">handleJobSubmitted</span></span>(jobId: <span class="type">Int</span>,</span><br><span class="line">    finalRDD: <span class="type">RDD</span>[_],</span><br><span class="line">    func: (<span class="type">TaskContext</span>, <span class="type">Iterator</span>[_]) =&gt; _,</span><br><span class="line">    partitions: <span class="type">Array</span>[<span class="type">Int</span>],</span><br><span class="line">    callSite: <span class="type">CallSite</span>,</span><br><span class="line">    listener: <span class="type">JobListener</span>,</span><br><span class="line">    properties: <span class="type">Properties</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="keyword">var</span> finalStage: <span class="type">ResultStage</span> = <span class="literal">null</span></span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="comment">// New stage creation may throw an exception if, for example, jobs are run on a</span></span><br><span class="line">    <span class="comment">// HadoopRDD whose underlying HDFS files have been deleted.</span></span><br><span class="line">    finalStage = createResultStage(finalRDD, func, partitions, jobId, callSite)</span><br><span class="line">  &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">	……</span><br></pre></td></tr></table></figure></li>
<li>这部分区域是对阶段进行划分。createResultStage方法用于ResultStage阶段。  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">createResultStage</span></span>(</span><br><span class="line">    rdd: <span class="type">RDD</span>[_],</span><br><span class="line">    func: (<span class="type">TaskContext</span>, <span class="type">Iterator</span>[_]) =&gt; _,</span><br><span class="line">    partitions: <span class="type">Array</span>[<span class="type">Int</span>],</span><br><span class="line">    jobId: <span class="type">Int</span>,</span><br><span class="line">    callSite: <span class="type">CallSite</span>): <span class="type">ResultStage</span> = &#123;</span><br><span class="line">  checkBarrierStageWithDynamicAllocation(rdd)</span><br><span class="line">  checkBarrierStageWithNumSlots(rdd)</span><br><span class="line">  checkBarrierStageWithRDDChainPattern(rdd, partitions.toSet.size)</span><br><span class="line">  <span class="keyword">val</span> parents = getOrCreateParentStages(rdd, jobId)</span><br><span class="line">  <span class="keyword">val</span> id = nextStageId.getAndIncrement()</span><br><span class="line">  <span class="keyword">val</span> stage = <span class="keyword">new</span> <span class="type">ResultStage</span>(id, rdd, func, partitions, parents, jobId, callSite)</span><br><span class="line">  stageIdToStage(id) = stage</span><br><span class="line">  updateJobIdStageIdMaps(jobId, stage)</span><br><span class="line">  stage</span><br><span class="line">&#125;</span><br><span class="line">​ <span class="type">ResultStage</span>中包含的rdd就是执行行动算子的那个rdd（下图中黄色表示的那个），也就是最后的那个rdd（下图中黄色图表示的rdd）。parents是<span class="type">ResultStage</span>的上一级阶段，parents是getOrCreateParentStages方法的返回值。getOrCreateParentStages用于获取或者创建给定<span class="type">RDD</span>的父阶段列表。</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">getOrCreateParentStages</span></span>(rdd: <span class="type">RDD</span>[_], firstJobId: <span class="type">Int</span>): <span class="type">List</span>[<span class="type">Stage</span>] = &#123;</span><br><span class="line">  getShuffleDependencies(rdd).map &#123; shuffleDep =&gt;</span><br><span class="line">    getOrCreateShuffleMapStage(shuffleDep, firstJobId)</span><br><span class="line">  &#125;.toList</span><br><span class="line">&#125;</span><br><span class="line">​ getShuffleDependencies方法用于获取给定rdd的shuffle依赖，其核心代码如下：</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span>[scheduler] <span class="function"><span class="keyword">def</span> <span class="title">getShuffleDependencies</span></span>(</span><br><span class="line">    rdd: <span class="type">RDD</span>[_]): <span class="type">HashSet</span>[<span class="type">ShuffleDependency</span>[_, _, _]] = &#123;</span><br><span class="line"> 		……</span><br><span class="line">  <span class="keyword">while</span> (waitingForVisit.nonEmpty) &#123;</span><br><span class="line">    <span class="keyword">val</span> toVisit = waitingForVisit.remove(<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">if</span> (!visited(toVisit)) &#123;</span><br><span class="line">      visited += toVisit</span><br><span class="line">      toVisit.dependencies.foreach &#123;</span><br><span class="line">        <span class="keyword">case</span> shuffleDep: <span class="type">ShuffleDependency</span>[_, _, _] =&gt;</span><br><span class="line">          parents += shuffleDep</span><br><span class="line">        <span class="keyword">case</span> dependency =&gt;</span><br><span class="line">          waitingForVisit.prepend(dependency.rdd)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  parents</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>核心代码用于判断给定rdd的依赖关系是不是shuffle依赖，如果是则加入结果列表。最终返回的结果列表，会通过map方法将列表中的每一个元素执行getOrCreateShuffleMapStage方法，该方法用于获取或者创建ShuffleMap阶段（写磁盘之前的阶段）。  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">getOrCreateShuffleMapStage(shuffleDep, firstJobId) =&gt; createShuffleMapStage(shuffleDep, firstJobId)</span><br></pre></td></tr></table></figure></li>
<li>createShuffleMapStage方法中会创建ShuffleMapStage对象，并当前rdd（调用行动算子的那个）依赖的rdd（下图紫色那个rdd）传给这个对象。  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createShuffleMapStage</span></span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">C</span>](</span><br><span class="line">    shuffleDep: <span class="type">ShuffleDependency</span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">C</span>], jobId: <span class="type">Int</span>): <span class="type">ShuffleMapStage</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> rdd = shuffleDep.rdd</span><br><span class="line">	……</span><br><span class="line">  <span class="keyword">val</span> numTasks = rdd.partitions.length</span><br><span class="line">  <span class="keyword">val</span> parents = getOrCreateParentStages(rdd, jobId)</span><br><span class="line">  <span class="keyword">val</span> id = nextStageId.getAndIncrement()</span><br><span class="line">  <span class="keyword">val</span> stage = <span class="keyword">new</span> <span class="type">ShuffleMapStage</span>(</span><br><span class="line">    id, rdd, numTasks, parents, jobId, rdd.creationSite, shuffleDep, mapOutputTracker)</span><br><span class="line"></span><br><span class="line">	……</span><br></pre></td></tr></table></figure></li>
<li>此时，ShuffleMap阶段（下图红色区域）就是Result阶段（蓝色区域）的上一级阶段。在上面的代码中，我们还可以看到，如果当前ShuffleMap阶段还有上一级阶段，那么getOrCreateParentStages(rdd, jobId)方法还是会获取它的上一级阶段的，此时这个方法中的rdd就不再是最后一个rdd，而是最后一个rdd的前一个rdd，也就是紫色表示的那个rdd。也就是说，阶段的寻找是一个不断往前的过程，只要含有shuffle过程，那么就会有新的阶段。<br>  <img src="https://s2.loli.net/2021/12/11/rvgSydiUjZf3pst.jpg"></li>
</ul>
</li>
<li><p>RDD 任务划分<br>RDD任务切分中间分为：Application、Job、Stage和Task</p>
</li>
</ol>
<ul>
<li>Application：初始化一个SparkContext即生成一个Application；</li>
<li>Job：一个Action算子就会生成一个Job；</li>
<li>Stage：Stage等于宽依赖(ShuffleDependency)的个数加1；</li>
<li>Task：一个Stage阶段中，最后一个RDD的分区个数就是Task的个数。<br><font color ='red' >注意：Application-&gt;Job-&gt;Stage-&gt;Task每一层都是1对n的关系。 </font></li>
</ul>
<ol start="8">
<li>RDD 任务划分源码<ul>
<li>DAGScheduler类的handleJobSubmitted方法中，有一个提交阶段的的方法：  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> finalStage: <span class="type">ResultStage</span> = <span class="literal">null</span></span><br><span class="line">	……</span><br><span class="line">finalStage = createResultStage(finalRDD, func, partitions, jobId, callSite)</span><br><span class="line">	……</span><br><span class="line">submitStage(finalStage)</span><br></pre></td></tr></table></figure></li>
<li>submitStage方法用于提交最终的ResultStage阶段，由于在最终的ResultStage可能包含了多个上级阶段，所以此处就相当于是提交整个应用程序的全部阶段。查看一下该方法的源码：  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">submitStage</span></span>(stage: <span class="type">Stage</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> jobId = activeJobForStage(stage)</span><br><span class="line">  <span class="keyword">if</span> (jobId.isDefined) &#123;</span><br><span class="line">    logDebug(<span class="string">s&quot;submitStage(<span class="subst">$stage</span> (name=<span class="subst">$&#123;stage.name&#125;</span>;&quot;</span> +</span><br><span class="line">      <span class="string">s&quot;jobs=<span class="subst">$&#123;stage.jobIds.toSeq.sorted.mkString(&quot;,&quot;)&#125;</span>))&quot;</span>)</span><br><span class="line">    <span class="keyword">if</span> (!waitingStages(stage) &amp;&amp; !runningStages(stage) &amp;&amp; !failedStages(stage)) &#123;</span><br><span class="line">      <span class="keyword">val</span> missing = getMissingParentStages(stage).sortBy(_.id)</span><br><span class="line">      logDebug(<span class="string">&quot;missing: &quot;</span> + missing)</span><br><span class="line">      <span class="keyword">if</span> (missing.isEmpty) &#123;</span><br><span class="line">        logInfo(<span class="string">&quot;Submitting &quot;</span> + stage + <span class="string">&quot; (&quot;</span> + stage.rdd + <span class="string">&quot;), which has no missing parents&quot;</span>)</span><br><span class="line">        submitMissingTasks(stage, jobId.get)</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">for</span> (parent &lt;- missing) &#123;</span><br><span class="line">          submitStage(parent)</span><br><span class="line">        &#125;</span><br><span class="line">        waitingStages += stage</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    abortStage(stage, <span class="string">&quot;No active job for stage &quot;</span> + stage.id, <span class="type">None</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>该方法的内部核心逻辑是先获取当前阶段的的所有父级阶段，如果其父级阶段为空那么直接执行submitMissingTasks方法，如果不为空，那么递归执行submitStage方法，只不过传入的参数是当前阶段的父级阶段，一直递归直到找到没有上级阶段的阶段，最终没有上级阶段的那个阶段会执行submitMissingTasks方法。下面查看一下该方法的核心源码部分：  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">submitMissingTasks</span></span>(stage: <span class="type">Stage</span>, jobId: <span class="type">Int</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    	 ……</span><br><span class="line">  <span class="keyword">val</span> partitionsToCompute: <span class="type">Seq</span>[<span class="type">Int</span>] = stage.findMissingPartitions()</span><br><span class="line">		 ……</span><br><span class="line">  <span class="keyword">val</span> tasks: <span class="type">Seq</span>[<span class="type">Task</span>[_]] = <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="keyword">val</span> serializedTaskMetrics = closureSerializer.serialize(stage.latestInfo.taskMetrics).array()</span><br><span class="line">    stage <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> stage: <span class="type">ShuffleMapStage</span> =&gt;</span><br><span class="line">        stage.pendingPartitions.clear()</span><br><span class="line">        partitionsToCompute.map &#123; id =&gt;</span><br><span class="line">          <span class="keyword">val</span> locs = taskIdToLocations(id)</span><br><span class="line">          <span class="keyword">val</span> part = partitions(id)</span><br><span class="line">          stage.pendingPartitions += id</span><br><span class="line">          <span class="keyword">new</span> <span class="type">ShuffleMapTask</span>(stage.id, stage.latestInfo.attemptNumber,</span><br><span class="line">            taskBinary, part, locs, properties, serializedTaskMetrics, <span class="type">Option</span>(jobId),</span><br><span class="line">            <span class="type">Option</span>(sc.applicationId), sc.applicationAttemptId, stage.rdd.isBarrier())</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">      <span class="keyword">case</span> stage: <span class="type">ResultStage</span> =&gt;</span><br><span class="line">        partitionsToCompute.map &#123; id =&gt;</span><br><span class="line">          <span class="keyword">val</span> p: <span class="type">Int</span> = stage.partitions(id)</span><br><span class="line">          <span class="keyword">val</span> part = partitions(p)</span><br><span class="line">          <span class="keyword">val</span> locs = taskIdToLocations(id)</span><br><span class="line">          <span class="keyword">new</span> <span class="type">ResultTask</span>(stage.id, stage.latestInfo.attemptNumber,</span><br><span class="line">            taskBinary, part, locs, id, properties, serializedTaskMetrics,</span><br><span class="line">            <span class="type">Option</span>(jobId), <span class="type">Option</span>(sc.applicationId), sc.applicationAttemptId,</span><br><span class="line">            stage.rdd.isBarrier())</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="type">NonFatal</span>(e) =&gt;</span><br><span class="line">      abortStage(stage, <span class="string">s&quot;Task creation failed: <span class="subst">$e</span>\n<span class="subst">$&#123;Utils.exceptionString(e)&#125;</span>&quot;</span>, <span class="type">Some</span>(e))</span><br><span class="line">      runningStages -= stage</span><br><span class="line">      <span class="keyword">return</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"> 	……</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>核心代码的逻辑在于根据传入的stage进行模式匹配，会根据不同类型的Satge创建的不同的Task，那么首先会计算分区得到分区索引集合，然后使用map方法将根据分区id创建xxxMapTask对象，有几个分区id就创建几个xxxMapTask对象。partitionsToCompute是stage.findMissingPartitions()的返回值，那么查看其源码，stage是一个抽象类的引用，调用的这个方法具体的实现在具体的xxxMapStage类中。分别查看一下在resultstage和中的源码：<ul>
<li>ResultStage：  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">findMissingPartitions</span></span>(): <span class="type">Seq</span>[<span class="type">Int</span>] = &#123;</span><br><span class="line">  <span class="keyword">val</span> job = activeJob.get</span><br><span class="line">  (<span class="number">0</span> until job.numPartitions).filter(id =&gt; !job.finished(id))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>ShuffleMapStage：  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">findMissingPartitions</span></span>(): <span class="type">Seq</span>[<span class="type">Int</span>] = &#123;</span><br><span class="line">  mapOutputTrackerMaster</span><br><span class="line">    .findMissingPartitions(shuffleDep.shuffleId)</span><br><span class="line">    .getOrElse(<span class="number">0</span> until numPartitions)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>所以可以看出，partitionsToCompute就是一个分区索引的集合。ResultStage和ShuffleMapStage的numPartitions的值计算方式一样，都是来自于它们所处阶段的最后一个rdd的分区数量值：<ul>
<li>job.numPartitions值：  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> numPartitions = finalStage <span class="keyword">match</span> &#123;</span><br><span class="line">  <span class="keyword">case</span> r: <span class="type">ResultStage</span> =&gt; r.partitions.length</span><br><span class="line">  <span class="keyword">case</span> m: <span class="type">ShuffleMapStage</span> =&gt; m.rdd.partitions.length</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>numPartitions：  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> numPartitions = rdd.partitions.length</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>所以总结一下，应用程序的总任务数量等于每个阶段的最后一个rdd的分区数量之和。</li>
</ul>
</li>
</ul>
</li>
</ol>
<h4 id="5-1-4-8-RDD持久化"><a href="#5-1-4-8-RDD持久化" class="headerlink" title="5.1.4.8 RDD持久化"></a>5.1.4.8 RDD持久化</h4><ol>
<li><p>RDD Cache缓存<br> RDD通过Cache或者Persist方法将前面的计算结果缓存，默认情况下会把数据以缓存在JVM的堆内存中。但是并不是这两个方法被调用时立即缓存，而是触发后面的action算子时，该RDD将会被缓存在计算节点的内存中，并供后面重用。</p>
 <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// cache操作会增加血缘关系，不改变原有的血缘关系</span></span><br><span class="line">println(wordToOneRdd.toDebugString)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 数据缓存。</span></span><br><span class="line">wordToOneRdd.cache()</span><br><span class="line"></span><br><span class="line"><span class="comment">// 可以更改存储级别</span></span><br><span class="line"><span class="comment">//mapRdd.persist(StorageLevel.MEMORY_AND_DISK_2)</span></span><br><span class="line">存储级别</span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">StorageLevel</span> </span>&#123;</span><br><span class="line">  <span class="keyword">val</span> <span class="type">NONE</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">false</span>, <span class="literal">false</span>, <span class="literal">false</span>, <span class="literal">false</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">DISK_ONLY</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">false</span>, <span class="literal">false</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">DISK_ONLY_2</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">false</span>, <span class="literal">false</span>, <span class="number">2</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">MEMORY_ONLY</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">false</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">true</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">MEMORY_ONLY_2</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">false</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">true</span>, <span class="number">2</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">MEMORY_ONLY_SER</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">false</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">false</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">MEMORY_ONLY_SER_2</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">false</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">false</span>, <span class="number">2</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">MEMORY_AND_DISK</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">true</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">MEMORY_AND_DISK_2</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">true</span>, <span class="number">2</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">MEMORY_AND_DISK_SER</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">false</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">MEMORY_AND_DISK_SER_2</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">false</span>, <span class="number">2</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">OFF_HEAP</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">true</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p> <img src="https://s2.loli.net/2021/12/11/lf4FvbAzEqIcYTn.jpg"><br> 缓存有可能丢失，或者存储于内存的数据由于内存不足而被删除，RDD的缓存容错机制保证了即使缓存丢失也能保证计算的正确执行。通过基于RDD的一系列转换，丢失的数据会被重算，由于RDD的各个Partition是相对独立的，因此只需要计算丢失的部分即可，并不需要重算全部Partition。<br> Spark会自动对一些Shuffle操作的中间数据做持久化操作(比如：reduceByKey)。这样做的目的是为了当一个节点Shuffle失败了避免重新计算整个输入。但是，在实际使用的时候，如果想重用数据，仍然建议调用persist或cache。</p>
</li>
<li><p>RDD CheckPoint检查点<br> 所谓的检查点其实就是将RDD中间结果写入磁盘<br> 由于血缘依赖过长会造成容错成本过高，这样就不如在中间阶段做检查点容错，如果检查点之后有节点出现问题，可以从检查点开始重做血缘，减少了开销。<br> 对RDD进行checkpoint操作并不会马上被执行，必须执行Action操作才能触发。</p>
 <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 设置检查点路径</span></span><br><span class="line">sc.setCheckpointDir(<span class="string">&quot;./checkpoint1&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 创建一个RDD，读取指定位置文件:hello atguigu atguigu</span></span><br><span class="line"><span class="keyword">val</span> lineRdd: <span class="type">RDD</span>[<span class="type">String</span>] = sc.textFile(<span class="string">&quot;input/1.txt&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 业务逻辑</span></span><br><span class="line"><span class="keyword">val</span> wordRdd: <span class="type">RDD</span>[<span class="type">String</span>] = lineRdd.flatMap(line =&gt; line.split(<span class="string">&quot; &quot;</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> wordToOneRdd: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Long</span>)] = wordRdd.map &#123;</span><br><span class="line">    word =&gt; &#123;</span><br><span class="line">        (word, <span class="type">System</span>.currentTimeMillis())</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 增加缓存,避免再重新跑一个job做checkpoint</span></span><br><span class="line">wordToOneRdd.cache()</span><br><span class="line"><span class="comment">// 数据检查点：针对wordToOneRdd做检查点计算</span></span><br><span class="line">wordToOneRdd.checkpoint()</span><br><span class="line"></span><br><span class="line"><span class="comment">// 触发执行逻辑</span></span><br><span class="line">wordToOneRdd.collect().foreach(println)</span><br></pre></td></tr></table></figure></li>
<li><p>缓存和检查点区别</p>
<ul>
<li>Cache缓存只是将数据保存起来，不切断血缘依赖。Checkpoint检查点切断血缘依赖。</li>
<li>Cache缓存的数据通常存储在磁盘、内存等地方，可靠性低。Checkpoint的数据通常存储在HDFS等容错、高可用的文件系统，可靠性高。</li>
<li>建议对checkpoint()的RDD使用Cache缓存，这样checkpoint的job只需从Cache缓存中读取数据即可，否则需要再从头计算一次RDD。</li>
</ul>
</li>
</ol>
<h4 id="5-1-4-9-RDD分区器"><a href="#5-1-4-9-RDD分区器" class="headerlink" title="5.1.4.9 RDD分区器"></a>5.1.4.9 RDD分区器</h4><p>Spark目前支持Hash分区和Range分区，和用户自定义分区。Hash分区为当前的默认分区。分区器直接决定了RDD中分区的个数、RDD中每条数据经过Shuffle后进入哪个分区，进而决定了Reduce的个数。</p>
<ul>
<li>只有Key-Value类型的RDD才有分区器，非Key-Value类型的RDD分区的值是None</li>
<li>每个RDD的分区ID范围：0 ~ (numPartitions - 1)，决定这个值是属于那个分区的。</li>
</ul>
<ol>
<li>Hash分区：对于给定的key，计算其hashCode,并除以分区个数取余 <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">HashPartitioner</span>(<span class="params">partitions: <span class="type">Int</span></span>) <span class="keyword">extends</span> <span class="title">Partitioner</span> </span>&#123;</span><br><span class="line">  require(partitions &gt;= <span class="number">0</span>, <span class="string">s&quot;Number of partitions (<span class="subst">$partitions</span>) cannot be negative.&quot;</span>)</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">numPartitions</span></span>: <span class="type">Int</span> = partitions</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getPartition</span></span>(key: <span class="type">Any</span>): <span class="type">Int</span> = key <span class="keyword">match</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="literal">null</span> =&gt; <span class="number">0</span></span><br><span class="line">    <span class="keyword">case</span> _ =&gt; <span class="type">Utils</span>.nonNegativeMod(key.hashCode, numPartitions)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">equals</span></span>(other: <span class="type">Any</span>): <span class="type">Boolean</span> = other <span class="keyword">match</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> h: <span class="type">HashPartitioner</span> =&gt;</span><br><span class="line">      h.numPartitions == numPartitions</span><br><span class="line">    <span class="keyword">case</span> _ =&gt;</span><br><span class="line">      <span class="literal">false</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">hashCode</span></span>: <span class="type">Int</span> = numPartitions</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>Range分区：将一定范围内的数据映射到一个分区中，尽量保证每个分区数据均匀，而且分区间有序 <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RangePartitioner</span>[<span class="type">K</span> : <span class="type">Ordering</span> : <span class="type">ClassTag</span>, <span class="type">V</span>](<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="class">    partitions: <span class="type">Int</span>,</span></span></span><br><span class="line"><span class="params"><span class="class">    rdd: <span class="type">RDD</span>[_ &lt;: <span class="type">Product2</span>[<span class="type">K</span>, <span class="type">V</span>]],</span></span></span><br><span class="line"><span class="params"><span class="class">    private var ascending: <span class="type">Boolean</span> = true</span>)</span></span><br><span class="line">  <span class="keyword">extends</span> <span class="type">Partitioner</span> &#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// We allow partitions = 0, which happens when sorting an empty RDD under the default settings.</span></span><br><span class="line">  require(partitions &gt;= <span class="number">0</span>, <span class="string">s&quot;Number of partitions cannot be negative but found <span class="subst">$partitions</span>.&quot;</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> ordering = implicitly[<span class="type">Ordering</span>[<span class="type">K</span>]]</span><br><span class="line"></span><br><span class="line">  <span class="comment">// An array of upper bounds for the first (partitions - 1) partitions</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> rangeBounds: <span class="type">Array</span>[<span class="type">K</span>] = &#123;</span><br><span class="line">  ...</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">numPartitions</span></span>: <span class="type">Int</span> = rangeBounds.length + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> binarySearch: ((<span class="type">Array</span>[<span class="type">K</span>], <span class="type">K</span>) =&gt; <span class="type">Int</span>) = <span class="type">CollectionsUtils</span>.makeBinarySearch[<span class="type">K</span>]</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getPartition</span></span>(key: <span class="type">Any</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> k = key.asInstanceOf[<span class="type">K</span>]</span><br><span class="line">    <span class="keyword">var</span> partition = <span class="number">0</span></span><br><span class="line">    <span class="keyword">if</span> (rangeBounds.length &lt;= <span class="number">128</span>) &#123;</span><br><span class="line">      <span class="comment">// If we have less than 128 partitions naive search</span></span><br><span class="line">      <span class="keyword">while</span> (partition &lt; rangeBounds.length &amp;&amp; ordering.gt(k, rangeBounds(partition))) &#123;</span><br><span class="line">        partition += <span class="number">1</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="comment">// Determine which binary search method to use only once.</span></span><br><span class="line">      partition = binarySearch(rangeBounds, k)</span><br><span class="line">      <span class="comment">// binarySearch either returns the match location or -[insertion point]-1</span></span><br><span class="line">      <span class="keyword">if</span> (partition &lt; <span class="number">0</span>) &#123;</span><br><span class="line">        partition = -partition<span class="number">-1</span></span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">if</span> (partition &gt; rangeBounds.length) &#123;</span><br><span class="line">        partition = rangeBounds.length</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (ascending) &#123;</span><br><span class="line">      partition</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      rangeBounds.length - partition</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">equals</span></span>(other: <span class="type">Any</span>): <span class="type">Boolean</span> = other <span class="keyword">match</span> &#123;</span><br><span class="line">  ...</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">hashCode</span></span>(): <span class="type">Int</span> = &#123;</span><br><span class="line">  ...</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@throws</span>(classOf[<span class="type">IOException</span>])</span><br><span class="line">  <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">writeObject</span></span>(out: <span class="type">ObjectOutputStream</span>): <span class="type">Unit</span> = <span class="type">Utils</span>.tryOrIOException &#123;</span><br><span class="line">  ...</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@throws</span>(classOf[<span class="type">IOException</span>])</span><br><span class="line">  <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">readObject</span></span>(in: <span class="type">ObjectInputStream</span>): <span class="type">Unit</span> = <span class="type">Utils</span>.tryOrIOException &#123;</span><br><span class="line">  ...</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ol>
<h4 id="5-1-4-10-RDD文件读取与保存"><a href="#5-1-4-10-RDD文件读取与保存" class="headerlink" title="5.1.4.10 RDD文件读取与保存"></a>5.1.4.10 RDD文件读取与保存</h4><p>Spark的数据读取及数据保存可以从两个维度来作区分：文件格式以及文件系统。</p>
<ul>
<li>文件格式分为：text文件、csv文件、sequence文件以及Object文件；</li>
<li>文件系统分为：本地文件系统、HDFS、HBASE以及数据库。<ul>
<li>text文件  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 读取输入文件</span></span><br><span class="line"><span class="keyword">val</span> inputRDD: <span class="type">RDD</span>[<span class="type">String</span>] = sc.textFile(<span class="string">&quot;input/1.txt&quot;</span>)</span><br><span class="line"><span class="comment">// 保存数据</span></span><br><span class="line">inputRDD.saveAsTextFile(<span class="string">&quot;output&quot;</span>)</span><br></pre></td></tr></table></figure></li>
<li>sequence文件<br>  SequenceFile文件是Hadoop用来存储二进制形式的key-value对而设计的一种平面文件(Flat File)。在SparkContext中，可以调用sequenceFile<a href="path">keyClass, valueClass</a>。  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 保存数据为SequenceFile</span></span><br><span class="line">dataRDD.saveAsSequenceFile(<span class="string">&quot;output&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 读取SequenceFile文件</span></span><br><span class="line">sc.sequenceFile[<span class="type">Int</span>,<span class="type">Int</span>](<span class="string">&quot;output&quot;</span>).collect().foreach(println)</span><br></pre></td></tr></table></figure></li>
<li>object对象文件<br>  对象文件是将对象序列化后保存的文件，采用Java的序列化机制。可以通过objectFile<a href="path">T: ClassTag</a>函数接收一个路径，读取对象文件，返回对应的RDD，也可以通过调用saveAsObjectFile()实现对对象文件的输出。因为是序列化所以要指定类型。  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 保存数据</span></span><br><span class="line">dataRDD.saveAsObjectFile(<span class="string">&quot;output&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 读取数据</span></span><br><span class="line">sc.objectFile[<span class="type">Int</span>](<span class="string">&quot;output&quot;</span>).collect().foreach(println)</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>
<h2 id="5-2-累加器"><a href="#5-2-累加器" class="headerlink" title="5.2 累加器"></a>5.2 累加器</h2><h3 id="5-2-1-实现原理"><a href="#5-2-1-实现原理" class="headerlink" title="5.2.1 实现原理"></a>5.2.1 实现原理</h3><p>累加器用来把Executor端变量信息聚合到Driver端。在Driver程序中定义的变量，在Executor端的每个Task都会得到这个变量的一份新的副本，每个task更新这些副本的值后，传回Driver端进行merge。</p>
<h3 id="5-2-2-基础编程"><a href="#5-2-2-基础编程" class="headerlink" title="5.2.2 基础编程"></a>5.2.2 基础编程</h3><h4 id="5-2-2-1-系统累加器"><a href="#5-2-2-1-系统累加器" class="headerlink" title="5.2.2.1 系统累加器"></a>5.2.2.1 系统累加器</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd = sc.makeRDD(<span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>))</span><br><span class="line"><span class="comment">// 声明累加器</span></span><br><span class="line"><span class="keyword">var</span> sum = sc.longAccumulator(<span class="string">&quot;sum&quot;</span>);</span><br><span class="line">rdd.foreach(</span><br><span class="line">  num =&gt; &#123;</span><br><span class="line">    <span class="comment">// 使用累加器</span></span><br><span class="line">    sum.add(num)</span><br><span class="line">  &#125;</span><br><span class="line">)</span><br><span class="line"><span class="comment">// 获取累加器的值</span></span><br><span class="line">println(<span class="string">&quot;sum = &quot;</span> + sum.value)</span><br></pre></td></tr></table></figure>

<h4 id="5-2-2-2-自定义累加器"><a href="#5-2-2-2-自定义累加器" class="headerlink" title="5.2.2.2 自定义累加器"></a>5.2.2.2 自定义累加器</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 自定义累加器</span></span><br><span class="line"><span class="comment">// 1. 继承AccumulatorV2，并设定泛型</span></span><br><span class="line"><span class="comment">// 2. 重写累加器的抽象方法</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">WordCountAccumulator</span> <span class="keyword">extends</span> <span class="title">AccumulatorV2</span>[<span class="type">String</span>, mutable.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">Long</span>]]</span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">var</span> map : mutable.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">Long</span>] = mutable.<span class="type">Map</span>()</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 累加器是否为初始状态</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">isZero</span></span>: <span class="type">Boolean</span> = &#123;</span><br><span class="line">      map.isEmpty</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 复制累加器</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">copy</span></span>(): <span class="type">AccumulatorV2</span>[<span class="type">String</span>, mutable.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">Long</span>]] = &#123;</span><br><span class="line">      <span class="keyword">new</span> <span class="type">WordCountAccumulator</span></span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 重置累加器</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">reset</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">      map.clear()</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 向累加器中增加数据 (In)</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">add</span></span>(word: <span class="type">String</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="comment">// 查询map中是否存在相同的单词</span></span><br><span class="line">        <span class="comment">// 如果有相同的单词，那么单词的数量加1</span></span><br><span class="line">        <span class="comment">// 如果没有相同的单词，那么在map中增加这个单词</span></span><br><span class="line">        map(word) = map.getOrElse(word, <span class="number">0</span>L) + <span class="number">1</span>L</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 合并累加器</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">merge</span></span>(other: <span class="type">AccumulatorV2</span>[<span class="type">String</span>, mutable.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">Long</span>]]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    </span><br><span class="line">      <span class="keyword">val</span> map1 = map</span><br><span class="line">      <span class="keyword">val</span> map2 = other.value</span><br><span class="line">    </span><br><span class="line">      <span class="comment">// 两个Map的合并</span></span><br><span class="line">      map = map1.foldLeft(map2)(</span><br><span class="line">        ( innerMap, kv ) =&gt; &#123;</span><br><span class="line">          innerMap(kv._1) = innerMap.getOrElse(kv._1, <span class="number">0</span>L) + kv._2</span><br><span class="line">          innerMap</span><br><span class="line">        &#125;</span><br><span class="line">      )</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 返回累加器的结果 （Out）</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">value</span></span>: mutable.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">Long</span>] = map</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="5-3-广播变量"><a href="#5-3-广播变量" class="headerlink" title="5.3 广播变量"></a>5.3 广播变量</h2><h3 id="5-3-1-实现原理"><a href="#5-3-1-实现原理" class="headerlink" title="5.3.1 实现原理"></a>5.3.1 实现原理</h3><p>广播变量用来高效分发较大的对象。向所有工作节点发送一个较大的只读值，以供一个或多个Spark操作使用。比如，如果你的应用需要向所有节点发送一个较大的只读查询表，广播变量用起来都很顺手。在多个并行操作中使用同一个变量，但是 Spark会为每个任务分别发送。</p>
<h3 id="5-3-2-基础编程"><a href="#5-3-2-基础编程" class="headerlink" title="5.3.2 基础编程"></a>5.3.2 基础编程</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd1 = sc.makeRDD(<span class="type">List</span>( (<span class="string">&quot;a&quot;</span>,<span class="number">1</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">2</span>), (<span class="string">&quot;c&quot;</span>, <span class="number">3</span>), (<span class="string">&quot;d&quot;</span>, <span class="number">4</span>) ),<span class="number">4</span>)</span><br><span class="line"><span class="keyword">val</span> list = <span class="type">List</span>( (<span class="string">&quot;a&quot;</span>,<span class="number">4</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">5</span>), (<span class="string">&quot;c&quot;</span>, <span class="number">6</span>), (<span class="string">&quot;d&quot;</span>, <span class="number">7</span>) )</span><br><span class="line"><span class="comment">// 声明广播变量</span></span><br><span class="line"><span class="keyword">val</span> broadcast: <span class="type">Broadcast</span>[<span class="type">List</span>[(<span class="type">String</span>, <span class="type">Int</span>)]] = sc.broadcast(list)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> resultRDD: <span class="type">RDD</span>[(<span class="type">String</span>, (<span class="type">Int</span>, <span class="type">Int</span>))] = rdd1.map &#123;</span><br><span class="line">  <span class="keyword">case</span> (key, num) =&gt; &#123;</span><br><span class="line">    <span class="keyword">var</span> num2 = <span class="number">0</span></span><br><span class="line">    <span class="comment">// 使用广播变量</span></span><br><span class="line">    <span class="keyword">for</span> ((k, v) &lt;- broadcast.value) &#123;</span><br><span class="line">      <span class="keyword">if</span> (k == key) &#123;</span><br><span class="line">        num2 = v</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    (key, (num, num2))</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


<h1 id="第6章-Spark案例实操"><a href="#第6章-Spark案例实操" class="headerlink" title="第6章 Spark案例实操"></a>第6章 Spark案例实操</h1><p>在实际的工作中如何使用这些API实现具体的需求。这些需求是电商网站的真实需求，所以在实现功能前，必须先将数据准备好。<br><img src="https://s2.loli.net/2021/12/11/TLAMWjC5Ybc4a19.jpg"></p>
<p>上面的数据图是从数据文件中截取的一部分内容，表示为电商网站的用户行为数据，主要包含用户的4种行为：搜索，点击，下单，支付。数据规则如下：</p>
<ul>
<li>数据文件中每行数据采用下划线分隔数据</li>
<li>每一行数据表示用户的一次行为，这个行为只能是4种行为的一种</li>
<li>如果搜索关键字为null,表示数据不是搜索数据</li>
<li>如果点击的品类ID和产品ID为-1，表示数据不是点击数据</li>
<li>针对于下单行为，一次可以下单多个商品，所以品类ID和产品ID可以是多个，id之间采用逗号分隔，如果本次不是下单行为，则数据采用null表示</li>
<li>支付行为和下单行为类似</li>
</ul>
<p>详细字段说明：<br>|编号        |字段名称      |字段类型      |字段含义       |<br>|——– |———–|———–|————-|<br>|1|  date| String|  用户点击行为的日期|<br>|2|  user_id|  Long| 用户的ID|<br>|3|  session_id|   String|    Session的ID|<br>|4|  page_id|  Long| 某个页面的ID|<br>|5|  action_time|  String|   动作的时间点|<br>|6|  search_keyword|   String|    用户搜索的关键词|<br>|7|  click_category_id|    Long|   某一个商品品类的ID|<br>|8|  click_product_id| Long|    某一个商品的ID|<br>|9|  order_category_ids|   String|    一次订单中所有品类的ID集合|<br>|10| order_product_ids|   String|    一次订单中所有商品的ID集合|<br>|11| pay_category_ids|    String| 一次支付中所有品类的ID集合|<br>|12| pay_product_ids| String|  一次支付中所有商品的ID集合|<br>|13| city_id| Long|    城市 id|</p>
<p>样例类：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//用户访问动作表</span></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">UserVisitAction</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="class">    date: <span class="type">String</span>,//用户点击行为的日期</span></span></span><br><span class="line"><span class="params"><span class="class">    user_id: <span class="type">Long</span>,//用户的<span class="type">ID</span></span></span></span><br><span class="line"><span class="params"><span class="class">    session_id: <span class="type">String</span>,//<span class="type">Session</span>的<span class="type">ID</span></span></span></span><br><span class="line"><span class="params"><span class="class">    page_id: <span class="type">Long</span>,//某个页面的<span class="type">ID</span></span></span></span><br><span class="line"><span class="params"><span class="class">    action_time: <span class="type">String</span>,//动作的时间点</span></span></span><br><span class="line"><span class="params"><span class="class">    search_keyword: <span class="type">String</span>,//用户搜索的关键词</span></span></span><br><span class="line"><span class="params"><span class="class">    click_category_id: <span class="type">Long</span>,//某一个商品品类的<span class="type">ID</span></span></span></span><br><span class="line"><span class="params"><span class="class">    click_product_id: <span class="type">Long</span>,//某一个商品的<span class="type">ID</span></span></span></span><br><span class="line"><span class="params"><span class="class">    order_category_ids: <span class="type">String</span>,//一次订单中所有品类的<span class="type">ID</span>集合</span></span></span><br><span class="line"><span class="params"><span class="class">    order_product_ids: <span class="type">String</span>,//一次订单中所有商品的<span class="type">ID</span>集合</span></span></span><br><span class="line"><span class="params"><span class="class">    pay_category_ids: <span class="type">String</span>,//一次支付中所有品类的<span class="type">ID</span>集合</span></span></span><br><span class="line"><span class="params"><span class="class">    pay_product_ids: <span class="type">String</span>,//一次支付中所有商品的<span class="type">ID</span>集合</span></span></span><br><span class="line"><span class="params"><span class="class">    city_id: <span class="type">Long</span>//城市 id</span></span></span><br><span class="line"><span class="params"><span class="class"></span>)</span></span><br></pre></td></tr></table></figure>

<h2 id="6-1-需求1：Top10热门品类"><a href="#6-1-需求1：Top10热门品类" class="headerlink" title="6.1 需求1：Top10热门品类"></a>6.1 需求1：Top10热门品类</h2><p><img src="https://s2.loli.net/2021/12/11/NHbitm2oXMDEnqZ.jpg"></p>
<h3 id="6-1-1-需求说明"><a href="#6-1-1-需求说明" class="headerlink" title="6.1.1 需求说明"></a>6.1.1 需求说明</h3><p>品类是指产品的分类，大型电商网站品类分多级，咱们的项目中品类只有一级，不同的公司可能对热门的定义不一样。我们按照每个品类的点击、下单、支付的量来统计热门品类。</p>
<p>鞋            点击数 下单数  支付数<br>衣服        点击数 下单数  支付数<br>电脑        点击数 下单数  支付数</p>
<p>例如，综合排名 = 点击数<em>20%+下单数</em>30%+支付数*50%</p>
<p>本项目需求优化为：<font color ='red' >先按照点击数排名，靠前的就排名高；如果点击数相同，再比较下单数；下单数再相同，就比较支付数。</font></p>
<h3 id="6-1-2-实现方案一"><a href="#6-1-2-实现方案一" class="headerlink" title="6.1.2 实现方案一"></a>6.1.2 实现方案一</h3><h4 id="6-1-2-1-需求分析"><a href="#6-1-2-1-需求分析" class="headerlink" title="6.1.2.1 需求分析"></a>6.1.2.1 需求分析</h4><p>分别统计每个品类点击的次数，下单的次数和支付的次数：<br>（品类，点击总数）（品类，下单总数）（品类，支付总数）</p>
<h4 id="6-1-2-2-需求实现"><a href="#6-1-2-2-需求实现" class="headerlink" title="6.1.2.2 需求实现"></a>6.1.2.2 需求实现</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">top10_1</span> </span>= &#123;</span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">&quot;WordCount&quot;</span>).setMaster(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//读取文件</span></span><br><span class="line">    <span class="keyword">val</span> userVisitRDD: <span class="type">RDD</span>[<span class="type">String</span>] = sparkContext.textFile(<span class="string">&quot;data/user_visit_action.txt&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//统计品类的点击数量</span></span><br><span class="line">    <span class="keyword">val</span> clickCountRDD: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = userVisitRDD.filter(</span><br><span class="line">        line =&gt; &#123;</span><br><span class="line">            (line.split(<span class="string">&quot;_&quot;</span>)(<span class="number">6</span>) != <span class="string">&quot;-1&quot;</span>) &amp;&amp; (line.split(<span class="string">&quot;_&quot;</span>)(<span class="number">7</span>) != <span class="string">&quot;-1&quot;</span>)</span><br><span class="line">        &#125;</span><br><span class="line">    ).map(</span><br><span class="line">        line =&gt; &#123;</span><br><span class="line">            <span class="keyword">val</span> datas: <span class="type">Array</span>[<span class="type">String</span>] = line.split(<span class="string">&quot;_&quot;</span>)</span><br><span class="line">            (datas(<span class="number">6</span>), <span class="number">1</span>)</span><br><span class="line">        &#125;</span><br><span class="line">    ).reduceByKey(_ + _)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//统计品类的下单数量</span></span><br><span class="line">    <span class="keyword">val</span> orderCountRDD: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = userVisitRDD.filter(</span><br><span class="line">        line =&gt; &#123;</span><br><span class="line">            <span class="keyword">val</span> datas = line.split(<span class="string">&quot;_&quot;</span>)</span><br><span class="line">            datas(<span class="number">8</span>) != <span class="string">&quot;null&quot;</span></span><br><span class="line">        &#125;</span><br><span class="line">    ).flatMap(</span><br><span class="line">        line =&gt; &#123;</span><br><span class="line">            <span class="keyword">val</span> datas: <span class="type">Array</span>[<span class="type">String</span>] = line.split(<span class="string">&quot;_&quot;</span>)</span><br><span class="line">            <span class="keyword">val</span> ids: <span class="type">Array</span>[<span class="type">String</span>] = datas(<span class="number">8</span>).split(<span class="string">&quot;,&quot;</span>)</span><br><span class="line">            ids.map((_, <span class="number">1</span>))</span><br><span class="line">        &#125;</span><br><span class="line">    ).reduceByKey(_ + _)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//统计品类的支付数量</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> payCountRDD: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = userVisitRDD.filter(</span><br><span class="line">        line =&gt; &#123;</span><br><span class="line">            <span class="keyword">val</span> datas = line.split(<span class="string">&quot;_&quot;</span>)</span><br><span class="line">            datas(<span class="number">10</span>) != <span class="string">&quot;null&quot;</span></span><br><span class="line">        &#125;</span><br><span class="line">    ).flatMap(</span><br><span class="line">        line =&gt; &#123;</span><br><span class="line">            <span class="keyword">val</span> datas = line.split(<span class="string">&quot;_&quot;</span>)</span><br><span class="line">            <span class="keyword">val</span> ids = datas(<span class="number">10</span>).split(<span class="string">&quot;,&quot;</span>)</span><br><span class="line">            ids.map((_, <span class="number">1</span>))</span><br><span class="line">        &#125;</span><br><span class="line">    ).reduceByKey(_ + _)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> mergeRDD: <span class="type">RDD</span>[(<span class="type">String</span>, (<span class="type">Iterable</span>[<span class="type">Int</span>], <span class="type">Iterable</span>[<span class="type">Int</span>], <span class="type">Iterable</span>[<span class="type">Int</span>]))] = clickCountRDD.cogroup(orderCountRDD, payCountRDD)</span><br><span class="line">    <span class="keyword">val</span> resultRDD: <span class="type">RDD</span>[(<span class="type">String</span>, (<span class="type">Int</span>, <span class="type">Int</span>, <span class="type">Int</span>))] = mergeRDD.mapValues &#123;</span><br><span class="line">        <span class="keyword">case</span> (click, order, pay) =&gt; &#123;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">var</span> clickCnt = <span class="number">0</span></span><br><span class="line">            <span class="keyword">var</span> orderCnt = <span class="number">0</span></span><br><span class="line">            <span class="keyword">var</span> payCnt = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">val</span> iterator1: <span class="type">Iterator</span>[<span class="type">Int</span>] = click.iterator</span><br><span class="line">            <span class="keyword">if</span> (iterator1.hasNext) &#123;</span><br><span class="line">                clickCnt = iterator1.next()</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">val</span> iterator2: <span class="type">Iterator</span>[<span class="type">Int</span>] = order.iterator</span><br><span class="line">            <span class="keyword">if</span> (iterator2.hasNext) &#123;</span><br><span class="line">                orderCnt = iterator2.next()</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">val</span> iterator3: <span class="type">Iterator</span>[<span class="type">Int</span>] = pay.iterator</span><br><span class="line">            <span class="keyword">if</span> (iterator3.hasNext) &#123;</span><br><span class="line">                payCnt = iterator3.next()</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            (clickCnt, orderCnt, payCnt)</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> top10 = resultRDD.sortBy(_._2, <span class="literal">false</span>).take(<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    top10.foreach(println)</span><br><span class="line"></span><br><span class="line">    sparkContext.stop()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="6-1-3-实现方案二"><a href="#6-1-3-实现方案二" class="headerlink" title="6.1.3 实现方案二"></a>6.1.3 实现方案二</h3><h4 id="6-1-3-1-需求分析"><a href="#6-1-3-1-需求分析" class="headerlink" title="6.1.3.1 需求分析"></a>6.1.3.1 需求分析</h4><p>一次性统计每个品类点击的次数，下单的次数和支付的次数：<br>（品类，（点击总数，下单总数，支付总数））</p>
<h4 id="6-1-3-2-需求实现"><a href="#6-1-3-2-需求实现" class="headerlink" title="6.1.3.2 需求实现"></a>6.1.3.2 需求实现</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//方式一</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">top10_2</span> </span>= &#123;</span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">&quot;WordCount&quot;</span>).setMaster(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//读取文件</span></span><br><span class="line">    <span class="keyword">val</span> userVisitRDD: <span class="type">RDD</span>[<span class="type">String</span>] = sparkContext.textFile(<span class="string">&quot;data/user_visit_action.txt&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//统计品类的点击数量</span></span><br><span class="line">    <span class="keyword">val</span> clickCountRDD: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = userVisitRDD.filter(</span><br><span class="line">        line =&gt; &#123;</span><br><span class="line">            (line.split(<span class="string">&quot;_&quot;</span>)(<span class="number">6</span>) != <span class="string">&quot;-1&quot;</span>) &amp;&amp; (line.split(<span class="string">&quot;_&quot;</span>)(<span class="number">7</span>) != <span class="string">&quot;-1&quot;</span>)</span><br><span class="line">        &#125;</span><br><span class="line">    ).map(</span><br><span class="line">        line =&gt; &#123;</span><br><span class="line">            <span class="keyword">val</span> datas: <span class="type">Array</span>[<span class="type">String</span>] = line.split(<span class="string">&quot;_&quot;</span>)</span><br><span class="line">            (datas(<span class="number">6</span>), <span class="number">1</span>)</span><br><span class="line">        &#125;</span><br><span class="line">    ).reduceByKey(_ + _)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//统计品类的下单数量</span></span><br><span class="line">    <span class="keyword">val</span> orderCountRDD: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = userVisitRDD.filter(</span><br><span class="line">        line =&gt; &#123;</span><br><span class="line">            <span class="keyword">val</span> datas = line.split(<span class="string">&quot;_&quot;</span>)</span><br><span class="line">            datas(<span class="number">8</span>) != <span class="string">&quot;null&quot;</span></span><br><span class="line">        &#125;</span><br><span class="line">    ).flatMap(</span><br><span class="line">        line =&gt; &#123;</span><br><span class="line">            <span class="keyword">val</span> datas: <span class="type">Array</span>[<span class="type">String</span>] = line.split(<span class="string">&quot;_&quot;</span>)</span><br><span class="line">            <span class="keyword">val</span> ids: <span class="type">Array</span>[<span class="type">String</span>] = datas(<span class="number">8</span>).split(<span class="string">&quot;,&quot;</span>)</span><br><span class="line">            ids.map((_, <span class="number">1</span>))</span><br><span class="line">        &#125;</span><br><span class="line">    ).reduceByKey(_ + _)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//统计品类的支付数量</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> payCountRDD: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = userVisitRDD.filter(</span><br><span class="line">        line =&gt; &#123;</span><br><span class="line">            <span class="keyword">val</span> datas = line.split(<span class="string">&quot;_&quot;</span>)</span><br><span class="line">            datas(<span class="number">10</span>) != <span class="string">&quot;null&quot;</span></span><br><span class="line">        &#125;</span><br><span class="line">    ).flatMap(</span><br><span class="line">        line =&gt; &#123;</span><br><span class="line">            <span class="keyword">val</span> datas = line.split(<span class="string">&quot;_&quot;</span>)</span><br><span class="line">            <span class="keyword">val</span> ids = datas(<span class="number">10</span>).split(<span class="string">&quot;,&quot;</span>)</span><br><span class="line">            ids.map((_, <span class="number">1</span>))</span><br><span class="line">        &#125;</span><br><span class="line">    ).reduceByKey(_ + _)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> unionRDD: <span class="type">RDD</span>[(<span class="type">String</span>, (<span class="type">Int</span>, <span class="type">Int</span>, <span class="type">Int</span>))] = clickCountRDD.map &#123;</span><br><span class="line">        <span class="keyword">case</span> (cid, clickCount) =&gt; &#123;</span><br><span class="line">            (cid, (clickCount, <span class="number">0</span>, <span class="number">0</span>))</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;.union(</span><br><span class="line">        orderCountRDD.map &#123;</span><br><span class="line">            <span class="keyword">case</span> (cid, orderCount) =&gt; &#123;</span><br><span class="line">                (cid, (<span class="number">0</span>, orderCount, <span class="number">0</span>))</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    ).union(</span><br><span class="line">        payCountRDD.map &#123;</span><br><span class="line">            <span class="keyword">case</span> (cid, payCount) =&gt; &#123;</span><br><span class="line">                (cid, (<span class="number">0</span>, <span class="number">0</span>, payCount))</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">val</span> top10: <span class="type">Array</span>[(<span class="type">String</span>, (<span class="type">Int</span>, <span class="type">Int</span>, <span class="type">Int</span>))] = unionRDD.reduceByKey(</span><br><span class="line">        (t1, t2) =&gt; &#123;</span><br><span class="line">            (t1._1 + t2._1, t1._2 + t2._2, t1._3 + t2._3)</span><br><span class="line">        &#125;</span><br><span class="line">    ).sortBy(_._2, <span class="literal">false</span>).take(<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    top10.foreach(println)</span><br><span class="line"></span><br><span class="line">    sparkContext.stop()</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//方式二</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">top10_3</span> </span>= &#123;</span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">&quot;WordCount&quot;</span>).setMaster(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">    <span class="comment">//读取文件</span></span><br><span class="line">    <span class="keyword">val</span> userVisitRDD: <span class="type">RDD</span>[<span class="type">String</span>] = sparkContext.textFile(<span class="string">&quot;data/user_visit_action.txt&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> top10: <span class="type">Array</span>[(<span class="type">String</span>, (<span class="type">Int</span>, <span class="type">Int</span>, <span class="type">Int</span>))] = userVisitRDD.flatMap(</span><br><span class="line">        line =&gt; &#123;</span><br><span class="line">            <span class="keyword">val</span> datas = line.split(<span class="string">&quot;_&quot;</span>)</span><br><span class="line">            <span class="keyword">if</span> ((line.split(<span class="string">&quot;_&quot;</span>)(<span class="number">6</span>) != <span class="string">&quot;-1&quot;</span>) &amp;&amp; (line.split(<span class="string">&quot;_&quot;</span>)(<span class="number">7</span>) != <span class="string">&quot;-1&quot;</span>)) &#123;</span><br><span class="line">                <span class="type">List</span>((datas(<span class="number">6</span>), (<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>)))</span><br><span class="line">            &#125; <span class="keyword">else</span> <span class="keyword">if</span> (datas(<span class="number">8</span>) != <span class="string">&quot;null&quot;</span>) &#123;</span><br><span class="line">                <span class="keyword">val</span> ids: <span class="type">Array</span>[<span class="type">String</span>] = datas(<span class="number">8</span>).split(<span class="string">&quot;,&quot;</span>)</span><br><span class="line">                ids.map((_, (<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>)))</span><br><span class="line">            &#125; <span class="keyword">else</span> <span class="keyword">if</span> (datas(<span class="number">10</span>) != <span class="string">&quot;null&quot;</span>) &#123;</span><br><span class="line">                <span class="keyword">val</span> ids: <span class="type">Array</span>[<span class="type">String</span>] = datas(<span class="number">10</span>).split(<span class="string">&quot;,&quot;</span>)</span><br><span class="line">                ids.map((_, (<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>)))</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="type">Nil</span></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    ).reduceByKey(</span><br><span class="line">        (t1, t2) =&gt; &#123;</span><br><span class="line">            (t1._1 + t2._1, t1._2 + t2._2, t1._3 + t2._3)</span><br><span class="line">        &#125;</span><br><span class="line">    ).sortBy(_._2, <span class="literal">false</span>).take(<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    top10.foreach(println)</span><br><span class="line"></span><br><span class="line">    sparkContext.stop()</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="6-1-4-实现方案三"><a href="#6-1-4-实现方案三" class="headerlink" title="6.1.4 实现方案三"></a>6.1.4 实现方案三</h3><h4 id="6-1-4-1-需求分析"><a href="#6-1-4-1-需求分析" class="headerlink" title="6.1.4.1 需求分析"></a>6.1.4.1 需求分析</h4><p>使用累加器的方式聚合数据</p>
<h4 id="6-1-4-2-需求实现"><a href="#6-1-4-2-需求实现" class="headerlink" title="6.1.4.2 需求实现"></a>6.1.4.2 需求实现</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">top10_4</span> </span>= &#123;</span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">&quot;WordCount&quot;</span>).setMaster(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">    <span class="comment">//创建累加器</span></span><br><span class="line">    <span class="keyword">val</span> accumulator = <span class="keyword">new</span> <span class="type">HotCategoryAccumulator</span>()</span><br><span class="line">    <span class="comment">//注册累加器</span></span><br><span class="line">    sparkContext.register(accumulator, <span class="string">&quot;HotCategoryAccumulator&quot;</span>)</span><br><span class="line">    <span class="comment">//读取文件</span></span><br><span class="line">    <span class="keyword">val</span> userVisitRDD: <span class="type">RDD</span>[<span class="type">String</span>] = sparkContext.textFile(<span class="string">&quot;data/user_visit_action.txt&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//遍历数据</span></span><br><span class="line">    userVisitRDD.foreach(</span><br><span class="line">        line =&gt; &#123;</span><br><span class="line">            <span class="keyword">val</span> datas = line.split(<span class="string">&quot;_&quot;</span>)</span><br><span class="line">            <span class="keyword">if</span> ((datas(<span class="number">6</span>) != <span class="string">&quot;-1&quot;</span>) &amp;&amp; (datas(<span class="number">7</span>) != <span class="string">&quot;-1&quot;</span>)) &#123;</span><br><span class="line">                accumulator.add(datas(<span class="number">6</span>), <span class="string">&quot;click&quot;</span>)</span><br><span class="line">            &#125; <span class="keyword">else</span> <span class="keyword">if</span> (datas(<span class="number">8</span>) != <span class="string">&quot;null&quot;</span>) &#123;</span><br><span class="line">                <span class="keyword">val</span> ids: <span class="type">Array</span>[<span class="type">String</span>] = datas(<span class="number">8</span>).split(<span class="string">&quot;,&quot;</span>)</span><br><span class="line">                ids.foreach(</span><br><span class="line">                    id =&gt; &#123;</span><br><span class="line">                        accumulator.add(id, <span class="string">&quot;order&quot;</span>)</span><br><span class="line">                    &#125;</span><br><span class="line">                )</span><br><span class="line">            &#125; <span class="keyword">else</span> <span class="keyword">if</span> (datas(<span class="number">10</span>) != <span class="string">&quot;null&quot;</span>) &#123;</span><br><span class="line">                <span class="keyword">val</span> ids: <span class="type">Array</span>[<span class="type">String</span>] = datas(<span class="number">10</span>).split(<span class="string">&quot;,&quot;</span>)</span><br><span class="line">                ids.foreach(</span><br><span class="line">                    id =&gt; &#123;</span><br><span class="line">                        accumulator.add(id, <span class="string">&quot;pay&quot;</span>)</span><br><span class="line">                    &#125;</span><br><span class="line">                )</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="type">Nil</span></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    )</span><br><span class="line">    <span class="comment">//获取累加器的值</span></span><br><span class="line">    <span class="keyword">val</span> accResult: mutable.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">HotCategory</span>] = accumulator.value</span><br><span class="line">    <span class="keyword">val</span> top10: <span class="type">List</span>[<span class="type">HotCategory</span>] = accResult.toList.map(_._2).sortBy(</span><br><span class="line">        obj =&gt; &#123;</span><br><span class="line">            (obj.clickCnt, obj.orderCnt, obj.payCnt)</span><br><span class="line">        &#125;</span><br><span class="line">    )(<span class="type">Ordering</span>.<span class="type">Tuple3</span>(<span class="type">Ordering</span>.<span class="type">Int</span>.reverse, <span class="type">Ordering</span>.<span class="type">Int</span>.reverse, <span class="type">Ordering</span>.<span class="type">Int</span>.reverse)).take(<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    top10.foreach(println)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    sparkContext.stop()</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//累加器</span></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">HotCategory</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="class">                          var cid: <span class="type">String</span>,</span></span></span><br><span class="line"><span class="params"><span class="class">                          var clickCnt: <span class="type">Int</span> = 0,</span></span></span><br><span class="line"><span class="params"><span class="class">                          var orderCnt: <span class="type">Int</span> = 0,</span></span></span><br><span class="line"><span class="params"><span class="class">                          var payCnt: <span class="type">Int</span> = 0</span></span></span><br><span class="line"><span class="params"><span class="class">                      </span>)</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">HotCategoryAccumulator</span> <span class="keyword">extends</span> <span class="title">AccumulatorV2</span>[(<span class="type">String</span>, <span class="type">String</span>), mutable.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">HotCategory</span>]] </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">val</span> hcMap = mutable.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">HotCategory</span>]()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">isZero</span></span>: <span class="type">Boolean</span> = &#123;</span><br><span class="line">        hcMap.isEmpty</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">copy</span></span>(): <span class="type">AccumulatorV2</span>[(<span class="type">String</span>, <span class="type">String</span>), mutable.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">HotCategory</span>]] = &#123;</span><br><span class="line">        <span class="keyword">new</span> <span class="type">HotCategoryAccumulator</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">reset</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">        hcMap.clear()</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">add</span></span>(v: (<span class="type">String</span>, <span class="type">String</span>)): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> (cid, actionType) = v</span><br><span class="line">        <span class="keyword">val</span> hotCategory: <span class="type">HotCategory</span> = hcMap.getOrElse(cid, <span class="type">HotCategory</span>(cid))</span><br><span class="line">        actionType <span class="keyword">match</span> &#123;</span><br><span class="line">            <span class="keyword">case</span> <span class="string">&quot;click&quot;</span> =&gt; &#123;</span><br><span class="line">                hotCategory.clickCnt += <span class="number">1</span></span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">case</span> <span class="string">&quot;order&quot;</span> =&gt; &#123;</span><br><span class="line">                hotCategory.orderCnt += <span class="number">1</span></span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">case</span> <span class="string">&quot;pay&quot;</span> =&gt; &#123;</span><br><span class="line">                hotCategory.payCnt += <span class="number">1</span></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        hcMap.update(cid, hotCategory)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">merge</span></span>(other: <span class="type">AccumulatorV2</span>[(<span class="type">String</span>, <span class="type">String</span>), mutable.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">HotCategory</span>]]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> otherMap: mutable.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">HotCategory</span>] = other.value</span><br><span class="line">        otherMap.foreach &#123;</span><br><span class="line">            <span class="keyword">case</span> (cid, accumulator) =&gt; &#123;</span><br><span class="line">                <span class="keyword">val</span> hotCategory: <span class="type">HotCategory</span> = hcMap.getOrElse(cid, <span class="type">HotCategory</span>(cid))</span><br><span class="line">                hotCategory.clickCnt += accumulator.clickCnt</span><br><span class="line">                hotCategory.orderCnt += accumulator.orderCnt</span><br><span class="line">                hotCategory.payCnt += accumulator.payCnt</span><br><span class="line">                hcMap.update(cid, hotCategory)</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">value</span></span>: mutable.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">HotCategory</span>] = &#123;</span><br><span class="line">        hcMap</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="6-2-需求2：Top10热门品类中每个品类的Top10活跃Session统计"><a href="#6-2-需求2：Top10热门品类中每个品类的Top10活跃Session统计" class="headerlink" title="6.2 需求2：Top10热门品类中每个品类的Top10活跃Session统计"></a>6.2 需求2：Top10热门品类中每个品类的Top10活跃Session统计</h2><h3 id="6-2-1-需求说明"><a href="#6-2-1-需求说明" class="headerlink" title="6.2.1 需求说明"></a>6.2.1 需求说明</h3><p>在需求一的基础上，增加每个品类用户session的点击统计</p>
<h3 id="6-2-2-需求分析"><a href="#6-2-2-需求分析" class="headerlink" title="6.2.2 需求分析"></a>6.2.2 需求分析</h3><ol>
<li>按照需求一的逻辑获取Top10品类id</li>
<li>过滤Top10品类相关数据</li>
<li>结构转换line =&gt; ((品类, session), 1) =&gt; ((品类, session), sum)</li>
<li>结构转换(品类, (session, sum)) =&gt; (品类, Iter[(session1, sum), (session2, sum), (session3, sum)])</li>
<li>将分组后的数据进行排序(降序)，取 Top10</li>
</ol>
<h3 id="6-2-3-功能实现"><a href="#6-2-3-功能实现" class="headerlink" title="6.2.3 功能实现"></a>6.2.3 功能实现</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.rdd.<span class="type">RDD</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">HotCategoryTop10Analysis_feature2</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">&quot;WordCount&quot;</span>).setMaster(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">        <span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">        <span class="comment">//读取文件</span></span><br><span class="line">        <span class="keyword">val</span> userVisitRDD: <span class="type">RDD</span>[<span class="type">String</span>] = sparkContext.textFile(<span class="string">&quot;data/user_visit_action.txt&quot;</span>)</span><br><span class="line"></span><br><span class="line">        userVisitRDD.cache()</span><br><span class="line"></span><br><span class="line">        <span class="comment">//需求1</span></span><br><span class="line">        <span class="keyword">val</span> top10: <span class="type">Array</span>[(<span class="type">String</span>, (<span class="type">Int</span>, <span class="type">Int</span>, <span class="type">Int</span>))] = userVisitRDD.flatMap(</span><br><span class="line">            line =&gt; &#123;</span><br><span class="line">                <span class="keyword">val</span> datas: <span class="type">Array</span>[<span class="type">String</span>] = line.split(<span class="string">&quot;_&quot;</span>)</span><br><span class="line">                <span class="keyword">if</span> ((line.split(<span class="string">&quot;_&quot;</span>)(<span class="number">6</span>) != <span class="string">&quot;-1&quot;</span>) &amp;&amp; (line.split(<span class="string">&quot;_&quot;</span>)(<span class="number">7</span>) != <span class="string">&quot;-1&quot;</span>)) &#123;</span><br><span class="line">                    <span class="type">List</span>((datas(<span class="number">6</span>), (<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>)))</span><br><span class="line">                &#125; <span class="keyword">else</span> <span class="keyword">if</span> (datas(<span class="number">8</span>) != <span class="string">&quot;null&quot;</span>) &#123;</span><br><span class="line">                    <span class="keyword">val</span> ids: <span class="type">Array</span>[<span class="type">String</span>] = datas(<span class="number">8</span>).split(<span class="string">&quot;,&quot;</span>)</span><br><span class="line">                    ids.map((_, (<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>)))</span><br><span class="line">                &#125; <span class="keyword">else</span> <span class="keyword">if</span> (datas(<span class="number">10</span>) != <span class="string">&quot;null&quot;</span>) &#123;</span><br><span class="line">                    <span class="keyword">val</span> ids: <span class="type">Array</span>[<span class="type">String</span>] = datas(<span class="number">10</span>).split(<span class="string">&quot;,&quot;</span>)</span><br><span class="line">                    ids.map((_, (<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>)))</span><br><span class="line">                &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                    <span class="type">Nil</span></span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        ).reduceByKey(</span><br><span class="line">            (t1, t2) =&gt; &#123;</span><br><span class="line">                (t1._1 + t2._1, t1._2 + t2._2, t1._3 + t2._3)</span><br><span class="line">            &#125;</span><br><span class="line">        ).sortBy(_._2, <span class="literal">false</span>).take(<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">//top10.foreach(println)</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> top10Ids: <span class="type">Array</span>[<span class="type">String</span>] = top10.map(_._1)</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1. 将数据进行筛选过滤，只保留 Top10 的热门品类的数据</span></span><br><span class="line">        <span class="comment">// 1.1 将数据转换为样例类</span></span><br><span class="line">        <span class="keyword">val</span> actionRDD: <span class="type">RDD</span>[<span class="type">UserVisitAction</span>] = userVisitRDD.map(</span><br><span class="line">            line =&gt; &#123;</span><br><span class="line">                <span class="keyword">val</span> datas: <span class="type">Array</span>[<span class="type">String</span>] = line.split(<span class="string">&quot;_&quot;</span>)</span><br><span class="line">                <span class="type">UserVisitAction</span>(</span><br><span class="line">                    datas(<span class="number">0</span>),</span><br><span class="line">                    datas(<span class="number">1</span>).toLong,</span><br><span class="line">                    datas(<span class="number">2</span>),</span><br><span class="line">                    datas(<span class="number">3</span>).toLong,</span><br><span class="line">                    datas(<span class="number">4</span>),</span><br><span class="line">                    datas(<span class="number">5</span>),</span><br><span class="line">                    datas(<span class="number">6</span>).toLong,</span><br><span class="line">                    datas(<span class="number">7</span>).toLong,</span><br><span class="line">                    datas(<span class="number">8</span>),</span><br><span class="line">                    datas(<span class="number">9</span>),</span><br><span class="line">                    datas(<span class="number">10</span>),</span><br><span class="line">                    datas(<span class="number">11</span>),</span><br><span class="line">                    datas(<span class="number">12</span>).toLong)</span><br><span class="line">            &#125;</span><br><span class="line">        )</span><br><span class="line">        <span class="comment">// 1.2 将数据进行过滤</span></span><br><span class="line">        <span class="keyword">val</span> filterRDD: <span class="type">RDD</span>[<span class="type">UserVisitAction</span>] = actionRDD.filter(</span><br><span class="line">            action =&gt; &#123;</span><br><span class="line">                <span class="keyword">if</span> (action.click_category_id != <span class="number">-1</span>) &#123;</span><br><span class="line">                    top10Ids.contains(action.click_category_id.toString)</span><br><span class="line">                &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                    <span class="literal">false</span></span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        )</span><br><span class="line">        <span class="comment">// 2. 将数据进行结构的转换</span></span><br><span class="line">        <span class="comment">//    line =&gt; ((品类, session), 1)</span></span><br><span class="line">        <span class="comment">// 3. 将转换结构后的数据进行统计</span></span><br><span class="line">        <span class="comment">//    line =&gt; ((品类, session), 1) =&gt; ((品类, session), sum)</span></span><br><span class="line">        <span class="keyword">val</span> reduceByKeyRDD: <span class="type">RDD</span>[((<span class="type">Long</span>, <span class="type">String</span>), <span class="type">Int</span>)] = filterRDD.map(</span><br><span class="line">            action =&gt; &#123;</span><br><span class="line">                ((action.click_category_id, action.session_id), <span class="number">1</span>)</span><br><span class="line">            &#125;</span><br><span class="line">        ).reduceByKey(_ + _)</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 4. 将统计结果进行结构的转换</span></span><br><span class="line">        <span class="comment">// ((品类, session), sum) =&gt; (品类, (session, sum))</span></span><br><span class="line">        <span class="comment">// 5. 将转换结构后的数据按照品类进行分组</span></span><br><span class="line">        <span class="comment">// (品类, (session, sum)) =&gt; (品类, Iter[(session1, sum), (session2, sum), (session3, sum)])</span></span><br><span class="line">        <span class="keyword">val</span> groupRDD: <span class="type">RDD</span>[(<span class="type">Long</span>, <span class="type">Iterable</span>[(<span class="type">String</span>, <span class="type">Int</span>)])] = reduceByKeyRDD.map &#123;</span><br><span class="line">            <span class="keyword">case</span> ((cid, sessionid), sum) =&gt; (cid, (sessionid, sum))</span><br><span class="line">        &#125;.groupByKey()</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 6. 将分组后的数据进行排序(降序)，取 Top10</span></span><br><span class="line">        <span class="keyword">val</span> result: <span class="type">RDD</span>[(<span class="type">Long</span>, <span class="type">List</span>[(<span class="type">String</span>, <span class="type">Int</span>)])] = groupRDD.mapValues(</span><br><span class="line">            iter =&gt; &#123;</span><br><span class="line">                iter.toList.sortBy(_._2)(<span class="type">Ordering</span>.<span class="type">Int</span>.reverse).take(<span class="number">10</span>)</span><br><span class="line">            &#125;</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 7. 将结果采集后打印在控制台</span></span><br><span class="line">        result.collect().sortBy(_._1).foreach(println)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        sparkContext.stop()</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//用户访问动作表</span></span><br><span class="line">    <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">UserVisitAction</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="class">                                  date: <span class="type">String</span>, //用户点击行为的日期</span></span></span><br><span class="line"><span class="params"><span class="class">                                  user_id: <span class="type">Long</span>, //用户的<span class="type">ID</span></span></span></span><br><span class="line"><span class="params"><span class="class">                                  session_id: <span class="type">String</span>, //<span class="type">Session</span>的<span class="type">ID</span></span></span></span><br><span class="line"><span class="params"><span class="class">                                  page_id: <span class="type">Long</span>, //某个页面的<span class="type">ID</span></span></span></span><br><span class="line"><span class="params"><span class="class">                                  action_time: <span class="type">String</span>, //动作的时间点</span></span></span><br><span class="line"><span class="params"><span class="class">                                  search_keyword: <span class="type">String</span>, //用户搜索的关键词</span></span></span><br><span class="line"><span class="params"><span class="class">                                  click_category_id: <span class="type">Long</span>, //某一个商品品类的<span class="type">ID</span></span></span></span><br><span class="line"><span class="params"><span class="class">                                  click_product_id: <span class="type">Long</span>, //某一个商品的<span class="type">ID</span></span></span></span><br><span class="line"><span class="params"><span class="class">                                  order_category_ids: <span class="type">String</span>, //一次订单中所有品类的<span class="type">ID</span>集合</span></span></span><br><span class="line"><span class="params"><span class="class">                                  order_product_ids: <span class="type">String</span>, //一次订单中所有商品的<span class="type">ID</span>集合</span></span></span><br><span class="line"><span class="params"><span class="class">                                  pay_category_ids: <span class="type">String</span>, //一次支付中所有品类的<span class="type">ID</span>集合</span></span></span><br><span class="line"><span class="params"><span class="class">                                  pay_product_ids: <span class="type">String</span>, //一次支付中所有商品的<span class="type">ID</span>集合</span></span></span><br><span class="line"><span class="params"><span class="class">                                  city_id: <span class="type">Long</span> //城市 id</span></span></span><br><span class="line"><span class="params"><span class="class">                              </span>)</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="6-3-需求3：页面单跳转换率统计"><a href="#6-3-需求3：页面单跳转换率统计" class="headerlink" title="6.3 需求3：页面单跳转换率统计"></a>6.3 需求3：页面单跳转换率统计</h2><h3 id="6-3-1-需求说明"><a href="#6-3-1-需求说明" class="headerlink" title="6.3.1 需求说明"></a>6.3.1 需求说明</h3><ol>
<li><p>页面单跳转化率<br> 计算页面单跳转化率，什么是页面单跳转换率，比如一个用户在一次 Session 过程中访问的页面路径 3,5,7,9,10,21，那么页面 3 跳到页面 5 叫一次单跳，7-9 也叫一次单跳，那么单跳转化率就是要统计页面点击的概率。<br> 比如：计算 3-5 的单跳转化率，先获取符合条件的 Session 对于页面 3 的访问次数（PV）为 A，然后获取符合条件的 Session 中访问了页面 3 又紧接着访问了页面 5 的次数为 B，那么 B/A 就是 3-5 的页面单跳转化率。</p>
</li>
<li><p>统计页面单跳转化率意义<br> 产品经理和运营总监，可以根据这个指标，去尝试分析，整个网站，产品，各个页面的表现怎么样，是不是需要去优化产品的布局；吸引用户最终可以进入最后的支付页面。<br> 数据分析师，可以此数据做更深一步的计算和分析。<br> 企业管理层，可以看到整个公司的网站，各个页面的之间的跳转的表现如何，可以适当调整公司的经营战略或策略。</p>
</li>
</ol>
<h3 id="6-3-2-需求分析"><a href="#6-3-2-需求分析" class="headerlink" title="6.3.2 需求分析"></a>6.3.2 需求分析</h3><ol>
<li>计算页面累计点击数量</li>
<li>根据session对访问数据分组</li>
<li>排序后出去所有单跳页面的组合</li>
<li>对组合执行wordCount统计</li>
<li>计算</li>
</ol>
<h3 id="6-3-3-功能实现"><a href="#6-3-3-功能实现" class="headerlink" title="6.3.3 功能实现"></a>6.3.3 功能实现</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.rdd.<span class="type">RDD</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">HotCategoryTop10Analysis_feature3</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">&quot;WordCount&quot;</span>).setMaster(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">        <span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">        <span class="comment">//读取文件</span></span><br><span class="line">        <span class="keyword">val</span> userVisitRDD: <span class="type">RDD</span>[<span class="type">String</span>] = sparkContext.textFile(<span class="string">&quot;data/user_visit_action.txt&quot;</span>)</span><br><span class="line"></span><br><span class="line">        userVisitRDD.cache()</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1. 将数据进行筛选过滤，只保留 Top10 的热门品类的数据</span></span><br><span class="line">        <span class="comment">// 1.1 将数据转换为样例类</span></span><br><span class="line">        <span class="keyword">val</span> actionRDD: <span class="type">RDD</span>[<span class="type">UserVisitAction</span>] = userVisitRDD.map(</span><br><span class="line">            line =&gt; &#123;</span><br><span class="line">                <span class="keyword">val</span> datas: <span class="type">Array</span>[<span class="type">String</span>] = line.split(<span class="string">&quot;_&quot;</span>)</span><br><span class="line">                <span class="type">UserVisitAction</span>(</span><br><span class="line">                    datas(<span class="number">0</span>),</span><br><span class="line">                    datas(<span class="number">1</span>).toLong,</span><br><span class="line">                    datas(<span class="number">2</span>),</span><br><span class="line">                    datas(<span class="number">3</span>).toLong,</span><br><span class="line">                    datas(<span class="number">4</span>),</span><br><span class="line">                    datas(<span class="number">5</span>),</span><br><span class="line">                    datas(<span class="number">6</span>).toLong,</span><br><span class="line">                    datas(<span class="number">7</span>).toLong,</span><br><span class="line">                    datas(<span class="number">8</span>),</span><br><span class="line">                    datas(<span class="number">9</span>),</span><br><span class="line">                    datas(<span class="number">10</span>),</span><br><span class="line">                    datas(<span class="number">11</span>),</span><br><span class="line">                    datas(<span class="number">12</span>).toLong)</span><br><span class="line">            &#125;</span><br><span class="line">        )</span><br><span class="line">        actionRDD.cache()</span><br><span class="line">        <span class="keyword">val</span> pageIdDistinct: <span class="type">Array</span>[<span class="type">Long</span>] = actionRDD.map(_.page_id).distinct().collect()</span><br><span class="line"></span><br><span class="line">        <span class="comment">//0. 计算分母(word(页面),count(数量))</span></span><br><span class="line">        <span class="keyword">val</span> pageCount: <span class="type">Array</span>[(<span class="type">Long</span>, <span class="type">Int</span>)] = actionRDD</span><br><span class="line">            .filter(</span><br><span class="line">                action =&gt; &#123;</span><br><span class="line">                    pageIdDistinct.contains(action.page_id)</span><br><span class="line">                &#125;</span><br><span class="line">            )</span><br><span class="line">            .map(</span><br><span class="line">                action =&gt; (action.page_id, <span class="number">1</span>)</span><br><span class="line">            ).reduceByKey(_ + _).collect()</span><br><span class="line">        <span class="keyword">val</span> pageCountMap: <span class="type">Map</span>[<span class="type">Long</span>, <span class="type">Int</span>] = pageCount.toMap</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">//1. 按照Session分组</span></span><br><span class="line">        <span class="keyword">val</span> sessionGroupRDD: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Iterable</span>[<span class="type">UserVisitAction</span>])] = actionRDD.groupBy(_.session_id)</span><br><span class="line">        <span class="comment">//2. 按时间排序</span></span><br><span class="line">        <span class="keyword">val</span> mapRDD: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">List</span>[(<span class="type">Long</span>, <span class="type">Long</span>)])] = sessionGroupRDD.mapValues(</span><br><span class="line">            iter =&gt; &#123;</span><br><span class="line">                <span class="keyword">val</span> sortAction: <span class="type">List</span>[<span class="type">UserVisitAction</span>] = iter.toList.sortBy(_.action_time)</span><br><span class="line">                <span class="keyword">val</span> pageIdList: <span class="type">List</span>[<span class="type">Long</span>] = sortAction.map(_.page_id)</span><br><span class="line">                <span class="comment">//val sliding: Iterator[List[Long]] = pageIdList.sliding(2)</span></span><br><span class="line">                <span class="comment">//sliding.map(</span></span><br><span class="line">                <span class="comment">//    list=&gt;&#123;</span></span><br><span class="line">                <span class="comment">//        list(0)+&quot;_&quot;+list(1)</span></span><br><span class="line">                <span class="comment">//    &#125;</span></span><br><span class="line">                <span class="comment">//)</span></span><br><span class="line">                <span class="keyword">val</span> zipList: <span class="type">List</span>[(<span class="type">Long</span>, <span class="type">Long</span>)] = pageIdList.zip(pageIdList.tail)</span><br><span class="line">                zipList</span><br><span class="line">            &#125;</span><br><span class="line">        )</span><br><span class="line">        <span class="comment">//3. 计算分子</span></span><br><span class="line">        <span class="comment">//  转换数据格式</span></span><br><span class="line">        <span class="comment">//  转换数据结构 3,5 =&gt; 3-5</span></span><br><span class="line">        <span class="comment">//  将连续页面跳转统计数量</span></span><br><span class="line">        <span class="keyword">val</span> pageListRDD: <span class="type">RDD</span>[<span class="type">List</span>[(<span class="type">Long</span>, <span class="type">Long</span>)]] = mapRDD.map(_._2)</span><br><span class="line">        <span class="keyword">val</span> pageRedirect: <span class="type">RDD</span>[(<span class="type">Long</span>, <span class="type">Long</span>)] = pageListRDD.flatMap(list =&gt; list)</span><br><span class="line">        <span class="keyword">val</span> pageRedirectCount: <span class="type">RDD</span>[((<span class="type">Long</span>, <span class="type">Long</span>), <span class="type">Int</span>)] = pageRedirect.map((_, <span class="number">1</span>)).reduceByKey(_ + _)</span><br><span class="line">        <span class="keyword">val</span> pageCountResult: <span class="type">Array</span>[((<span class="type">Long</span>, <span class="type">Long</span>), <span class="type">Int</span>)] = pageRedirectCount.collect()</span><br><span class="line"></span><br><span class="line">        <span class="comment">//4 计算</span></span><br><span class="line">        pageCountResult.foreach&#123;</span><br><span class="line">            <span class="keyword">case</span> ((page1,page2), count) =&gt;&#123;</span><br><span class="line">                println(<span class="string">s&quot;页面id:<span class="subst">$&#123;page1&#125;</span>-&gt;<span class="subst">$&#123;page2&#125;</span> 单跳转换率:&quot;</span>+(count.toDouble/pageCountMap.getOrElse(page1,<span class="number">0</span>)))</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        sparkContext.stop()</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//用户访问动作表</span></span><br><span class="line">    <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">UserVisitAction</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="class">          date: <span class="type">String</span>, //用户点击行为的日期</span></span></span><br><span class="line"><span class="params"><span class="class">          user_id: <span class="type">Long</span>, //用户的<span class="type">ID</span></span></span></span><br><span class="line"><span class="params"><span class="class">          session_id: <span class="type">String</span>, //<span class="type">Session</span>的<span class="type">ID</span></span></span></span><br><span class="line"><span class="params"><span class="class">          page_id: <span class="type">Long</span>, //某个页面的<span class="type">ID</span></span></span></span><br><span class="line"><span class="params"><span class="class">          action_time: <span class="type">String</span>, //动作的时间点</span></span></span><br><span class="line"><span class="params"><span class="class">          search_keyword: <span class="type">String</span>, //用户搜索的关键词</span></span></span><br><span class="line"><span class="params"><span class="class">          click_category_id: <span class="type">Long</span>, //某一个商品品类的<span class="type">ID</span></span></span></span><br><span class="line"><span class="params"><span class="class">          click_product_id: <span class="type">Long</span>, //某一个商品的<span class="type">ID</span></span></span></span><br><span class="line"><span class="params"><span class="class">          order_category_ids: <span class="type">String</span>, //一次订单中所有品类的<span class="type">ID</span>集合</span></span></span><br><span class="line"><span class="params"><span class="class">          order_product_ids: <span class="type">String</span>, //一次订单中所有商品的<span class="type">ID</span>集合</span></span></span><br><span class="line"><span class="params"><span class="class">          pay_category_ids: <span class="type">String</span>, //一次支付中所有品类的<span class="type">ID</span>集合</span></span></span><br><span class="line"><span class="params"><span class="class">          pay_product_ids: <span class="type">String</span>, //一次支付中所有商品的<span class="type">ID</span>集合</span></span></span><br><span class="line"><span class="params"><span class="class">          city_id: <span class="type">Long</span> //城市 id</span></span></span><br><span class="line"><span class="params"><span class="class">      </span>)</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h1 id="第7章-Spark-Core-源码解析"><a href="#第7章-Spark-Core-源码解析" class="headerlink" title="第7章 Spark Core 源码解析"></a>第7章 Spark Core 源码解析</h1><p><img src="https://s2.loli.net/2022/01/22/E5xmZGhjOsIqJ4M.png" alt="源码图解"></p>
<h2 id="7-1-提交任务源码"><a href="#7-1-提交任务源码" class="headerlink" title="7.1 提交任务源码"></a>7.1 提交任务源码</h2><ul>
<li>提交任务入口  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">bin/spark-submit \</span><br><span class="line">--class org.apache.spark.examples.SparkPi \</span><br><span class="line">--master <span class="built_in">local</span>[2] \</span><br><span class="line">./examples/jars/spark-examples_2.12-3.0.0.jar \</span><br><span class="line">10</span><br></pre></td></tr></table></figure>
<ul>
<li>跟进spark-submit脚本  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">。。。</span><br><span class="line">。。。</span><br><span class="line"><span class="built_in">exec</span> <span class="string">&quot;<span class="variable">$&#123;SPARK_HOME&#125;</span>&quot;</span>/bin/spark-class org.apache.spark.deploy.SparkSubmit <span class="string">&quot;<span class="variable">$@</span>&quot;</span></span><br></pre></td></tr></table></figure>
<ul>
<li>关注 类名为<code>org.apache.spark.deploy.SparkSubmit</code>  </li>
</ul>
</li>
<li>跟进spark-class脚本  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">。。。</span><br><span class="line">。。。</span><br><span class="line">CMD=(<span class="string">&quot;<span class="variable">$&#123;CMD[@]:0:$LAST&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;<span class="variable">$&#123;CMD[@]&#125;</span>&quot;</span></span><br><span class="line"><span class="built_in">exec</span> <span class="string">&quot;<span class="variable">$&#123;CMD[@]&#125;</span>&quot;</span></span><br><span class="line">。。。</span><br><span class="line">。。。</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<ul>
<li>添加echo打印执行命令为  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$JAVA_HOME</span>/bin/java -cp <span class="variable">$SPARK_HOME</span>/conf/:/Users/zhenan/app/spark-3.0.0-bin-hadoop3.2/jars/* -Xmx1g org.apache.spark.deploy.SparkSubmit --master <span class="built_in">local</span>[2] --class org.apache.spark.examples.SparkPi ./examples/jars/spark-examples_2.12-3.0.0.jar 10</span><br></pre></td></tr></table></figure></li>
<li>确定执行类为<code>org.apache.spark.deploy.SparkSubmit</code>  </li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="7-1-1-SparkSubmit源码"><a href="#7-1-1-SparkSubmit源码" class="headerlink" title="7.1.1 SparkSubmit源码"></a>7.1.1 SparkSubmit源码</h3><ul>
<li>跟进<code>org.apache.spark.deploy.SparkSubmit</code>  </li>
<li>进入伴生对象的main方法<code>org.apache.spark.deploy.SparkSubmit#main</code>  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> submit = <span class="keyword">new</span> <span class="type">SparkSubmit</span>() &#123;</span><br><span class="line">    self =&gt;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">override</span> <span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">parseArguments</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">SparkSubmitArguments</span> = &#123;</span><br><span class="line">      <span class="keyword">new</span> <span class="type">SparkSubmitArguments</span>(args) &#123;</span><br><span class="line">        <span class="keyword">override</span> <span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">logInfo</span></span>(msg: =&gt; <span class="type">String</span>): <span class="type">Unit</span> = self.logInfo(msg)</span><br><span class="line">    </span><br><span class="line">        <span class="keyword">override</span> <span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">logWarning</span></span>(msg: =&gt; <span class="type">String</span>): <span class="type">Unit</span> = self.logWarning(msg)</span><br><span class="line">    </span><br><span class="line">        <span class="keyword">override</span> <span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">logError</span></span>(msg: =&gt; <span class="type">String</span>): <span class="type">Unit</span> = self.logError(msg)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">override</span> <span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">logInfo</span></span>(msg: =&gt; <span class="type">String</span>): <span class="type">Unit</span> = printMessage(msg)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">override</span> <span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">logWarning</span></span>(msg: =&gt; <span class="type">String</span>): <span class="type">Unit</span> = printMessage(<span class="string">s&quot;Warning: <span class="subst">$msg</span>&quot;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">override</span> <span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">logError</span></span>(msg: =&gt; <span class="type">String</span>): <span class="type">Unit</span> = printMessage(<span class="string">s&quot;Error: <span class="subst">$msg</span>&quot;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">doSubmit</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="keyword">super</span>.doSubmit(args)</span><br><span class="line">      &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">        <span class="keyword">case</span> e: <span class="type">SparkUserAppException</span> =&gt;</span><br><span class="line">          exitFn(e.exitCode)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">  &#125;</span><br><span class="line">    </span><br><span class="line">  submit.doSubmit(args)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>跟进<code>super.doSubmit(args)</code> </li>
<li>-&gt; 解析参数：<code>val appArgs = parseArguments(args)</code> <ul>
<li>-&gt; <code>new SparkSubmitArguments(args)</code></li>
<li>-&gt; <code>parse(args.asJava)</code>  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Set parameters from command line arguments</span></span><br><span class="line">parse(args.asJava)</span><br></pre></td></tr></table></figure></li>
<li>-&gt; 参数校验：由具体实现类提供逻辑  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (!handle(name, value)) &#123;</span><br><span class="line">  <span class="keyword">break</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
  <img src="https://s2.loli.net/2022/01/22/UOB4kK3TzWZSvtp.jpg"><ul>
<li>-&gt;实现类: <code>org.apache.spark.deploy.SparkSubmitArguments#handle</code></li>
<li>获取到   <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">--master  =&gt; master    =&gt; yarn</span><br><span class="line">--class   =&gt; mainClass =&gt; SparkPI(WordCount)</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>
</li>
<li>-&gt; 执行action动作  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">appArgs.action <span class="keyword">match</span> &#123;</span><br><span class="line">  <span class="keyword">case</span> <span class="type">SparkSubmitAction</span>.<span class="type">SUBMIT</span> =&gt; submit(appArgs, uninitLog)</span><br><span class="line">  <span class="keyword">case</span> <span class="type">SparkSubmitAction</span>.<span class="type">KILL</span> =&gt; kill(appArgs)</span><br><span class="line">  <span class="keyword">case</span> <span class="type">SparkSubmitAction</span>.<span class="type">REQUEST_STATUS</span> =&gt; requestStatus(appArgs)</span><br><span class="line">  <span class="keyword">case</span> <span class="type">SparkSubmitAction</span>.<span class="type">PRINT_VERSION</span> =&gt; printVersion()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>-&gt; 跟进appArgs.action赋值  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Action should be SUBMIT unless otherwise specified</span></span><br><span class="line">action = <span class="type">Option</span>(action).getOrElse(<span class="type">SUBMIT</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>默认值为<code>SUBMIT</code></li>
</ul>
</li>
<li>-&gt; 跟进任务提交方法<code>case SparkSubmitAction.SUBMIT =&gt; submit(appArgs, uninitLog)</code><ul>
<li>-&gt; <code>doRunMain</code></li>
<li>-&gt; <code>runMain</code><ul>
<li>-&gt; 准备提交环境的参数，跟进<code>val (childArgs, childClasspath, sparkConf, childMainClass) = prepareSubmitEnvironment(args)</code><ul>
<li>-&gt; 跟进childMainClass取值,查找赋值的位置</li>
<li>-&gt; <code>val isYarnCluster = clusterManager == YARN &amp;&amp; deployMode == CLUSTER</code></li>
<li>-&gt; 确定类型childMainClass  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (isYarnCluster) &#123;</span><br><span class="line">  childMainClass = <span class="type">YARN_CLUSTER_SUBMIT_CLASS</span></span><br><span class="line">  。。。</span><br><span class="line">  。。。</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>-&gt; <code>private[deploy] val YARN_CLUSTER_SUBMIT_CLASS = &quot;org.apache.spark.deploy.yarn.YarnClusterApplication&quot;</code></li>
</ul>
</li>
<li>-&gt; 添加依赖jar  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> loader = getSubmitClassLoader(sparkConf)</span><br><span class="line"><span class="keyword">for</span> (jar &lt;- childClasspath) &#123;</span><br><span class="line">  addJarToClasspath(jar, loader)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>-&gt; <code>mainClass = Utils.classForName(childMainClass)</code></li>
<li>-&gt; 判断类型，创建Spark应用程序，<font color ='red' >SparkApplication</font>  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> app: <span class="type">SparkApplication</span> = <span class="keyword">if</span> (classOf[<span class="type">SparkApplication</span>].isAssignableFrom(mainClass)) &#123;</span><br><span class="line">  mainClass.getConstructor().newInstance().asInstanceOf[<span class="type">SparkApplication</span>]</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">  <span class="keyword">new</span> <span class="type">JavaMainApplication</span>(mainClass)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>-&gt; 启动应用程序<code>app.start</code>,具体逻辑由childMainClass实现，Yarn模式为<code>org.apache.spark.deploy.yarn.YarnClusterApplication</code></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="7-1-2-YarnClusterApplication-源码"><a href="#7-1-2-YarnClusterApplication-源码" class="headerlink" title="7.1.2 YarnClusterApplication 源码"></a>7.1.2 YarnClusterApplication 源码</h3><ul>
<li>跟进YarnClusterApplication  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span>[spark] <span class="class"><span class="keyword">class</span> <span class="title">YarnClusterApplication</span> <span class="keyword">extends</span> <span class="title">SparkApplication</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">start</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>], conf: <span class="type">SparkConf</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// SparkSubmit would use yarn cache to distribute files &amp; jars in yarn mode,</span></span><br><span class="line">    <span class="comment">// so remove them from sparkConf here for yarn mode.</span></span><br><span class="line">    conf.remove(<span class="type">JARS</span>)</span><br><span class="line">    conf.remove(<span class="type">FILES</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">new</span> <span class="type">Client</span>(<span class="keyword">new</span> <span class="type">ClientArguments</span>(args), conf, <span class="literal">null</span>).run()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>-&gt; 定位start方法  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">new</span> <span class="type">Client</span>(<span class="keyword">new</span> <span class="type">ClientArguments</span>(args), conf, <span class="literal">null</span>).run()</span><br></pre></td></tr></table></figure>
<ul>
<li>-&gt; 跟进ClientArguments：<code>--class =&gt; userClass =&gt; SparkPi(WordCount)</code>  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">case</span> (<span class="string">&quot;--class&quot;</span>) :: value :: tail =&gt;</span><br><span class="line"> userClass = value</span><br><span class="line"> args = tail</span><br></pre></td></tr></table></figure></li>
<li>-&gt; 跟进<code>new Client()</code><ul>
<li>-&gt;创建YarnClient  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">val</span> yarnClient = <span class="type">YarnClient</span>.createYarnClient</span><br></pre></td></tr></table></figure>
<ul>
<li>-&gt; 跟进 <code>createYarnClient</code>  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">public static <span class="type">YarnClient</span> createYarnClient() &#123;</span><br><span class="line">  <span class="type">YarnClient</span> client = <span class="keyword">new</span> <span class="type">YarnClientImpl</span>();</span><br><span class="line">  <span class="keyword">return</span> client;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>-&gt; 查找rmClient<code>org.apache.hadoop.yarn.client.api.impl.YarnClientImpl#rmClient</code>  <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">protected</span> ApplicationClientProtocol rmClient;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>
</li>
<li>-&gt; 跟进<code>org.apache.spark.deploy.yarn.Client#run</code><ul>
<li>-&gt; 跟进<code>org.apache.spark.deploy.yarn.Client#submitApplication</code><ul>
<li>创建Yran客户端,建立连接<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">submitApplication</span></span>(): <span class="type">ApplicationId</span> = &#123;</span><br><span class="line"><span class="type">ResourceRequestHelper</span>.validateResources(sparkConf)</span><br><span class="line">  。。。</span><br><span class="line">  。。。</span><br><span class="line">  launcherBackend.connect()</span><br><span class="line">  yarnClient.init(hadoopConf)</span><br><span class="line">  yarnClient.start()</span><br><span class="line">  。。。</span><br><span class="line">  。。。</span><br><span class="line">  <span class="comment">// Set up the appropriate contexts to launch our AM</span></span><br><span class="line">  <span class="keyword">val</span> containerContext = createContainerLaunchContext(newAppResponse)</span><br><span class="line">  <span class="keyword">val</span> appContext = createApplicationSubmissionContext(newApp, containerContext)</span><br><span class="line">  。。。</span><br><span class="line">  。。。</span><br><span class="line">  logInfo(<span class="string">s&quot;Submitting application <span class="subst">$appId</span> to ResourceManager&quot;</span>)</span><br><span class="line">  yarnClient.submitApplication(appContext)</span><br><span class="line">  。。。</span><br><span class="line">  。。。</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>-&gt; 跟进 <code>createContainerLaunchContext</code> <code>Set up a ContainerLaunchContext to launch our ApplicationMaster container. This sets up the launch environment, java options, and the command for launching the AM.</code><ul>
<li>-&gt; java虚拟机配置  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">javaOpts += <span class="string">&quot;-Xmx&quot;</span> + amMemory + <span class="string">&quot;m&quot;</span></span><br></pre></td></tr></table></figure></li>
<li>-&gt; 垃圾回收  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (useConcurrentAndIncrementalGC) &#123;</span><br><span class="line">  <span class="comment">// In our expts, using (default) throughput collector has severe perf ramifications in</span></span><br><span class="line">  <span class="comment">// multi-tenant machines</span></span><br><span class="line">  javaOpts += <span class="string">&quot;-XX:+UseConcMarkSweepGC&quot;</span></span><br><span class="line">  javaOpts += <span class="string">&quot;-XX:MaxTenuringThreshold=31&quot;</span></span><br><span class="line">  javaOpts += <span class="string">&quot;-XX:SurvivorRatio=8&quot;</span></span><br><span class="line">  javaOpts += <span class="string">&quot;-XX:+CMSIncrementalMode&quot;</span></span><br><span class="line">  javaOpts += <span class="string">&quot;-XX:+CMSIncrementalPacing&quot;</span></span><br><span class="line">  javaOpts += <span class="string">&quot;-XX:CMSIncrementalDutyCycleMin=0&quot;</span></span><br><span class="line">  javaOpts += <span class="string">&quot;-XX:CMSIncrementalDutyCycle=10&quot;</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>-&gt; 拼接容器运行命令<ul>
<li>-&gt; 确定amClass  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> amClass =</span><br><span class="line">  <span class="keyword">if</span> (isClusterMode) &#123;</span><br><span class="line">    <span class="type">Utils</span>.classForName(<span class="string">&quot;org.apache.spark.deploy.yarn.ApplicationMaster&quot;</span>).getName</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="type">Utils</span>.classForName(<span class="string">&quot;org.apache.spark.deploy.yarn.ExecutorLauncher&quot;</span>).getName</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>Cluster:org.apache.spark.deploy.yarn.ApplicationMaster</li>
<li>Client:org.apache.spark.deploy.yarn.ExecutorLauncher</li>
</ul>
</li>
<li>-&gt; 拼接amArgs  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> amArgs =</span><br><span class="line">  <span class="type">Seq</span>(amClass) ++ userClass ++ userJar ++ primaryPyFile ++ primaryRFile ++ userArgs ++</span><br><span class="line">  <span class="type">Seq</span>(<span class="string">&quot;--properties-file&quot;</span>,</span><br><span class="line">    buildPath(<span class="type">Environment</span>.<span class="type">PWD</span>.$$(), <span class="type">LOCALIZED_CONF_DIR</span>, <span class="type">SPARK_CONF_FILE</span>)) ++</span><br><span class="line">  <span class="type">Seq</span>(<span class="string">&quot;--dist-cache-conf&quot;</span>,</span><br><span class="line">    buildPath(<span class="type">Environment</span>.<span class="type">PWD</span>.$$(), <span class="type">LOCALIZED_CONF_DIR</span>, <span class="type">DIST_CACHE_CONF_FILE</span>))</span><br></pre></td></tr></table></figure></li>
<li>-&gt; 拼接Command 最终结果为<code>bin/java org.apache.spark.deploy.yarn.ApplicationMaster</code><ul>
<li>此时启动的是AM(<code>ApplicationMaster</code>),位于NodeManager<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Command for the ApplicationMaster</span></span><br><span class="line"><span class="keyword">val</span> commands = prefixEnv ++</span><br><span class="line">  <span class="type">Seq</span>(<span class="type">Environment</span>.<span class="type">JAVA_HOME</span>.$$() + <span class="string">&quot;/bin/java&quot;</span>, <span class="string">&quot;-server&quot;</span>) ++</span><br><span class="line">  javaOpts ++ amArgs ++</span><br><span class="line">  <span class="type">Seq</span>(</span><br><span class="line">    <span class="string">&quot;1&gt;&quot;</span>, <span class="type">ApplicationConstants</span>.<span class="type">LOG_DIR_EXPANSION_VAR</span> + <span class="string">&quot;/stdout&quot;</span>,</span><br><span class="line">    <span class="string">&quot;2&gt;&quot;</span>, <span class="type">ApplicationConstants</span>.<span class="type">LOG_DIR_EXPANSION_VAR</span> + <span class="string">&quot;/stderr&quot;</span>)</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="7-1-3-ApplicationMaster-源码"><a href="#7-1-3-ApplicationMaster-源码" class="headerlink" title="7.1.3 ApplicationMaster 源码"></a>7.1.3 ApplicationMaster 源码</h3><ul>
<li>跟进伴生对象<code>org.apache.spark.deploy.yarn.ApplicationMaster</code></li>
<li>定位main方法<ul>
<li>-&gt; 跟进参数封装 <code>val amArgs = new ApplicationMasterArguments(args)</code><ul>
<li>-&gt; 定位<code>parseArgs</code></li>
<li>-&gt; <code>--class =&gt; userClass =&gt; SparkPi/WordCount</code>  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">case</span> (<span class="string">&quot;--class&quot;</span>) :: value :: tail =&gt;</span><br><span class="line">  userClass = value</span><br><span class="line">  args = tail</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>-&gt; 跟进创建ApplicationMaster  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">master = <span class="keyword">new</span> <span class="type">ApplicationMaster</span>(amArgs, sparkConf, yarnConf)</span><br></pre></td></tr></table></figure>
<ul>
<li>-&gt; 创建YarnRMClient  <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> val client = <span class="keyword">new</span> YarnRMClient()</span><br></pre></td></tr></table></figure></li>
<li>-&gt; 定位 run方法<code>org.apache.spark.deploy.yarn.ApplicationMaster#run</code>  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">。。。</span><br><span class="line">。。。</span><br><span class="line"><span class="keyword">if</span> (isClusterMode) &#123;</span><br><span class="line">    runDriver()</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    runExecutorLauncher()</span><br><span class="line">&#125;</span><br><span class="line">。。。</span><br><span class="line">。。。</span><br></pre></td></tr></table></figure>
<ul>
<li>Cluster:runDriver</li>
<li>Client:runExecutorLauncher</li>
</ul>
</li>
<li>-&gt;跟进 <code>org.apache.spark.deploy.yarn.ApplicationMaster#runDriver</code></li>
<li>-&gt; 定位<code>userClassThread = startUserApplication()</code></li>
<li>-&gt; 跟进 <code>startUserApplication</code></li>
<li>-&gt; 获取main方法(–class =&gt; userClass =&gt; SparkPi/WordCount 的main)  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> mainMethod = userClassLoader.loadClass(args.userClass)</span><br><span class="line">    .getMethod(<span class="string">&quot;main&quot;</span>, classOf[<span class="type">Array</span>[<span class="type">String</span>]])</span><br></pre></td></tr></table></figure></li>
<li>启动线程执行main(<font color ='red' >启动Driver</font>)  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> userThread = <span class="keyword">new</span> <span class="type">Thread</span> &#123;</span><br><span class="line">      <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">          。。。</span><br><span class="line">          。。。  </span><br><span class="line">          mainMethod.invoke(<span class="literal">null</span>, userArgs.toArray)</span><br><span class="line">          。。。</span><br><span class="line">          。。。</span><br><span class="line">      &#125;</span><br><span class="line">    userThread.setContextClassLoader(userClassLoader)</span><br><span class="line">    userThread.setName(<span class="string">&quot;Driver&quot;</span>)</span><br><span class="line">    userThread.start()</span><br><span class="line">    userThread</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>此处启动的线程就是Driver</li>
<li>run方法调动的内容为应用程序的main方法(SparkPi/WordCount 的main)</li>
</ul>
</li>
<li>-&gt; ApplicationMaster中<font color ='red' >资源线程阻塞等待</font>  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sc = <span class="type">ThreadUtils</span>.awaitResult(sparkContextPromise.future,</span><br><span class="line"><span class="type">Duration</span>(totalWaitTime, <span class="type">TimeUnit</span>.<span class="type">MILLISECONDS</span>))</span><br></pre></td></tr></table></figure></li>
<li>-&gt; 计算线程初始化SparkContext完成后退出阻塞状态继续执行<ul>
<li><code>mainMethod.invoke(null, userArgs.toArray)</code>启动SparkPi/WordCount 的main<ul>
<li>new SparkContext()</li>
<li><code>org.apache.spark.scheduler.cluster.YarnClusterScheduler#postStartHook</code>  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">postStartHook</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="type">ApplicationMaster</span>.sparkContextInitialized(sc)</span><br><span class="line">  <span class="keyword">super</span>.postStartHook()</span><br><span class="line">  logInfo(<span class="string">&quot;YarnClusterScheduler.postStartHook done&quot;</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li><font color ='red' >计算线程进入阻塞状态</font>  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">sparkContextInitialized</span></span>(sc: <span class="type">SparkContext</span>) = &#123;</span><br><span class="line">  sparkContextPromise.synchronized &#123;</span><br><span class="line">    <span class="comment">// Notify runDriver function that SparkContext is available</span></span><br><span class="line">    sparkContextPromise.success(sc)</span><br><span class="line">    <span class="comment">// Pause the user class thread in order to make proper initialization in runDriver function.</span></span><br><span class="line">     sparkContextPromise.wait()</span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>此时资源线程条件满足继续执行</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>-&gt; 注册AM 定位 <code>registerAM(host, port, userConf, sc.ui.map(_.webUrl), appAttemptId)</code><ul>
<li>由Client反馈ResourceManager，注册AM的信息</li>
</ul>
</li>
<li>-&gt; 跟进<code>createAllocator(driverRef, userConf, rpcEnv, appAttemptId, distCacheConf)</code><ul>
<li>申请计算资源   <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">。。。</span><br><span class="line">。。。</span><br><span class="line">。。。</span><br><span class="line">allocator = client.createAllocator(</span><br><span class="line">yarnConf,</span><br><span class="line">_sparkConf,</span><br><span class="line">appAttemptId,</span><br><span class="line">driverUrl,</span><br><span class="line">driverRef,</span><br><span class="line">securityMgr,</span><br><span class="line">localResources)</span><br><span class="line">。。。</span><br><span class="line">。。。</span><br><span class="line">。。。</span><br></pre></td></tr></table></figure></li>
<li>分配资源   <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">allocator.allocateResources()</span><br></pre></td></tr></table></figure>
<ul>
<li>获取运行容器  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> allocatedContainers = allocateResponse.getAllocatedContainers()</span><br></pre></td></tr></table></figure></li>
<li>处理容器 跟进<code>handleAllocatedContainers(allocatedContainers.asScala)</code></li>
<li>分配容器  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Assign remaining that are neither node-local nor rack-local</span></span><br><span class="line"><span class="keyword">val</span> remainingAfterOffRackMatches = <span class="keyword">new</span> <span class="type">ArrayBuffer</span>[<span class="type">Container</span>]</span><br><span class="line"><span class="keyword">for</span> (allocatedContainer &lt;- remainingAfterRackMatches) &#123;</span><br><span class="line">  matchContainerToRequest(allocatedContainer, <span class="type">ANY_HOST</span>, containersToUse,</span><br><span class="line">    remainingAfterOffRackMatches)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>启动容器 跟进  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">runAllocatedContainers(containersToUse)</span><br></pre></td></tr></table></figure></li>
<li><font color ='red' >启动Executors</font>  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">    <span class="keyword">new</span> <span class="type">ExecutorRunnable</span>(</span><br><span class="line">    <span class="type">Some</span>(container),</span><br><span class="line">    conf,</span><br><span class="line">    sparkConf,</span><br><span class="line">    driverUrl,</span><br><span class="line">    executorId,</span><br><span class="line">    executorHostname,</span><br><span class="line">    executorMemory,</span><br><span class="line">    executorCores,</span><br><span class="line">    appAttemptId.getApplicationId.toString,</span><br><span class="line">    securityMgr,</span><br><span class="line">    localResources,</span><br><span class="line">    <span class="type">ResourceProfile</span>.<span class="type">DEFAULT_RESOURCE_PROFILE_ID</span> <span class="comment">// use until fully supported</span></span><br><span class="line">  ).run()</span><br><span class="line">    ```  </span><br><span class="line">- -&gt; 跟进 run方法 &lt;font color =<span class="symbol">&#x27;re</span>d&#x27; &gt;nmClient启动&lt;/font&gt;</span><br><span class="line">    - `org.apache.spark.deploy.yarn.<span class="type">ExecutorRunnable</span>#run`</span><br><span class="line">    - -&gt; nmClient启动容器 `org.apache.spark.deploy.yarn.<span class="type">ExecutorRunnable</span>#startContainer` </span><br><span class="line">        ```scala</span><br><span class="line">        <span class="comment">// Send the start request to the ContainerManager</span></span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">          nmClient.startContainer(container.get, ctx)</span><br><span class="line">        &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">          <span class="keyword">case</span> ex: <span class="type">Exception</span> =&gt;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">SparkException</span>(<span class="string">s&quot;Exception while starting container <span class="subst">$&#123;container.get.getId&#125;</span>&quot;</span> +</span><br><span class="line">              <span class="string">s&quot; on host <span class="subst">$hostname</span>&quot;</span>, ex)</span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>-&gt; 准备运行命令<code>org.apache.spark.deploy.yarn.ExecutorRunnable#prepareCommand</code></li>
<li>确定执行命令  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">  <span class="keyword">val</span> commands = prefixEnv ++</span><br><span class="line"><span class="type">Seq</span>(<span class="type">Environment</span>.<span class="type">JAVA_HOME</span>.$$() + <span class="string">&quot;/bin/java&quot;</span>, <span class="string">&quot;-server&quot;</span>) ++</span><br><span class="line">javaOpts ++</span><br><span class="line"><span class="type">Seq</span>(<span class="string">&quot;org.apache.spark.executor.YarnCoarseGrainedExecutorBackend&quot;</span>,</span><br><span class="line">  <span class="string">&quot;--driver-url&quot;</span>, masterAddress,</span><br><span class="line">  <span class="string">&quot;--executor-id&quot;</span>, executorId,</span><br><span class="line">  <span class="string">&quot;--hostname&quot;</span>, hostname,</span><br><span class="line">  <span class="string">&quot;--cores&quot;</span>, executorCores.toString,</span><br><span class="line">  <span class="string">&quot;--app-id&quot;</span>, appId,</span><br><span class="line">  <span class="string">&quot;--resourceProfileId&quot;</span>, resourceProfileId.toString) ++</span><br><span class="line">userClassPath ++</span><br><span class="line"><span class="type">Seq</span>(</span><br><span class="line">  <span class="string">s&quot;1&gt;<span class="subst">$&#123;ApplicationConstants.LOG_DIR_EXPANSION_VAR&#125;</span>/stdout&quot;</span>,</span><br><span class="line">  <span class="string">s&quot;2&gt;<span class="subst">$&#123;ApplicationConstants.LOG_DIR_EXPANSION_VAR&#125;</span>/stderr&quot;</span>)</span><br></pre></td></tr></table></figure></li>
<li><font color ='red' >容器执行类</font><code>org.apache.spark.executor.YarnCoarseGrainedExecutorBackend</code></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>Driver中mainMethod即为用户程序的main方法入口，workCount的main方法<ul>
<li>-&gt; 跟进<code>new SparkContext(conf)</code></li>
<li>-&gt; 定位<code>Post init</code>  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">_taskScheduler.postStartHook()</span><br></pre></td></tr></table></figure></li>
<li>-&gt; 跟进 <code>org.apache.spark.scheduler.cluster.YarnClusterScheduler#postStartHook</code></li>
<li>-&gt; 跟进 (注意：这里的方法为半生对象的静态方法)<code>org.apache.spark.deploy.yarn.ApplicationMaster#sparkContextInitialized</code></li>
<li>-&gt; 跟进 (注意：这里的方法为类的成员方法)<code>org.apache.spark.deploy.yarn.ApplicationMaster#sparkContextInitialized</code>  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">  <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">sparkContextInitialized</span></span>(sc: <span class="type">SparkContext</span>) = &#123;</span><br><span class="line">      sparkContextPromise.synchronized &#123;</span><br><span class="line">      <span class="comment">// Notify runDriver function that SparkContext is available</span></span><br><span class="line">      sparkContextPromise.success(sc)</span><br><span class="line">      <span class="comment">// Pause the user class thread in order to make proper initialization in runDriver function.</span></span><br><span class="line">      sparkContextPromise.wait()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>通知runDriver方法SparkContext可用，退出阻塞状态</li>
<li>当前计算线程进入阻塞状态，Driver执行准备工作</li>
</ul>
</li>
</ul>
</li>
<li> 资源申请结束 恢复Driver <code>resumeDriver()</code></li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="7-1-4-ExecutorBackend-源码"><a href="#7-1-4-ExecutorBackend-源码" class="headerlink" title="7.1.4 ExecutorBackend 源码"></a>7.1.4 ExecutorBackend 源码</h3><ul>
<li>入口<code>org.apache.spark.executor.YarnCoarseGrainedExecutorBackend</code></li>
<li>伴生对象的main方法<code>org.apache.spark.executor.YarnCoarseGrainedExecutorBackend#main</code></li>
<li>跟进run方法  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">CoarseGrainedExecutorBackend</span>.run(backendArgs, createFn)</span><br></pre></td></tr></table></figure></li>
<li>定位 Executor  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">env.rpcEnv.setupEndpoint(<span class="string">&quot;Executor&quot;</span>,</span><br><span class="line">    backendCreateFn(env.rpcEnv, arguments, env, cfg.resourceProfile))</span><br></pre></td></tr></table></figure>
<ul>
<li>Backend：后台</li>
<li>Endpoint：终端</li>
<li>RpcEnv：rpc通信环境</li>
<li>RpcEndpoint：rpc通信终端  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">The life-cycle of an endpoint is:</span><br><span class="line">constructor -&gt; onStart -&gt; receive* -&gt; onStop</span><br></pre></td></tr></table></figure>
<ul>
<li>YarnCoarseGrainedExecutorBackend父类 -&gt; CoarseGrainedExecutorBackend  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onStart</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">  logInfo(<span class="string">&quot;Connecting to driver: &quot;</span> + driverUrl)</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    _resources = parseOrFindResources(resourcesFileOpt)</span><br><span class="line">  &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="type">NonFatal</span>(e) =&gt;</span><br><span class="line">      exitExecutor(<span class="number">1</span>, <span class="string">&quot;Unable to create executor due to &quot;</span> + e.getMessage, e)</span><br><span class="line">  &#125;</span><br><span class="line">  rpcEnv.asyncSetupEndpointRefByURI(driverUrl).flatMap &#123; ref =&gt;</span><br><span class="line">    <span class="comment">// This is a very fast action so we can use &quot;ThreadUtils.sameThread&quot;</span></span><br><span class="line">    driver = <span class="type">Some</span>(ref)</span><br><span class="line">    ref.ask[<span class="type">Boolean</span>](<span class="type">RegisterExecutor</span>(executorId, self, hostname, cores, extractLogUrls,</span><br><span class="line">      extractAttributes, _resources, resourceProfile.id))</span><br><span class="line">  &#125;(<span class="type">ThreadUtils</span>.sameThread).onComplete &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="type">Success</span>(_) =&gt;</span><br><span class="line">      self.send(<span class="type">RegisteredExecutor</span>)</span><br><span class="line">    <span class="keyword">case</span> <span class="type">Failure</span>(e) =&gt;</span><br><span class="line">      exitExecutor(<span class="number">1</span>, <span class="string">s&quot;Cannot register with driver: <span class="subst">$driverUrl</span>&quot;</span>, e, notifyDriver = <span class="literal">false</span>)</span><br><span class="line">  &#125;(<span class="type">ThreadUtils</span>.sameThread)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>Backend和Driver通讯<code>rpcEnv.asyncSetupEndpointRefByURI</code><ul>
<li>注册Executor  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ref.ask[<span class="type">Boolean</span>](<span class="type">RegisterExecutor</span>(executorId, self, hostname, cores, extractLogUrls,</span><br><span class="line">    extractAttributes, _resources, resourceProfile.id))</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>Driver应答<code>org.apache.spark.SparkContext#_schedulerBackend</code><ul>
<li><code>org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend.DriverEndpoint#receiveAndReply</code>  <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">  listenerBus.post(</span><br><span class="line">  <span class="type">SparkListenerExecutorAdded</span>(<span class="type">System</span>.currentTimeMillis(), executorId, data))</span><br><span class="line"><span class="comment">// Note: some tests expect the reply to come after we put the executor in the map</span></span><br><span class="line">context.reply(<span class="literal">true</span>)</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
 
      <!-- reward -->
      
      <div id="reword-out">
        <div id="reward-btn">
          打赏
        </div>
      </div>
      
    </div>
    

    <!-- copyright -->
    
    <div class="declare">
      <ul class="post-copyright">
        <li>
          <i class="ri-copyright-line"></i>
          <strong>版权声明： </strong>
          
          本博客所有文章除特别声明外，著作权归作者所有。转载请注明出处！
          
        </li>
      </ul>
    </div>
    
    <footer class="article-footer">
       
<div class="share-btn">
      <span class="share-sns share-outer">
        <i class="ri-share-forward-line"></i>
        分享
      </span>
      <div class="share-wrap">
        <i class="arrow"></i>
        <div class="share-icons">
          
          <a class="weibo share-sns" href="javascript:;" data-type="weibo">
            <i class="ri-weibo-fill"></i>
          </a>
          <a class="weixin share-sns wxFab" href="javascript:;" data-type="weixin">
            <i class="ri-wechat-fill"></i>
          </a>
          <a class="qq share-sns" href="javascript:;" data-type="qq">
            <i class="ri-qq-fill"></i>
          </a>
          <a class="douban share-sns" href="javascript:;" data-type="douban">
            <i class="ri-douban-line"></i>
          </a>
          <!-- <a class="qzone share-sns" href="javascript:;" data-type="qzone">
            <i class="icon icon-qzone"></i>
          </a> -->
          
          <a class="facebook share-sns" href="javascript:;" data-type="facebook">
            <i class="ri-facebook-circle-fill"></i>
          </a>
          <a class="twitter share-sns" href="javascript:;" data-type="twitter">
            <i class="ri-twitter-fill"></i>
          </a>
          <a class="google share-sns" href="javascript:;" data-type="google">
            <i class="ri-google-fill"></i>
          </a>
        </div>
      </div>
</div>

<div class="wx-share-modal">
    <a class="modal-close" href="javascript:;"><i class="ri-close-circle-line"></i></a>
    <p>扫一扫，分享到微信</p>
    <div class="wx-qrcode">
      <img src="//api.qrserver.com/v1/create-qr-code/?size=150x150&data=https://anzhen-tech.gitee.io/2021/12/11/SparkCore/" alt="微信分享二维码">
    </div>
</div>

<div id="share-mask"></div>  
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Spark/" rel="tag">Spark</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/SparkCore/" rel="tag">SparkCore</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/" rel="tag">大数据</a></li></ul>

    </footer>
  </div>

   
  <nav class="article-nav">
    
      <a href="/2021/12/13/SparkSQL/" class="article-nav-link">
        <strong class="article-nav-caption">上一篇</strong>
        <div class="article-nav-title">
          
            SparkSQL
          
        </div>
      </a>
    
    
      <a href="/2021/12/04/Scala%E5%85%A5%E9%97%A8/" class="article-nav-link">
        <strong class="article-nav-caption">下一篇</strong>
        <div class="article-nav-title">Scala入门</div>
      </a>
    
  </nav>

   
<!-- valine评论 -->
<div id="vcomments-box">
  <div id="vcomments"></div>
</div>
<script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/valine@1.4.14/dist/Valine.min.js"></script>
<script>
  new Valine({
    el: "#vcomments",
    app_id: "XCKHv09pYxF5EmF2ezNgFfLS-gzGzoHsz",
    app_key: "gyCHBp787fNNfXDiHGIcj7Am",
    path: window.location.pathname,
    avatar: "monsterid",
    placeholder: "给我的文章加点评论吧~",
    recordIP: true,
  });
  const infoEle = document.querySelector("#vcomments .info");
  if (infoEle && infoEle.childNodes && infoEle.childNodes.length > 0) {
    infoEle.childNodes.forEach(function (item) {
      item.parentNode.removeChild(item);
    });
  }
</script>
<style>
  #vcomments-box {
    padding: 5px 30px;
  }

  @media screen and (max-width: 800px) {
    #vcomments-box {
      padding: 5px 0px;
    }
  }

  #vcomments-box #vcomments {
    background-color: #fff;
  }

  .v .vlist .vcard .vh {
    padding-right: 20px;
  }

  .v .vlist .vcard {
    padding-left: 10px;
  }
</style>

 
   
     
</article>

</section>
      <footer class="footer">
  <div class="outer">
    <ul>
      <li>
        Copyrights &copy;
        2019-2022
        <i class="ri-heart-fill heart_icon"></i> Anzhen
      </li>
    </ul>
    <ul>
      <li>
        
      </li>
    </ul>
    <ul>
      <li>
        
        
        <span>
  <span><i class="ri-user-3-fill"></i>访问人数:<span id="busuanzi_value_site_uv"></span></span>
  <span class="division">|</span>
  <span><i class="ri-eye-fill"></i>浏览次数:<span id="busuanzi_value_page_pv"></span></span>
</span>
        
      </li>
    </ul>
    <ul>
      
    </ul>
    <ul>
      
    </ul>
    <ul>
      <li>
        <!-- cnzz统计 -->
        
        <script type="text/javascript" src=''></script>
        
      </li>
    </ul>
  </div>
</footer>    
    </main>
    <div class="float_btns">
      <div class="totop" id="totop">
  <i class="ri-arrow-up-line"></i>
</div>

<div class="todark" id="todark">
  <i class="ri-moon-line"></i>
</div>

    </div>
    <aside class="sidebar on">
      <button class="navbar-toggle"></button>
<nav class="navbar">
  
  <div class="logo">
    <a href="/"><img src="/images/ayer-side.svg" alt="anzhen.tech"></a>
  </div>
  
  <ul class="nav nav-main">
    
    <li class="nav-item">
      <a class="nav-item-link" href="/">主页</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/archives">归档</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/categories">分类</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags">标签</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags/HDFS">HDFS</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags/Yarn">Yarn</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags/MR">MR</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags/Hive">Hive</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags/%E6%95%B0%E6%8D%AE%E9%87%87%E9%9B%86">数据采集</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags/HBase">HBase</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags/Kafka">Kafka</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags/Spark">Spark</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags/Flink">Flink</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags/MySQL">MySQL</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags/Java">Java</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags/interview">面试宝典</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/2019/11/07/about-me">关于我</a>
    </li>
    
  </ul>
</nav>
<nav class="navbar navbar-bottom">
  <ul class="nav">
    <li class="nav-item">
      
      <a class="nav-item-link nav-item-search"  title="搜索">
        <i class="ri-search-line"></i>
      </a>
      
      
      <a class="nav-item-link" target="_blank" href="/atom.xml" title="RSS Feed">
        <i class="ri-rss-line"></i>
      </a>
      
    </li>
  </ul>
</nav>
<div class="search-form-wrap">
  <div class="local-search local-search-plugin">
  <input type="search" id="local-search-input" class="local-search-input" placeholder="Search...">
  <div id="local-search-result" class="local-search-result"></div>
</div>
</div>
    </aside>
    <div id="mask"></div>

<!-- #reward -->
<div id="reward">
  <span class="close"><i class="ri-close-line"></i></span>
  <p class="reward-p"><i class="ri-cup-line"></i>请我喝杯咖啡吧~</p>
  <div class="reward-box">
    
    <div class="reward-item">
      <img class="reward-img" src="/images/alipay.png">
      <span class="reward-type">支付宝</span>
    </div>
    
    
    <div class="reward-item">
      <img class="reward-img" src="/images/wechat.png">
      <span class="reward-type">微信</span>
    </div>
    
  </div>
</div>
    
<script src="/js/jquery-3.6.0.min.js"></script>
 
<script src="/js/lazyload.min.js"></script>

<!-- Tocbot -->
 
<script src="/js/tocbot.min.js"></script>

<script>
  tocbot.init({
    tocSelector: ".tocbot",
    contentSelector: ".article-entry",
    headingSelector: "h1, h2, h3, h4, h5, h6",
    hasInnerContainers: true,
    scrollSmooth: true,
    scrollContainer: "main",
    positionFixedSelector: ".tocbot",
    positionFixedClass: "is-position-fixed",
    fixedSidebarOffset: "auto",
  });
</script>

<script src="https://cdn.jsdelivr.net/npm/jquery-modal@0.9.2/jquery.modal.min.js"></script>
<link
  rel="stylesheet"
  href="https://cdn.jsdelivr.net/npm/jquery-modal@0.9.2/jquery.modal.min.css"
/>
<script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js"></script>

<script src="/dist/main.js"></script>

<!-- ImageViewer -->
 <!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                <!--  Controls are self-explanatory. Order can be changed. -->

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" style="display:none" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css">
<script src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js"></script>

<script>
    function viewer_init() {
        let pswpElement = document.querySelectorAll('.pswp')[0];
        let $imgArr = document.querySelectorAll(('.article-entry img:not(.reward-img)'))

        $imgArr.forEach(($em, i) => {
            $em.onclick = () => {
                // slider展开状态
                // todo: 这样不好，后面改成状态
                if (document.querySelector('.left-col.show')) return
                let items = []
                $imgArr.forEach(($em2, i2) => {
                    let img = $em2.getAttribute('data-idx', i2)
                    let src = $em2.getAttribute('data-target') || $em2.getAttribute('src')
                    let title = $em2.getAttribute('alt')
                    // 获得原图尺寸
                    const image = new Image()
                    image.src = src
                    items.push({
                        src: src,
                        w: image.width || $em2.width,
                        h: image.height || $em2.height,
                        title: title
                    })
                })
                var gallery = new PhotoSwipe(pswpElement, PhotoSwipeUI_Default, items, {
                    index: parseInt(i)
                });
                gallery.init()
            }
        })
    }
    viewer_init()
</script> 
<!-- MathJax -->
 <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
      tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
  });

  MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
      for(i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
      }
  });
</script>

<script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.6/unpacked/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script>
  var ayerConfig = {
    mathjax: true,
  };
</script>

<!-- Katex -->

<!-- busuanzi  -->
 
<script src="/js/busuanzi-2.3.pure.min.js"></script>
 
<!-- ClickLove -->

<!-- ClickBoom1 -->

<!-- ClickBoom2 -->

<!-- CodeCopy -->
 
<link rel="stylesheet" href="/css/clipboard.css">
 <script src="https://cdn.jsdelivr.net/npm/clipboard@2/dist/clipboard.min.js"></script>
<script>
  function wait(callback, seconds) {
    var timelag = null;
    timelag = window.setTimeout(callback, seconds);
  }
  !function (e, t, a) {
    var initCopyCode = function(){
      var copyHtml = '';
      copyHtml += '<button class="btn-copy" data-clipboard-snippet="">';
      copyHtml += '<i class="ri-file-copy-2-line"></i><span>COPY</span>';
      copyHtml += '</button>';
      $(".highlight .code pre").before(copyHtml);
      $(".article pre code").before(copyHtml);
      var clipboard = new ClipboardJS('.btn-copy', {
        target: function(trigger) {
          return trigger.nextElementSibling;
        }
      });
      clipboard.on('success', function(e) {
        let $btn = $(e.trigger);
        $btn.addClass('copied');
        let $icon = $($btn.find('i'));
        $icon.removeClass('ri-file-copy-2-line');
        $icon.addClass('ri-checkbox-circle-line');
        let $span = $($btn.find('span'));
        $span[0].innerText = 'COPIED';
        
        wait(function () { // 等待两秒钟后恢复
          $icon.removeClass('ri-checkbox-circle-line');
          $icon.addClass('ri-file-copy-2-line');
          $span[0].innerText = 'COPY';
        }, 2000);
      });
      clipboard.on('error', function(e) {
        e.clearSelection();
        let $btn = $(e.trigger);
        $btn.addClass('copy-failed');
        let $icon = $($btn.find('i'));
        $icon.removeClass('ri-file-copy-2-line');
        $icon.addClass('ri-time-line');
        let $span = $($btn.find('span'));
        $span[0].innerText = 'COPY FAILED';
        
        wait(function () { // 等待两秒钟后恢复
          $icon.removeClass('ri-time-line');
          $icon.addClass('ri-file-copy-2-line');
          $span[0].innerText = 'COPY';
        }, 2000);
      });
    }
    initCopyCode();
  }(window, document);
</script>
 
<!-- CanvasBackground -->

<script>
  if (window.mermaid) {
    mermaid.initialize({ theme: "forest" });
  }
</script>


    
    <div id="music">
    
    
    
    <iframe frameborder="no" border="1" marginwidth="0" marginheight="0" width="200" height="52"
        src="//music.163.com/outchain/player?type=2&id=318916815&auto=1&height=32"></iframe>
</div>

<style>
    #music {
        position: fixed;
        right: 15px;
        bottom: 0;
        z-index: 998;
    }
</style>
    
    

  </div>
<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>